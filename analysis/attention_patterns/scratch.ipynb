{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da24c203-a034-4a5a-bdfd-6c85ebd5bb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elleven/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e368dd6-d8dd-46a5-9c73-3f49c6127251",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoderbase-1b\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbbdb8f1-fc8e-4942-8680-b369ae2aac66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.05k/1.05k [00:00<00:00, 10.8MB/s]\n",
      "Downloading model.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.55G/4.55G [06:43<00:00, 11.3MB/s]\n",
      "Downloading (…)neration_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 111/111 [00:00<00:00, 491kB/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"bigcode/starcoderbase-1b\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "5a7c46a6-8b31-474d-ab05-ce75d6253b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"\n",
    "-- Task\n",
    "-- We are given two strings s and c, you have to deleted all the characters in s \n",
    "-- that are equal to any character in c\n",
    "-- then check if the result string is palindrome.\n",
    "-- A string is called palindrome if it reads the same backward as forward.\n",
    "-- You should return a tuple containing the result string and True/False for the check.\n",
    "--\n",
    "-- Example\n",
    "-- For s = \"abcde\", c = \"ae\", the result should be ('bcd',False)\n",
    "-- For s = \"abcdef\", c = \"b\" the result should be ('acdef',False)\n",
    "-- For s = \"abcdedcba\", c = \"ab\", the result should be ('cdedc',True)\n",
    "local function reverse_delete(s, c)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "40e1d5dd-de12-4735-89a3-0d52db46a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_end_tok_i(tokenizer, enc, stop_seqs=[\"\\nend\", \"\\n--\"]):\n",
    "    def stop_in_enc(enc):\n",
    "        dec = tokenizer.decode(enc)\n",
    "        for stop in stop_seqs:\n",
    "            if stop in dec:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "        \n",
    "    i = 0\n",
    "    \n",
    "    while i < len(enc) - 1 and not stop_in_enc(enc[:i]):\n",
    "        i += 1\n",
    "\n",
    "    return i\n",
    "    \n",
    "    \n",
    "toks = tokenizer.encode(PROMPT, return_tensors=\"pt\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "b7f3250f-44ab-424a-9f0f-6b11b13ae092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(toks, do_sample=True, max_new_tokens=150, temperature=0.2, top_p=0.95)\n",
    "end_tok = find_end_tok_i(tokenizer, out[0][len(toks[0])-1:]) + len(toks[0])\n",
    "out = out[0][:end_tok-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "138eeb53-4996-43a1-8368-d09f7b8a0b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Task\n",
      "-- We are given two strings s and c, you have to deleted all the characters in s \n",
      "-- that are equal to any character in c\n",
      "-- then check if the result string is palindrome.\n",
      "-- A string is called palindrome if it reads the same backward as forward.\n",
      "-- You should return a tuple containing the result string and True/False for the check.\n",
      "--\n",
      "-- Example\n",
      "-- For s = \"abcde\", c = \"ae\", the result should be ('bcd',False)\n",
      "-- For s = \"abcdef\", c = \"b\" the result should be ('acdef',False)\n",
      "-- For s = \"abcdedcba\", c = \"ab\", the result should be ('cdedc',True)\n",
      "local function reverse_delete(s, c)\n",
      "\tlocal result = \"\"\n",
      "\tfor i = 1, #s do\n",
      "\t\tif s:sub(i, i) ~= c:sub(1, 1) then\n",
      "\t\t\tresult = result.. s:sub(i, i)\n",
      "\t\tend\n",
      "\tend\n",
      "\treturn result, true\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "bf3a79c5-4d45-40b1-9182-6ac496286fe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238\n",
      "torch.Size([16, 237, 237])\n"
     ]
    }
   ],
   "source": [
    "enc = model(out, output_attentions=True)\n",
    "attns = enc[\"attentions\"]\n",
    "# quite a deep tensor...\n",
    "layer_i = 0\n",
    "batch_i = 0 # we only have one prompt\n",
    "attn_head_i = 0\n",
    "print(end_tok)\n",
    "# attns[layer_i][batch_i][attn_head_i][end_tok]\n",
    "\n",
    "# get last layer attns\n",
    "last_layer_attns = attns[-1][batch_i]\n",
    "print(last_layer_attns.size())\n",
    "last_layer_attns_head_mean = last_layer_attns.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "dee1c769-67fc-4399-bedc-4c5aa3a71566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "ced8415d-862c-46a6-b0c5-3e843a3bda9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0254, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar = torch.flip(torch.arange(1, len(enc[0]) + 1), [0]).cuda()\n",
    "summed = last_layer_attns_head_mean.sum(0)\n",
    "mean_pooled = summed / ar\n",
    "mean_pooled[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "f8ee2519-a60c-4faa-aa3e-eb31c5a4e5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "1c622696-a84b-472a-a168-f935197acfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    237.000000\n",
       "mean       0.004219\n",
       "std        0.038850\n",
       "min        0.000117\n",
       "25%        0.000318\n",
       "50%        0.000708\n",
       "75%        0.001851\n",
       "max        0.598300\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(last_layer_attns_head_mean[-1].detach().cpu().numpy()).describe()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "c0d0d036-c846-4ba9-b9f0-06cf704c9c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41m\u001b[30m\n",
      "\u001b[0m\u001b[41m\u001b[30m--\u001b[0m\u001b[43m\u001b[30m Task\u001b[0m\u001b[41m\u001b[30m\n",
      "\u001b[0m\u001b[43m\u001b[30m--\u001b[0m\u001b[41m\u001b[30m We\u001b[0m\u001b[43m\u001b[30m are\u001b[0m\u001b[43m\u001b[30m given\u001b[0m\u001b[43m\u001b[30m two\u001b[0m\u001b[41m\u001b[30m strings\u001b[0m\u001b[43m\u001b[30m s\u001b[0m\u001b[42m\u001b[30m and\u001b[0m\u001b[42m\u001b[30m c\u001b[0m\u001b[41m\u001b[30m,\u001b[0m\u001b[41m\u001b[30m you\u001b[0m\u001b[42m\u001b[30m have\u001b[0m\u001b[43m\u001b[30m to\u001b[0m\u001b[43m\u001b[30m deleted\u001b[0m\u001b[43m\u001b[30m all\u001b[0m\u001b[43m\u001b[30m the\u001b[0m\u001b[43m\u001b[30m characters\u001b[0m\u001b[42m\u001b[30m in\u001b[0m\u001b[43m\u001b[30m s\u001b[0m\u001b[42m\u001b[30m \u001b[0m\u001b[43m\u001b[30m\n",
      "\u001b[0m\u001b[47m\u001b[30m--\u001b[0m\u001b[43m\u001b[30m that\u001b[0m\u001b[43m\u001b[30m are\u001b[0m\u001b[42m\u001b[30m equal\u001b[0m\u001b[47m\u001b[30m to\u001b[0m\u001b[47m\u001b[30m any\u001b[0m\u001b[47m\u001b[30m character\u001b[0m\u001b[42m\u001b[30m in\u001b[0m\u001b[42m\u001b[30m c\u001b[0m\u001b[41m\u001b[30m\n",
      "\u001b[0m\u001b[41m\u001b[30m--\u001b[0m\u001b[42m\u001b[30m then\u001b[0m\u001b[42m\u001b[30m check\u001b[0m\u001b[47m\u001b[30m if\u001b[0m\u001b[42m\u001b[30m the\u001b[0m\u001b[42m\u001b[30m result\u001b[0m\u001b[42m\u001b[30m string\u001b[0m\u001b[42m\u001b[30m is\u001b[0m\u001b[41m\u001b[30m pal\u001b[0m\u001b[42m\u001b[30mindrome\u001b[0m\u001b[41m\u001b[30m.\u001b[0m\u001b[41m\u001b[30m\n",
      "\u001b[0m\u001b[43m\u001b[30m--\u001b[0m\u001b[43m\u001b[30m A\u001b[0m\u001b[47m\u001b[30m string\u001b[0m\u001b[43m\u001b[30m is\u001b[0m\u001b[42m\u001b[30m called\u001b[0m\u001b[43m\u001b[30m pal\u001b[0m\u001b[42m\u001b[30mindrome\u001b[0m\u001b[42m\u001b[30m if\u001b[0m\u001b[47m\u001b[30m it\u001b[0m\u001b[47m\u001b[30m reads\u001b[0m\u001b[47m\u001b[30m the\u001b[0m\u001b[47m\u001b[30m same\u001b[0m\u001b[42m\u001b[30m backward\u001b[0m\u001b[47m\u001b[30m as\u001b[0m\u001b[47m\u001b[30m forward\u001b[0m\u001b[43m\u001b[30m.\u001b[0m\u001b[41m\u001b[30m\n",
      "\u001b[0m\u001b[42m\u001b[30m--\u001b[0m\u001b[43m\u001b[30m You\u001b[0m\u001b[42m\u001b[30m should\u001b[0m\u001b[42m\u001b[30m return\u001b[0m\u001b[42m\u001b[30m a\u001b[0m\u001b[43m\u001b[30m tuple\u001b[0m\u001b[42m\u001b[30m containing\u001b[0m\u001b[42m\u001b[30m the\u001b[0m\u001b[42m\u001b[30m result\u001b[0m\u001b[47m\u001b[30m string\u001b[0m\u001b[47m\u001b[30m and\u001b[0m\u001b[43m\u001b[30m True\u001b[0m\u001b[41m\u001b[30m/\u001b[0m\u001b[42m\u001b[30mFalse\u001b[0m\u001b[47m\u001b[30m for\u001b[0m\u001b[42m\u001b[30m the\u001b[0m\u001b[47m\u001b[30m check\u001b[0m\u001b[43m\u001b[30m.\u001b[0m\u001b[41m\u001b[30m\n",
      "\u001b[0m\u001b[42m\u001b[30m--\u001b[0m\u001b[41m\u001b[30m\n",
      "\u001b[0m\u001b[42m\u001b[30m--\u001b[0m\u001b[43m\u001b[30m Example\u001b[0m\u001b[43m\u001b[30m\n",
      "\u001b[0m\u001b[43m\u001b[30m--\u001b[0m\u001b[43m\u001b[30m For\u001b[0m\u001b[42m\u001b[30m s\u001b[0m\u001b[43m\u001b[30m =\u001b[0m\u001b[41m\u001b[30m \"\u001b[0m\u001b[41m\u001b[30mabc\u001b[0m\u001b[42m\u001b[30mde\u001b[0m\u001b[43m\u001b[30m\",\u001b[0m\u001b[42m\u001b[30m c\u001b[0m\u001b[47m\u001b[30m =\u001b[0m\u001b[42m\u001b[30m \"\u001b[0m\u001b[42m\u001b[30mae\u001b[0m\u001b[47m\u001b[30m\",\u001b[0m\u001b[42m\u001b[30m the\u001b[0m\u001b[42m\u001b[30m result\u001b[0m\u001b[42m\u001b[30m should\u001b[0m\u001b[47m\u001b[30m be\u001b[0m\u001b[43m\u001b[30m ('\u001b[0m\u001b[47m\u001b[30mbcd\u001b[0m\u001b[47m\u001b[30m',\u001b[0m\u001b[41m\u001b[30mFalse\u001b[0m\u001b[42m\u001b[30m)\u001b[0m\u001b[42m\u001b[30m\n",
      "\u001b[0m\u001b[47m\u001b[30m--\u001b[0m\u001b[47m\u001b[30m For\u001b[0m\u001b[42m\u001b[30m s\u001b[0m\u001b[47m\u001b[30m =\u001b[0m\u001b[42m\u001b[30m \"\u001b[0m\u001b[42m\u001b[30mabcdef\u001b[0m\u001b[42m\u001b[30m\",\u001b[0m\u001b[47m\u001b[30m c\u001b[0m\u001b[47m\u001b[30m =\u001b[0m\u001b[47m\u001b[30m \"\u001b[0m\u001b[47m\u001b[30mb\u001b[0m\u001b[47m\u001b[30m\"\u001b[0m\u001b[42m\u001b[30m the\u001b[0m\u001b[42m\u001b[30m result\u001b[0m\u001b[47m\u001b[30m should\u001b[0m\u001b[47m\u001b[30m be\u001b[0m\u001b[42m\u001b[30m ('\u001b[0m\u001b[47m\u001b[30mac\u001b[0m\u001b[47m\u001b[30mdef\u001b[0m\u001b[42m\u001b[30m',\u001b[0m\u001b[43m\u001b[30mFalse\u001b[0m\u001b[43m\u001b[30m)\u001b[0m\u001b[41m\u001b[30m\n",
      "\u001b[0m\u001b[47m\u001b[30m--\u001b[0m\u001b[47m\u001b[30m For\u001b[0m\u001b[47m\u001b[30m s\u001b[0m\u001b[47m\u001b[30m =\u001b[0m\u001b[47m\u001b[30m \"\u001b[0m\u001b[47m\u001b[30mabc\u001b[0m\u001b[47m\u001b[30mded\u001b[0m\u001b[47m\u001b[30mcba\u001b[0m\u001b[47m\u001b[30m\",\u001b[0m\u001b[47m\u001b[30m c\u001b[0m\u001b[41m\u001b[30m =\u001b[0m\u001b[47m\u001b[30m \"\u001b[0m\u001b[47m\u001b[30mab\u001b[0m\u001b[47m\u001b[30m\",\u001b[0m\u001b[47m\u001b[30m the\u001b[0m\u001b[47m\u001b[30m result\u001b[0m\u001b[47m\u001b[30m should\u001b[0m\u001b[47m\u001b[30m be\u001b[0m\u001b[47m\u001b[30m ('\u001b[0m\u001b[47m\u001b[30mc\u001b[0m\u001b[47m\u001b[30mded\u001b[0m\u001b[47m\u001b[30mc\u001b[0m\u001b[47m\u001b[30m',\u001b[0m\u001b[42m\u001b[30mTrue\u001b[0m\u001b[47m\u001b[30m)\u001b[0m\u001b[41m\u001b[30m\n",
      "\u001b[0m\u001b[41m\u001b[30mlocal\u001b[0m\u001b[41m\u001b[30m function\u001b[0m\u001b[41m\u001b[30m reverse\u001b[0m\u001b[43m\u001b[30m_\u001b[0m\u001b[43m\u001b[30mdelete\u001b[0m\u001b[41m\u001b[30m(\u001b[0m\u001b[41m\u001b[30ms\u001b[0m\u001b[41m\u001b[30m,\u001b[0m\u001b[43m\u001b[30m c\u001b[0m\u001b[41m\u001b[30m)\u001b[0m\u001b[41m\u001b[30m\n",
      "\u001b[0m\u001b[41m\u001b[30m\t\u001b[0m\u001b[41m\u001b[30mlocal\u001b[0m\u001b[41m\u001b[30m result\u001b[0m\u001b[41m\u001b[30m =\u001b[0m\u001b[41m\u001b[30m \"\"\u001b[0m\u001b[41m\u001b[30m\n",
      "\u001b[0m\u001b[41m\u001b[30m\t\u001b[0m\u001b[41m\u001b[30mfor\u001b[0m\u001b[41m\u001b[30m i\u001b[0m\u001b[41m\u001b[30m =\u001b[0m\u001b[41m\u001b[30m \u001b[0m\u001b[41m\u001b[30m1\u001b[0m\u001b[41m\u001b[30m,\u001b[0m\u001b[41m\u001b[30m #\u001b[0m\u001b[41m\u001b[30ms\u001b[0m\u001b[41m\u001b[30m do\u001b[0m\u001b[41m\u001b[30m\n",
      "\t\u001b[0m\u001b[43m\u001b[30m\t\u001b[0m\u001b[43m\u001b[30mif\u001b[0m\u001b[43m\u001b[30m s\u001b[0m\u001b[41m\u001b[30m:\u001b[0m\u001b[41m\u001b[30msub\u001b[0m\u001b[43m\u001b[30m(\u001b[0m\u001b[43m\u001b[30mi\u001b[0m\u001b[43m\u001b[30m,\u001b[0m\u001b[43m\u001b[30m i\u001b[0m\u001b[41m\u001b[30m)\u001b[0m\u001b[43m\u001b[30m ~=\u001b[0m\u001b[43m\u001b[30m c\u001b[0m\u001b[43m\u001b[30m:\u001b[0m\u001b[42m\u001b[30msub\u001b[0m\u001b[47m\u001b[30m(\u001b[0m\u001b[43m\u001b[30m1\u001b[0m\u001b[43m\u001b[30m,\u001b[0m\u001b[43m\u001b[30m \u001b[0m\u001b[43m\u001b[30m1\u001b[0m\u001b[41m\u001b[30m)\u001b[0m\u001b[41m\u001b[30m then\u001b[0m\u001b[41m\u001b[30m\n",
      "\t\t\u001b[0m\u001b[43m\u001b[30m\t\u001b[0m\u001b[43m\u001b[30mresult\u001b[0m\u001b[42m\u001b[30m =\u001b[0m\u001b[43m\u001b[30m result\u001b[0m\u001b[42m\u001b[30m..\u001b[0m\u001b[42m\u001b[30m s\u001b[0m\u001b[42m\u001b[30m:\u001b[0m\u001b[42m\u001b[30msub\u001b[0m\u001b[42m\u001b[30m(\u001b[0m\u001b[41m\u001b[30mi\u001b[0m\u001b[42m\u001b[30m,\u001b[0m\u001b[43m\u001b[30m i\u001b[0m\u001b[41m\u001b[30m)\u001b[0m\u001b[41m\u001b[30m\n",
      "\t\u001b[0m\u001b[42m\u001b[30m\t\u001b[0m\u001b[41m\u001b[30mend\u001b[0m\u001b[47m\u001b[30m\n",
      "\u001b[0m\u001b[47m\u001b[30m\t\u001b[0m\u001b[41m\u001b[30mend\u001b[0m\u001b[43m\u001b[30m\n",
      "\u001b[0m\u001b[43m\u001b[30m\t\u001b[0m\u001b[43m\u001b[30mreturn\u001b[0m\u001b[43m\u001b[30m result\u001b[0m\u001b[43m\u001b[30m,\u001b[0m\u001b[41m\u001b[30m true\u001b[0m\u001b[42m\u001b[30m\n",
      "\u001b[0m\u001b[41m\u001b[30mend\u001b[0m"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "\n",
    "FORE=\"black\"\n",
    "def color_tok(attn, tok, distr) -> str:\n",
    "    if attn < distr[\"25%\"]:\n",
    "        return colored(tok, color=FORE, on_color='on_light_grey')\n",
    "    elif attn < distr[\"50%\"]:\n",
    "        return colored(tok, color=FORE, on_color='on_green')\n",
    "    elif attn < distr[\"75%\"]:\n",
    "        return colored(tok, color=FORE, on_color='on_yellow')\n",
    "    else:\n",
    "        return colored(tok, color=FORE, on_color='on_red')\n",
    "    \n",
    "\n",
    "def visualize_attn(tokenizer, out, meaned_attns):\n",
    "    distr = pd.DataFrame(meaned_attns.detach().cpu().numpy()).describe()[0]\n",
    "    for i, tok in enumerate(out):\n",
    "        colored = color_tok(meaned_attns[i], tokenizer.decode(tok), distr)\n",
    "        print(colored, end=\"\")\n",
    "\n",
    "visualize_attn(tokenizer, out, mean_pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "13603ebe-d0b0-4192-bbd4-16b8e69c20eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "ded21d30-3af7-4269-8ca2-9490ee1a6b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\e[48;5;4m%03d\n"
     ]
    }
   ],
   "source": [
    "print(\"\\e[48;5;4m%03d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "d79b6a90-8797-45ed-a4c3-db581f16b450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  203,   287,  4187,   203,   287,  2688,   884,  2702,  3134,  5852,\n",
       "          309,   461,   281,    30,   844,  1159,   372,  8128,  1169,   322,\n",
       "         7125,   328,   309,   225,   203,   287,   688,   884,  5040,   372,\n",
       "         1346,  5341,   328,   281,   203,   287,  1615,  1505,   415,   322,\n",
       "         1056,   802,   438, 18375, 39379,    32,   203,   287,   399,   802,\n",
       "          438,  3823, 18375, 39379,   415,   561, 14822,   322,  2432, 21851,\n",
       "          619,  7386,    32,   203,   287,  2448,  1395,   442,   312,  8825,\n",
       "         6621,   322,  1056,   802,   461,  2933,    33,  2700,   436,   322,\n",
       "         1505,    32,   203,   287,   203,   287,  5938,   203,   287,  2616,\n",
       "          309,   280,   313,  8183,   286,   392,   281,   280,   313,  3633,\n",
       "          392,   322,  1056,  1395,   526,  2726, 23550,   370,  2700,    27,\n",
       "          203,   287,  2616,   309,   280,   313, 25870,   392,   281,   280,\n",
       "          313,    84,    20,   322,  1056,  1395,   526,  2726,   329,   589,\n",
       "          370,  2700,    27,   203,   287,  2616,   309,   280,   313,  8183,\n",
       "          981, 35285,   392,   281,   280,   313,   360,   392,   322,  1056,\n",
       "         1395,   526,  2726,    85,   981,    85,   370,  1815,    27,   203,\n",
       "         1696,   667, 10156,    81,  2560,    26,   101,    30,   281,    27,\n",
       "          203,   202,  1696,  1056,   280,  2149,   203,   202,   979,   595,\n",
       "          280,   225,    35,    30,   588,   101,   745,   357,   202,   325,\n",
       "          309,    44,  1113,    26,    91,    30,   595,    27,   610,   281,\n",
       "         1615,   355,   202,  1267, 27561,  1267,   474,   225,    35,    79,\n",
       "          280,   309,    44,  1113,    26,    91,   474,   225,    35,    30,\n",
       "          588,   101,    27,   355,   202,   101,   280,   309,    44,  1113,\n",
       "           26,    35,    30,   595,   429,   225,    35,    27,  4377,   309,\n",
       "           44,  1113,    26,    91,   474,   225,    35,    30,   588,   101,\n",
       "           27,   357,   202,   416,   203,   202,   416,   203,   202,   601,\n",
       "          309,    30,  1056,   203,   416], device='cuda:0')"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full mean pool fn\n",
    "def mean_pool_attn_from_toks(toks):\n",
    "    assert len(toks.size()) == 1, \"mean pooling batched toks is currently not supported\"\n",
    "    enc = model(toks, output_attentions=True)\n",
    "    attns = enc[\"attentions\"]\n",
    "    # quite a deep tensor...\n",
    "    layer_i = 0\n",
    "    batch_i = 0 # we only have one prompt\n",
    "    attn_head_i = 0\n",
    "    # attns[layer_i][batch_i][attn_head_i][tok]\n",
    "\n",
    "    # get last layer attns\n",
    "    last_layer_attns = attns[-1][batch_i]\n",
    "    last_layer_attns_head_mean = last_layer_attns.mean(dim=0)\n",
    "\n",
    "    ar = torch.flip(torch.arange(1, len(enc[0]) + 1), [0]).cuda()\n",
    "    summed = last_layer_attns_head_mean.sum(0)\n",
    "    mean_pooled = summed / ar\n",
    "    return mean_pooled\n",
    "\n",
    "def generate_with_stop(model, tokenizer, prompt):\n",
    "    toks = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(toks, do_sample=True, max_new_tokens=150, temperature=0.2, top_p=0.95)\n",
    "    end_tok = find_end_tok_i(tokenizer, out[0][len(toks[0])-1:]) + len(toks[0])\n",
    "    out = out[0][:end_tok-1]\n",
    "    return out\n",
    "    \n",
    "generate_with_stop(model, tokenizer, PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28047d57-a003-4e60-a679-3b790ef5864a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
