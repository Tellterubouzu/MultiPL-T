{"name": "HumanEval_100473_bit_ceil", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if n:\n-- #     # see https://stackoverflow.com/a/14267825/17332200\n-- #     exp = (n - 1).bit_length()\n-- #     res = 1 << exp\n-- #     return res\n-- # else:\n-- #     return 1\n-- \n-- Calculate the smallest power of 2 not smaller than n.\nlocal function bit_ceil(n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_100473_bit_ceil.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = bit_ceil\n    lu.assertEquals(candidate(28), 32)\n    lu.assertEquals(candidate(10), 16)\n    lu.assertEquals(candidate(8), 8)\n    lu.assertEquals(candidate(13), 16)\n    lu.assertEquals(candidate(31), 32)\n    lu.assertEquals(candidate(22), 32)\n    lu.assertEquals(candidate(7), 8)\n    lu.assertEquals(candidate(20), 32)\n    lu.assertEquals(candidate(26), 32)\n    lu.assertEquals(candidate(12), 16)\n    lu.assertEquals(candidate(3), 4)\n    lu.assertEquals(candidate(19), 32)\n    lu.assertEquals(candidate(16), 16)\n    lu.assertEquals(candidate(23), 32)\n    lu.assertEquals(candidate(4), 4)\n    lu.assertEquals(candidate(9), 16)\n    lu.assertEquals(candidate(17), 32)\n    lu.assertEquals(candidate(24), 32)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(0), 1)\n    lu.assertEquals(candidate(15), 16)\n    lu.assertEquals(candidate(27), 32)\n    lu.assertEquals(candidate(25), 32)\n    lu.assertEquals(candidate(11), 16)\n    lu.assertEquals(candidate(6), 8)\n    lu.assertEquals(candidate(14), 16)\n    lu.assertEquals(candidate(5), 8)\n    lu.assertEquals(candidate(30), 32)\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(21), 32)\n    lu.assertEquals(candidate(18), 32)\n    lu.assertEquals(candidate(29), 32)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_100527_ell2ang", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return 2 * 10800. / (ell + 1)\n-- \n-- Convert the given ell(s) to its analogous angular scale(s) in arcmins.\n-- Return the angular scale(s) in arcmins corresponding to the Fourier mode \n-- ell(s).\n-- Parameters\n-- ----------\n-- ell: value, array of values\n--     The ell mode(s).\n-- Returns\n-- -------\n-- a: float, array of floats\n--     The angular scale(s) corresponding to ell.\nlocal function ell2ang(ell)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_100527_ell2ang.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = ell2ang\n    lu.assertEquals(candidate(1), 10800)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_101579_about_me", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"The wise {} loves Python.\".format(your_name)\n-- \n-- Return the most important thing about a person.\n-- Parameters\n-- ----------\n-- your_name\n--     A string indicating the name of the person.\nlocal function about_me(your_name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_101579_about_me.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = about_me\n    lu.assertEquals(candidate('Jose'), 'The wise Jose loves Python.')\n    lu.assertEquals(candidate('<NAME>'), 'The wise <NAME> loves Python.')\n    lu.assertEquals(candidate('Peter'), 'The wise Peter loves Python.')\n    lu.assertEquals(candidate('Donald'), 'The wise Donald loves Python.')\n    lu.assertEquals(candidate('Bob'), 'The wise Bob loves Python.')\n    lu.assertEquals(candidate('Bernhard'), 'The wise Bernhard loves Python.')\n    lu.assertEquals(candidate('Abraham'), 'The wise Abraham loves Python.')\n    lu.assertEquals(candidate('Bob'), 'The wise Bob loves Python.')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_101630_maybe_quote", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # NEED_QUOTE = u\" \\t\\\"\\\\'\"\n-- # clean = True\n-- # for c in s:\n-- #     if c in NEED_QUOTE:\n-- #         clean = False\n-- #         break\n-- # if clean:\n-- #     return s\n-- # # Something needs quoting or escaping.\n-- # QUOTE = u\"'\"\n-- # ESC = u\"\\\\\"\n-- # arg = [QUOTE]\n-- # for c in s:\n-- #     if c == QUOTE:\n-- #         arg.append(QUOTE)\n-- #         arg.append(ESC)\n-- #         arg.append(QUOTE)\n-- #     elif c == ESC:\n-- #         arg.append(ESC)\n-- #     arg.append(c)\n-- # arg.append(QUOTE)\n-- # return ''.join(arg)\n-- \n--  Enclose the string argument in single quotes if it looks like it needs it.\n-- Spaces and quotes will trigger; single quotes in the argument are escaped.\n-- This is only used to compose the --print output so need only satisfy shlex.\nlocal function maybe_quote(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_101630_maybe_quote.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = maybe_quote\n    lu.assertEquals(candidate('foo bar'), \"'foo bar'\")\n    lu.assertEquals(candidate('x'), 'x')\n    lu.assertEquals(candidate('fo\\\\\"o'), '\\'fo\\\\\\\\\"o\\'')\n    lu.assertEquals(candidate('a\\\\tb'), \"'a\\\\\\\\tb'\")\n    lu.assertEquals(candidate('fo\"o'), '\\'fo\"o\\'')\n    lu.assertEquals(candidate('Hello World!'), \"'Hello World!'\")\n    lu.assertEquals(candidate('a   b'), \"'a   b'\")\n    lu.assertEquals(candidate('a'), 'a')\n    lu.assertEquals(candidate('ab c\\\\'), \"'ab c\\\\\\\\'\")\n    lu.assertEquals(candidate('Hello World?'), \"'Hello World?'\")\n    lu.assertEquals(candidate('fo\\\\\\\\o'), \"'fo\\\\\\\\\\\\\\\\o'\")\n    lu.assertEquals(candidate('\\\\'), \"'\\\\\\\\'\")\n    lu.assertEquals(candidate('a\\\\nb\\\\tc'), \"'a\\\\\\\\nb\\\\\\\\tc'\")\n    lu.assertEquals(candidate('foo bar'), \"'foo bar'\")\n    lu.assertEquals(candidate(' x'), \"' x'\")\n    lu.assertEquals(candidate('fo o'), \"'fo o'\")\n    lu.assertEquals(candidate('Hello World?'), \"'Hello World?'\")\n    lu.assertEquals(candidate('fo\\\\\\\\\\\\o'), \"'fo\\\\\\\\\\\\\\\\\\\\\\\\o'\")\n    lu.assertEquals(candidate('ab c'), \"'ab c'\")\n    lu.assertEquals(candidate('a\\\\nb'), \"'a\\\\\\\\nb'\")\n    lu.assertEquals(candidate('x '), \"'x '\")\n    lu.assertEquals(candidate('a  b'), \"'a  b'\")\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('\"'), '\\'\"\\'')\n    lu.assertEquals(candidate('foo'), 'foo')\n    lu.assertEquals(candidate('Hello \"World\"!'), '\\'Hello \"World\"!\\'')\n    lu.assertEquals(candidate('Hello World!'), \"'Hello World!'\")\n    lu.assertEquals(candidate('a b'), \"'a b'\")\n    lu.assertEquals(candidate('hello'), 'hello')\n    lu.assertEquals(candidate('abc'), 'abc')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_101962_is_categorical", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return var_type == \"categorical\"\n-- \n-- Checks whether the variable type for the\n-- variable of interest is categorical.\nlocal function is_categorical(var_type)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_101962_is_categorical.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_categorical\n    lu.assertEquals(candidate(None), false)\n    lu.assertEquals(candidate('string'), false)\n    lu.assertEquals(candidate('date'), false)\n    lu.assertEquals(candidate('number'), false)\n    lu.assertEquals(candidate('categorical'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_10216_split3", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # part1, text = text.split(pat1, 1)\n-- # part2, part3 = text.split(pat2, 1)\n-- # return part1, part2, part3\n-- \n-- Split text in 3 parts: before pat1, between, and after pat2.\nlocal function split3(text, pat1, pat2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_10216_split3.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = split3\n    lu.assertEquals(candidate('a:b:c', ':', ':'), {'a', 'b', 'c'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_10218_parts", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # q, r = divmod(a, b)\n-- # return [q + 1] * r + [q] * (b - r)\n-- \n-- https://stackoverflow.com/a/52698110\nlocal function parts(a, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_10218_parts.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parts\n    lu.assertEquals(candidate(0, 4), {0, 0, 0, 0})\n    lu.assertEquals(candidate(3, 2), {2, 1})\n    lu.assertEquals(candidate(11, 2), {6, 5})\n    lu.assertEquals(candidate(10, 1), {10})\n    lu.assertEquals(candidate(4, 2), {2, 2})\n    lu.assertEquals(candidate(0, 10), {0, 0, 0, 0, 0, 0, 0, 0, 0, 0})\n    lu.assertEquals(candidate(4, 3), {2, 1, 1})\n    lu.assertEquals(candidate(10, 3), {4, 3, 3})\n    lu.assertEquals(candidate(5, 3), {2, 2, 1})\n    lu.assertEquals(candidate(0, 2), {0, 0})\n    lu.assertEquals(candidate(2, 3), {1, 1, 0})\n    lu.assertEquals(candidate(0, 1), {0})\n    lu.assertEquals(candidate(1, 1), {1})\n    lu.assertEquals(candidate(2, 2), {1, 1})\n    lu.assertEquals(candidate(10, 2), {5, 5})\n    lu.assertEquals(candidate(4, 1), {4})\n    lu.assertEquals(candidate(6, 3), {2, 2, 2})\n    lu.assertEquals(candidate(1, 4), {1, 0, 0, 0})\n    lu.assertEquals(candidate(1, 2), {1, 0})\n    lu.assertEquals(candidate(1, 3), {1, 0, 0})\n    lu.assertEquals(candidate(3, 1), {3})\n    lu.assertEquals(candidate(5, 1), {5})\n    lu.assertEquals(candidate(100, 10), {10, 10, 10, 10, 10, 10, 10, 10, 10, 10})\n    lu.assertEquals(candidate(99, 10), {10, 10, 10, 10, 10, 10, 10, 10, 10, 9})\n    lu.assertEquals(candidate(0, 3), {0, 0, 0})\n    lu.assertEquals(candidate(2, 1), {2})\n    lu.assertEquals(candidate(3, 3), {1, 1, 1})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_102630_get_last_step_id", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return steps[-1]['id']\n-- \n-- Returns the id of the last step in |steps|.\nlocal function get_last_step_id(steps)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_102630_get_last_step_id.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_last_step_id\n    lu.assertEquals(candidate({{['id'] = 's1'}, {['id'] = 's2'}, {['id'] = 's3'}}), 's3')\n    lu.assertEquals(candidate({{['id'] = 's1'}}), 's1')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_102704_bit_length", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # `- 2` is due the output format of `bin`: `0bXXXXXXX`\n-- # return len(bin(num)) - 2\n-- \n-- Measure the length of the number in bits.\n-- :param num: The number to measure.\n-- :return:    The minimal amount of bits needed to represent the number.\nlocal function bit_length(num)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_102704_bit_length.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = bit_length\n    lu.assertEquals(candidate(6), 3)\n    lu.assertEquals(candidate(281474976710656), 49)\n    lu.assertEquals(candidate(27), 5)\n    lu.assertEquals(candidate(255), 8)\n    lu.assertEquals(candidate(8), 4)\n    lu.assertEquals(candidate(65535), 16)\n    lu.assertEquals(candidate(13), 4)\n    lu.assertEquals(candidate(12), 4)\n    lu.assertEquals(candidate(14), 4)\n    lu.assertEquals(candidate(128), 8)\n    lu.assertEquals(candidate(5), 3)\n    lu.assertEquals(candidate(11), 4)\n    lu.assertEquals(candidate(30), 5)\n    lu.assertEquals(candidate(281474976710655), 48)\n    lu.assertEquals(candidate(1099511627775), 40)\n    lu.assertEquals(candidate(0), 1)\n    lu.assertEquals(candidate(256), 9)\n    lu.assertEquals(candidate(7), 3)\n    lu.assertEquals(candidate(1023), 10)\n    lu.assertEquals(candidate(23), 5)\n    lu.assertEquals(candidate(15), 4)\n    lu.assertEquals(candidate(4294967297), 33)\n    lu.assertEquals(candidate(9), 4)\n    lu.assertEquals(candidate(29), 5)\n    lu.assertEquals(candidate(4294967295), 32)\n    lu.assertEquals(candidate(1099511627776), 41)\n    lu.assertEquals(candidate(28), 5)\n    lu.assertEquals(candidate(16777216), 25)\n    lu.assertEquals(candidate(16), 5)\n    lu.assertEquals(candidate(17), 5)\n    lu.assertEquals(candidate(10), 4)\n    lu.assertEquals(candidate(20), 5)\n    lu.assertEquals(candidate(32), 6)\n    lu.assertEquals(candidate(25), 5)\n    lu.assertEquals(candidate(22), 5)\n    lu.assertEquals(candidate(1099511627777), 41)\n    lu.assertEquals(candidate(21), 5)\n    lu.assertEquals(candidate(31), 5)\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(1024), 11)\n    lu.assertEquals(candidate(19), 5)\n    lu.assertEquals(candidate(18), 5)\n    lu.assertEquals(candidate(127), 7)\n    lu.assertEquals(candidate(1025), 11)\n    lu.assertEquals(candidate(65536), 17)\n    lu.assertEquals(candidate(3), 2)\n    lu.assertEquals(candidate(33), 6)\n    lu.assertEquals(candidate(24), 5)\n    lu.assertEquals(candidate(16777215), 24)\n    lu.assertEquals(candidate(4), 3)\n    lu.assertEquals(candidate(16777217), 25)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(26), 5)\n    lu.assertEquals(candidate(4294967296), 33)\n    lu.assertEquals(candidate(65537), 17)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_103381_get_cindex", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # summ = 0\n-- # pair = 0\n-- # for i in range(0, len(Y)):\n-- #     for j in range(0, len(Y)):\n-- #         if i != j:\n-- #             if (Y[i] > Y[j]):\n-- #                 pair += 1\n-- #                 summ += 1 * (P[i] > P[j]) + 0.5 * (P[i] == P[j])\n-- # if pair != 0:\n-- #     return summ/pair\n-- # else:\n-- #     return 0\n-- \n-- ******NOTE; CAUTION******\n-- This code is from the original repository of \"DeepDTA; Bioinformatics\"\n-- Now the get_cindex is invalid (order dependent of given pairs).\n-- We will use lifelines.utils for \nlocal function get_cindex(Y, P)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_103381_get_cindex.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_cindex\n    lu.assertEquals(candidate({1, 0, 0, 0, 1, 1, 0, 0}, {1, 0, 0, 0, 1, 1, 0, 0}), 1.0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_103969_split_compound", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # merged_segs = []\n-- # # These occur only as prefixes, so they can always be merged:\n-- # mergeable_prefixes = ['on', 'bright', 'on_bright']\n-- # for s in compound.split('_'):\n-- #     if merged_segs and merged_segs[-1] in mergeable_prefixes:\n-- #         merged_segs[-1] += '_' + s\n-- #     else:\n-- #         merged_segs.append(s)\n-- # return merged_segs\n-- \n-- Split a possibly compound format string into segments.\n-- >>> split_compound('bold_underline_bright_blue_on_red')\n-- ['bold', 'underline', 'bright_blue', 'on_red']\nlocal function split_compound(compound)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_103969_split_compound.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = split_compound\n    lu.assertEquals(candidate('foo_bar'), {'foo', 'bar'})\n    lu.assertEquals(candidate('bold_underline_bright_blue_on_red'), {'bold', 'underline', 'bright_blue', 'on_red'})\n    lu.assertEquals(candidate('bold_underline'), {'bold', 'underline'})\n    lu.assertEquals(candidate('underline_bright_blue_on_red'), {'underline', 'bright_blue', 'on_red'})\n    lu.assertEquals(candidate('bold'), {'bold'})\n    lu.assertEquals(candidate('underline_on_red'), {'underline', 'on_red'})\n    lu.assertEquals(candidate('bold_underline_bright_blue_on_red'), {'bold', 'underline', 'bright_blue', 'on_red'})\n    lu.assertEquals(candidate('bold_underline_bright_green'), {'bold', 'underline', 'bright_green'})\n    lu.assertEquals(candidate('on_red'), {'on_red'})\n    lu.assertEquals(candidate('underline'), {'underline'})\n    lu.assertEquals(candidate('foo_on_bar'), {'foo', 'on_bar'})\n    lu.assertEquals(candidate('on_green'), {'on_green'})\n    lu.assertEquals(candidate('foo_bar_baz'), {'foo', 'bar', 'baz'})\n    lu.assertEquals(candidate('foo'), {'foo'})\n    lu.assertEquals(candidate('bold'), {'bold'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_104481_compliment", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return pattern.replace(\"A\", \"t\").replace(\"T\", \"a\").replace(\"G\", \"c\").replace(\"C\", \"g\").upper()\n-- \n-- [finds the complimentary strand of dna \"pattern\"]\n-- Args:\n--     pattern ([string]): [dna strand of which compliment is found]\n-- Returns:\n--     [string]: [compliment of dna pattern: A -> T, G -> C, T -> A, C -> G]\nlocal function compliment(pattern)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_104481_compliment.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = compliment\n    lu.assertEquals(candidate('T'), 'A')\n    lu.assertEquals(candidate('A'), 'T')\n    lu.assertEquals(candidate('C'), 'G')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('G'), 'C')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_104902__escape", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # txt = txt.replace('&', '&amp;')\n-- # txt = txt.replace('<', '&lt;')\n-- # txt = txt.replace('>', '&gt;')\n-- # return txt\n-- \n-- Basic html escaping.\nlocal function _escape(txt)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_104902__escape.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _escape\n    lu.assertEquals(candidate('&'), '&amp;')\n    lu.assertEquals(candidate('hello'), 'hello')\n    lu.assertEquals(candidate('foo<bar>'), 'foo&lt;bar&gt;')\n    lu.assertEquals(candidate('<html>'), '&lt;html&gt;')\n    lu.assertEquals(candidate('<world>hello&'), '&lt;world&gt;hello&amp;')\n    lu.assertEquals(candidate('<&>'), '&lt;&amp;&gt;')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('world'), 'world')\n    lu.assertEquals(candidate('<hello>'), '&lt;hello&gt;')\n    lu.assertEquals(candidate('<hello/>'), '&lt;hello/&gt;')\n    lu.assertEquals(candidate('<html>'), '&lt;html&gt;')\n    lu.assertEquals(candidate('&<hello/>'), '&amp;&lt;hello/&gt;')\n    lu.assertEquals(candidate('&<hello>'), '&amp;&lt;hello&gt;')\n    lu.assertEquals(candidate('<world/>'), '&lt;world/&gt;')\n    lu.assertEquals(candidate('&<world>hello'), '&amp;&lt;world&gt;hello')\n    lu.assertEquals(candidate('foo<bar/>'), 'foo&lt;bar/&gt;')\n    lu.assertEquals(candidate('hello'), 'hello')\n    lu.assertEquals(candidate('hello'), 'hello')\n    lu.assertEquals(candidate('&<world/>'), '&amp;&lt;world/&gt;')\n    lu.assertEquals(candidate('<hello>world&'), '&lt;hello&gt;world&amp;')\n    lu.assertEquals(candidate('>'), '&gt;')\n    lu.assertEquals(candidate('<'), '&lt;')\n    lu.assertEquals(candidate('&<world>'), '&amp;&lt;world&gt;')\n    lu.assertEquals(candidate('&<>'), '&amp;&lt;&gt;')\n    lu.assertEquals(candidate('&<hello>world'), '&amp;&lt;hello&gt;world')\n    lu.assertEquals(candidate('<world>'), '&lt;world&gt;')\n    lu.assertEquals(candidate('&<>'), '&amp;&lt;&gt;')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_105314_user_match", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if username is None:\n-- #     return True\n-- # def field_match(field):\n-- #     if field not in user:\n-- #         return False\n-- #     return user[field] == username\n-- # return field_match('email' if '@' in username else 'username')\n-- \n-- Return True if this is the user we are looking for.\n-- Compares a user structure from a Gerrit record to a username (or email\n-- address). All users match if the username is None.\nlocal function user_match(user, username)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_105314_user_match.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = user_match\n    lu.assertEquals(candidate({['username'] = 'foo'}, 'bar'), false)\n    lu.assertEquals(candidate({['username'] = 'foo'}, 'foo'), true)\n    lu.assertEquals(candidate({['email'] = '<EMAIL>'}, '<EMAIL>'), false)\n    lu.assertEquals(candidate({['username'] = 'foo', ['email'] = '<EMAIL>'}, 'bar'), false)\n    lu.assertEquals(candidate({['username'] = 'foo', ['email'] = '<EMAIL>'}, 'foo'), true)\n    lu.assertEquals(candidate({['username'] = 'foo', ['email'] = '<EMAIL>'}, '<EMAIL>'), false)\n    lu.assertEquals(candidate({['username'] = 'foo'}, None), true)\n    lu.assertEquals(candidate({['username'] = 'alice'}, 'bob'), false)\n    lu.assertEquals(candidate({['username'] = 'alice'}, None), true)\n    lu.assertEquals(candidate({['username'] = 'alice'}, 'alice'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_105342_get_month", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if month == \"Jan\":\n-- #     return 1\n-- # elif month == \"Feb\":\n-- #     return 2\n-- # elif month == \"Mar\":\n-- #     return 3\n-- # elif month == \"Apr\":\n-- #     return 4\n-- # elif month == \"May\":\n-- #     return 5\n-- # elif month == \"Jun\":\n-- #     return 6\n-- # elif month == \"Jul\":\n-- #     return 7\n-- # elif month == \"Aug\":\n-- #     return 8\n-- # elif month == \"Sep\":\n-- #     return 9\n-- # elif month == \"Oct\":\n-- #     return 10\n-- # elif month == \"Nov\":\n-- #     return 11\n-- # elif month == \"Dec\":\n-- #     return 12\n-- # else:\n-- #     raise Exception(\"Invalid month string \" + month)\n-- \n--     convert from a month string to a month integer.\nlocal function get_month(month)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_105342_get_month.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_month\n    lu.assertEquals(candidate('Nov'), 11)\n    lu.assertEquals(candidate('Jan'), 1)\n    lu.assertEquals(candidate('Jun'), 6)\n    lu.assertEquals(candidate('Sep'), 9)\n    lu.assertEquals(candidate('Oct'), 10)\n    lu.assertEquals(candidate('Apr'), 4)\n    lu.assertEquals(candidate('Mar'), 3)\n    lu.assertEquals(candidate('Feb'), 2)\n    lu.assertEquals(candidate('Aug'), 8)\n    lu.assertEquals(candidate('Dec'), 12)\n    lu.assertEquals(candidate('Jul'), 7)\n    lu.assertEquals(candidate('May'), 5)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_105509_normalize_case", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # text = str(text)\n-- # return text.lower()\n-- \n-- Convert `text' to lower case.\nlocal function normalize_case(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_105509_normalize_case.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = normalize_case\n    lu.assertEquals(candidate('Hi there!'), 'hi there!')\n    lu.assertEquals(candidate('a b C'), 'a b c')\n    lu.assertEquals(candidate('1 2 3 4 5'), '1 2 3 4 5')\n    lu.assertEquals(candidate('THAT'), 'that')\n    lu.assertEquals(candidate('none'), 'none')\n    lu.assertEquals(candidate('12345'), '12345')\n    lu.assertEquals(candidate('I like pie.'), 'i like pie.')\n    lu.assertEquals(candidate('Hello world!'), 'hello world!')\n    lu.assertEquals(candidate('None'), 'none')\n    lu.assertEquals(candidate('abc'), 'abc')\n    lu.assertEquals(candidate('this'), 'this')\n    lu.assertEquals(candidate(None), 'none')\n    lu.assertEquals(candidate('AbC'), 'abc')\n    lu.assertEquals(candidate('hi there!'), 'hi there!')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('Hey You!'), 'hey you!')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_105531_bisect_right", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if hi is None:\n-- #     hi = len(sorted_collection)\n-- # while lo < hi:\n-- #     mid = (lo + hi) // 2\n-- #     if sorted_collection[mid] <= item:\n-- #         lo = mid + 1\n-- #     else:\n-- #         hi = mid\n-- # return lo\n-- \n-- Locates the first element in a sorted array that is larger than a given value.\n-- It has the same interface as https://docs.python.org/3/library/bisect.html#bisect.bisect_right .\n-- :param sorted_collection: some ascending sorted collection with comparable items\n-- :param item: item to bisect\n-- :param lo: lowest index to consider (as in sorted_collection[lo:hi])\n-- :param hi: past the highest index to consider (as in sorted_collection[lo:hi])\n-- :return: index i such that all values in sorted_collection[lo:i] are <= item and all values in sorted_collection[i:hi] are > item.\n-- Examples:\n-- >>> bisect_right([0, 5, 7, 10, 15], 0)\n-- 1\n-- >>> bisect_right([0, 5, 7, 10, 15], 15)\n-- 5\n-- >>> bisect_right([0, 5, 7, 10, 15], 6)\n-- 2\n-- >>> bisect_right([0, 5, 7, 10, 15], 15, 1, 3)\n-- 3\n-- >>> bisect_right([0, 5, 7, 10, 15], 6, 2)\n-- 2\nlocal function bisect_right(sorted_collection, item, lo, hi)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_105531_bisect_right.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = bisect_right\n    lu.assertEquals(candidate({0, 5, 7, 10, 15}, 6, 2), 2)\n    lu.assertEquals(candidate({0, 5, 7, 10, 15}, 15, 1, 3), 3)\n    lu.assertEquals(candidate({0, 5, 7, 10, 15}, 0), 1)\n    lu.assertEquals(candidate({0, 5, 7, 10, 15}, 15, 1, 3), 3)\n    lu.assertEquals(candidate({0, 5, 7, 10, 15}, 15, 3), 5)\n    lu.assertEquals(candidate({0, 5, 7, 10, 15}, 15), 5)\n    lu.assertEquals(candidate({0, 5, 7, 10, 15}, 0), 1)\n    lu.assertEquals(candidate({0, 5, 7, 10, 15}, 6), 2)\n    lu.assertEquals(candidate({0, 5, 7, 10, 15}, 6), 2)\n    lu.assertEquals(candidate({0, 5, 7, 10, 15}, 15), 5)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_105585_concat_key_value", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return [key + ' ' + value for key, value in str_dict.items()]\n-- \n-- concat keys and values of dict containing strings elements\n-- into a list\n-- Parameters\n-- ----------\n-- str_dict: dict\n--     dict to process\n--     {key1: value1, key2: value2}\n-- Returns\n-- -------\n-- list\n--     ['key1 value1', 'key2 value2']\nlocal function concat_key_value(str_dict)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_105585_concat_key_value.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = concat_key_value\n    lu.assertEquals(candidate({}), {})\n    lu.assertEquals(candidate({['key3'] = 'value3', ['key4'] = 'value4'}), {'key3 value3', 'key4 value4'})\n    lu.assertEquals(candidate({['k1'] = 'v1', ['k2'] = 'v2', ['k3'] = 'v3', ['k4'] = 'v4', ['k5'] = 'v5'}), {'k1 v1', 'k2 v2', 'k3 v3', 'k4 v4', 'k5 v5'})\n    lu.assertEquals(candidate({['k1'] = 'v1'}), {'k1 v1'})\n    lu.assertEquals(candidate({['k1'] = 'v1', ['k2'] = 'v2', ['k3'] = 'v3'}), {'k1 v1', 'k2 v2', 'k3 v3'})\n    lu.assertEquals(candidate({['key5'] = 'value5', ['key6'] = 'value6'}), {'key5 value5', 'key6 value6'})\n    lu.assertEquals(candidate({['k1'] = 'v1', ['k2'] = 'v2', ['k3'] = 'v3', ['k4'] = 'v4'}), {'k1 v1', 'k2 v2', 'k3 v3', 'k4 v4'})\n    lu.assertEquals(candidate({['k1'] = 'v1', ['k2'] = 'v2', ['k3'] = 'v3', ['k4'] = 'v4', ['k5'] = 'v5', ['k6'] = 'v6'}), {'k1 v1', 'k2 v2', 'k3 v3', 'k4 v4', 'k5 v5', 'k6 v6'})\n    lu.assertEquals(candidate({['key7'] = 'value7', ['key8'] = 'value8'}), {'key7 value7', 'key8 value8'})\n    lu.assertEquals(candidate({['k1'] = 'v1', ['k2'] = 'v2'}), {'k1 v1', 'k2 v2'})\n    lu.assertEquals(candidate({['key1'] = 'value1', ['key2'] = 'value2'}), {'key1 value1', 'key2 value2'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_105820_jaccard_dependency", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # words1, _ = exp1\n-- # words2, _ = exp2\n-- # l1 = len(words1)\n-- # l2 = len(words2)\n-- # l_intersect = len(set(words1).intersection(set(words2)))\n-- # return 0 if l_intersect / float(l1) >= l_intersect / float(l2) else 1\n-- \n-- calculate the direction of dependence of 2 experiments \n-- if exp1 is parent of exp2 return 0 \n--                 otherwise return 1\nlocal function jaccard_dependency(exp1, exp2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_105820_jaccard_dependency.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = jaccard_dependency\n    lu.assertEquals(candidate({'word1', 'word2'}, {'word1', 'word3'}), 0)\n    lu.assertEquals(candidate({'word1', 'word2'}, {'word2', 'word2'}), 0)\n    lu.assertEquals(candidate({'word1', 'word2'}, {'word1', 'word1'}), 0)\n    lu.assertEquals(candidate({'word1', 'word2'}, {'word1', 'word2'}), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_106177__divides", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if (a1 == 0) and (a2 == 0):\n-- #     return a1, a2, 0\n-- # j = 0\n-- # while (a1 + a2) % 3 == 0:\n-- #     tmpa1 = a1\n-- #     a1 = ((a1 + a1) - a2) / 3\n-- #     a2 = (tmpa1 + a2) / 3\n-- #     j += 1\n-- # return a1, a2, j\n-- \n-- divide 1-w\n-- (i.e. a1+a2*w -> (1-w)^k * (x+y*w))\nlocal function _divides(a1, a2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_106177__divides.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _divides\n    lu.assertEquals(candidate(0, 1), {0, 1, 0})\n    lu.assertEquals(candidate(1, 0), {1, 0, 0})\n    lu.assertEquals(candidate(1, 2), {0, 1, 1})\n    lu.assertEquals(candidate(0, 0), {0, 0, 0})\n    lu.assertEquals(candidate(1, 1), {1, 1, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_106375_calib_time", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for acq_time, stim in normalized_triggers:\n-- #     if stim == 'calibration_trigger':\n-- #         return acq_time\n-- # return None\n-- \n-- Given a list of normalized triggers, return the time (acquisition) clock\n-- when the calibration trigger was displayed.\nlocal function calib_time(normalized_triggers)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_106375_calib_time.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = calib_time\n    lu.assertEquals(candidate({{10, 'calibration_trigger'}}), 10)\n    lu.assertEquals(candidate({{1, 'other_stim'}, {2, 'calibration_trigger'}, {3, 'calibration_trigger'}, {4, 'other_stim'}}), 2)\n    lu.assertEquals(candidate({{0, 'other_stim'}, {1, 'other_stim'}, {2, 'other_stim'}, {3, 'other_stim'}, {4, 'calibration_trigger'}}), 4)\n    lu.assertEquals(candidate({{0, 'calibration_trigger'}, {1, 'other_stim'}, {2, 'calibration_trigger'}, {3, 'other_stim'}, {4, 'other_stim'}}), 0)\n    lu.assertEquals(candidate({{0, 'calibration_trigger'}, {1, 'calibration_trigger'}, {2, 'other_stim'}, {3, 'other_stim'}}), 0)\n    lu.assertEquals(candidate({{5, 'calibration_trigger'}, {6, 'not_calibration_trigger'}}), 5)\n    lu.assertEquals(candidate({{0, 'other_stim'}, {1, 'other_stim'}, {2, 'calibration_trigger'}, {3, 'other_stim'}, {4, 'other_stim'}}), 2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_106887_symmetric_residue", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if a <= m // 2:\n-- #     return a\n-- # return a - m\n-- \n-- Return the residual mod m such that it is within half of the modulus.\n-- >>> symmetric_residue(1, 6)\n-- 1\n-- >>> symmetric_residue(4, 6)\n-- -2\nlocal function symmetric_residue(a, m)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_106887_symmetric_residue.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = symmetric_residue\n    lu.assertEquals(candidate(1, 13), 1)\n    lu.assertEquals(candidate(10, 11), -1)\n    lu.assertEquals(candidate(1, 6), 1)\n    lu.assertEquals(candidate(4, 6), -2)\n    lu.assertEquals(candidate(1, 11), 1)\n    lu.assertEquals(candidate(1, 10), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_107516_extract_uri_schema", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if uri:\n-- #     try:\n-- #         schema, data = uri.split(':', 1)\n-- #         return schema\n-- #     except:\n-- #         pass\n-- # return None\n-- \n--  Extract schema of given uri \nlocal function extract_uri_schema(uri)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_107516_extract_uri_schema.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = extract_uri_schema\n    lu.assertEquals(candidate('https://example.com:80'), 'https')\n    lu.assertEquals(candidate('http://'), 'http')\n    lu.assertEquals(candidate('http://www.google.com/something?a=1'), 'http')\n    lu.assertEquals(candidate('ftp://a.com/p?a=1'), 'ftp')\n    lu.assertEquals(candidate('ssh://example.com'), 'ssh')\n    lu.assertEquals(candidate(None), None)\n    lu.assertEquals(candidate('http://google.com'), 'http')\n    lu.assertEquals(candidate('http://example.com'), 'http')\n    lu.assertEquals(candidate('https://foo.bar'), 'https')\n    lu.assertEquals(candidate('http://a.com'), 'http')\n    lu.assertEquals(candidate('https://:1234'), 'https')\n    lu.assertEquals(candidate('https://'), 'https')\n    lu.assertEquals(candidate('https://foo.bar/baz?qux=1'), 'https')\n    lu.assertEquals(candidate('https://a.com/p?a=1'), 'https')\n    lu.assertEquals(candidate('https://a.com/'), 'https')\n    lu.assertEquals(candidate('telnet://example.com'), 'telnet')\n    lu.assertEquals(candidate('ftp://google.com'), 'ftp')\n    lu.assertEquals(candidate('http://a.com/p?a=1'), 'http')\n    lu.assertEquals(candidate('https://foo.bar/baz?qux=1#quux'), 'https')\n    lu.assertEquals(candidate('file:///path/to/file.ext'), 'file')\n    lu.assertEquals(candidate('ftp://a.com'), 'ftp')\n    lu.assertEquals(candidate('https://www.google.com'), 'https')\n    lu.assertEquals(candidate('http://www.google.com'), 'http')\n    lu.assertEquals(candidate('a.com'), None)\n    lu.assertEquals(candidate('ftp://www.google.com/something'), 'ftp')\n    lu.assertEquals(candidate('https://a.com'), 'https')\n    lu.assertEquals(candidate('https:///:1234'), 'https')\n    lu.assertEquals(candidate('https://a.com:1234/'), 'https')\n    lu.assertEquals(candidate('ldaps://example.com'), 'ldaps')\n    lu.assertEquals(candidate('https://www.google.com/something'), 'https')\n    lu.assertEquals(candidate('foo:bar'), 'foo')\n    lu.assertEquals(candidate('https://google.com'), 'https')\n    lu.assertEquals(candidate('http://www.google.com/something'), 'http')\n    lu.assertEquals(candidate('https://a.com:1234'), 'https')\n    lu.assertEquals(candidate('https://www.google.com/something?a=1'), 'https')\n    lu.assertEquals(candidate('foo:bar:baz:qux?quux#corge?grault'), 'foo')\n    lu.assertEquals(candidate('ftp://a.com:1234'), 'ftp')\n    lu.assertEquals(candidate('foo:bar:baz:qux'), 'foo')\n    lu.assertEquals(candidate('https:///'), 'https')\n    lu.assertEquals(candidate('ldap://example.com'), 'ldap')\n    lu.assertEquals(candidate('foo:bar:baz:qux?quux#corge?grault#garply'), 'foo')\n    lu.assertEquals(candidate('file://localhost/path/to/something.txt'), 'file')\n    lu.assertEquals(candidate('mailto:<EMAIL>'), 'mailto')\n    lu.assertEquals(candidate('https://foo.bar/baz'), 'https')\n    lu.assertEquals(candidate('https://example.com'), 'https')\n    lu.assertEquals(candidate('foo:bar:baz'), 'foo')\n    lu.assertEquals(candidate('http:///'), 'http')\n    lu.assertEquals(candidate('https://example.com:443'), 'https')\n    lu.assertEquals(candidate('http://a.com/'), 'http')\n    lu.assertEquals(candidate('foo:bar:baz:qux?quux'), 'foo')\n    lu.assertEquals(candidate('foo:bar:baz:qux?quux#corge'), 'foo')\n    lu.assertEquals(candidate('ftp://www.google.com/something?a=1'), 'ftp')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_107566_hour_min_sec", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if secs is not None:\n-- #     m, s = divmod(secs, 60)\n-- #     m = int(m)\n-- #     s = int(s)\n-- #     if m > 59:\n-- #         h, m = divmod(m, 60)\n-- #         h = int(h)\n-- #         m = int(m)\n-- #         if hms:\n-- #             return f'{h}h {m:02d}m {s:02d}s'\n-- #         else:\n-- #             return f'{h}:{m:02d}:{s:02d}'\n-- #     else:\n-- #         if hms:\n-- #             return f'{m}m {s:02d}s'\n-- #         else:\n-- #             return f'{m}:{s:02d}'\n-- # else:\n-- #     return '_:__'\n-- \n-- Convert seconds into a more readable hh:mm:ss representation\n-- :secs Number of seconds\n-- :hms Hours:Minutes:Seconds representation, rather than the default seconds.\nlocal function hour_min_sec(secs, hms)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_107566_hour_min_sec.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = hour_min_sec\n    lu.assertEquals(candidate(86399), '23:59:59')\n    lu.assertEquals(candidate(10), '0:10')\n    lu.assertEquals(candidate(3601), '1:00:01')\n    lu.assertEquals(candidate(7200), '2:00:00')\n    lu.assertEquals(candidate(3660), '1:01:00')\n    lu.assertEquals(candidate(7199), '1:59:59')\n    lu.assertEquals(candidate(None), '_:__')\n    lu.assertEquals(candidate(100), '1:40')\n    lu.assertEquals(candidate(61), '1:01')\n    lu.assertEquals(candidate(3610), '1:00:10')\n    lu.assertEquals(candidate(3600), '1:00:00')\n    lu.assertEquals(candidate(3), '0:03')\n    lu.assertEquals(candidate(3721), '1:02:01')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_107578_preprocessText", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Remove unwanted characters\n-- # unwanted_chars = set([\"@\", \"+\", '/', \"'\", '\"', '\\\\', '', '\\\\n', '\\n',\n-- #                       '?', '#', '%', '$', '&', ';', '!', ';', ':', \"*\", \"_\", \"=\"])\n-- # for char in unwanted_chars:\n-- #     text = text.replace(char, '')\n-- # # Convert all text into lowercase\n-- # text = text.lower()\n-- # return text\n-- \n--     This script parses text and removes stop words\nlocal function preprocessText(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_107578_preprocessText.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = preprocessText\n    lu.assertEquals(candidate('hello @name?'), 'hello name')\n    lu.assertEquals(candidate('hello @name;'), 'hello name')\n    lu.assertEquals(candidate('hello @name/'), 'hello name')\n    lu.assertEquals(candidate('@'), '')\n    lu.assertEquals(candidate(\"I'm\"), 'im')\n    lu.assertEquals(candidate(\"I'll\"), 'ill')\n    lu.assertEquals(candidate('hello @name!'), 'hello name')\n    lu.assertEquals(candidate('hello @name_'), 'hello name')\n    lu.assertEquals(candidate('hello @name'), 'hello name')\n    lu.assertEquals(candidate('hello @name:'), 'hello name')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('hello @name+'), 'hello name')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_10767_regenerate_response", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Init a blank json response\n-- # response_data = {'wallet_address': db_entry['wallet_address'], 'contract_address': db_entry['contract_address'],\n-- #                  'tokenId': db_entry['tokenId'], 'random_str': db_entry['random_str'],\n-- #                  'message': db_entry['message']}\n-- # # Form JSON response\n-- # return response_data\n-- \n-- Unique message generator.\n-- Args:\n--     db_entry (dict?): Stored response from the database that has already been created.\n-- Returns:\n--     JSON string which contains the message response\nlocal function regenerate_response(db_entry)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_10767_regenerate_response.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = regenerate_response\n    lu.assertEquals(candidate({['wallet_address'] = '0x12345', ['contract_address'] = '0x67890', ['tokenId'] = '3', ['random_str'] = '98765', ['message'] = 'Hello World!'}), {['wallet_address'] = '0x12345', ['contract_address'] = '0x67890', ['tokenId'] = '3', ['random_str'] = '98765', ['message'] = 'Hello World!'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_107867_bottom_up_partition_problem", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # sum_of_numbers = sum(numbers)\n-- # target_sum = sum_of_numbers // 2\n-- # is_even = sum_of_numbers % 2 == 0\n-- # if not is_even:\n-- #     return False\n-- # list_size = len(numbers)\n-- # cache = [[False for _ in range(target_sum + 1)] for _ in range(list_size + 1)]\n-- # # initialize all values in when the target is 0 to True\n-- # for i in range(list_size + 1):\n-- #     cache[i][0] = True\n-- # for number_index in range(1, list_size + 1):\n-- #     current_number = numbers[number_index - 1]\n-- #     for current_target in range(1, target_sum + 1):\n-- #         if current_number <= current_target:\n-- #             cache[number_index][current_target] = cache[number_index - 1][current_target - current_number] or \\\n-- #                 cache[number_index - 1][current_target]\n-- #         else:\n-- #             cache[number_index][current_target] = cache[number_index - 1][current_target]\n-- # return cache[list_size][target_sum]\n-- \n-- Parameters\n-- ----------\n-- numbers : list of integers\n-- Returns\n-- -------\n-- bool\n--     Boolean representing whether subsets exist which are equal to each other\n-- >>> bottom_up_partition_problem([1, 2, 3, 4])\n-- True\nlocal function bottom_up_partition_problem(numbers)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_107867_bottom_up_partition_problem.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = bottom_up_partition_problem\n    lu.assertEquals(candidate({1, 2, 3, 4, 5, 6}), false)\n    lu.assertEquals(candidate(list(range(100, 0, -1))), true)\n    lu.assertEquals(candidate({1, 1, 1, 1, 5, 6}), false)\n    lu.assertEquals(candidate({2, 2, 1, 1}), true)\n    lu.assertEquals(candidate({1, 1, 3, 4, 5, 6}), true)\n    lu.assertEquals(candidate({1, 2, 3, 4, 5}), false)\n    lu.assertEquals(candidate({10, 7, 8, 9}), true)\n    lu.assertEquals(candidate({1, 2, 3, 5}), false)\n    lu.assertEquals(candidate({1, 2, 3, 4}), true)\n    lu.assertEquals(candidate({100000}), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_108043_nearly_equal", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return a == b or int(a*10**sig_fig) == int(b*10**sig_fig)\n-- \n-- A helper function to determine whether two floats are nearly equal.\n-- Can be replaced by math.isclose in Py3\nlocal function nearly_equal(a, b, sig_fig)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_108043_nearly_equal.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = nearly_equal\n    lu.assertEquals(candidate(1e-09, 1e-09), true)\n    lu.assertEquals(candidate(0.123456789, 0.1234567891, 3), true)\n    lu.assertEquals(candidate(12345678900.0, 12345678900.0), true)\n    lu.assertEquals(candidate(0.12345, 0.12346, 7), false)\n    lu.assertEquals(candidate(0.123456789, 0.1234567891, 5), true)\n    lu.assertEquals(candidate(2.5, 2.5, 1), true)\n    lu.assertEquals(candidate(0.12345, 0.12346, 3), true)\n    lu.assertEquals(candidate(1e-09, 2e-09, 1), true)\n    lu.assertEquals(candidate(1.25, 1.25), true)\n    lu.assertEquals(candidate(1e-09, 2e-09, 0), true)\n    lu.assertEquals(candidate(1e-09, 2e-09, 5), true)\n    lu.assertEquals(candidate(2.5, 2.5, 3), true)\n    lu.assertEquals(candidate(1e-09, 1e-09, 5), true)\n    lu.assertEquals(candidate(0.12345, 0.12346, 6), false)\n    lu.assertEquals(candidate(2.5, 2.5, 1), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_10822_isIrrelevantManualRules", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if len(s) < 4:\n-- #     return True, cdoc, len(s)  # too short\n-- # if \"Monster\" in s[:15]:\n-- #     return True, cdoc, len(s)  # Paragraph with monster\n-- # if \"Newspaper logo is conditionally\" in s:\n-- #     return True, [], sum([0]+[len(s) for s in cdoc])  # reset doc, everything above is not relevant for monster jobs, this also includes \"Skip to main content\" in s: continue\n-- # if \"About the Job\" in s:\n-- #     return True, cdoc, len(s)\n-- # if '=\"' in s:\n-- #     return True, cdoc, len(s)  # often html gets not removed properly, what remains is sth like &lt; link rel=\"stylesheet\" href=\"https:, , coda. newjobs. com,\n-- # return False, cdoc, 0\n-- \n-- Hand-crafted rules to remove paragraphs or entire documents from job advertisements\n-- :param s: String\n-- :param cdoc: List of parts of document (String)\n-- :returns: Boolean (ie. True if removal is needed), list of parts of documents (String), length of irrelevant parts\nlocal function isIrrelevantManualRules(s, cdoc)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_10822_isIrrelevantManualRules.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = isIrrelevantManualRules\n    lu.assertEquals(candidate('Job Responsibilities', {'Job Responsibilities', \"We're hiring\"}), {false, {'Job Responsibilities', \"We're hiring\"}, 0})\n    lu.assertEquals(candidate('Why Work with Us', {'About the Job', \"We're hiring\"}), {false, {'About the Job', \"We're hiring\"}, 0})\n    lu.assertEquals(candidate('What We Do', {'What We Do', \"We're hiring\"}), {false, {'What We Do', \"We're hiring\"}, 0})\n    lu.assertEquals(candidate('Why Work with Us', {'Why Work with Us', \"We're hiring\"}), {false, {'Why Work with Us', \"We're hiring\"}, 0})\n    lu.assertEquals(candidate('What We Offer', {'What We Offer', \"We're hiring\"}), {false, {'What We Offer', \"We're hiring\"}, 0})\n    lu.assertEquals(candidate('a b c d e f g h i j k l m n o p q r s t u v w x y z 1 2 3 4 5 6 7 8 9 0', {'1', '2', '3', '4', '5', '6', '7', '8', '9', '0'}), {false, {'1', '2', '3', '4', '5', '6', '7', '8', '9', '0'}, 0})\n    lu.assertEquals(candidate('What We Do', {'About the Job', \"We're hiring\"}), {false, {'About the Job', \"We're hiring\"}, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_109004_temp_to_str", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # temp = str(temp)\n-- # while len(temp) < 5:\n-- #     temp = '{}0'.format(temp)\n-- # return temp\n-- \n--     converts temperature from format 0.1 to format 0.100 by adding three places after decimal\nlocal function temp_to_str(temp)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_109004_temp_to_str.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = temp_to_str\n    lu.assertEquals(candidate(-12.345), '-12.345')\n    lu.assertEquals(candidate(0.001), '0.001')\n    lu.assertEquals(candidate(0.1), '0.100')\n    lu.assertEquals(candidate(0.01), '0.010')\n    lu.assertEquals(candidate(1.1), '1.100')\n    lu.assertEquals(candidate(123.456), '123.456')\n    lu.assertEquals(candidate(0.1), '0.100')\n    lu.assertEquals(candidate(2.0), '2.000')\n    lu.assertEquals(candidate(0.0), '0.000')\n    lu.assertEquals(candidate(12.345), '12.345')\n    lu.assertEquals(candidate(0.12), '0.120')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_109336_all_is_same", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return all(el == vec[0] for el in vec)\n-- \n--     Test that all elements in a vector are the same\nlocal function all_is_same(vec)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_109336_all_is_same.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = all_is_same\n    lu.assertEquals(candidate({1, 1}), true)\n    lu.assertEquals(candidate({1, 1, 1, 1, 1, 1, 1}), true)\n    lu.assertEquals(candidate({1, 1, 1}), true)\n    lu.assertEquals(candidate({}), true)\n    lu.assertEquals(candidate({1, 1, 1, 1, 1}), true)\n    lu.assertEquals(candidate({1}), true)\n    lu.assertEquals(candidate({1, 1, 1, 2}), false)\n    lu.assertEquals(candidate({1, 1, 1, 1, 1, 1}), true)\n    lu.assertEquals(candidate({1, 1, 1, 1}), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_109878__to_initials", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # states = {\n-- #     'Alaska': 'AK',\n-- #     'Alabama': 'AL',\n-- #     'Arkansas': 'AR',\n-- #     'American Samoa': 'AS',\n-- #     'Arizona': 'AZ',\n-- #     'California': 'CA',\n-- #     'Colorado': 'CO',\n-- #     'Connecticut': 'CT',\n-- #     'District of Columbia': 'DC',\n-- #     'Delaware': 'DE',\n-- #     'Florida': 'FL',\n-- #     'Georgia': 'GA',\n-- #     'Guam': 'GU',\n-- #     'Hawaii': 'HI',\n-- #     'Iowa': 'IA',\n-- #     'Idaho': 'ID',\n-- #     'Illinois': 'IL',\n-- #     'Indiana': 'IN',\n-- #     'Kansas': 'KS',\n-- #     'Kentucky': 'KY',\n-- #     'Louisiana': 'LA',\n-- #     'Massachusetts': 'MA',\n-- #     'Maryland': 'MD',\n-- #     'Maine': 'ME',\n-- #     'Michigan': 'MI',\n-- #     'Minnesota': 'MN',\n-- #     'Missouri': 'MO',\n-- #     'Northern Mariana Islands': 'MP',\n-- #     'Mississippi': 'MS',\n-- #     'Montana': 'MT',\n-- #     'National': 'NA',\n-- #     'North Carolina': 'NC',\n-- #     'North Dakota': 'ND',\n-- #     'Nebraska': 'NE',\n-- #     'New Hampshire': 'NH',\n-- #     'New Jersey': 'NJ',\n-- #     'New Mexico': 'NM',\n-- #     'Nevada': 'NV',\n-- #     'New York': 'NY',\n-- #     'Ohio': 'OH',\n-- #     'Oklahoma': 'OK',\n-- #     'Oregon': 'OR',\n-- #     'Pennsylvania': 'PA',\n-- #     'Puerto Rico': 'PR',\n-- #     'Rhode Island': 'RI',\n-- #     'South Carolina': 'SC',\n-- #     'South Dakota': 'SD',\n-- #     'Tennessee': 'TN',\n-- #     'Texas': 'TX',\n-- #     'Utah': 'UT',\n-- #     'Virginia': 'VA',\n-- #     'Virgin Islands': 'VI',\n-- #     'Vermont': 'VT',\n-- #     'Washington': 'WA',\n-- #     'Wisconsin': 'WI',\n-- #     'West Virginia': 'WV',\n-- #     'Wyoming': 'WY'\n-- # }\n-- # return states[state]\n-- \n--     Given a state, returns its initials\nlocal function _to_initials(state)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_109878__to_initials.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _to_initials\n    lu.assertEquals(candidate('American Samoa'), 'AS')\n    lu.assertEquals(candidate('Indiana'), 'IN')\n    lu.assertEquals(candidate('Guam'), 'GU')\n    lu.assertEquals(candidate('Georgia'), 'GA')\n    lu.assertEquals(candidate('Kentucky'), 'KY')\n    lu.assertEquals(candidate('Maryland'), 'MD')\n    lu.assertEquals(candidate('Alabama'), 'AL')\n    lu.assertEquals(candidate('Hawaii'), 'HI')\n    lu.assertEquals(candidate('Texas'), 'TX')\n    lu.assertEquals(candidate('Arizona'), 'AZ')\n    lu.assertEquals(candidate('Arkansas'), 'AR')\n    lu.assertEquals(candidate('Louisiana'), 'LA')\n    lu.assertEquals(candidate('Connecticut'), 'CT')\n    lu.assertEquals(candidate('Maine'), 'ME')\n    lu.assertEquals(candidate('Iowa'), 'IA')\n    lu.assertEquals(candidate('Florida'), 'FL')\n    lu.assertEquals(candidate('District of Columbia'), 'DC')\n    lu.assertEquals(candidate('Colorado'), 'CO')\n    lu.assertEquals(candidate('California'), 'CA')\n    lu.assertEquals(candidate('Alaska'), 'AK')\n    lu.assertEquals(candidate('Kansas'), 'KS')\n    lu.assertEquals(candidate('Delaware'), 'DE')\n    lu.assertEquals(candidate('Illinois'), 'IL')\n    lu.assertEquals(candidate('Idaho'), 'ID')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_110506_fish", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # fis = [0] * 9\n-- # for f in fs:\n-- #     fis[f] += 1\n-- # return fis\n-- \n-- Transform a list of fish represented by their time-to-spawn state into a list of the\n-- number of fish in each state indexed by their time-to-spawn.\n-- Under this strategy, the example input [3, 4, 3, 1, 2] would be transformed into\n-- [0, 1, 1, 2, 1, 0, 0, 0, 0].\nlocal function fish(fs)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_110506_fish.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = fish\n    lu.assertEquals(candidate(list()), {0, 0, 0, 0, 0, 0, 0, 0, 0})\n    lu.assertEquals(candidate({3, 4, 3, 1, 2}), {0, 1, 1, 2, 1, 0, 0, 0, 0})\n    lu.assertEquals(candidate({3, 4, 3, 1, 2}), {0, 1, 1, 2, 1, 0, 0, 0, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_110590_check_hcl", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # allowed = set('0123456789abcdef')\n-- # return s.startswith('#') and len(s) == 7 and set(s[1:]).issubset(allowed)\n-- \n-- >>> check_hcl(\"#333333\")\n-- True\n-- >>> check_hcl(\"#eeeee\")\n-- False\n-- >>> check_hcl(\"3eeeee\")\n-- False\nlocal function check_hcl(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_110590_check_hcl.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = check_hcl\n    lu.assertEquals(candidate('#123abc'), true)\n    lu.assertEquals(candidate('#333333'), true)\n    lu.assertEquals(candidate('#123abz'), false)\n    lu.assertEquals(candidate('eeeee'), false)\n    lu.assertEquals(candidate('#eeeee'), false)\n    lu.assertEquals(candidate('#eeeee'), false)\n    lu.assertEquals(candidate('123abc'), false)\n    lu.assertEquals(candidate('3eeeee'), false)\n    lu.assertEquals(candidate('#333333'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_110596_discretisation_length", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return (N - 1) * (d + 1) + 1\n-- \n-- Returns the length of a linspace where there are N points with d intermediate (in-between) points.\nlocal function discretisation_length(N, d)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_110596_discretisation_length.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = discretisation_length\n    lu.assertEquals(candidate(5, 5), 25)\n    lu.assertEquals(candidate(1, 1), 1)\n    lu.assertEquals(candidate(2, 3), 5)\n    lu.assertEquals(candidate(3, 2), 7)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_11081_gcd_looping_with_divrem", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # while n != 0:\n-- #     m, n = n, m % n\n-- # return m\n-- \n-- Computes the greatest common divisor of two numbers by getting remainder from division in a\n-- loop.\n-- :param int m: First number.\n-- :param int n: Second number.\n-- :returns: GCD as a number.\nlocal function gcd_looping_with_divrem(m, n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_11081_gcd_looping_with_divrem.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = gcd_looping_with_divrem\n    lu.assertEquals(candidate(100000, 1), 1)\n    lu.assertEquals(candidate(1, 3), 1)\n    lu.assertEquals(candidate(2, 5), 1)\n    lu.assertEquals(candidate(15, 5), 5)\n    lu.assertEquals(candidate(1, 7), 1)\n    lu.assertEquals(candidate(1, 6), 1)\n    lu.assertEquals(candidate(1, 1), 1)\n    lu.assertEquals(candidate(1, 100000), 1)\n    lu.assertEquals(candidate(2, 4), 2)\n    lu.assertEquals(candidate(2, 8), 2)\n    lu.assertEquals(candidate(10, 20), 10)\n    lu.assertEquals(candidate(1, 4), 1)\n    lu.assertEquals(candidate(2, 0), 2)\n    lu.assertEquals(candidate(1, 9), 1)\n    lu.assertEquals(candidate(42, 12), 6)\n    lu.assertEquals(candidate(2, 3), 1)\n    lu.assertEquals(candidate(2, 6), 2)\n    lu.assertEquals(candidate(3, 6), 3)\n    lu.assertEquals(candidate(20, 10), 10)\n    lu.assertEquals(candidate(3, 3), 3)\n    lu.assertEquals(candidate(5, 15), 5)\n    lu.assertEquals(candidate(1000, 900), 100)\n    lu.assertEquals(candidate(2, 7), 1)\n    lu.assertEquals(candidate(4, 3), 1)\n    lu.assertEquals(candidate(100000, 100000), 100000)\n    lu.assertEquals(candidate(10, 15), 5)\n    lu.assertEquals(candidate(1, 5), 1)\n    lu.assertEquals(candidate(2, 1), 1)\n    lu.assertEquals(candidate(1, 8), 1)\n    lu.assertEquals(candidate(2, 9), 1)\n    lu.assertEquals(candidate(15, 10), 5)\n    lu.assertEquals(candidate(5, 6), 1)\n    lu.assertEquals(candidate(10, 5), 5)\n    lu.assertEquals(candidate(1, 0), 1)\n    lu.assertEquals(candidate(2, 2), 2)\n    lu.assertEquals(candidate(3, 2), 1)\n    lu.assertEquals(candidate(1, 2), 1)\n    lu.assertEquals(candidate(10, 3), 1)\n    lu.assertEquals(candidate(12, 15), 3)\n    lu.assertEquals(candidate(5, 10), 5)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_110834_get_overlap", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return max(0, min(a[1], b[1]) - max(a[0], b[0]))\n-- \n--     report overlap of coordinates\nlocal function get_overlap(a, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_110834_get_overlap.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_overlap\n    lu.assertEquals(candidate({1, 5}, {8, 8}), 0)\n    lu.assertEquals(candidate({1, 5}, {7, 8}), 0)\n    lu.assertEquals(candidate({1, 5}, {3, 4}), 1)\n    lu.assertEquals(candidate({0, 2}, {1, 5}), 1)\n    lu.assertEquals(candidate({1, 4}, {4, 6}), 0)\n    lu.assertEquals(candidate({1, 5}, {6, 8}), 0)\n    lu.assertEquals(candidate({1, 3}, {3, 4}), 0)\n    lu.assertEquals(candidate({1, 4}, {2, 3}), 1)\n    lu.assertEquals(candidate({1, 4}, {-2, -1}), 0)\n    lu.assertEquals(candidate({3, 4}, {1, 3}), 0)\n    lu.assertEquals(candidate({1, 5}, {6, 7}), 0)\n    lu.assertEquals(candidate({2, 4}, {1, 3}), 1)\n    lu.assertEquals(candidate({3, 4}, {1, 5}), 1)\n    lu.assertEquals(candidate({1, 3}, {2, 4}), 1)\n    lu.assertEquals(candidate({4, 6}, {1, 4}), 0)\n    lu.assertEquals(candidate({-1, 0}, {0, 1}), 0)\n    lu.assertEquals(candidate({-1, -1}, {1, 4}), 0)\n    lu.assertEquals(candidate({2, 3}, {1, 4}), 1)\n    lu.assertEquals(candidate({1, 2}, {3, 4}), 0)\n    lu.assertEquals(candidate({1, 5}, {0, 2}), 1)\n    lu.assertEquals(candidate({5, 9}, {8, 11}), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_111495_scale_vars", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return [v * scale for v in var_seq]\n-- \n--     Scale a variable sequence.\nlocal function scale_vars(var_seq, scale)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_111495_scale_vars.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = scale_vars\n    lu.assertEquals(candidate(list(range(1, 6)), 5), {5, 10, 15, 20, 25})\n    lu.assertEquals(candidate({1, 2, 3}, 5), {5, 10, 15})\n    lu.assertEquals(candidate(list(range(1, 6)), 10), {10, 20, 30, 40, 50})\n    lu.assertEquals(candidate({1, 2, 3}, -1), {-1, -2, -3})\n    lu.assertEquals(candidate({1, 2, 3}, 10), {10, 20, 30})\n    lu.assertEquals(candidate({2, 3}, 4), {8, 12})\n    lu.assertEquals(candidate({1, 2, 3}, 0), {0, 0, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_1122_get_bit", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return (byteval & (1 << index)) != 0\n-- \n-- retrieve bit value from byte at provided index\nlocal function get_bit(byteval, index)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_1122_get_bit.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_bit\n    lu.assertEquals(candidate(254, 3), true)\n    lu.assertEquals(candidate(0, 5), false)\n    lu.assertEquals(candidate(0, 4), false)\n    lu.assertEquals(candidate(0, 1), false)\n    lu.assertEquals(candidate(2, 0), 0)\n    lu.assertEquals(candidate(0, 0), false)\n    lu.assertEquals(candidate(1, 5), false)\n    lu.assertEquals(candidate(0, 6), false)\n    lu.assertEquals(candidate(128, 1), false)\n    lu.assertEquals(candidate(254, 0), false)\n    lu.assertEquals(candidate(1, 1), false)\n    lu.assertEquals(candidate(0, 5), false)\n    lu.assertEquals(candidate(0, 2), false)\n    lu.assertEquals(candidate(2, 1), true)\n    lu.assertEquals(candidate(32, 0), false)\n    lu.assertEquals(candidate(1, 3), false)\n    lu.assertEquals(candidate(2, 0), false)\n    lu.assertEquals(candidate(128, 0), false)\n    lu.assertEquals(candidate(32, 1), false)\n    lu.assertEquals(candidate(0, 7), false)\n    lu.assertEquals(candidate(0, 0), false)\n    lu.assertEquals(candidate(0, 3), false)\n    lu.assertEquals(candidate(1, 0), true)\n    lu.assertEquals(candidate(2, 2), false)\n    lu.assertEquals(candidate(0, 6), false)\n    lu.assertEquals(candidate(254, 2), true)\n    lu.assertEquals(candidate(128, 7), 1)\n    lu.assertEquals(candidate(64, 0), false)\n    lu.assertEquals(candidate(255, 2), true)\n    lu.assertEquals(candidate(128, 7), true)\n    lu.assertEquals(candidate(2, 2), 0)\n    lu.assertEquals(candidate(1, 3), false)\n    lu.assertEquals(candidate(255, 1), true)\n    lu.assertEquals(candidate(2, 3), 0)\n    lu.assertEquals(candidate(0, 4), false)\n    lu.assertEquals(candidate(0, 7), false)\n    lu.assertEquals(candidate(16, 0), false)\n    lu.assertEquals(candidate(255, 5), true)\n    lu.assertEquals(candidate(4, 1), false)\n    lu.assertEquals(candidate(16, 1), false)\n    lu.assertEquals(candidate(1, 0), true)\n    lu.assertEquals(candidate(255, 6), true)\n    lu.assertEquals(candidate(0, 2), false)\n    lu.assertEquals(candidate(1, 2), false)\n    lu.assertEquals(candidate(1, 6), false)\n    lu.assertEquals(candidate(8, 0), false)\n    lu.assertEquals(candidate(254, 6), true)\n    lu.assertEquals(candidate(1, 4), false)\n    lu.assertEquals(candidate(254, 1), true)\n    lu.assertEquals(candidate(4, 0), false)\n    lu.assertEquals(candidate(0, 1), false)\n    lu.assertEquals(candidate(0, 3), false)\n    lu.assertEquals(candidate(255, 7), true)\n    lu.assertEquals(candidate(255, 3), true)\n    lu.assertEquals(candidate(2, 0), false)\n    lu.assertEquals(candidate(1, 0), 1)\n    lu.assertEquals(candidate(1, 7), false)\n    lu.assertEquals(candidate(8, 1), false)\n    lu.assertEquals(candidate(2, 1), true)\n    lu.assertEquals(candidate(1, 1), false)\n    lu.assertEquals(candidate(1, 2), false)\n    lu.assertEquals(candidate(254, 5), true)\n    lu.assertEquals(candidate(2, 3), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_112319_find_divisor", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # divisors = []\n-- # for i in range(1, x + 1):\n-- #     if x % i == 0:\n-- #         divisors.append(i)\n-- # divisors.reverse()\n-- # return divisors\n-- \n-- Find all divisor of an integer\n-- Args:\n--     x (int)\n-- Returns:\n--     list: list of divisor\nlocal function find_divisor(x)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_112319_find_divisor.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = find_divisor\n    lu.assertEquals(candidate(1), {1})\n    lu.assertEquals(candidate(0), {})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_112396_process_args", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # ks = []\n-- # vs = []\n-- # for arg in args[1:]:\n-- #     k, v = arg.split('=')\n-- #     ks.append(k)\n-- #     vs.append(v)\n-- # args_dict = dict(zip(ks, vs))\n-- # return args_dict\n-- \n-- Basic kwarg handler to process input list of keyword arguments from the \n-- commandline and return a dictionary of values.\n-- Parameters\n-- ----------\n-- args : LIST\n--     List of kwargs from command line.\n-- Returns\n-- -------\n-- args_dict : DICT\n--     Dictionary of kwargs, split by '=' sign.\nlocal function process_args(args)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_112396_process_args.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = process_args\n    lu.assertEquals(candidate({'candidate', 'in_filename=test.csv'}), {['in_filename'] = 'test.csv'})\n    lu.assertEquals(candidate({'candidate', 'in_filename=test.csv', 'in_column=a', 'out_column=b'}), {['in_filename'] = 'test.csv', ['in_column'] = 'a', ['out_column'] = 'b'})\n    lu.assertEquals(candidate({'prog', 'a=b', 'c=d'}), {['a'] = 'b', ['c'] = 'd'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_11241_match_with_batchsize", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if lim % batchsize == 0:\n-- #     return lim\n-- # else:\n-- #     return lim - lim % batchsize\n-- \n-- Function used by modify_datasets below to match return the integer closest to lim\n-- which is multiple of batchsize, i.e., lim%batchsize=0.\nlocal function match_with_batchsize(lim, batchsize)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_11241_match_with_batchsize.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = match_with_batchsize\n    lu.assertEquals(candidate(17, 4), 16)\n    lu.assertEquals(candidate(2, 1), 2)\n    lu.assertEquals(candidate(16, 2), 16)\n    lu.assertEquals(candidate(1, 1), 1)\n    lu.assertEquals(candidate(13, 4), 12)\n    lu.assertEquals(candidate(200, 100), 200)\n    lu.assertEquals(candidate(10, 1), 10)\n    lu.assertEquals(candidate(19, 4), 16)\n    lu.assertEquals(candidate(16, 4), 16)\n    lu.assertEquals(candidate(3, 6), 0)\n    lu.assertEquals(candidate(5, 5), 5)\n    lu.assertEquals(candidate(10, 6), 6)\n    lu.assertEquals(candidate(4, 2), 4)\n    lu.assertEquals(candidate(4, 4), 4)\n    lu.assertEquals(candidate(10, 3), 9)\n    lu.assertEquals(candidate(4, 1), 4)\n    lu.assertEquals(candidate(20, 4), 20)\n    lu.assertEquals(candidate(3, 1), 3)\n    lu.assertEquals(candidate(2, 2), 2)\n    lu.assertEquals(candidate(3, 3), 3)\n    lu.assertEquals(candidate(12, 4), 12)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_112930_numPointsInSpans", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # timeline = []\n-- # for start, end in spans:\n-- #     assert start <= end\n-- #     timeline.append((start, 0, 1))\n-- #     timeline.append((end, 1, -1))\n-- # prevStart = None\n-- # value = 0\n-- # ret = 0\n-- # for position, _, diff in sorted(timeline):\n-- #     prevValue = value\n-- #     value += diff\n-- #     if prevValue == 0 and value > 0:\n-- #         prevStart = position\n-- #     elif prevValue > 0 and value == 0:\n-- #         ret += position - prevStart + 1\n-- # return ret\n-- \n-- >>> numPointsInSpans([(1, 3)])\n-- 3\n-- >>> numPointsInSpans([(1, 3), (5, 7)])\n-- 6\n-- >>> numPointsInSpans([(1, 3), (3, 5)])\n-- 5\n-- >>> numPointsInSpans([(1, 3), (2, 5)])\n-- 5\nlocal function numPointsInSpans(spans)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_112930_numPointsInSpans.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = numPointsInSpans\n    lu.assertEquals(candidate({{1, 3}, {3, 5}}), 5)\n    lu.assertEquals(candidate({{1, 3}, {5, 7}}), 6)\n    lu.assertEquals(candidate({{1, 3}}), 3)\n    lu.assertEquals(candidate({{1, 3}, {5, 7}}), 6)\n    lu.assertEquals(candidate({{1, 3}, {2, 5}}), 5)\n    lu.assertEquals(candidate({{1, 3}, {2, 5}}), 5)\n    lu.assertEquals(candidate({{1, 3}, {3, 5}}), 5)\n    lu.assertEquals(candidate({{1, 3}}), 3)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_113074_linearized_best_response", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return -1.0*y + 0.0\n-- \n-- A linearization of the best-response of the weights to some hyperparameter at some point.\n-- :param y: The hyperparameter to evaluate the linearization at.\n-- :return: The linearized best-response.\nlocal function linearized_best_response(y)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_113074_linearized_best_response.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = linearized_best_response\n    lu.assertEquals(candidate(0.5), -0.5)\n    lu.assertEquals(candidate(1.0), -1.0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_113300__get_alignments_skipped_bad_char_rule", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # reverse_pattern_prefix = pattern_prefix[::-1]  # checking from right to left\n-- # len_pattern_prefix = len(pattern_prefix)\n-- # for index in range(len_pattern_prefix):\n-- #     if reverse_pattern_prefix[index] == mismatched_char:\n-- #         return index\n-- # return len(pattern_prefix)\n-- \n-- Get the number of alignments that can be skipped according to bad\n-- character rule in Boyer Moore's exact matching algorithm\n-- >>> _get_alignments_skipped_bad_char_rule(\"C\", \"ATCTTTATCATA\")\n-- 3\n-- >>> _get_alignments_skipped_bad_char_rule(\"G\", \"ATCTTTATCATA\")\n-- 12\n-- >>> _get_alignments_skipped_bad_char_rule(\"T\", \"GTAGCGGC\")\n-- 6\n-- >>> _get_alignments_skipped_bad_char_rule(\"C\", \"GTAGC\")\n-- 0\n-- >>> _get_alignments_skipped_bad_char_rule(\"C\", \"GT\")\n-- 2\nlocal function _get_alignments_skipped_bad_char_rule(mismatched_char, pattern_prefix)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_113300__get_alignments_skipped_bad_char_rule.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _get_alignments_skipped_bad_char_rule\n    lu.assertEquals(candidate('G', 'ATCTTTATCATA'), 12)\n    lu.assertEquals(candidate('C', 'GTAGC'), 0)\n    lu.assertEquals(candidate('C', 'GTAGC'), 0)\n    lu.assertEquals(candidate('C', 'GT'), 2)\n    lu.assertEquals(candidate('T', 'GTAGCGGC'), 6)\n    lu.assertEquals(candidate('C', 'GT'), 2)\n    lu.assertEquals(candidate('T', 'GTAGCGGC'), 6)\n    lu.assertEquals(candidate('C', 'ATCTTTATCATA'), 3)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_113934_color_distance", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # r1, g1, b1 = rgb1\n-- # r2, g2, b2 = rgb2\n-- # return abs(r2-r1) + abs(g2-g1) + abs(b2-b1)\n-- \n--  Compute absolute difference between 3-channels. \nlocal function color_distance(rgb1, rgb2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_113934_color_distance.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = color_distance\n    lu.assertEquals(candidate({255, 255, 255}, {254, 254, 254}), 3)\n    lu.assertEquals(candidate({255, 255, 255}, {254, 255, 254}), 2)\n    lu.assertEquals(candidate({0, 0, 0}, {0, 0, 0}), 0)\n    lu.assertEquals(candidate({255, 255, 255}, {255, 255, 255}), 0)\n    lu.assertEquals(candidate({255, 255, 255}, {255, 254, 255}), 1)\n    lu.assertEquals(candidate({0, 0, 0}, {0, 1, 2}), 3)\n    lu.assertEquals(candidate({255, 255, 255}, {255, 255, 254}), 1)\n    lu.assertEquals(candidate({255, 255, 255}, {254, 254, 255}), 2)\n    lu.assertEquals(candidate({10, 10, 10}, {10, 10, 10}), 0)\n    lu.assertEquals(candidate({255, 255, 255}, {254, 255, 255}), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_114068_format_value", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return value.replace('|', ' ')\n-- \n-- When scraping indexable content from the search API, we joined\n-- organisation and topic titles with pipes. Since we are combining\n-- all the columns together here we need to make sure these get treated as\n-- separate words.\nlocal function format_value(value)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_114068_format_value.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = format_value\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('foo|bar'), 'foo bar')\n    lu.assertEquals(candidate('foo|bar|baz'), 'foo bar baz')\n    lu.assertEquals(candidate('A|B'), 'A B')\n    lu.assertEquals(candidate('Foo Bar'), 'Foo Bar')\n    lu.assertEquals(candidate('foo'), 'foo')\n    lu.assertEquals(candidate('Foo'), 'Foo')\n    lu.assertEquals(candidate('Foo|Bar|Baz'), 'Foo Bar Baz')\n    lu.assertEquals(candidate('Foo|Bar'), 'Foo Bar')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_114884_parse_duration", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # import re\n-- # def filter_digits(s): return \"\".join(re.findall('\\d+', s))\n-- # seconds = 0\n-- # if 'hr' in row:\n-- #     hr, row = row.split('hr')\n-- #     seconds += int(filter_digits(hr)) * 3600\n-- # if 'min' in row:\n-- #     min, row = row.split('min')\n-- #     seconds += int(filter_digits(min)) * 60\n-- # if 'sec' in row:\n-- #     sec, _ = row.split('sec')\n-- #     seconds += int(filter_digits(sec))\n-- # return seconds\n-- \n-- Parses duration value string 'Xhr', 'Ymin' or 'Zsec' and returns (X::Y::Z) as seconds\nlocal function parse_duration(row)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_114884_parse_duration.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_duration\n    lu.assertEquals(candidate('6hr'), 21600)\n    lu.assertEquals(candidate('5min'), 300)\n    lu.assertEquals(candidate('1hr 30min'), 5400)\n    lu.assertEquals(candidate('6sec'), 6)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_114921_generate_wms_and_wfs_query_urls", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # urls = []\n-- # for lyr in wms:\n-- #     urls.append(wms_base.format(lyr))\n-- # for veclyr in wfs:\n-- #     urls.append(wfs_base.format(veclyr, wfs[veclyr]))\n-- # return urls\n-- \n-- Generate WMS and WFS query URLs for individual data layers.\nlocal function generate_wms_and_wfs_query_urls(wms, wms_base, wfs, wfs_base)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_114921_generate_wms_and_wfs_query_urls.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = generate_wms_and_wfs_query_urls\n    lu.assertEquals(candidate({'1', '2'}, '{0}?request=GetCapabilities', {['3'] = '4'}, '{1}?service=WFS&version=1.0.0&request=GetFeature&typeName={0}&outputFormat=SHAPE-ZIP'), {'1?request=GetCapabilities', '2?request=GetCapabilities', '4?service=WFS&version=1.0.0&request=GetFeature&typeName=3&outputFormat=SHAPE-ZIP'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_115127_plusone", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # b = a+1\n-- # return b\n-- \n-- add 1 to a\nlocal function plusone(a)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_115127_plusone.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = plusone\n    lu.assertEquals(candidate(3000), 3001)\n    lu.assertEquals(candidate(6), 7)\n    lu.assertEquals(candidate(10), 11)\n    lu.assertEquals(candidate(30), 31)\n    lu.assertEquals(candidate(3), 4)\n    lu.assertEquals(candidate(300000), 300001)\n    lu.assertEquals(candidate(5), 6)\n    lu.assertEquals(candidate(30000000000), 30000000001)\n    lu.assertEquals(candidate(8), 9)\n    lu.assertEquals(candidate(4), 5)\n    lu.assertEquals(candidate(-1), 0)\n    lu.assertEquals(candidate(3000000000), 3000000001)\n    lu.assertEquals(candidate(2), 3)\n    lu.assertEquals(candidate(3000000000000), 3000000000001)\n    lu.assertEquals(candidate(3000000), 3000001)\n    lu.assertEquals(candidate(1), 2)\n    lu.assertEquals(candidate(30000), 30001)\n    lu.assertEquals(candidate(300000000), 300000001)\n    lu.assertEquals(candidate(300), 301)\n    lu.assertEquals(candidate(30000000), 30000001)\n    lu.assertEquals(candidate(0), 1)\n    lu.assertEquals(candidate(300000000000), 300000000001)\n    lu.assertEquals(candidate(7), 8)\n    lu.assertEquals(candidate(9), 10)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_116084_trans", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # merge\n-- # if n == m and n != 0 and n < 15:\n-- #     return 0, n + 1, True\n-- # # move\n-- # if m == 0:\n-- #     return 0, n, False\n-- # # nothing to do\n-- # return n, m, False\n-- \n-- transformation between 2 tiles\n-- [n,m] -> [n',m']\nlocal function trans(n, m)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_116084_trans.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = trans\n    lu.assertEquals(candidate(2, 5), {2, 5, false})\n    lu.assertEquals(candidate(0, 12), {0, 12, false})\n    lu.assertEquals(candidate(0, 0), {0, 0, false})\n    lu.assertEquals(candidate(0, 2), {0, 2, false})\n    lu.assertEquals(candidate(14, 2), {14, 2, false})\n    lu.assertEquals(candidate(0, 1), {0, 1, false})\n    lu.assertEquals(candidate(3, 5), {3, 5, false})\n    lu.assertEquals(candidate(1, 1), {0, 2, true})\n    lu.assertEquals(candidate(1, 0), {0, 1, false})\n    lu.assertEquals(candidate(4, 5), {4, 5, false})\n    lu.assertEquals(candidate(6, 2), {6, 2, false})\n    lu.assertEquals(candidate(3, 2), {3, 2, false})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_116189_convert_time", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # much better than the original one lol\n-- # # man I suck at docstrings lol\n-- # try:\n-- #     times = {}\n-- #     return_times = []\n-- #     time_dict = {\n-- #         \"years\": 31536000,\n-- #         \"months\": 2628000,\n-- #         \"weeks\": 604800,\n-- #         \"days\": 86400,\n-- #         \"hours\": 3600,\n-- #         \"minutes\": 60,\n-- #         \"seconds\": 1\n-- #     }\n-- #     for key, value in time_dict.items():\n-- #         times[str(key)] = {}\n-- #         times[str(key)][\"value\"] = int(_time // value)\n-- #         _time %= value\n-- #     for key, value in times.items():\n-- #         if not value['value']:\n-- #             continue\n-- #         return_times.append(\"{0} {1}\".format(value['value'], key))\n-- #     return ' '.join(return_times) if return_times else '0 seconds'\n-- # except Exception:\n-- #     return 'indefinitely'\n-- \n--     Convert time into a years, hours, minute, seconds thing.\nlocal function convert_time(_time)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_116189_convert_time.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = convert_time\n    lu.assertEquals(candidate(172800), '2 days')\n    lu.assertEquals(candidate(0), '0 seconds')\n    lu.assertEquals(candidate(86399), '23 hours 59 minutes 59 seconds')\n    lu.assertEquals(candidate(3), '3 seconds')\n    lu.assertEquals(candidate(7200), '2 hours')\n    lu.assertEquals(candidate(0), '0 seconds')\n    lu.assertEquals(candidate(59), '59 seconds')\n    lu.assertEquals(candidate(120), '2 minutes')\n    lu.assertEquals(candidate(3599), '59 minutes 59 seconds')\n    lu.assertEquals(candidate(1209600), '2 weeks')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_116230_hash_function", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return (a * num_row + b) % m\n-- \n-- Hash function for calculating signature matrix\n-- :return hash number for each row number\nlocal function hash_function(num_row, a, b, m)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_116230_hash_function.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = hash_function\n    lu.assertEquals(candidate(3, 1, 3, 4), 2)\n    lu.assertEquals(candidate(4, 3, 1, 6), 1)\n    lu.assertEquals(candidate(10, 2, 5, 6), 1)\n    lu.assertEquals(candidate(7, 1, 1, 2), 0)\n    lu.assertEquals(candidate(4, 2, 5, 6), 1)\n    lu.assertEquals(candidate(1, 3, 1, 6), 4)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_117035_getR", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # R = []\n-- # for r in range(len(t)):\n-- #     qrow = []\n-- #     for c in range(len(a)):\n-- #         qrow.append(m[t[r]][a[c]])\n-- #     R.append(qrow)\n-- # return R\n-- \n-- Returns the matrix R as in the canonical form [[Q,R],[0,I]]\n-- m is the input matrix, t is the list of the transient states,\n-- a is the list of the absorbing states.\nlocal function getR(m, t, a)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_117035_getR.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = getR\n    lu.assertEquals(candidate({{0, 1, 0}, {1, 0, 1}, {0, 1, 0}}, {0}, {1, 2}), {{1, 0}})\n    lu.assertEquals(candidate({{0, 1, 0}, {1, 0, 1}, {0, 1, 0}}, {}, {1, 2}), {})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_117164_PEER_cmd", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return f\"time Rscript {PEER_exec_path} {phenotype_file} {output_prefix} {num_peer} -o {output_dir} --covariates {covariates_file}\"\n-- \n--     Command to execute PEER covariate correction. Be sure to use r-4.0.3\nlocal function PEER_cmd(PEER_exec_path, phenotype_file, covariates_file, num_peer, output_prefix, output_dir)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_117164_PEER_cmd.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = PEER_cmd\n    lu.assertEquals(candidate('peer.R', 'peer_test_phenotype_file.txt', 'peer_test_covariates_file.txt', 100, 'peer_test_output_prefix', 'peer_test_output_dir'), 'time Rscript peer.R peer_test_phenotype_file.txt peer_test_output_prefix 100 -o peer_test_output_dir --covariates peer_test_covariates_file.txt')\n    lu.assertEquals(candidate('/home/user/PEER/PEER/peer.R', '/home/user/PEER/PEER/peer_example/pheno1.csv', '/home/user/PEER/PEER/peer_example/covar1.csv', 10, '10_peer', '/home/user/PEER/PEER/peer_example/out'), 'time Rscript /home/user/PEER/PEER/peer.R /home/user/PEER/PEER/peer_example/pheno1.csv 10_peer 10 -o /home/user/PEER/PEER/peer_example/out --covariates /home/user/PEER/PEER/peer_example/covar1.csv')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_117496_is_leap", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)\n-- \n--  https://stackoverflow.com/a/30714165 \nlocal function is_leap(year)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_117496_is_leap.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_leap\n    lu.assertEquals(candidate(2019), false)\n    lu.assertEquals(candidate(2401), false)\n    lu.assertEquals(candidate(1900), false)\n    lu.assertEquals(candidate(2012), true)\n    lu.assertEquals(candidate(2100), false)\n    lu.assertEquals(candidate(2800), true)\n    lu.assertEquals(candidate(1988), true)\n    lu.assertEquals(candidate(1999), false)\n    lu.assertEquals(candidate(2801), false)\n    lu.assertEquals(candidate(2021), false)\n    lu.assertEquals(candidate(2900), false)\n    lu.assertEquals(candidate(2004), true)\n    lu.assertEquals(candidate(2001), false)\n    lu.assertEquals(candidate(2400), true)\n    lu.assertEquals(candidate(3000), false)\n    lu.assertEquals(candidate(2000), true)\n    lu.assertEquals(candidate(2008), true)\n    lu.assertEquals(candidate(2020), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_118038__textTypeForDefStyleName", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if 'here' in attribute.lower() and defStyleName == 'dsOthers':\n-- #     return 'h'  # ruby\n-- # elif 'block' in attribute.lower() and defStyleName == 'dsComment':\n-- #     return 'b'\n-- # elif defStyleName in ('dsString', 'dsRegionMarker', 'dsChar', 'dsOthers'):\n-- #     return 's'\n-- # elif defStyleName == 'dsComment':\n-- #     return 'c'\n-- # else:\n-- #     return ' '\n-- \n--  ' ' for code\n-- 'c' for comments\n-- 'b' for block comments\n-- 'h' for here documents\nlocal function _textTypeForDefStyleName(attribute, defStyleName)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_118038__textTypeForDefStyleName.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _textTypeForDefStyleName\n    lu.assertEquals(candidate('string', 'dsString'), 's')\n    lu.assertEquals(candidate('char', 'dsChar'), 's')\n    lu.assertEquals(candidate('comment', 'dsComment'), 'c')\n    lu.assertEquals(candidate('block', 'dsComment'), 'b')\n    lu.assertEquals(candidate('here', 'dsOthers'), 'h')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_118563_parse_centrifuge", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if line.startswith('readID'):\n-- #     return None\n-- # x = line.rstrip().split('\\t')\n-- # return x[0], x[1], int(x[3]), int(x[5])\n-- \n-- Parse a line in a Centrifuge mapping file.\n-- Parameters\n-- ----------\n-- line : str\n--     Line to parse.\n-- Returns\n-- -------\n-- tuple of (str, str, int, int)\n--     Query, subject, score, length.\n-- Notes\n-- -----\n-- Centrifuge output format:\n--     readID, seqID, taxID, score, 2ndBestScore, hitLength, queryLength,\n--     numMatches\n-- .. _Centrifuge manual:\n--     https://ccb.jhu.edu/software/centrifuge/manual.shtml\nlocal function parse_centrifuge(line)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_118563_parse_centrifuge.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_centrifuge\n    lu.assertEquals(candidate('readID,seqID,taxID,score,2ndBestScore,hitLength,queryLength,numMatches'), None)\n    lu.assertEquals(candidate('1\\tseq1\\t1\\t500\\t2\\t1000\\t2000\\t3'), {'1', 'seq1', 500, 1000})\n    lu.assertEquals(candidate('read1\\tseq2\\t1\\t123\\t456\\t100\\t200\\t3'), {'read1', 'seq2', 123, 100})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_118723_replace_newlines", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # html_str = html_str.replace('<br>', '\\n')\n-- # return html_str\n-- \n--     remove newlines\nlocal function replace_newlines(html_str)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_118723_replace_newlines.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = replace_newlines\n    lu.assertEquals(candidate('<html><body><p>This is a paragraph.</p> This is another.</body></html>'), '<html><body><p>This is a paragraph.</p> This is another.</body></html>')\n    lu.assertEquals(candidate('<html><head><title>test</title></head><body>test\\n1</body></html>'), '<html><head><title>test</title></head><body>test\\n1</body></html>')\n    lu.assertEquals(candidate('<html><head><title>test</title></head><body>test\\n1\\n2\\n3\\n4\\n5</body></html>'), '<html><head><title>test</title></head><body>test\\n1\\n2\\n3\\n4\\n5</body></html>')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_119408_get_content_type", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # file_extension = file_extension.lower()\n-- # if file_extension == 'pdf':\n-- #     content_type = 'application/pdf'\n-- # elif file_extension == 'bmp':\n-- #     content_type = 'image/bmp'\n-- # elif file_extension == 'png':\n-- #     content_type = 'image/png'\n-- # elif file_extension in ['jpeg', 'jpg']:\n-- #     content_type = 'image/jpeg'\n-- # elif file_extension == 'doc':\n-- #     content_type = 'application/msword'\n-- # elif file_extension == 'docx':\n-- #     content_type = 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'\n-- # elif file_extension == 'text':\n-- #     content_type = 'text/plain'\n-- # else:\n-- #     return 'content_type not found'\n-- # return content_type\n-- \n-- getting content_type via file_type.\n-- :param file_extension\n-- :return: content_type\nlocal function get_content_type(file_extension)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_119408_get_content_type.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_content_type\n    lu.assertEquals(candidate('jpg'), 'image/jpeg')\n    lu.assertEquals(candidate('jpeg'), 'image/jpeg')\n    lu.assertEquals(candidate('docx'), 'application/vnd.openxmlformats-officedocument.wordprocessingml.document')\n    lu.assertEquals(candidate('png'), 'image/png')\n    lu.assertEquals(candidate('doc'), 'application/msword')\n    lu.assertEquals(candidate('bmp'), 'image/bmp')\n    lu.assertEquals(candidate('pdf'), 'application/pdf')\n    lu.assertEquals(candidate('text'), 'text/plain')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_120250_is_contained_in", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if frst[0] != scnd[0]:\n-- #     return False\n-- # if frst[1] >= scnd[1] and frst[2] <= scnd[2]:\n-- #     return True\n-- # return False\n-- \n-- Is the first region contained in the second.\n-- :param frst: a tuple representing the first region\n--              with chromosome, start, end as the first\n--              3 columns\n-- :param scnd: a tuple representing the second region\n--              with chromosome, start, end as the first\n--              3 columns\n-- :return: True or False\nlocal function is_contained_in(frst, scnd)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_120250_is_contained_in.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_contained_in\n    lu.assertEquals(candidate({'chr1', 1, 100}, {'chr1', 1, 150}), true)\n    lu.assertEquals(candidate({'1', 10, 15}, {'1', 10, 20}), true)\n    lu.assertEquals(candidate({'1', 0, 2}, {'1', 0, 1}), false)\n    lu.assertEquals(candidate({'chr1', 1, 100}, {'chr1', 50, 150}), false)\n    lu.assertEquals(candidate({'1', 10, 15}, {'1', 12, 16}), false)\n    lu.assertEquals(candidate({'1', 0, 1}, {'1', 0, 1}), true)\n    lu.assertEquals(candidate({'chr1', 1, 100}, {'chr1', 1, 200}), true)\n    lu.assertEquals(candidate({'1', 10, 15}, {'1', 9, 9}), false)\n    lu.assertEquals(candidate({'1', 10, 15}, {'1', 15, 16}), false)\n    lu.assertEquals(candidate({'1', 10, 15}, {'1', 9, 14}), false)\n    lu.assertEquals(candidate({'1', 10, 15}, {'1', 9, 8}), false)\n    lu.assertEquals(candidate({'1', 10, 15}, {'1', 15, 15}), false)\n    lu.assertEquals(candidate({'1', 10, 15}, {'1', 16, 16}), false)\n    lu.assertEquals(candidate({'1', 0, 1}, {'2', 0, 2}), false)\n    lu.assertEquals(candidate({'1', 0, 2}, {'1', 0, 2}), true)\n    lu.assertEquals(candidate({'1', 0, 2}, {'1', 1, 2}), false)\n    lu.assertEquals(candidate({'1', 10, 15}, {'1', 10, 15}), true)\n    lu.assertEquals(candidate({'1', 0, 1}, {'1', 0, 2}), true)\n    lu.assertEquals(candidate({'1', 10, 15}, {'1', 10, 10}), false)\n    lu.assertEquals(candidate({'chr2', 1, 100}, {'chr1', 1, 100}), false)\n    lu.assertEquals(candidate({'chr1', 1, 100}, {'chr1', 1, 100}), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_120570_IsEven", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return i % 2 == 0\n-- \n-- Returns a Z3 condition for if an Int is even\nlocal function IsEven(i)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_120570_IsEven.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = IsEven\n    lu.assertEquals(candidate(-1), false)\n    lu.assertEquals(candidate(4), true)\n    lu.assertEquals(candidate(5), false)\n    lu.assertEquals(candidate(-2), true)\n    lu.assertEquals(candidate(8), true)\n    lu.assertEquals(candidate(2), true)\n    lu.assertEquals(candidate(1), false)\n    lu.assertEquals(candidate(7), false)\n    lu.assertEquals(candidate(9), false)\n    lu.assertEquals(candidate(6), true)\n    lu.assertEquals(candidate(0), true)\n    lu.assertEquals(candidate(3), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_120905_fibonacci", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if n_terms <= 1:\n-- #     return n_terms\n-- # return (fibonacci(n_terms - 1) + fibonacci(n_terms - 2))\n-- \n-- Method that prints the fibonacci sequence until the n-th number\nlocal function fibonacci(n_terms)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_120905_fibonacci.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = fibonacci\n    lu.assertEquals(candidate(12), 144)\n    lu.assertEquals(candidate(2), 1)\n    lu.assertEquals(candidate(13), 233)\n    lu.assertEquals(candidate(24), 46368)\n    lu.assertEquals(candidate(16), 987)\n    lu.assertEquals(candidate(25), 75025)\n    lu.assertEquals(candidate(20), 6765)\n    lu.assertEquals(candidate(28), 317811)\n    lu.assertEquals(candidate(11), 89)\n    lu.assertEquals(candidate(29), 514229)\n    lu.assertEquals(candidate(30), 832040)\n    lu.assertEquals(candidate(19), 4181)\n    lu.assertEquals(candidate(3), 2)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(18), 2584)\n    lu.assertEquals(candidate(5), 5)\n    lu.assertEquals(candidate(4), 3)\n    lu.assertEquals(candidate(8), 21)\n    lu.assertEquals(candidate(21), 10946)\n    lu.assertEquals(candidate(15), 610)\n    lu.assertEquals(candidate(9), 34)\n    lu.assertEquals(candidate(10), 55)\n    lu.assertEquals(candidate(6), 8)\n    lu.assertEquals(candidate(7), 13)\n    lu.assertEquals(candidate(22), 17711)\n    lu.assertEquals(candidate(14), 377)\n    lu.assertEquals(candidate(23), 28657)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(26), 121393)\n    lu.assertEquals(candidate(27), 196418)\n    lu.assertEquals(candidate(17), 1597)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_121456__GenerateMakePrivateGetter", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # code = [\n-- #     \"\\n\\nprivate Delegate MakeGetter(DataViewRow input, int iinfo)\",\n-- #     \"\\n{\",\n-- #     \"\\nValueGetter<{0}> result = (ref {0} dst) =>\".format(return_type),\n-- #     \"\\n{\"\n-- # ]\n-- # # generate holders for intermediate values\n-- # for variable_name in variable_names:\n-- #     code.append(\"\\nvar {0}Val =  _parent._{0}.GetValue(input);\".format(variable_name))\n-- # code.append(\n-- #     \"\\ndst = {function_name}(\".format(\n-- #         function_name=function_name\n-- #     )\n-- # )\n-- # length = len(variable_names)\n-- # for index in range(length):\n-- #     code.append(\n-- #         \"{0}Val{1}\".format(\n-- #             variable_names[index],\n-- #             \", \" if index != length - 1 else \"\"\n-- #         )\n-- #     )\n-- # code.extend(\n-- #     [\n-- #         ');',\n-- #         \"\\n};\",\n-- #         \"\\n\\nreturn result;\\n}\"\n-- #     ]\n-- # )\n-- # return \"\".join(code)\n-- \n-- Internal helper method to actually get the data. It calls the DLL and passes the correct data to it.\n-- It then returns that value back to ML.NET.\nlocal function _GenerateMakePrivateGetter(return_type, variable_names, function_name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_121456__GenerateMakePrivateGetter.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _GenerateMakePrivateGetter\n    lu.assertEquals(candidate('System.Single', {'x'}, 'Foo'), '\\n\\nprivate Delegate MakeGetter(DataViewRow input, int iinfo)\\n{\\nValueGetter<System.Single> result = (ref System.Single dst) =>\\n{\\nvar xVal =  _parent._x.GetValue(input);\\ndst = Foo(xVal);\\n};\\n\\nreturn result;\\n}')\n    lu.assertEquals(candidate('System.Single', {'x', 'y', 'z'}, 'Foo'), '\\n\\nprivate Delegate MakeGetter(DataViewRow input, int iinfo)\\n{\\nValueGetter<System.Single> result = (ref System.Single dst) =>\\n{\\nvar xVal =  _parent._x.GetValue(input);\\nvar yVal =  _parent._y.GetValue(input);\\nvar zVal =  _parent._z.GetValue(input);\\ndst = Foo(xVal, yVal, zVal);\\n};\\n\\nreturn result;\\n}')\n    lu.assertEquals(candidate('System.Single', {'x', 'y'}, 'Foo'), '\\n\\nprivate Delegate MakeGetter(DataViewRow input, int iinfo)\\n{\\nValueGetter<System.Single> result = (ref System.Single dst) =>\\n{\\nvar xVal =  _parent._x.GetValue(input);\\nvar yVal =  _parent._y.GetValue(input);\\ndst = Foo(xVal, yVal);\\n};\\n\\nreturn result;\\n}')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_121971_months_of_gregorian_calendar", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return {1: 'January', 2: 'February', 3: 'March', 4: 'April', 5: 'May', 6: 'June',\n-- #         7: 'July', 8: 'August', 9: 'September', 10: 'October', 11: 'November',\n-- #         12: 'December'}\n-- \n-- Months of the Gregorian calendar.\n-- Parameters\n-- ----------\n-- year : int, optional\n--     (dummy value).\n-- Returns\n-- -------\n-- out : dict\n--     integers as keys, months of the Gregorian calendar as values.\n-- Notes\n-- -----\n-- Appropriate for use as 'year_cycles' function in :class:`Calendar`.\n-- This module has a built-in calendar with months only:\n-- :data:`CalMonthsOnly`.\nlocal function months_of_gregorian_calendar(year)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_121971_months_of_gregorian_calendar.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = months_of_gregorian_calendar\n    lu.assertEquals(candidate(2000), {[1] = 'January', [2] = 'February', [3] = 'March', [4] = 'April', [5] = 'May', [6] = 'June', [7] = 'July', [8] = 'August', [9] = 'September', [10] = 'October', [11] = 'November', [12] = 'December'})\n    lu.assertEquals(candidate(2009), {[1] = 'January', [2] = 'February', [3] = 'March', [4] = 'April', [5] = 'May', [6] = 'June', [7] = 'July', [8] = 'August', [9] = 'September', [10] = 'October', [11] = 'November', [12] = 'December'})\n    lu.assertEquals(candidate(2020), {[1] = 'January', [2] = 'February', [3] = 'March', [4] = 'April', [5] = 'May', [6] = 'June', [7] = 'July', [8] = 'August', [9] = 'September', [10] = 'October', [11] = 'November', [12] = 'December'})\n    lu.assertEquals(candidate(1999), {[1] = 'January', [2] = 'February', [3] = 'March', [4] = 'April', [5] = 'May', [6] = 'June', [7] = 'July', [8] = 'August', [9] = 'September', [10] = 'October', [11] = 'November', [12] = 'December'})\n    lu.assertEquals(candidate(2002), {[1] = 'January', [2] = 'February', [3] = 'March', [4] = 'April', [5] = 'May', [6] = 'June', [7] = 'July', [8] = 'August', [9] = 'September', [10] = 'October', [11] = 'November', [12] = 'December'})\n    lu.assertEquals(candidate(2001), {[1] = 'January', [2] = 'February', [3] = 'March', [4] = 'April', [5] = 'May', [6] = 'June', [7] = 'July', [8] = 'August', [9] = 'September', [10] = 'October', [11] = 'November', [12] = 'December'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_122909_limit_count", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if count is None:\n-- #     return on_device\n-- # elif count > 0:\n-- #     return min(count, on_device)\n-- # else:\n-- #     return max(on_device + count, 0)\n-- \n-- Handling of the optional \"count\" parameter, common in many commands.\n-- Parameters:\n--   - count     -- desired number of elements\n--   - on_device -- number of elements on device\n-- If count is None or 0 return what's on device\n-- If count > 0 use count, unless it is more than what's on device. \n-- If count < 0 that means \"abs(count) less than what's on device\n-- Typical usage: \n--    count = limit_count(count, mc.mgrp_get_count())\nlocal function limit_count(count, on_device)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_122909_limit_count.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = limit_count\n    lu.assertEquals(candidate(-10, -10), 0)\n    lu.assertEquals(candidate(-2, 0), 0)\n    lu.assertEquals(candidate(3, 0), 0)\n    lu.assertEquals(candidate(None, 2), 2)\n    lu.assertEquals(candidate(10, 10), 10)\n    lu.assertEquals(candidate(None, 1), 1)\n    lu.assertEquals(candidate(None, 0), 0)\n    lu.assertEquals(candidate(0, -10), 0)\n    lu.assertEquals(candidate(3, 2), 2)\n    lu.assertEquals(candidate(-3, 1), 0)\n    lu.assertEquals(candidate(1, 10), 1)\n    lu.assertEquals(candidate(-1, 0), 0)\n    lu.assertEquals(candidate(3, 1), 1)\n    lu.assertEquals(candidate(10, 0), 0)\n    lu.assertEquals(candidate(None, 10), 10)\n    lu.assertEquals(candidate(-1, 10), 9)\n    lu.assertEquals(candidate(-3, 2), 0)\n    lu.assertEquals(candidate(-10, 10), 0)\n    lu.assertEquals(candidate(0, 0), 0)\n    lu.assertEquals(candidate(-2, 2), 0)\n    lu.assertEquals(candidate(1, 0), 0)\n    lu.assertEquals(candidate(None, 5), 5)\n    lu.assertEquals(candidate(2, 2), 2)\n    lu.assertEquals(candidate(10, 1), 1)\n    lu.assertEquals(candidate(-1, 2), 1)\n    lu.assertEquals(candidate(2, 0), 0)\n    lu.assertEquals(candidate(-3, 0), 0)\n    lu.assertEquals(candidate(2, 1), 1)\n    lu.assertEquals(candidate(10, 5), 5)\n    lu.assertEquals(candidate(1, 1), 1)\n    lu.assertEquals(candidate(-2, 1), 0)\n    lu.assertEquals(candidate(10, 2), 2)\n    lu.assertEquals(candidate(1, 2), 1)\n    lu.assertEquals(candidate(-10, 0), 0)\n    lu.assertEquals(candidate(0, -5), 0)\n    lu.assertEquals(candidate(-5, 0), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_122974_distManhattan", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # (x1, y1) = p1\n-- # (x2, y2) = p2\n-- # return abs(x1-x2)+abs(y1-y2)\n-- \n--  calcule la distance de Manhattan entre le tuple \n-- p1 et le tuple p2\nlocal function distManhattan(p1, p2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_122974_distManhattan.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = distManhattan\n    lu.assertEquals(candidate({1, 1}, {0, 0}), 2)\n    lu.assertEquals(candidate({-1, -1}, {-1, -1}), 0)\n    lu.assertEquals(candidate({0, 0}, {1, 2}), 3)\n    lu.assertEquals(candidate({-1, -1}, {1, -1}), 2)\n    lu.assertEquals(candidate({-1, 0}, {0, -1}), 2)\n    lu.assertEquals(candidate({-1, -1}, {1, 1}), 4)\n    lu.assertEquals(candidate({1, 1}, {1, 1}), 0)\n    lu.assertEquals(candidate({0, 0}, {1, 1}), 2)\n    lu.assertEquals(candidate({-1, 1}, {1, 1}), 2)\n    lu.assertEquals(candidate({1, 1}, {-1, -1}), 4)\n    lu.assertEquals(candidate({1, 0}, {0, 1}), 2)\n    lu.assertEquals(candidate({-2, 0}, {2, 0}), 4)\n    lu.assertEquals(candidate({0, 0}, {3, 3}), 6)\n    lu.assertEquals(candidate({1, 1}, {-1, -1}), 4)\n    lu.assertEquals(candidate({0, 0}, {-1, -1}), 2)\n    lu.assertEquals(candidate({0, 0}, {3, 4}), 7)\n    lu.assertEquals(candidate({0, 0}, {-1, -1}), 2)\n    lu.assertEquals(candidate({0, 0}, {1, 1}), 2)\n    lu.assertEquals(candidate({100, 100}, {0, 0}), 200)\n    lu.assertEquals(candidate({0, 0}, {0, 1}), 1)\n    lu.assertEquals(candidate({0, 0}, {100, 100}), 200)\n    lu.assertEquals(candidate({1, 1}, {0, 0}), 2)\n    lu.assertEquals(candidate({0, 0}, {0, 0}), 0)\n    lu.assertEquals(candidate({0, 0}, {-2, -2}), 4)\n    lu.assertEquals(candidate({1, 1}, {2, 2}), 2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_123440_is_power_of_2", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return not val & (val-1)\n-- \n-- Returns True if an integer is a power of 2. Only works for x > 0.\nlocal function is_power_of_2(val)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_123440_is_power_of_2.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_power_of_2\n    lu.assertEquals(candidate(4), true)\n    lu.assertEquals(candidate(17), false)\n    lu.assertEquals(candidate(26), false)\n    lu.assertEquals(candidate(6), false)\n    lu.assertEquals(candidate(22), false)\n    lu.assertEquals(candidate(25), false)\n    lu.assertEquals(candidate(7), false)\n    lu.assertEquals(candidate(1), true)\n    lu.assertEquals(candidate(14), false)\n    lu.assertEquals(candidate(10), false)\n    lu.assertEquals(candidate(65), false)\n    lu.assertEquals(candidate(18), false)\n    lu.assertEquals(candidate(21), false)\n    lu.assertEquals(candidate(11), false)\n    lu.assertEquals(candidate(2), true)\n    lu.assertEquals(candidate(15), false)\n    lu.assertEquals(candidate(13), false)\n    lu.assertEquals(candidate(16), true)\n    lu.assertEquals(candidate(12), false)\n    lu.assertEquals(candidate(23), false)\n    lu.assertEquals(candidate(27), false)\n    lu.assertEquals(candidate(19), false)\n    lu.assertEquals(candidate(9), false)\n    lu.assertEquals(candidate(8), true)\n    lu.assertEquals(candidate(64), true)\n    lu.assertEquals(candidate(5), false)\n    lu.assertEquals(candidate(3), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_123581_htmlentities", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for lookfor, replacewith in [\n-- #     (\"&\", \"&amp;\"),\n-- #     (\">\", \"&gt;\"),\n-- #     (\"<\", \"&lt;\"),\n-- #     (\"'\", \"&#39;\"),\n-- #     ('\"', \"&quot;\"),\n-- # ]:\n-- #     text = text.replace(lookfor, replacewith)\n-- # return text\n-- \n-- Escape chars in the text for HTML presentation\n-- Args:\n--   text (str): subject to replace\n-- Returns:\n--   str : result of replacement\nlocal function htmlentities(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_123581_htmlentities.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = htmlentities\n    lu.assertEquals(candidate('>'), '&gt;')\n    lu.assertEquals(candidate('a < b'), 'a &lt; b')\n    lu.assertEquals(candidate('<'), '&lt;')\n    lu.assertEquals(candidate('\"'), '&quot;')\n    lu.assertEquals(candidate('Testing & testing'), 'Testing &amp; testing')\n    lu.assertEquals(candidate('Testing & testing > testing < testing'), 'Testing &amp; testing &gt; testing &lt; testing')\n    lu.assertEquals(candidate('a & b'), 'a &amp; b')\n    lu.assertEquals(candidate('I have a new line of text\\n'), 'I have a new line of text\\n')\n    lu.assertEquals(candidate('a > b'), 'a &gt; b')\n    lu.assertEquals(candidate('&'), '&amp;')\n    lu.assertEquals(candidate(\"'\"), '&#39;')\n    lu.assertEquals(candidate('I have a new line of text'), 'I have a new line of text')\n    lu.assertEquals(candidate('Testing'), 'Testing')\n    lu.assertEquals(candidate('Testing & testing > testing <'), 'Testing &amp; testing &gt; testing &lt;')\n    lu.assertEquals(candidate('a \" b'), 'a &quot; b')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_123984_decimal_to_hexadecimal", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # result = hex(number)[2:]\n-- # return result\n-- \n-- Return hexadecimal version of the specified decimal number.\nlocal function decimal_to_hexadecimal(number)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_123984_decimal_to_hexadecimal.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = decimal_to_hexadecimal\n    lu.assertEquals(candidate(7), '7')\n    lu.assertEquals(candidate(5), '5')\n    lu.assertEquals(candidate(10), 'a')\n    lu.assertEquals(candidate(3), '3')\n    lu.assertEquals(candidate(15), 'f')\n    lu.assertEquals(candidate(257), '101')\n    lu.assertEquals(candidate(1), '1')\n    lu.assertEquals(candidate(32767), '7fff')\n    lu.assertEquals(candidate(255), 'ff')\n    lu.assertEquals(candidate(0), '0')\n    lu.assertEquals(candidate(25), '19')\n    lu.assertEquals(candidate(255), 'ff')\n    lu.assertEquals(candidate(10), 'a')\n    lu.assertEquals(candidate(12345), '3039')\n    lu.assertEquals(candidate(11), 'b')\n    lu.assertEquals(candidate(100), '64')\n    lu.assertEquals(candidate(16), '10')\n    lu.assertEquals(candidate(256), '100')\n    lu.assertEquals(candidate(174), 'ae')\n    lu.assertEquals(candidate(0), '0')\n    lu.assertEquals(candidate(256), '100')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_12403_degrees_to_meters", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # ret_val = 111195 * degrees\n-- # return ret_val\n-- \n-- 111195 = (Earth mean radius)*PI/180\n-- (supposedly 'maximum error using this method is ~ 0.1%')\n-- :see: https://stackoverflow.com/questions/12204834/get-distance-in-meters-instead-of-degrees-in-spatialite\nlocal function degrees_to_meters(degrees)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_12403_degrees_to_meters.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = degrees_to_meters\n    lu.assertEquals(candidate(6), 667170)\n    lu.assertEquals(candidate(10000), 1111950000)\n    lu.assertEquals(candidate(3), 333585)\n    lu.assertEquals(candidate(10), 1111950)\n    lu.assertEquals(candidate(5), 555975)\n    lu.assertEquals(candidate(4), 444780)\n    lu.assertEquals(candidate(1), 111195)\n    lu.assertEquals(candidate(1000), 111195000)\n    lu.assertEquals(candidate(100000), 11119500000)\n    lu.assertEquals(candidate(100), 11119500)\n    lu.assertEquals(candidate(2), 222390)\n    lu.assertEquals(candidate(0), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_125020__is_temp_garbage", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return filename.startswith(\"~$\") or filename.endswith(\".tmp\")\n-- \n-- Is this a Microsoft Office temp file?\nlocal function _is_temp_garbage(filename)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_125020__is_temp_garbage.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _is_temp_garbage\n    lu.assertEquals(candidate('c:\\\\users\\\\someone\\\\somefile.docx~123.tmp'), true)\n    lu.assertEquals(candidate('c:\\\\users\\\\someone\\\\somefile.docx~123.tmp~123'), false)\n    lu.assertEquals(candidate('c:\\\\users\\\\someone\\\\somefile.docx~1'), false)\n    lu.assertEquals(candidate('c:\\\\users\\\\someone\\\\somefile.docx.tmp'), true)\n    lu.assertEquals(candidate('c:\\\\users\\\\someone\\\\somefile.docx~123'), false)\n    lu.assertEquals(candidate('c:\\\\users\\\\someone\\\\somefile.docx~123.tmp~123.tmp'), true)\n    lu.assertEquals(candidate('c:\\\\users\\\\someone\\\\somefile.docx'), false)\n    lu.assertEquals(candidate('c:\\\\users\\\\someone\\\\~somefile.docx'), false)\n    lu.assertEquals(candidate('c:\\\\users\\\\someone\\\\somefile.docx~123.tmp~'), false)\n    lu.assertEquals(candidate('c:\\\\users\\\\someone\\\\somefile.docx~'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_125164_debugsquare", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # print(\"DEBUG: the value of x is\", x, \"in the function debugsquare\")\n-- # return x * x\n-- \n-- Return x squared but also print a debug value of x.\nlocal function debugsquare(x)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_125164_debugsquare.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = debugsquare\n    lu.assertEquals(candidate(-2), 4)\n    lu.assertEquals(candidate(4), 16)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(5), 25)\n    lu.assertEquals(candidate(3), 9)\n    lu.assertEquals(candidate(2), 4)\n    lu.assertEquals(candidate(12), 144)\n    lu.assertEquals(candidate(10), 100)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_125500_cmds_to_bash", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"\"\"\\\n-- # in/bash\n-- # -vexubE -o pipefail\n-- # # % '\\n'.join(cmds)\n-- \n-- Turn a list of cmds into a bash script.\nlocal function cmds_to_bash(cmds)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_125500_cmds_to_bash.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = cmds_to_bash\n    lu.assertEquals(candidate({'echo hello', 'echo world'}), '#!/bin/bash\\nset -vexubE -o pipefail\\necho hello\\necho world\\n')\n    lu.assertEquals(candidate({'echo hello world'}), '#!/bin/bash\\nset -vexubE -o pipefail\\necho hello world\\n')\n    lu.assertEquals(candidate({'set -ex', 'echo hello world'}), '#!/bin/bash\\nset -vexubE -o pipefail\\nset -ex\\necho hello world\\n')\n    lu.assertEquals(candidate({'a', 'b c'}), '#!/bin/bash\\nset -vexubE -o pipefail\\na\\nb c\\n')\n    lu.assertEquals(candidate({}), '#!/bin/bash\\nset -vexubE -o pipefail\\n\\n')\n    lu.assertEquals(candidate({'ls', 'non-existent', 'file'}), '#!/bin/bash\\nset -vexubE -o pipefail\\nls\\nnon-existent\\nfile\\n')\n    lu.assertEquals(candidate({'set -v', 'echo hello world'}), '#!/bin/bash\\nset -vexubE -o pipefail\\nset -v\\necho hello world\\n')\n    lu.assertEquals(candidate({'set -u', 'echo hello world'}), '#!/bin/bash\\nset -vexubE -o pipefail\\nset -u\\necho hello world\\n')\n    lu.assertEquals(candidate({'do a thing', 'do another thing'}), '#!/bin/bash\\nset -vexubE -o pipefail\\ndo a thing\\ndo another thing\\n')\n    lu.assertEquals(candidate({\"echo hello 'world'\", 'echo hello $var'}), \"#!/bin/bash\\nset -vexubE -o pipefail\\necho hello 'world'\\necho hello $var\\n\")\n    lu.assertEquals(candidate({'set -e', 'echo hello world'}), '#!/bin/bash\\nset -vexubE -o pipefail\\nset -e\\necho hello world\\n')\n    lu.assertEquals(candidate({'set -x', 'echo hello world'}), '#!/bin/bash\\nset -vexubE -o pipefail\\nset -x\\necho hello world\\n')\n    lu.assertEquals(candidate({'do a thing', 'do another thing'}), '#!/bin/bash\\nset -vexubE -o pipefail\\ndo a thing\\ndo another thing\\n')\n    lu.assertEquals(candidate({'a', 'b c | d'}), '#!/bin/bash\\nset -vexubE -o pipefail\\na\\nb c | d\\n')\n    lu.assertEquals(candidate({'echo \"hello\"', 'grep f', 'cat out.txt'}), '#!/bin/bash\\nset -vexubE -o pipefail\\necho \"hello\"\\ngrep f\\ncat out.txt\\n')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_125653_reverse_number", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return int(str(num)[::-1])\n-- \n--     Reverse a number.\nlocal function reverse_number(num)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_125653_reverse_number.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = reverse_number\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(123), 321)\n    lu.assertEquals(candidate(981), 189)\n    lu.assertEquals(candidate(1234), 4321)\n    lu.assertEquals(candidate(1000), 1)\n    lu.assertEquals(candidate(321), 123)\n    lu.assertEquals(candidate(12), 21)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(23), 32)\n    lu.assertEquals(candidate(100), 1)\n    lu.assertEquals(candidate(10), 1)\n    lu.assertEquals(candidate(999), 999)\n    lu.assertEquals(candidate(500), 5)\n    lu.assertEquals(candidate(123456789), 987654321)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_125811_compare_bits", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # rows = len(olds)\n-- # cols = len(olds[0])\n-- # delta = [[0] * cols for i in range(rows)]\n-- # for i in range(0, rows):\n-- #     for j in range(0, cols):\n-- #         delta[i][j] = news[i][j] - olds[i][j]\n-- # return delta\n-- \n-- Subtract 2D list to determine changes to bit state.\nlocal function compare_bits(olds, news)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_125811_compare_bits.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = compare_bits\n    lu.assertEquals(candidate({{1, 1}, {1, 1}}, {{1, 1}, {1, 1}}), {{0, 0}, {0, 0}})\n    lu.assertEquals(candidate({{0, 0, 0}, {0, 0, 0}, {0, 0, 0}}, {{0, 0, 0}, {0, 0, 0}, {0, 0, 0}}), {{0, 0, 0}, {0, 0, 0}, {0, 0, 0}})\n    lu.assertEquals(candidate({{0, 0}, {0, 0}}, {{0, 0}, {0, 0}}), {{0, 0}, {0, 0}})\n    lu.assertEquals(candidate({{0, 0}, {0, 0}}, {{1, 1}, {1, 1}}), {{1, 1}, {1, 1}})\n    lu.assertEquals(candidate({{1, 1}, {1, 1}}, {{0, 0}, {0, 0}}), {{-1, -1}, {-1, -1}})\n    lu.assertEquals(candidate({{1, 1}, {0, 1}}, {{1, 1}, {1, 1}}), {{0, 0}, {1, 0}})\n    lu.assertEquals(candidate({{1, 0}, {1, 1}}, {{0, 1}, {0, 0}}), {{-1, 1}, {-1, -1}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_126107__compressed_name_to_c_string", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # rendered_string = ''\n-- # in_normal_string = False\n-- # for c in compressed_name:\n-- #     if ord(c) < 0x20:\n-- #         if in_normal_string:\n-- #             rendered_string += f'\" \"\\\\x{ord(c):02x}\" '\n-- #             in_normal_string = False\n-- #         else:\n-- #             rendered_string += f'\"\\\\x{ord(c):02x}\" '\n-- #     else:\n-- #         if in_normal_string:\n-- #             rendered_string += c\n-- #         else:\n-- #             rendered_string += f'\"{c}'\n-- #         in_normal_string = True\n-- # if in_normal_string:\n-- #     rendered_string += '\"'\n-- # return rendered_string.strip()\n-- \n-- Convert a compressed name (with fragment references) to a string that\n-- the C++ compiler will accept. The primary reason for this function is\n-- because the hex escape sequence (\\xHH) in C/C++ has no length limit, so\n-- will happily run into the characters after the HH. So we have to break\n-- those references into separate strings. Example: converts (\"\u0001ab\")\n-- into (\"\u0001\" \"ab\").\nlocal function _compressed_name_to_c_string(compressed_name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_126107__compressed_name_to_c_string.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _compressed_name_to_c_string\n    lu.assertEquals(candidate('a\"b\"c\"d'), '\"a\"b\"c\"d\"')\n    lu.assertEquals(candidate('abc\\\\nabc'), '\"abc\\\\nabc\"')\n    lu.assertEquals(candidate('a\\x01\\x02b'), '\"a\" \"\\\\x01\" \"\\\\x02\" \"b\"')\n    lu.assertEquals(candidate('a\\\\x00b'), '\"a\\\\x00b\"')\n    lu.assertEquals(candidate('a\\x01b'), '\"a\" \"\\\\x01\" \"b\"')\n    lu.assertEquals(candidate('a'), '\"a\"')\n    lu.assertEquals(candidate('\\x01ab'), '\"\\\\x01\" \"ab\"')\n    lu.assertEquals(candidate('a\\x01\\x02\\x03\\x04b'), '\"a\" \"\\\\x01\" \"\\\\x02\" \"\\\\x03\" \"\\\\x04\" \"b\"')\n    lu.assertEquals(candidate('a\\x01\\x02\\x03b'), '\"a\" \"\\\\x01\" \"\\\\x02\" \"\\\\x03\" \"b\"')\n    lu.assertEquals(candidate('a\\x01\\x02\\x03\\x04\\x05b'), '\"a\" \"\\\\x01\" \"\\\\x02\" \"\\\\x03\" \"\\\\x04\" \"\\\\x05\" \"b\"')\n    lu.assertEquals(candidate('a\\\\b'), '\"a\\\\b\"')\n    lu.assertEquals(candidate('a\"b'), '\"a\"b\"')\n    lu.assertEquals(candidate('ab'), '\"ab\"')\n    lu.assertEquals(candidate('a\\\\b\\\\c'), '\"a\\\\b\\\\c\"')\n    lu.assertEquals(candidate('a\"b\"c'), '\"a\"b\"c\"')\n    lu.assertEquals(candidate('abc'), '\"abc\"')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_12626_mysql_quote", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not x:\n-- #     return \"NULL\"\n-- # x = x.replace(\"\\\\\", \"\\\\\\\\\")\n-- # x = x.replace(\"'\", \"''\")\n-- # x = x.replace(\"\\n\", \"\\\\n\")\n-- # return \"'{}'\".format(x)\n-- \n-- Quote the string x using MySQL quoting rules. If x is the empty string,\n-- return \"NULL\". Probably not safe against maliciously formed strings, but\n-- our input is fixed and from a basically trustable source.\nlocal function mysql_quote(x)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_12626_mysql_quote.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = mysql_quote\n    lu.assertEquals(candidate('foo\\\\bar'), \"'foo\\\\\\\\bar'\")\n    lu.assertEquals(candidate('\\n'), \"'\\\\n'\")\n    lu.assertEquals(candidate(None), 'NULL')\n    lu.assertEquals(candidate('hello'), \"'hello'\")\n    lu.assertEquals(candidate(''), 'NULL')\n    lu.assertEquals(candidate('foo'), \"'foo'\")\n    lu.assertEquals(candidate('c\\\\'), \"'c\\\\\\\\'\")\n    lu.assertEquals(candidate('a'), \"'a'\")\n    lu.assertEquals(candidate(\"'\"), \"''''\")\n    lu.assertEquals(candidate('\\n\\n'), \"'\\\\n\\\\n'\")\n    lu.assertEquals(candidate('\\\\'), \"'\\\\\\\\'\")\n    lu.assertEquals(candidate(None), 'NULL')\n    lu.assertEquals(candidate('\\n\\n\\n'), \"'\\\\n\\\\n\\\\n'\")\n    lu.assertEquals(candidate('hello\\nworld'), \"'hello\\\\nworld'\")\n    lu.assertEquals(candidate('foo\\nbar'), \"'foo\\\\nbar'\")\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_126701__is_file_valid", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return not name.startswith('.')\n-- \n-- Decide if a file is valid.\nlocal function _is_file_valid(name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_126701__is_file_valid.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _is_file_valid\n    lu.assertEquals(candidate('.foo'), false)\n    lu.assertEquals(candidate('foo.txt'), true)\n    lu.assertEquals(candidate('.foo.bak'), false)\n    lu.assertEquals(candidate('.foo.txt'), false)\n    lu.assertEquals(candidate('.foo.txt.bak'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_12710_splitRPMFilename", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if filename[-4:] == '.rpm':\n-- #     filename = filename[:-4]\n-- # archIndex = filename.rfind('.')\n-- # arch = filename[archIndex + 1:]\n-- # relIndex = filename[:archIndex].rfind('-')\n-- # rel = filename[relIndex + 1:archIndex]\n-- # verIndex = filename[:relIndex].rfind('-')\n-- # ver = filename[verIndex + 1:relIndex]\n-- # epochIndex = filename.find(':')\n-- # if epochIndex == -1:\n-- #     epoch = ''\n-- # else:\n-- #     epoch = filename[:epochIndex]\n-- # name = filename[epochIndex + 1:verIndex]\n-- # return name, ver, rel, epoch, arch\n-- \n-- Pass in a standard style rpm fullname\n-- Return a name, version, release, epoch, arch, e.g.::\n--     foo-1.0-1.i386.rpm returns foo, 1.0, 1, i386\n--     1:bar-9-123a.ia64.rpm returns bar, 9, 123a, 1, ia64\nlocal function splitRPMFilename(filename)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_12710_splitRPMFilename.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = splitRPMFilename\n    lu.assertEquals(candidate('1:baz-999-1.i386.rpm'), {'baz', '999', '1', '1', 'i386'})\n    lu.assertEquals(candidate('foo-1.0-1.i386.rpm'), {'foo', '1.0', '1', '', 'i386'})\n    lu.assertEquals(candidate('1:foo-9-123a.ia64.rpm'), {'foo', '9', '123a', '1', 'ia64'})\n    lu.assertEquals(candidate('foo-1.0-1.i386.rpm'), {'foo', '1.0', '1', '', 'i386'})\n    lu.assertEquals(candidate('1:bar-9-123a.ia64.rpm'), {'bar', '9', '123a', '1', 'ia64'})\n    lu.assertEquals(candidate('bar-9-123a.ia64.rpm'), {'bar', '9', '123a', '', 'ia64'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_127284_matches", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # case insensitive, change the variables cases\n-- # if \"-i\" in flags:\n-- #     pattern = pattern.lower()\n-- #     line = line.lower()\n-- # # invert matching, will check if the patter is not in line, return True/False whether it is.\n-- # if \"-v\" in flags:\n-- #     return pattern not in line\n-- # # match an entire line\n-- # if \"-x\" in flags:\n-- #     if len(line.rstrip()) != len(pattern):\n-- #         return False\n-- # return pattern in line\n-- \n-- Checks if the pattern is in the line currently being searched. Uses the flags to modify the patter and/or the\n-- line being searched through.\n-- `-i` Match line using a case-insensitive comparison.\n-- `-v` Invert the program -- collect all lines that fail to match the pattern.\n-- `-x` Only match entire lines, instead of lines that contain a match.\n-- :param line: the line to search through in the file or the string provided\n-- :param pattern: the pattern to use for the search\n-- :param flags: the flags to narrow down the search\n-- :return: Boolean value, returns True if the pattern is in the line\n-- :rtype: bool\nlocal function matches(line, pattern, flags)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_127284_matches.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = matches\n    lu.assertEquals(candidate('hello', 'hello', {'-x'}), true)\n    lu.assertEquals(candidate('test line', 'test line', {'-v'}), false)\n    lu.assertEquals(candidate('This line contains a python pattern.', 'python', {'-i', '-x'}), false)\n    lu.assertEquals(candidate('hello', 'Hello', {}), false)\n    lu.assertEquals(candidate('hello', 'hello', {'-x', '-v'}), false)\n    lu.assertEquals(candidate('This line contains a python pattern.', 'Python', {'-i', '-x', '-v'}), false)\n    lu.assertEquals(candidate('This line contains a python pattern.', 'python', {}), true)\n    lu.assertEquals(candidate('test line', 'test line', {'-x'}), true)\n    lu.assertEquals(candidate('hello', 'ell', {}), true)\n    lu.assertEquals(candidate('This line contains a python pattern.', 'python', {'-v'}), false)\n    lu.assertEquals(candidate('This line contains a python pattern.', 'PYTHON', {'-i'}), true)\n    lu.assertEquals(candidate('test line', 'TEST LINE', {'-x'}), false)\n    lu.assertEquals(candidate('hello', 'Hello', {'-i'}), true)\n    lu.assertEquals(candidate('This line contains a python pattern.', 'python', {'-x'}), false)\n    lu.assertEquals(candidate('hello', 'ell', {'-v'}), false)\n    lu.assertEquals(candidate('test line', 'TEST LINE', {'-i'}), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_127444_parse_host_port", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not address:\n-- #     return (None, None)\n-- # if address[0] == '[':\n-- #     # Escaped ipv6\n-- #     _host, _port = address[1:].split(']')\n-- #     host = _host\n-- #     if ':' in _port:\n-- #         port = _port.split(':')[1]\n-- #     else:\n-- #         port = default_port\n-- # else:\n-- #     if address.count(':') == 1:\n-- #         host, port = address.split(':')\n-- #     else:\n-- #         # 0 means ipv4, >1 means ipv6.\n-- #         # We prohibit unescaped ipv6 addresses with port.\n-- #         host = address\n-- #         port = default_port\n-- # return (host, None if port is None else int(port))\n-- \n-- Interpret a string as a host:port pair.\n-- An IPv6 address MUST be escaped if accompanied by a port,\n-- because otherwise ambiguity ensues: 2001:db8:85a3::8a2e:370:7334\n-- means both [2001:db8:85a3::8a2e:370:7334] and\n-- [2001:db8:85a3::8a2e:370]:7334.\n-- >>> parse_host_port('server01:80')\n-- ('server01', 80)\n-- >>> parse_host_port('server01')\n-- ('server01', None)\n-- >>> parse_host_port('server01', default_port=1234)\n-- ('server01', 1234)\n-- >>> parse_host_port('[::1]:80')\n-- ('::1', 80)\n-- >>> parse_host_port('[::1]')\n-- ('::1', None)\n-- >>> parse_host_port('[::1]', default_port=1234)\n-- ('::1', 1234)\n-- >>> parse_host_port('2001:db8:85a3::8a2e:370:7334', default_port=1234)\n-- ('2001:db8:85a3::8a2e:370:7334', 1234)\n-- >>> parse_host_port(None)\n-- (None, None)\nlocal function parse_host_port(address, default_port)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_127444_parse_host_port.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_host_port\n    lu.assertEquals(candidate('a:1234', None), {'a', 1234})\n    lu.assertEquals(candidate('server01:80'), {'server01', 80})\n    lu.assertEquals(candidate('a:1234', 123), {'a', 1234})\n    lu.assertEquals(candidate('foo'), {'foo', None})\n    lu.assertEquals(candidate('[::1]:80'), {'::1', 80})\n    lu.assertEquals(candidate('[::1]:1234', 123), {'::1', 1234})\n    lu.assertEquals(candidate(''), {None, None})\n    lu.assertEquals(candidate('server01'), {'server01', None})\n    lu.assertEquals(candidate('a'), {'a', None})\n    lu.assertEquals(candidate('foo:80'), {'foo', 80})\n    lu.assertEquals(candidate('[::1]:1234', None), {'::1', 1234})\n    lu.assertEquals(candidate(None), {None, None})\n    lu.assertEquals(candidate('[::1]'), {'::1', None})\n    lu.assertEquals(candidate('localhost:80'), {'localhost', 80})\n    lu.assertEquals(candidate('a:123'), {'a', 123})\n    lu.assertEquals(candidate('localhost'), {'localhost', None})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_127862_convert_to_cents", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # price = price.replace(\"$\", \"\").replace(\",\", \".\")\n-- # converted_price: float = float(price) * 100\n-- # return int(converted_price)\n-- \n-- Convert the price to cents, stripping the currency sign\n-- Parameters\n-- ----------\n-- price : str\n--     Price provided in string format\n-- Returns\n-- -------\n-- int\n--     Price converted to cents\nlocal function convert_to_cents(price)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_127862_convert_to_cents.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = convert_to_cents\n    lu.assertEquals(candidate('$0'), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_12854_get_metric", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # start = line.find(name) + len(name) + 1\n-- # end = line.find(split, start)\n-- # return line[start:end]\n-- \n-- Get metric value from output line\n-- :param line: console output line\n-- :param name: name of metric\n-- :param split: split character\n-- :return: metric value\nlocal function get_metric(line, name, split)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_12854_get_metric.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_metric\n    lu.assertEquals(candidate('MetricA', 'MetricA', ' '), '')\n    lu.assertEquals(candidate('MetricA : 0.1', 'MetricB', ' : '), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_128799_match_subroutine_call", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if len(names) < 1:\n-- #     return \"\"\n-- # try:\n-- #     i = names.index(\"CALL\")\n-- # except ValueError:\n-- #     return \"\"\n-- # return names[i+1]\n-- \n-- \nlocal function match_subroutine_call(names)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_128799_match_subroutine_call.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = match_subroutine_call\n    lu.assertEquals(candidate(list(' ')), '')\n    lu.assertEquals(candidate(list('')), '')\n    lu.assertEquals(candidate(list('SUB1 SUB2')), '')\n    lu.assertEquals(candidate({'FOO', 'CALL', 'BAR', 'BAZ'}), 'BAR')\n    lu.assertEquals(candidate(list()), '')\n    lu.assertEquals(candidate(list('SUB1 CALL SUB2')), '')\n    lu.assertEquals(candidate(list('CALL ')), '')\n    lu.assertEquals(candidate(list('SUB1 SUB2 SUB3 SUB4')), '')\n    lu.assertEquals(candidate({'FOO', 'BAR', 'BAZ'}), '')\n    lu.assertEquals(candidate(list('SUB1')), '')\n    lu.assertEquals(candidate(list('SUB1 SUB2 SUB3')), '')\n    lu.assertEquals(candidate(list('SUB1 SUB2 CALL')), '')\n    lu.assertEquals(candidate({'CALL', 'FOO'}), 'FOO')\n    lu.assertEquals(candidate(list('CALL')), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_129353_has_QRp", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if p % 2 == 0:\n-- #     return False\n-- # return pow(a, (p-1) >> 1, p) == 1\n-- \n-- Euler's criterion\n-- x**2 = a % p -> does x exist? \n-- not all a's in p have an x\nlocal function has_QRp(a, p)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_129353_has_QRp.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = has_QRp\n    lu.assertEquals(candidate(22, 31), false)\n    lu.assertEquals(candidate(7, 7), false)\n    lu.assertEquals(candidate(2, 91), false)\n    lu.assertEquals(candidate(1, 31), true)\n    lu.assertEquals(candidate(42, 43), false)\n    lu.assertEquals(candidate(19, 31), true)\n    lu.assertEquals(candidate(11, 20), false)\n    lu.assertEquals(candidate(4, 11), true)\n    lu.assertEquals(candidate(3, 11), true)\n    lu.assertEquals(candidate(8, 11), false)\n    lu.assertEquals(candidate(7, 31), true)\n    lu.assertEquals(candidate(15, 13), false)\n    lu.assertEquals(candidate(2, 5), false)\n    lu.assertEquals(candidate(7, 5), false)\n    lu.assertEquals(candidate(5, 31), true)\n    lu.assertEquals(candidate(12, 31), false)\n    lu.assertEquals(candidate(5, 11), true)\n    lu.assertEquals(candidate(6, 31), false)\n    lu.assertEquals(candidate(2, 7), true)\n    lu.assertEquals(candidate(5, 5), false)\n    lu.assertEquals(candidate(6, 5), true)\n    lu.assertEquals(candidate(24, 31), false)\n    lu.assertEquals(candidate(9, 31), true)\n    lu.assertEquals(candidate(0, 31), false)\n    lu.assertEquals(candidate(3, 5), false)\n    lu.assertEquals(candidate(8, 5), false)\n    lu.assertEquals(candidate(11, 13), false)\n    lu.assertEquals(candidate(25, 31), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_129399_hex_to_rgb", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if value:\n-- #     value = value.lstrip(\"#\")\n-- #     lenv = len(value)\n-- #     return tuple(int(value[i: i + lenv // 3], 16) for i in range(0, lenv, lenv // 3))\n-- # return None\n-- \n-- Convert a hex value to RGB\n-- :param value: the hex color\n-- :type value: string\n-- :returns: rgb tuple\n-- :rtype: tuple\nlocal function hex_to_rgb(value)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_129399_hex_to_rgb.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = hex_to_rgb\n    lu.assertEquals(candidate('#FFFFFF'), {255, 255, 255})\n    lu.assertEquals(candidate('000'), {0, 0, 0})\n    lu.assertEquals(candidate('#ff0000'), {255, 0, 0})\n    lu.assertEquals(candidate('999999'), {153, 153, 153})\n    lu.assertEquals(candidate('#000000'), {0, 0, 0})\n    lu.assertEquals(candidate('112233'), {17, 34, 51})\n    lu.assertEquals(candidate(None), None)\n    lu.assertEquals(candidate('#808080'), {128, 128, 128})\n    lu.assertEquals(candidate(''), None)\n    lu.assertEquals(candidate('FFFFFF'), {255, 255, 255})\n    lu.assertEquals(candidate('abcdef'), {171, 205, 239})\n    lu.assertEquals(candidate('ff0000'), {255, 0, 0})\n    lu.assertEquals(candidate('#000'), {0, 0, 0})\n    lu.assertEquals(candidate('#123456'), {18, 52, 86})\n    lu.assertEquals(candidate('F0F0F0'), {240, 240, 240})\n    lu.assertEquals(candidate('#999999'), {153, 153, 153})\n    lu.assertEquals(candidate('000000'), {0, 0, 0})\n    lu.assertEquals(candidate('#F0F0F0'), {240, 240, 240})\n    lu.assertEquals(candidate('FF0000'), {255, 0, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_129402_join_as_compacted_paragraphs", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # paragraphs[:] = [' '.join(p.split()) for p in paragraphs]  # Remove extra whitespace & newlines\n-- # return '\\n'.join(paragraphs)\n-- \n-- :param paragraphs: List containing individual paragraphs; potentially with extraneous whitespace within\n-- :return: String with \n--  separated paragraphs and no extra whitespace\nlocal function join_as_compacted_paragraphs(paragraphs)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_129402_join_as_compacted_paragraphs.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = join_as_compacted_paragraphs\n    lu.assertEquals(candidate({'A paragraph with a \\n newline character, and a couple of\\t tabs'}), 'A paragraph with a newline character, and a couple of tabs')\n    lu.assertEquals(candidate({'I ate a\\n\\n\\n\\n\\n\\n\\n\\nball', 'I drank a\\n\\n\\n\\n\\n\\n\\n\\ncoffee'}), 'I ate a ball\\nI drank a coffee')\n    lu.assertEquals(candidate({'A paragraph with a\\nnewline character, and a couple of\\ttabs'}), 'A paragraph with a newline character, and a couple of tabs')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_129406_R10_yield", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # #\n-- # Pmax = 1.40 * (FMTab / Apmin)   # (R10/3)\n-- # # Alternative safety verification\n-- # Sp = PG / Pmax     # (R10/4)\n-- # if Sp > 1.0:\n-- #     print('Sp > {} --> PASS'.format(Sp))\n-- # else:\n-- #     print('Sp < {} --> FAIL'.format(Sp))\n-- # #\n-- # return Pmax\n-- # #\n-- \n--     R10 Determining the surface pressure Pmax\n-- (Sec 5.5.4)\n--     For the maximum surface pressure with yield\n--     or angle controlled tightening techniques.\nlocal function R10_yield(FMTab, Apmin, PG)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_129406_R10_yield.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = R10_yield\n    lu.assertEquals(candidate(0.1, 0.1, 1.0), 1.4)\n    lu.assertEquals(candidate(1.0, 1.0, 0.1), 1.4)\n    lu.assertEquals(candidate(2.0, 1.0, 2.0), 2.8)\n    lu.assertEquals(candidate(0.1, 0.1, 0.1), 1.4)\n    lu.assertEquals(candidate(1.0, 1.0, 2.0), 1.4)\n    lu.assertEquals(candidate(1.0, 1.0, 1.0), 1.4)\n    lu.assertEquals(candidate(1.0, 1.0, 1.0), 1.4)\n    lu.assertEquals(candidate(2.0, 1.0, 1.0), 2.8)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_129442_solution", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # reversed_list = []\n-- # for i in num_list:\n-- #     num = str(i)[::-1].strip('0')\n-- #     if num.endswith('-'):\n-- #         num = int(num.strip('-')) * -1\n-- #     reversed_list.append(int(num))\n-- # return (reversed_list, max(reversed_list))\n-- \n-- Complete the function such that:\n-- Given a list of digits LD, reverse all the digits in LD to a new list LR.\n-- Return the LR and largest element in LR in a tuple such that:\n--     - [123] -> ([321], 321)\n--     - [-789, 10] -> ([-987, 1], 1)\n--     - [11020, 3512] -> ([2011, 2153], 2153)\n-- Constraints:\n-- Eliminate leading zeros.\n-- Note the position of the negative operator after the reversal.\nlocal function solution(num_list)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_129442_solution.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = solution\n    lu.assertEquals(candidate({11020, 3512}), {{2011, 2153}, 2153})\n    lu.assertEquals(candidate({-789, 10}), {{-987, 1}, 1})\n    lu.assertEquals(candidate({123}), {{321}, 321})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_129631_levenshtein_distance", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # n1 = len(w1) + 1\n-- # n2 = len(w2) + 1\n-- # dist = [[0]*n2 for _ in range(n1)]\n-- # for i in range(n1):\n-- #     dist[i][0] = i\n-- # for j in range(n2):\n-- #     dist[0][j] = j\n-- # for x in range(1, n1):\n-- #     for y in range(1, n2):\n-- #         if w1[x-1] == w2[y-1]:\n-- #             dist[x][y] = min(dist[x-1][y-1],\n-- #                              dist[x-1][y] + 1,\n-- #                              dist[x][y-1] + 1)\n-- #         else:\n-- #             dist[x][y] = min(dist[x-1][y-1] + 1,\n-- #                              dist[x-1][y] + 1,\n-- #                              dist[x][y-1] + 1)\n-- # return dist[n1-1][n2-1]\n-- \n-- Parameters:\n-- ----------\n--   w1: str\n--   w2: str\n-- Returns:\n-- --------\n--   int:\n--     Returns Levenshtein edit distance between the two strings \nlocal function levenshtein_distance(w1, w2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_129631_levenshtein_distance.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = levenshtein_distance\n    lu.assertEquals(candidate('aa', 'aa'), 0)\n    lu.assertEquals(candidate('hello', 'halo'), 2)\n    lu.assertEquals(candidate('kitten', 'kitt'), 2)\n    lu.assertEquals(candidate('a', ''), 1)\n    lu.assertEquals(candidate('', ''), 0)\n    lu.assertEquals(candidate('5678', '1234'), 4)\n    lu.assertEquals(candidate('b', 'a'), 1)\n    lu.assertEquals(candidate('1234', '5678'), 4)\n    lu.assertEquals(candidate('', 'a'), 1)\n    lu.assertEquals(candidate('kitten', 'sitting'), 3)\n    lu.assertEquals(candidate('kitten', 'sittin'), 2)\n    lu.assertEquals(candidate('cut', 'cat'), 1)\n    lu.assertEquals(candidate('a', 'b'), 1)\n    lu.assertEquals(candidate('kitten', 'kit'), 3)\n    lu.assertEquals(candidate('dog', 'cat'), 3)\n    lu.assertEquals(candidate('kitten', 'kittten'), 1)\n    lu.assertEquals(candidate('kitten', 'sitting'), 3)\n    lu.assertEquals(candidate('b', 'a'), 1)\n    lu.assertEquals(candidate('hello', 'hallo'), 1)\n    lu.assertEquals(candidate('k', ''), 1)\n    lu.assertEquals(candidate('sitting', 'kitten'), 3)\n    lu.assertEquals(candidate('kitten', 'kitte'), 1)\n    lu.assertEquals(candidate('cat', 'dog'), 3)\n    lu.assertEquals(candidate('a', 'b'), 1)\n    lu.assertEquals(candidate('cat', 'cut'), 1)\n    lu.assertEquals(candidate('', ''), 0)\n    lu.assertEquals(candidate('s', 's'), 0)\n    lu.assertEquals(candidate('kitten', 'k'), 5)\n    lu.assertEquals(candidate('', 'k'), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_130049_get_beta_skewness", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return 2*(b-a)*((a+b+1)**0.5) / (a+b+2) / (a*b)**0.5\n-- \n-- Compute skewness of beta distribution based on shape parameters `a` and `b`.\nlocal function get_beta_skewness(a, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_130049_get_beta_skewness.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_beta_skewness\n    lu.assertEquals(candidate(0.5, 0.5), 0.0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_130400_sub", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # res = [x[0] - x[1] for x in zip(p1, p2)]\n-- # n = len(res)\n-- # T = max(p1, p2, key=len)[n:]\n-- # res.extend((-t for t in T) if len(p2) > len(p1) else T)\n-- # return res\n-- \n--  Subtracts the two Polynomials, 'p1' and 'p2'\n-- The arguments can be a sequence of coefficients or an instance of the Polynomial class. \nlocal function sub(p1, p2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_130400_sub.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sub\n    lu.assertEquals(candidate({3, 4}, {1, 2}), {2, 2})\n    lu.assertEquals(candidate({0, 0, 0, 0, 0, 0, 0}, {1, 1, 1, 1, 1, 1, 1}), {-1, -1, -1, -1, -1, -1, -1})\n    lu.assertEquals(candidate({1, 2}, {3, 4}), {-2, -2})\n    lu.assertEquals(candidate({2}, {1}), {1})\n    lu.assertEquals(candidate({1}, {}), {1})\n    lu.assertEquals(candidate({}, {3, 2, 1}), {-3, -2, -1})\n    lu.assertEquals(candidate({1, 2, 3, 4}, {5, 6, 7, 8}), {-4, -4, -4, -4})\n    lu.assertEquals(candidate({1, 0}, {0, 1}), {1, -1})\n    lu.assertEquals(candidate({1, 2, 3}, {4, 5, 6}), {-3, -3, -3})\n    lu.assertEquals(candidate({0}, {0}), {0})\n    lu.assertEquals(candidate({5, 6, 7, 8}, {1, 2, 3, 4}), {4, 4, 4, 4})\n    lu.assertEquals(candidate({1}, {2}), {-1})\n    lu.assertEquals(candidate({4, 5, 6}, {1, 2, 3}), {3, 3, 3})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_130569_get_command_from_state", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # command = None\n-- # if state == 'present':\n-- #     command = 'vrouter-create'\n-- # if state == 'absent':\n-- #     command = 'vrouter-delete'\n-- # if state == 'update':\n-- #     command = 'vrouter-modify'\n-- # return command\n-- \n-- This method gets appropriate command name for the state specified. It\n-- returns the command name for the specified state.\n-- :param state: The state for which the respective command name is required.\nlocal function get_command_from_state(state)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_130569_get_command_from_state.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_command_from_state\n    lu.assertEquals(candidate('present'), 'vrouter-create')\n    lu.assertEquals(candidate('update'), 'vrouter-modify')\n    lu.assertEquals(candidate('absent'), 'vrouter-delete')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_13090_get_urn_from_raw_update", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return raw_string.split(\"(\")[1].split(\",\")[0]\n-- \n-- Return the URN of a raw group update\n-- Example: urn:li:fs_miniProfile:<id>\n-- Example: urn:li:fs_updateV2:(<urn>,GROUP_FEED,EMPTY,DEFAULT,false)\nlocal function get_urn_from_raw_update(raw_string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_13090_get_urn_from_raw_update.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_urn_from_raw_update\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:32423423432,MEMBER_COUNT_UPDATED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:32423423432')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:6161898594178132904,GROUP_FEED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:6161898594178132904')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:3243252353,GROUP_FEED,EMPTY,DEFAULT,false),EMPTY,EMPTY,EMPTY,false)'), 'urn:li:fs_miniProfile:3243252353')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:1,GROUP_FEED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:1')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:1,GROUP_FEED,EMPTY,DEFAULT,false),(urn:li:fs_miniProfile:2,GROUP_FEED,EMPTY,DEFAULT,false),(urn:li:fs_miniProfile:3,GROUP_FEED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:1')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:123456789,GROUP_JOIN_REQUEST_ACCEPTED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:123456789')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:32423423432,UPCOMING_EVENT_CREATED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:32423423432')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:921474837697135657,GROUP_FEED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:921474837697135657')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(<urn>,PROMOTED,EMPTY,DEFAULT,false)'), '<urn>')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:1234567890123456789,GROUP_FEED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:1234567890123456789')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:123456789,GROUP_FEED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:123456789')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:1159,GROUP_FEED,EMPTY,DEFAULT,false)garbage'), 'urn:li:fs_miniProfile:1159')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:32423423432,EVENT_ORGANIZER_CREATED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:32423423432')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:55555,GROUP_FEED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:55555')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:123456,MEMBERSHIP_UPDATED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:123456')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:123456,FEED_UPDATED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:123456')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:1,GROUP_FEED,EMPTY,DEFAULT,false),(urn:li:fs_updateV2:(urn:li:fs_miniProfile:2,GROUP_FEED,EMPTY,DEFAULT,false),GROUP_FEED,EMPTY,DEFAULT,false),(urn:li:fs_miniProfile:3,GROUP_FEED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:1')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:3243252353,GROUP_FEED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:3243252353')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(<urn>,GROUP_FEED,EMPTY,DEFAULT,false)'), '<urn>')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:1159,GROUP_FEED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:1159')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:32423423432,GROUP_FEED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:32423423432')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:5dc5e35e3e5e0340a5a1872e,GROUP_FEED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:5dc5e35e3e5e0340a5a1872e')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:123456789,OWNERSHIP_TRANSFER,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:123456789')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(<urn>,REMOVED,EMPTY,DEFAULT,false)'), '<urn>')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:55555,GROUP_FEED,EMPTY,DEFAULT,true)'), 'urn:li:fs_miniProfile:55555')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:123456,GROUP_FEED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:123456')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(<urn>,UPDATED,EMPTY,DEFAULT,false)'), '<urn>')\n    lu.assertEquals(candidate('urn:li:fs_miniProfile:(urn:li:fs_miniProfile:1234567890123456789,urn:li:organization:1234567890123456789)'), 'urn:li:fs_miniProfile:1234567890123456789')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(<urn>,GROUP_FEED,EMPTY,DEFAULT,false)'), '<urn>')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(<urn>,GROUP_FEED,EMPTY,DEFAULT,true)'), '<urn>')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:123456789,MEMBERSHIP_CHANGE,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:123456789')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(<urn>,UNSUBSCRIBED,EMPTY,DEFAULT,false)'), '<urn>')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:d52191c1-9b2c-45a9-a721-2a3317a8047c,GROUP_FEED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:d52191c1-9b2c-45a9-a721-2a3317a8047c')\n    lu.assertEquals(candidate('urn:li:fs_updateV2:(urn:li:fs_miniProfile:123456,POSTS_UPDATED,EMPTY,DEFAULT,false)'), 'urn:li:fs_miniProfile:123456')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_131292_GetEnvCall", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if is_constructor:\n-- #     return 'NewObject'\n-- # env_call_map = {'boolean': 'Boolean',\n-- #                 'byte': 'Byte',\n-- #                 'char': 'Char',\n-- #                 'short': 'Short',\n-- #                 'int': 'Int',\n-- #                 'long': 'Long',\n-- #                 'float': 'Float',\n-- #                 'void': 'Void',\n-- #                 'double': 'Double',\n-- #                 'Object': 'Object',\n-- #                 }\n-- # call = env_call_map.get(return_type, 'Object')\n-- # if is_static:\n-- #     call = 'Static' + call\n-- # return 'Call' + call + 'Method'\n-- \n-- Maps the types availabe via env->Call__Method.\nlocal function GetEnvCall(is_constructor, is_static, return_type)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_131292_GetEnvCall.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = GetEnvCall\n    lu.assertEquals(candidate(false, false, 'void'), 'CallVoidMethod')\n    lu.assertEquals(candidate(false, false, 'Object'), 'CallObjectMethod')\n    lu.assertEquals(candidate(true, false, 'boolean'), 'NewObject')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_132314_get_full_policy_path", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # split_arn = arn.split(\":\")\n-- # resource_string = \":\".join(split_arn[5:])\n-- # resource_string = resource_string.split(\"/\")[1:]\n-- # resource_string = \"/\".join(resource_string)\n-- # return resource_string\n-- \n-- Resource string will output strings like the following examples.\n-- Case 1:\n--   Input: arn:aws:iam::aws:policy/aws-service-role/AmazonGuardDutyServiceRolePolicy\n-- Output:\n--   aws-service-role/AmazonGuardDutyServiceRolePolicy\n-- Case 2:\n--   Input: arn:aws:iam::123456789012:role/ExampleRole\n--   Output: ExampleRole\n-- :param arn:\n-- :return:\nlocal function get_full_policy_path(arn)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_132314_get_full_policy_path.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_full_policy_path\n    lu.assertEquals(candidate('arn:aws:iam::aws:policy/aws-service-role/AmazonGuardDutyServiceRolePolicy'), 'aws-service-role/AmazonGuardDutyServiceRolePolicy')\n    lu.assertEquals(candidate('arn:aws:iam::aws:policy/aws-service-role/AmazonGuardDutyServiceRolePolicy'), 'aws-service-role/AmazonGuardDutyServiceRolePolicy')\n    lu.assertEquals(candidate('arn:aws:iam::123456789012:role/ExampleRole'), 'ExampleRole')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_132458_digitToInt", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if s not in \"0123456789abcdefABCDEF\":\n-- #     raise ValueError(\"not a digit %s\" % s)\n-- # return \"0123456789abcdef\".index(s.lower())\n-- \n-- digitToInt :: str -> int\n-- Convert a single digit Char to the corresponding Int. This function fails\n-- unless its argument satisfies isHexDigit, but recognises both upper and\n-- lower-case hexadecimal digits (i.e. '0'..'9', 'a'..'f', 'A'..'F').\nlocal function digitToInt(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_132458_digitToInt.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = digitToInt\n    lu.assertEquals(candidate('A'), 10)\n    lu.assertEquals(candidate('7'), 7)\n    lu.assertEquals(candidate('D'), 13)\n    lu.assertEquals(candidate('6'), 6)\n    lu.assertEquals(candidate('2'), 2)\n    lu.assertEquals(candidate('1'), 1)\n    lu.assertEquals(candidate('B'), 11)\n    lu.assertEquals(candidate('f'), 15)\n    lu.assertEquals(candidate('d'), 13)\n    lu.assertEquals(candidate('9'), 9)\n    lu.assertEquals(candidate('4'), 4)\n    lu.assertEquals(candidate('F'), 15)\n    lu.assertEquals(candidate('C'), 12)\n    lu.assertEquals(candidate('c'), 12)\n    lu.assertEquals(candidate('5'), 5)\n    lu.assertEquals(candidate('f'), 15)\n    lu.assertEquals(candidate('f'), 15)\n    lu.assertEquals(candidate('1'), 1)\n    lu.assertEquals(candidate('a'), 10)\n    lu.assertEquals(candidate('3'), 3)\n    lu.assertEquals(candidate('A'), 10)\n    lu.assertEquals(candidate('a'), 10)\n    lu.assertEquals(candidate('e'), 14)\n    lu.assertEquals(candidate('0'), 0)\n    lu.assertEquals(candidate('f'), int('f', 16))\n    lu.assertEquals(candidate('1'), 1)\n    lu.assertEquals(candidate('b'), 11)\n    lu.assertEquals(candidate('5'), 5)\n    lu.assertEquals(candidate('1'), int('1', 10))\n    lu.assertEquals(candidate('8'), 8)\n    lu.assertEquals(candidate('E'), 14)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_132637_drop_command", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return message[len(command) + 1:]\n-- \n--     Given a message text, drops the command prefix from the string.\nlocal function drop_command(message, command)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_132637_drop_command.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = drop_command\n    lu.assertEquals(candidate('!command parameter1 parameter2 parameter3', '!COMMAND'), 'parameter1 parameter2 parameter3')\n    lu.assertEquals(candidate('!command parameter1 parameter2 parameter3', '!command'), 'parameter1 parameter2 parameter3')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_132755_appartient_au_triangle", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # C = (Y[1]-Y[0])*X[2]+(X[0]-X[1])*Y[2]-((Y[1]-Y[0])*X[0]+(X[0]-X[1])*Y[0])\n-- # c = (Y[1]-Y[0])*x+(X[0]-X[1])*y-((Y[1]-Y[0])*X[0]+(X[0]-X[1])*Y[0])\n-- # A = (Y[2]-Y[1])*X[0]+(X[1]-X[2])*Y[0]-((Y[2]-Y[1])*X[1]+(X[1]-X[2])*Y[1])\n-- # a = (Y[2]-Y[1])*x+(X[1]-X[2])*y-((Y[2]-Y[1])*X[1]+(X[1]-X[2])*Y[1])\n-- # B = (Y[0]-Y[2])*X[1]+(X[2]-X[0])*Y[1]-((Y[0]-Y[2])*X[2]+(X[2]-X[0])*Y[2])\n-- # b = (Y[0]-Y[2])*x+(X[2]-X[0])*y-((Y[0]-Y[2])*X[2]+(X[2]-X[0])*Y[2])\n-- # if C*c >= 0 and B*b >= 0 and A*a >= 0:\n-- #     return True\n-- # else:\n-- #     return False\n-- \n-- verifie si le point P(x;y) appartient au triangle ABC\nlocal function appartient_au_triangle(X, Y, x, y)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_132755_appartient_au_triangle.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = appartient_au_triangle\n    lu.assertEquals(candidate({10, 10, 10}, {10, 10, 10}, 10, 10), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_133001_dequebecify", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # import unicodedata\n-- # return ''.join(c for c in unicodedata.normalize('NFD', input)\n-- #                if unicodedata.category(c) != 'Mn')\n-- \n--  Normalizes text to pure english\n-- From <http://stackoverflow.com/questions/517923>\n-- This function only transliterates French diacritics.\n-- If you need to separate Quebec from Canada, try:\n-- roc,qc = canada.sovereignty_referendum('quebec')\nlocal function dequebecify(input)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_133001_dequebecify.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = dequebecify\n    lu.assertEquals(candidate(\"C'est pas faux\"), \"C'est pas faux\")\n    lu.assertEquals(candidate('Je suis une phrase.'), 'Je suis une phrase.')\n    lu.assertEquals(candidate('\u00c9vang\u00e9liste fran\u00e7ais'), 'Evangeliste francais')\n    lu.assertEquals(candidate('test'), 'test')\n    lu.assertEquals(candidate('\u00c9quipe fran\u00e7aise'), 'Equipe francaise')\n    lu.assertEquals(candidate('Mais comment?'), 'Mais comment?')\n    lu.assertEquals(candidate('Deja vu'), 'Deja vu')\n    lu.assertEquals(candidate(\"L'\u00e9cole\"), \"L'ecole\")\n    lu.assertEquals(candidate('\u00c7a va?'), 'Ca va?')\n    lu.assertEquals(candidate('Et ca?'), 'Et ca?')\n    lu.assertEquals(candidate('Le \u00e9v\u00eaque'), 'Le eveque')\n    lu.assertEquals(candidate('R\u00e9sidence de la Madeleine'), 'Residence de la Madeleine')\n    lu.assertEquals(candidate('M\u00e8re de la Madeleine'), 'Mere de la Madeleine')\n    lu.assertEquals(candidate('Bonjour'), 'Bonjour')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_133468_filesafe", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"\".join(c for c in str_ if c.isalnum() or c in (' ', '.', '_', '-')).rstrip()\n-- \n-- Convert a string to something safe for filenames.\nlocal function filesafe(str_)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_133468_filesafe.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = filesafe\n    lu.assertEquals(candidate('This is a normal filename'), 'This is a normal filename')\n    lu.assertEquals(candidate('A very normal filename'), 'A very normal filename')\n    lu.assertEquals(candidate('True.0'), 'True.0')\n    lu.assertEquals(candidate('True'), 'True')\n    lu.assertEquals(candidate('.'), '.')\n    lu.assertEquals(candidate('..'), '..')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_135211_transform_basis_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # name = name.lower()\n-- # name = name.replace('/', '_sl_')\n-- # name = name.replace('*', '_st_')\n-- # return name\n-- \n-- Transforms the name of a basis set to an internal representation\n-- This makes comparison of basis set names easier by, for example,\n-- converting the name to all lower case.\nlocal function transform_basis_name(name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_135211_transform_basis_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = transform_basis_name\n    lu.assertEquals(candidate('foo_sl_bar'), 'foo_sl_bar')\n    lu.assertEquals(candidate('foo123'), 'foo123')\n    lu.assertEquals(candidate('foo/bar/baz'), 'foo_sl_bar_sl_baz')\n    lu.assertEquals(candidate('A'), 'a')\n    lu.assertEquals(candidate('foo*BAR'), 'foo_st_bar')\n    lu.assertEquals(candidate('foo*bar'), 'foo_st_bar')\n    lu.assertEquals(candidate('foo_st_bar'), 'foo_st_bar')\n    lu.assertEquals(candidate('a'), 'a')\n    lu.assertEquals(candidate('FOO'), 'foo')\n    lu.assertEquals(candidate('foo_sl_BAR'), 'foo_sl_bar')\n    lu.assertEquals(candidate('foo_st_BAR'), 'foo_st_bar')\n    lu.assertEquals(candidate('FOO123'), 'foo123')\n    lu.assertEquals(candidate('foo/BAR'), 'foo_sl_bar')\n    lu.assertEquals(candidate('foo*bar*baz'), 'foo_st_bar_st_baz')\n    lu.assertEquals(candidate('foo/bar'), 'foo_sl_bar')\n    lu.assertEquals(candidate('foo'), 'foo')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_135424_people_in_rec_area", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if length_1 <= 0 or length_2 <= 0 or social_d <= 0:  # We want our lengths and social_d to be positive numbers.\n-- #     raise Exception(\"Lengths of rectangle and the social distance should be positve numbers.\")  # Raises exception to inform user of mistake.\n-- # else:\n-- #     area = length_1*length_2  # We model our waiting area as a rectangle whose area we define as the product of its length and width.\n-- #     max_people_in_rec = int(area/(social_d**2))  # We take A = n*m*social_d^2, where n*m yields the max number of people that can fit in the area. We get: n*m = number of people = area/social_d^2\n-- #     return (max_people_in_rec)\n-- \n-- This function's job is to tell us how many people can fit in a rectangular area. We model people as equally sized circles, with the distance between the center of two circles being the \"social distance\" between them. Our function takes the lengths of the rectangle and the distance between the center of two adjacent circles as inputs.\n-- The circles are laid out in a rectangular grid within the rectangular area. This way, the lengths of the sides of the rectange can be given by L1=n*2r and L2 = m*2r where n and m are the number of circles and r is the radius of each circle. We also let social_d = 2r, the distane between the center of two circles.\nlocal function people_in_rec_area(length_1, length_2, social_d)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_135424_people_in_rec_area.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = people_in_rec_area\n    lu.assertEquals(candidate(1, 1, 1), 1)\n    lu.assertEquals(candidate(1, 4, 1), 4)\n    lu.assertEquals(candidate(20, 20, 20), 1)\n    lu.assertEquals(candidate(5, 10, 1), 50)\n    lu.assertEquals(candidate(1, 20, 1), 20)\n    lu.assertEquals(candidate(1, 2, 1), 2)\n    lu.assertEquals(candidate(1, 6, 1), 6)\n    lu.assertEquals(candidate(10, 10, 10), 1)\n    lu.assertEquals(candidate(5, 5, 1), 25)\n    lu.assertEquals(candidate(1, 1, 1), 1)\n    lu.assertEquals(candidate(1, 5, 1), 5)\n    lu.assertEquals(candidate(1, 3, 1), 3)\n    lu.assertEquals(candidate(5, 5, 3), 2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_13659__split_left", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # result = []\n-- # temp = u\"\"\n-- # escaped = False\n-- # index = 0\n-- # left = True\n-- # for c in val:\n-- #     left = False\n-- #     temp += c\n-- #     if c == sep[index] and not escaped:\n-- #         index += 1\n-- #     else:\n-- #         index = 0\n-- #         if c == u\"\\\\\":\n-- #             escaped ^= True\n-- #         else:\n-- #             escaped = False\n-- #     if index >= len(sep):\n-- #         left = True\n-- #         index = 0\n-- #         result.append(temp[:-len(sep)])\n-- #         temp = u\"\"\n-- # if temp or left:\n-- #     result.append(temp)\n-- # return result\n-- \n-- Split a string by a delimiter which can be escaped by \\\nlocal function _split_left(val, sep)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_13659__split_left.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _split_left\n    lu.assertEquals(candidate('::foo:', ':'), {'', '', 'foo', ''})\n    lu.assertEquals(candidate('foo\\\\', ','), {'foo\\\\'})\n    lu.assertEquals(candidate('foo', ','), {'foo'})\n    lu.assertEquals(candidate('foo:bar', ':'), {'foo', 'bar'})\n    lu.assertEquals(candidate(':::foo:', ':'), {'', '', '', 'foo', ''})\n    lu.assertEquals(candidate(':foo:', ':'), {'', 'foo', ''})\n    lu.assertEquals(candidate('foo\\\\x', 'x\\\\y'), {'foo\\\\x'})\n    lu.assertEquals(candidate('foo\\\\x', '\\\\x\\\\y'), {'foo\\\\x'})\n    lu.assertEquals(candidate('a,b,c,', ','), {'a', 'b', 'c', ''})\n    lu.assertEquals(candidate('foo,bar', ',,'), {'foo,bar'})\n    lu.assertEquals(candidate('foo', 'x'), {'foo'})\n    lu.assertEquals(candidate('a,b,c', ','), {'a', 'b', 'c'})\n    lu.assertEquals(candidate('foo:', ':'), {'foo', ''})\n    lu.assertEquals(candidate('foo', ':'), {'foo'})\n    lu.assertEquals(candidate('foo', '\\\\x'), {'foo'})\n    lu.assertEquals(candidate('foo,bar', ','), {'foo', 'bar'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_136605_is_tissue_compatible", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # import hashlib\n-- # # This code is a convoluted way to approximate a random oracle on the ids (so the\n-- # # output is uniformly random in [0, 1), but for a given ID pair, the same output is always\n-- # # produced).\n-- # hexval = hashlib.md5((str(recv_id) + str(don_id)).encode()).hexdigest()\n-- # intval = int(hexval, 16)\n-- # b = intval / (1 << 129 - 1)  # md5 generates 128-bit number; we normalize it to [0, 1]\n-- # if recv_pra == 'Low':\n-- #     return b <= 0.95\n-- # elif recv_pra == 'Medium':\n-- #     return b <= 0.45\n-- # else:  # high pra\n-- #     assert recv_pra == 'High'\n-- #     return b <= 0.10\n-- \n-- Modeling actual  compatibility is complex, and depends on\n-- properties of different HLA markers and various other complications.\n-- Instead of dealing with the medical complexities, we use a simple \n-- model that produces a uniformly-distributed value that is dependent\n-- on the two inputs, and outputs a discretized probability.\n-- It's not important to understand the following code. But you should \n-- call this function with the receiver's PRA-type, receiver's ID, \n-- and the donor's  ID to check if their tissues are compatible or not.\n-- Example usage: is_tissue_compatible('Low', 4474, 3587)\nlocal function is_tissue_compatible(recv_pra, recv_id, don_id)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_136605_is_tissue_compatible.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_tissue_compatible\n    lu.assertEquals(candidate('High', 4474, 3587), false)\n    lu.assertEquals(candidate('Medium', 4474, 3587), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_136714_blasius", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return 0.3164 * re ** (-0.25)\n-- \n-- Calculate friction coefficient according to Blasius.\n-- Parameters\n-- ----------\n-- re : float\n--     Reynolds number.\n-- Returns\n-- -------\n-- darcy_friction_factor : float\n--     Darcy friction factor.\nlocal function blasius(re)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_136714_blasius.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = blasius\n    lu.assertEquals(candidate(1.0), 0.3164)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_136879_clamp", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if MIN < MAX:\n-- #     return max(MIN, min(n, MAX - 1))\n-- # else:\n-- #     raise ValueError(\"MAX value must be greater than MIN value\")\n-- \n-- Clam n in [MIN, MAX[\nlocal function clamp(MIN, n, MAX)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_136879_clamp.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = clamp\n    lu.assertEquals(candidate(0, 5, 10), 5)\n    lu.assertEquals(candidate(0, 2, 2), 1)\n    lu.assertEquals(candidate(3, 3, 6), 3)\n    lu.assertEquals(candidate(3, 5, 6), 5)\n    lu.assertEquals(candidate(3, 4, 6), 4)\n    lu.assertEquals(candidate(0, -2, 2), 0)\n    lu.assertEquals(candidate(2, 3, 4), 3)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_137389_unit_form", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if disc & 3 == 0:\n-- #     a = 1\n-- #     b = 0\n-- #     c = disc // -4\n-- # elif disc & 3 == 1:\n-- #     a = 1\n-- #     b = 1\n-- #     c = (disc - 1) // -4\n-- # else:\n-- #     raise ValueError(\"discriminant is not 0 or 1 mod 4.\")\n-- # return (a, b, c)\n-- \n--     Return generated quadratic form with the given discriminant.\nlocal function unit_form(disc)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_137389_unit_form.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = unit_form\n    lu.assertEquals(candidate(0), {1, 0, 0})\n    lu.assertEquals(candidate(1), {1, 1, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_13746_rotation_cs", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # Xrot = X*c - Y*s\n-- # Yrot = Y*c + X*s\n-- # return Xrot, Yrot\n-- \n-- For numpy arrays X and Y returns the numpy arrays of Xrot and Yrot\n-- for specified rotation angle cosine and sine values.\nlocal function rotation_cs(X, Y, c, s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_13746_rotation_cs.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = rotation_cs\n    lu.assertEquals(candidate(0, 0, 1, 0), {0, 0})\n    lu.assertEquals(candidate(0, 0, 0, 1), {0, 0})\n    lu.assertEquals(candidate(0, 0, 0, 0), {0, 0})\n    lu.assertEquals(candidate(1, 0, 0, -1), {0, -1})\n    lu.assertEquals(candidate(1, 1, 0, 1), {-1, 1})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_13749_get_parent_technique_id", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return sub_tid.split(\".\")[0]\n-- \n-- Given a sub-technique id, return parent\nlocal function get_parent_technique_id(sub_tid)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_13749_get_parent_technique_id.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_parent_technique_id\n    lu.assertEquals(candidate('T1001.001'), 'T1001')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_137905_parse_function_path", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # path_components = function_path.split(\".\")\n-- # if len(path_components) > 2:\n-- #     function_name = path_components.pop()\n-- #     module_path = \".\".join(path_components)\n-- #     return module_path, function_name\n-- # return path_components[0], path_components[1]\n-- \n-- Util to parse path to functions.\nlocal function parse_function_path(function_path)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_137905_parse_function_path.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_function_path\n    lu.assertEquals(candidate('a_module.a_function.another_function'), {'a_module.a_function', 'another_function'})\n    lu.assertEquals(candidate('a_module.a_function'), {'a_module', 'a_function'})\n    lu.assertEquals(candidate('test.func'), {'test', 'func'})\n    lu.assertEquals(candidate('test.func.func2'), {'test.func', 'func2'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_139038_prazen_kvadrat_n", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # BEGIN SOLUTION\n-- # result = ''\n-- # # END SOLUTION\n-- # return result\n-- \n--  vrni string, ki bo narisal prazen kvadrat v velikost n_vrstic\nlocal function prazen_kvadrat_n(n_vrstic)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_139038_prazen_kvadrat_n.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = prazen_kvadrat_n\n    lu.assertEquals(candidate(0), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_140092_parse_addr", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # This code is based on\n-- # # django.test.testcases.LiveServerTestCase.setUpClass\n-- # # The specified ports may be of the form '8000-8010,8080,9200-9300'\n-- # # i.e. a comma-separated list of ports or ranges of ports, so we break\n-- # # it down into a detailed list of all possible ports.\n-- # possible_ports = []\n-- # try:\n-- #     host, port_ranges = specified_address.split(':')\n-- #     for port_range in port_ranges.split(','):\n-- #         # A port range can be of either form: '8000' or '8000-8010'.\n-- #         extremes = list(map(int, port_range.split('-')))\n-- #         assert len(extremes) in [1, 2]\n-- #         if len(extremes) == 1:\n-- #             # Port range of the form '8000'\n-- #             possible_ports.append(extremes[0])\n-- #         else:\n-- #             # Port range of the form '8000-8010'\n-- #             for port in range(extremes[0], extremes[1] + 1):\n-- #                 possible_ports.append(port)\n-- # except Exception:\n-- #     raise Exception(\n-- #         'Invalid address (\"%s\") for live server.' % specified_address)\n-- # return (host, possible_ports)\n-- \n-- Parse the --liveserver argument into a host/IP address and port range\nlocal function parse_addr(specified_address)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_140092_parse_addr.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_addr\n    lu.assertEquals(candidate('127.0.0.1:8000-8010'), {'127.0.0.1', {8000, 8001, 8002, 8003, 8004, 8005, 8006, 8007, 8008, 8009, 8010}})\n    lu.assertEquals(candidate('127.0.0.1:8000'), {'127.0.0.1', {8000}})\n    lu.assertEquals(candidate('127.0.0.1:8000-8010,8080'), {'127.0.0.1', {8000, 8001, 8002, 8003, 8004, 8005, 8006, 8007, 8008, 8009, 8010, 8080}})\n    lu.assertEquals(candidate('127.0.0.1:8000,8001'), {'127.0.0.1', {8000, 8001}})\n    lu.assertEquals(candidate('localhost:8000,8001,8003-8010'), {'localhost', {8000, 8001, 8003, 8004, 8005, 8006, 8007, 8008, 8009, 8010}})\n    lu.assertEquals(candidate('127.0.0.1:8000,8001,8002-8005'), {'127.0.0.1', {8000, 8001, 8002, 8003, 8004, 8005}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_140262_DNAtoRNA", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return dna.replace('T', 'U')\n-- \n--  dna_to_rna == PEP8 (forced camelCase by CodeWars) \nlocal function DNAtoRNA(dna)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_140262_DNAtoRNA.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = DNAtoRNA\n    lu.assertEquals(candidate(candidate('ACGT')), 'ACGU')\n    lu.assertEquals(candidate('GCAT'), 'GCAU')\n    lu.assertEquals(candidate('TTTT'), 'UUUU')\n    lu.assertEquals(candidate('GATGGAACTTGACTACGTAAATT'), 'GAUGGAACUUGACUACGUAAAUU')\n    lu.assertEquals(candidate('GCAT'), 'GCAU')\n    lu.assertEquals(candidate('GCAT'), 'GCAU')\n    lu.assertEquals(candidate('GACCGCCGCC'), 'GACCGCCGCC')\n    lu.assertEquals(candidate('TTTT'), 'UUUU')\n    lu.assertEquals(candidate('GACCGCCGCC'), 'GACCGCCGCC')\n    lu.assertEquals(candidate('TTTT'), 'UUUU')\n    lu.assertEquals(candidate('ACGT'), 'ACGU')\n    lu.assertEquals(candidate('TTTT'), 'UUUU')\n    lu.assertEquals(candidate('GACCGCCGCC'), 'GACCGCCGCC')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_14050_box_sizing", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return keyword in ('padding-box', 'border-box', 'content-box')\n-- \n-- Validation for the ``box-sizing`` property from css3-ui\nlocal function box_sizing(keyword)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_14050_box_sizing.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = box_sizing\n    lu.assertEquals(candidate('content-box'), true)\n    lu.assertEquals(candidate('foo'), false)\n    lu.assertEquals(candidate('padding-box'), true)\n    lu.assertEquals(candidate('border-box'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_140877_parse_photo_link", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # base_url = photo_url.split('?')[0]\n-- # name = base_url.split('/')[-1]\n-- # return base_url, name\n-- \n-- Extracts the base URL (URL without query parameters) and the photo name from a Onedrive photo URL\n-- :param photo_url: photo URL\n-- :return: base URL and photo name\nlocal function parse_photo_link(photo_url)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_140877_parse_photo_link.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_photo_link\n    lu.assertEquals(candidate('https://graph.microsoft.com/v1.0/me/drive/items/root/children/F22222222222222222222222222222222'), {'https://graph.microsoft.com/v1.0/me/drive/items/root/children/F22222222222222222222222222222222', 'F22222222222222222222222222222222'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_142183_is_parenthetical", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if len(s) < 2:\n-- #     return False\n-- # else:\n-- #     return s[0] == \"(\" and s[-1] == \")\"\n-- \n-- (str) -> bool\n-- Returns True if s starts with '(' and ends with ')'\nlocal function is_parenthetical(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_142183_is_parenthetical.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_parenthetical\n    lu.assertEquals(candidate('This string is not parenthetical (()())())'), false)\n    lu.assertEquals(candidate('(abc(def)(ghi)jkl)'), true)\n    lu.assertEquals(candidate(''), false)\n    lu.assertEquals(candidate(')'), false)\n    lu.assertEquals(candidate('This string is not parenthetical ()()'), false)\n    lu.assertEquals(candidate('this is not a test('), false)\n    lu.assertEquals(candidate('(abc(def)ghi)'), true)\n    lu.assertEquals(candidate('(abc)'), true)\n    lu.assertEquals(candidate('This string is not parenthetical'), false)\n    lu.assertEquals(candidate('(this is a test)'), true)\n    lu.assertEquals(candidate('This string is not parenthetical (()'), false)\n    lu.assertEquals(candidate('this is not a test)'), false)\n    lu.assertEquals(candidate('This string is not parenthetical (()())('), false)\n    lu.assertEquals(candidate('('), false)\n    lu.assertEquals(candidate('((abc)def)ghi'), false)\n    lu.assertEquals(candidate(''), false)\n    lu.assertEquals(candidate('(abc)def(ghi)jkl'), false)\n    lu.assertEquals(candidate('()'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_142478_construct_vocab", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # all_words = set(words1 + words2)\n-- # vocab = {}\n-- # for word in all_words:\n-- #     # Give words in this list higher weighting as it almost certainly means a promotion\n-- #     if word in ['virtual-assist']:\n-- #         vocab[word] = 2\n-- #     else:\n-- #         vocab[word] = 1\n-- # return vocab\n-- \n-- Combines words from two sentences into a single\n-- dictionary\n-- Input: words1 - List of strings\n--        words2 - List of strings\n-- Output: vocab - dictionary where key is word,\n--                     value is weight of word\nlocal function construct_vocab(words1, words2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_142478_construct_vocab.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = construct_vocab\n    lu.assertEquals(candidate({'orange', 'banana', 'apple'}, {'orange'}), {['orange'] = 1, ['banana'] = 1, ['apple'] = 1})\n    lu.assertEquals(candidate({'a', 'b', 'c'}, {'d', 'e', 'f'}), {['a'] = 1, ['b'] = 1, ['c'] = 1, ['d'] = 1, ['e'] = 1, ['f'] = 1})\n    lu.assertEquals(candidate({'orange', 'banana', 'apple'}, {}), {['orange'] = 1, ['banana'] = 1, ['apple'] = 1})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_142835_has_divider_smaller_than", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # print(n, i)\n-- # if i <= 1:\n-- #     return False\n-- # else:\n-- #     if n % i == 0:\n-- #         return True\n-- #     else:\n-- #         return has_divider_smaller_than(n, i - 1)\n-- \n-- Get True if n has a divided smaller than i\n-- :param n: number\n-- :param i: number\n-- :return: boolean\nlocal function has_divider_smaller_than(n, i)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_142835_has_divider_smaller_than.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = has_divider_smaller_than\n    lu.assertEquals(candidate(234, 22), true)\n    lu.assertEquals(candidate(7, 5), false)\n    lu.assertEquals(candidate(7, 3), false)\n    lu.assertEquals(candidate(10, 4), true)\n    lu.assertEquals(candidate(6, 5), true)\n    lu.assertEquals(candidate(234, 1000), true)\n    lu.assertEquals(candidate(6, 3), true)\n    lu.assertEquals(candidate(9, 5), true)\n    lu.assertEquals(candidate(1, 1), false)\n    lu.assertEquals(candidate(10, 5), true)\n    lu.assertEquals(candidate(-10, 5), true)\n    lu.assertEquals(candidate(3, 3), true)\n    lu.assertEquals(candidate(234, 12), true)\n    lu.assertEquals(candidate(1, 2), false)\n    lu.assertEquals(candidate(3, 1), false)\n    lu.assertEquals(candidate(2, 1), false)\n    lu.assertEquals(candidate(6, 2), true)\n    lu.assertEquals(candidate(5, 1), false)\n    lu.assertEquals(candidate(13, 2), false)\n    lu.assertEquals(candidate(-9, 5), true)\n    lu.assertEquals(candidate(7, 1), false)\n    lu.assertEquals(candidate(6, 4), true)\n    lu.assertEquals(candidate(-1, 5), false)\n    lu.assertEquals(candidate(6, 1), false)\n    lu.assertEquals(candidate(200, 5), true)\n    lu.assertEquals(candidate(4, 2), true)\n    lu.assertEquals(candidate(12, 2), true)\n    lu.assertEquals(candidate(1, 5), false)\n    lu.assertEquals(candidate(4, 1), false)\n    lu.assertEquals(candidate(-7, 5), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_143409_get_chunk_label", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # hours, minutes = divmod(tot_minutes, 60)\n-- # return f\"{hours:02}h:{minutes:02}m\"\n-- \n-- Returns a readable elapsed time.\nlocal function get_chunk_label(tot_minutes)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_143409_get_chunk_label.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_chunk_label\n    lu.assertEquals(candidate(122), '02h:02m')\n    lu.assertEquals(candidate(12), '00h:12m')\n    lu.assertEquals(candidate(65), '01h:05m')\n    lu.assertEquals(candidate(135), '02h:15m')\n    lu.assertEquals(candidate(61), '01h:01m')\n    lu.assertEquals(candidate(98), '01h:38m')\n    lu.assertEquals(candidate(3599), '59h:59m')\n    lu.assertEquals(candidate(132), '02h:12m')\n    lu.assertEquals(candidate(1), '00h:01m')\n    lu.assertEquals(candidate(0), '00h:00m')\n    lu.assertEquals(candidate(60), '01h:00m')\n    lu.assertEquals(candidate(2), '00h:02m')\n    lu.assertEquals(candidate(1220), '20h:20m')\n    lu.assertEquals(candidate(20), '00h:20m')\n    lu.assertEquals(candidate(18), '00h:18m')\n    lu.assertEquals(candidate(120), '02h:00m')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_1437_check_host", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not ('http' in host and '//' in host) and host[len(host) - 1] == '/':\n-- #     return ''.join(['http://', host[:len(host) - 1]])\n-- # elif not ('http' in host and '//' in host):\n-- #     return ''.join(['http://', host])\n-- # elif host[len(host) - 1] == '/':\n-- #     return host[:len(host) - 1]\n-- # else:\n-- #     return host\n-- \n--  Helper function to get the hostname in desired format \nlocal function check_host(host)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_1437_check_host.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = check_host\n    lu.assertEquals(candidate(candidate('https://example.com')), 'https://example.com')\n    lu.assertEquals(candidate('example.com'), 'http://example.com')\n    lu.assertEquals(candidate('https://example.com/'), 'https://example.com')\n    lu.assertEquals(candidate('codecademy.com'), 'http://codecademy.com')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_144168_even", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if (x % 2 == 0):\n-- #     return True\n-- # else:\n-- #     return False\n-- \n-- returns true if even number, false if odd\nlocal function even(x)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_144168_even.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = even\n    lu.assertEquals(candidate(2), true)\n    lu.assertEquals(candidate(0), true)\n    lu.assertEquals(candidate(100), true)\n    lu.assertEquals(candidate(3), false)\n    lu.assertEquals(candidate(-2), true)\n    lu.assertEquals(candidate(42), true)\n    lu.assertEquals(candidate(15), false)\n    lu.assertEquals(candidate(2017), false)\n    lu.assertEquals(candidate(1), false)\n    lu.assertEquals(candidate(-1), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_144200_curate_list", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # final_list = []\n-- # for token in input_list:\n-- #     if len(token.strip()) == 0:\n-- #         continue\n-- #     if token.strip() in words_list:\n-- #         final_list.append(token.strip())\n-- # return final_list\n-- \n-- :param input_list:\n-- :type input_list:\n-- :param words_list:\n-- :type words_list:\n-- :return:\n-- :rtype:\nlocal function curate_list(input_list, words_list)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_144200_curate_list.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = curate_list\n    lu.assertEquals(candidate({}, {}), {})\n    lu.assertEquals(candidate({'a', 'b', 'c', 'd', 'e', 'f', 'g'}, {'a', 'b', 'c', 'd', 'e'}), {'a', 'b', 'c', 'd', 'e'})\n    lu.assertEquals(candidate({'a', 'b', 'c', 'd', 'e'}, {'a', 'b', 'c', 'd', 'e', 'f', 'g'}), {'a', 'b', 'c', 'd', 'e'})\n    lu.assertEquals(candidate({'a', 'b', 'c', 'd', 'e'}, {'a', 'b', 'c', 'd', 'e'}), {'a', 'b', 'c', 'd', 'e'})\n    lu.assertEquals(candidate({}, {'a', 'b', 'c'}), {})\n    lu.assertEquals(candidate({'a', 'b', 'c'}, {}), {})\n    lu.assertEquals(candidate({'a', 'b', 'c', 'd', 'e', 'f', 'g'}, {'h', 'i', 'j', 'k', 'l', 'm', 'n'}), {})\n    lu.assertEquals(candidate({'the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog'}, {'the', 'quick', 'brown', 'fox', 'jumped', 'the', 'lazy', 'dog'}), {'the', 'quick', 'brown', 'fox', 'jumped', 'the', 'lazy', 'dog'})\n    lu.assertEquals(candidate({'a', 'b', 'c', 'd', 'e', 'f', 'g'}, {}), {})\n    lu.assertEquals(candidate({'the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog'}, {'the', 'quick', 'brown', 'fox', 'over', 'the', 'lazy', 'dog'}), {'the', 'quick', 'brown', 'fox', 'over', 'the', 'lazy', 'dog'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_144223_line_id_2_txt", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # line_id_str = str(line_id)\n-- # if len(line_id_str) == 1:\n-- #     line_id_txt = \"000\" + line_id_str\n-- # elif len(line_id_str) == 2:\n-- #     line_id_txt = \"00\" + line_id_str\n-- # elif len(line_id_str) == 3:\n-- #     line_id_txt = \"0\" + line_id_str\n-- # else:\n-- #     line_id_txt = line_id_str\n-- # return line_id_txt\n-- \n-- Convert line id (integer) to string nnnn\n-- :return: line_id_txt -> <string> ID de la linia introduit en format text\nlocal function line_id_2_txt(line_id)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_144223_line_id_2_txt.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = line_id_2_txt\n    lu.assertEquals(candidate(3), '0003')\n    lu.assertEquals(candidate(19), '0019')\n    lu.assertEquals(candidate(999), '0999')\n    lu.assertEquals(candidate(24), '0024')\n    lu.assertEquals(candidate(9999), '9999')\n    lu.assertEquals(candidate(17), '0017')\n    lu.assertEquals(candidate(25), '0025')\n    lu.assertEquals(candidate(255), '0255')\n    lu.assertEquals(candidate(0), '0000')\n    lu.assertEquals(candidate(9), '0009')\n    lu.assertEquals(candidate(6), '0006')\n    lu.assertEquals(candidate(1), '0001')\n    lu.assertEquals(candidate(12), '0012')\n    lu.assertEquals(candidate(16), '0016')\n    lu.assertEquals(candidate(7), '0007')\n    lu.assertEquals(candidate(12), '0012')\n    lu.assertEquals(candidate(10), '0010')\n    lu.assertEquals(candidate(18), '0018')\n    lu.assertEquals(candidate(5000), '5000')\n    lu.assertEquals(candidate(15), '0015')\n    lu.assertEquals(candidate(2), '0002')\n    lu.assertEquals(candidate(8), '0008')\n    lu.assertEquals(candidate(1), '0001')\n    lu.assertEquals(candidate(99), '0099')\n    lu.assertEquals(candidate(1000), '1000')\n    lu.assertEquals(candidate(13), '0013')\n    lu.assertEquals(candidate(20), '0020')\n    lu.assertEquals(candidate(240), '0240')\n    lu.assertEquals(candidate(5), '0005')\n    lu.assertEquals(candidate(0), '0000')\n    lu.assertEquals(candidate(100), '0100')\n    lu.assertEquals(candidate(4), '0004')\n    lu.assertEquals(candidate(14), '0014')\n    lu.assertEquals(candidate(99999), '99999')\n    lu.assertEquals(candidate(12345), '12345')\n    lu.assertEquals(candidate(11), '0011')\n    lu.assertEquals(candidate(111), '0111')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_144251_keyword_filter", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not keywords:\n-- #     return True\n-- # return all(keyword in url for keyword in keywords)\n-- \n-- return true if url contains all keywords,\n-- return false otherwise \nlocal function keyword_filter(url, keywords)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_144251_keyword_filter.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = keyword_filter\n    lu.assertEquals(candidate('http://foo.com/bar?a=1&b=2', {'a', 'b'}), true)\n    lu.assertEquals(candidate('http://foo.com/bar?a=1&b=2', {'b'}), true)\n    lu.assertEquals(candidate('http://foo.com/bar?a=1&b=2', {'a', 'b', 'c', 'd'}), false)\n    lu.assertEquals(candidate('http://foo.com/bar?a=1&b=2', {'a'}), true)\n    lu.assertEquals(candidate('http://foo.com/bar?a=1&b=2', {}), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_144645__remove_prefix", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # _result = path.split('/')\n-- # _last_index = len(_result)-1\n-- # return _result[_last_index]\n-- \n-- Removes prefixed from absolute etcd paths\nlocal function _remove_prefix(path)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_144645__remove_prefix.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _remove_prefix\n    lu.assertEquals(candidate('/a/b/c'), 'c')\n    lu.assertEquals(candidate('/a/b'), 'b')\n    lu.assertEquals(candidate('/a'), 'a')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('/foo/bar'), 'bar')\n    lu.assertEquals(candidate('/'), '')\n    lu.assertEquals(candidate('/foo'), 'foo')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_144790_iscsi_portal_with_port", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"%(address)s:3260\" % {\"address\": address}\n-- \n-- Add default port 3260 to iSCSI portal\n-- :param address: iSCSI portal without port\n-- :return: iSCSI portal with default port 3260\nlocal function iscsi_portal_with_port(address)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_144790_iscsi_portal_with_port.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = iscsi_portal_with_port\n    lu.assertEquals(candidate('10.1.2.3'), '10.1.2.3:3260')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_14508_betweenness_index", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # index = n_nodes * n_times * layer_index + n_nodes * time_index + node_index\n-- # return index\n-- \n--     find the index associated to a point in the static graph. See betweenness_centrality.\nlocal function betweenness_index(n_nodes, n_times, node_index, time_index, layer_index)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_14508_betweenness_index.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = betweenness_index\n    lu.assertEquals(candidate(1, 1, 0, 0, 0), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_145532_middle_me", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # string = N * Y\n-- # if len(string) % 2:\n-- #     return X\n-- # else:\n-- #     id_x = len(string) // 2\n-- #     return f'{string[:id_x]}{X}{string[id_x:]}'\n-- \n--  This function takes a key of X and place it in the middle of Y repeated N times. \nlocal function middle_me(N, X, Y)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_145532_middle_me.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = middle_me\n    lu.assertEquals(candidate(1, 'abc', 'xyz'), 'abc')\n    lu.assertEquals(candidate(2, '', 'abc'), 'abcabc')\n    lu.assertEquals(candidate(2, 'abc', 'z'), 'zabcz')\n    lu.assertEquals(candidate(2, '', ''), '')\n    lu.assertEquals(candidate(2, '', 'def'), 'defdef')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_145712_valid_ip", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # import socket\n-- # try:\n-- #     socket.inet_aton(query)\n-- #     return True\n-- # except socket.error:\n-- #     return False\n-- \n-- Check if an IP address is valid.\nlocal function valid_ip(query)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_145712_valid_ip.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = valid_ip\n    lu.assertEquals(candidate('127.0.0.1:8080'), false)\n    lu.assertEquals(candidate('127.0.0.1'), true)\n    lu.assertEquals(candidate('192.168.1.1.1.1'), false)\n    lu.assertEquals(candidate('192.168.1.1.1.1.1.1'), false)\n    lu.assertEquals(candidate('192.168.255.255'), true)\n    lu.assertEquals(candidate('0.0.0.0.0'), false)\n    lu.assertEquals(candidate('192.168.1.1'), true)\n    lu.assertEquals(candidate('255.255.255.255'), true)\n    lu.assertEquals(candidate('0.0.0.0'), true)\n    lu.assertEquals(candidate('255.255.255.255'), true)\n    lu.assertEquals(candidate('1.2.3.4'), true)\n    lu.assertEquals(candidate(''), false)\n    lu.assertEquals(candidate('192.168.3.11'), true)\n    lu.assertEquals(candidate('256.127.0.1'), false)\n    lu.assertEquals(candidate('abc'), false)\n    lu.assertEquals(candidate('127.0.0.256'), false)\n    lu.assertEquals(candidate('abc'), false)\n    lu.assertEquals(candidate('192.168.1.255'), true)\n    lu.assertEquals(candidate('127.0.256.1'), false)\n    lu.assertEquals(candidate('192.168.1.1.1.1.1'), false)\n    lu.assertEquals(candidate('127.0.0.300'), false)\n    lu.assertEquals(candidate('256.0.0.0'), false)\n    lu.assertEquals(candidate('192.168.1.1.1'), false)\n    lu.assertEquals(candidate('1.2.3.4.5'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_145929_containsExploit", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return ('https://' in text.lower() or\n-- #         'http://' in text.lower() or\n-- #         'javascript:' in text.lower() or\n-- #         'example.com' in text.lower())\n-- \n--  Returns whether or not the given str contains evidence that it is an open redirect exploit \nlocal function containsExploit(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_145929_containsExploit.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = containsExploit\n    lu.assertEquals(candidate('hi there! http://localhost/'), true)\n    lu.assertEquals(candidate('hi there! http://[::1]'), true)\n    lu.assertEquals(candidate('Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nunc euismod purus sit amet ante facilisis, quis rhoncus urna tempor. In hac habitasse platea dictumst. Aenean ullamcorper, diam in ornare posuere, eros odio tempus purus, vel vehicula sem dolor ut lorem. Vivamus a tincidunt turpis. Etiam eu dui vel odio commodo fringilla. Vestibulum ultrices, neque in facilisis pellentesque, nunc nibh interdum tortor, at venenatis risus nulla nec libero. Quisque viverra sapien vel lectus ultricies, quis mollis nisl fermentum. Morbi semper auctor mi, id hendrerit nunc. Mauris mattis eros non magna faucibus, id tincidunt diam interdum. Quisque congue lorem at arcu rhoncus, a gravida urna aliquet. Nunc vel diam lacinia, ullamcorper felis ut, convallis magna.'), false)\n    lu.assertEquals(candidate('hi there! https://[::1]/foo'), true)\n    lu.assertEquals(candidate('hi there! https://[::1]:8000/'), true)\n    lu.assertEquals(candidate('hello'), false)\n    lu.assertEquals(candidate('hi there! http://[::1]:8000/'), true)\n    lu.assertEquals(candidate('hi there! http://[::1]/foo'), true)\n    lu.assertEquals(candidate('hi there! http://example.com/test/123456?abc=1'), true)\n    lu.assertEquals(candidate('javascript:alert(1)'), true)\n    lu.assertEquals(candidate('Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer vel dolor ac diam bibendum placerat non non erat. Praesent non massa sit amet erat dapibus posuere. Sed quis orci et nisl dapibus iaculis.'), false)\n    lu.assertEquals(candidate('http://example.com'), true)\n    lu.assertEquals(candidate('https://example.com'), true)\n    lu.assertEquals(candidate('hi there! https://[::1]'), true)\n    lu.assertEquals(candidate('hi there! http://127.0.0.1/'), true)\n    lu.assertEquals(candidate(''), false)\n    lu.assertEquals(candidate('hi there! https://[::1]/'), true)\n    lu.assertEquals(candidate('hi there! http://example.com'), true)\n    lu.assertEquals(candidate('Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.'), false)\n    lu.assertEquals(candidate('hi there! javascript:alert(1)'), true)\n    lu.assertEquals(candidate('hi there! http://[::1]:8000'), true)\n    lu.assertEquals(candidate('hi there'), false)\n    lu.assertEquals(candidate('hi there! https://example.com/test/123456'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_146824_is_fq", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return name.startswith('refs/')\n-- \n-- Return True if the supplied 'name' is fully-qualified, False otherwise.\n-- Usage examples:\n--     >>> is_fq('master')\n--     False\n--     >>> is_fq('refs/heads/master')\n--     True\n-- :name: string name of the ref to test\n-- :returns: bool\nlocal function is_fq(name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_146824_is_fq.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_fq\n    lu.assertEquals(candidate('master'), false)\n    lu.assertEquals(candidate('master'), false)\n    lu.assertEquals(candidate('refs/heads/master'), true)\n    lu.assertEquals(candidate('refs/heads/master'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_147099_style_to_dict", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if isinstance(style, dict):\n-- #     return style\n-- # d = {}\n-- # styles = style.split(';')\n-- # for s in styles:\n-- #     # noinspection PyBroadException\n-- #     try:\n-- #         key, value = s.split(':')\n-- #         d[key.strip()] = value.strip()\n-- #     except:\n-- #         pass\n-- # return d\n-- \n-- Parses an HTML tag style attribute.\n-- :param style:\nlocal function style_to_dict(style)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_147099_style_to_dict.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = style_to_dict\n    lu.assertEquals(candidate('font-style: italic; font-style: normal'), {['font-style'] = 'normal'})\n    lu.assertEquals(candidate('font-weight: bold; font-style: italic'), {['font-weight'] = 'bold', ['font-style'] = 'italic'})\n    lu.assertEquals(candidate('color: rgb(255, 0, 0)'), {['color'] = 'rgb(255, 0, 0)'})\n    lu.assertEquals(candidate('display: block; display: inline-block; display: none'), {['display'] = 'none'})\n    lu.assertEquals(candidate('color: red; margin: 5px; font-size: 20px;'), {['color'] = 'red', ['margin'] = '5px', ['font-size'] = '20px'})\n    lu.assertEquals(candidate('foo'), {})\n    lu.assertEquals(candidate('color: red; margin: 5px;'), {['color'] = 'red', ['margin'] = '5px'})\n    lu.assertEquals(candidate('foo;'), {})\n    lu.assertEquals(candidate('margin:10px'), {['margin'] = '10px'})\n    lu.assertEquals(candidate('color: rgba(255, 0, 0, 0.5)'), {['color'] = 'rgba(255, 0, 0, 0.5)'})\n    lu.assertEquals(candidate(';;'), {})\n    lu.assertEquals(candidate('font-size: 123;'), {['font-size'] = '123'})\n    lu.assertEquals(candidate(''), {})\n    lu.assertEquals(candidate('font-size: 14px'), {['font-size'] = '14px'})\n    lu.assertEquals(candidate('font-size: 123; color: #fff; color: #abc;'), {['font-size'] = '123', ['color'] = '#abc'})\n    lu.assertEquals(candidate('background-color:red;'), {['background-color'] = 'red'})\n    lu.assertEquals(candidate(' '), {})\n    lu.assertEquals(candidate('font-size: 13px; color: white; background-color: blue;'), {['font-size'] = '13px', ['color'] = 'white', ['background-color'] = 'blue'})\n    lu.assertEquals(candidate(';'), {})\n    lu.assertEquals(candidate('color: #f00'), {['color'] = '#f00'})\n    lu.assertEquals(candidate('margin-top:10px; margin-right:10px; margin-bottom:10px; margin-left:10px;'), {['margin-top'] = '10px', ['margin-right'] = '10px', ['margin-bottom'] = '10px', ['margin-left'] = '10px'})\n    lu.assertEquals(candidate('color: #ff0000'), {['color'] = '#ff0000'})\n    lu.assertEquals(candidate('color: red; margin: 5px; font-size: 20px;;'), {['color'] = 'red', ['margin'] = '5px', ['font-size'] = '20px'})\n    lu.assertEquals(candidate(''), {})\n    lu.assertEquals(candidate('margin-top:10px; padding:20px'), {['margin-top'] = '10px', ['padding'] = '20px'})\n    lu.assertEquals(candidate('color: #ff0000;'), {['color'] = '#ff0000'})\n    lu.assertEquals(candidate('color: red;'), {['color'] = 'red'})\n    lu.assertEquals(candidate(' ; '), {})\n    lu.assertEquals(candidate('font-size: 123; color: #fff;'), {['font-size'] = '123', ['color'] = '#fff'})\n    lu.assertEquals(candidate(';'), {})\n    lu.assertEquals(candidate('font-size: 123; font-size: 456;'), {['font-size'] = '456'})\n    lu.assertEquals(candidate('foo;bar'), {})\n    lu.assertEquals(candidate('border:1px solid blue; font-size:18pt'), {['border'] = '1px solid blue', ['font-size'] = '18pt'})\n    lu.assertEquals(candidate('color: red; font-size: 12; display: block; display: inline-block'), {['color'] = 'red', ['font-size'] = '12', ['display'] = 'inline-block'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_14777_sqlite3_quote_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # XXX Could omit quotes in some cases, but safer this way.\n-- # return '\"' + name.replace('\"', '\"\"') + '\"'\n-- \n-- Quote `name` as a SQL identifier, e.g. a table or column name.\n-- Do NOT use this for strings, e.g. inserting data into a table.\n-- Use query parameters instead.\nlocal function sqlite3_quote_name(name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_14777_sqlite3_quote_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sqlite3_quote_name\n    lu.assertEquals(candidate('a\\rb\\nc'), '\"a\\rb\\nc\"')\n    lu.assertEquals(candidate('a.b.c'), '\"a.b.c\"')\n    lu.assertEquals(candidate(''), '\"\"')\n    lu.assertEquals(candidate('a1'), '\"a1\"')\n    lu.assertEquals(candidate('ABC'), '\"ABC\"')\n    lu.assertEquals(candidate('a'), '\"a\"')\n    lu.assertEquals(candidate('a\"b.c.d'), '\"a\"\"b.c.d\"')\n    lu.assertEquals(candidate('a.b.c\"d'), '\"a.b.c\"\"d\"')\n    lu.assertEquals(candidate('a.b\"c.d'), '\"a.b\"\"c.d\"')\n    lu.assertEquals(candidate('a.b.c.d'), '\"a.b.c.d\"')\n    lu.assertEquals(candidate('a.b'), '\"a.b\"')\n    lu.assertEquals(candidate('a b\"c d'), '\"a b\"\"c d\"')\n    lu.assertEquals(candidate('a.b.c.d\"'), '\"a.b.c.d\"\"\"')\n    lu.assertEquals(candidate('abc\"def'), '\"abc\"\"def\"')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_148003_Name_Validation", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for Char in Name:\n-- #     if (\"A\" <= Char <= \"Z\" or \"a\" <= Char <= \"z\"\n-- #             or Char == \"-\" or Char == \"'\"):\n-- #         continue\n-- #     else:\n-- #         return False\n-- # return True\n-- \n--  Function to Validate a Name for Input: Allowing Spaces, - and '\nlocal function Name_Validation(Name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_148003_Name_Validation.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = Name_Validation\n    lu.assertEquals(candidate('Anne-Marie'), true)\n    lu.assertEquals(candidate('Sabrina'), true)\n    lu.assertEquals(candidate('<NAME>'), false)\n    lu.assertEquals(candidate('Anne Marie'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_148266_get_env", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # try:\n-- #     # test if our custom arg is present and delete it so that setuptools doesn't\n-- #     # throw an error\n-- #     args.remove(\"pypi-dev\")\n-- #     return \"dev\"\n-- # except ValueError:\n-- #     return \"prod\"\n-- \n--  If args contain `pypi-dev`, the package name will be `knackpy-dev`. Else the\n-- package name will be Knackpy.\nlocal function get_env(args)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_148266_get_env.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_env\n    lu.assertEquals(candidate({'pypi-dev'}), 'dev')\n    lu.assertEquals(candidate({}), 'prod')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_148854_toHex", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # hex = \"0123456789abcdef\"\n-- # return hex[int(val / 16)] + hex[int(val - int(val / 16) * 16)]\n-- \n-- Converts the given value (0-255) into its hexadecimal representation\nlocal function toHex(val)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_148854_toHex.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = toHex\n    lu.assertEquals(candidate(128), '80')\n    lu.assertEquals(candidate(1), '01')\n    lu.assertEquals(candidate(47), '2f')\n    lu.assertEquals(candidate(167), 'a7')\n    lu.assertEquals(candidate(17), '11')\n    lu.assertEquals(candidate(5), '05')\n    lu.assertEquals(candidate(10), '0a')\n    lu.assertEquals(candidate(118), '76')\n    lu.assertEquals(candidate(20), '14')\n    lu.assertEquals(candidate(16), '10')\n    lu.assertEquals(candidate(23), '17')\n    lu.assertEquals(candidate(15), '0f')\n    lu.assertEquals(candidate(2), '02')\n    lu.assertEquals(candidate(12), '0c')\n    lu.assertEquals(candidate(254), 'fe')\n    lu.assertEquals(candidate(0), '00')\n    lu.assertEquals(candidate(255), 'ff')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_14894_copy_dict", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # result = dict(source_dict)\n-- # result.update(diffs)\n-- # return result\n-- \n-- Returns a copy of source_dict, updated with the new key-value pairs in diffs.\nlocal function copy_dict(source_dict, diffs)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_14894_copy_dict.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = copy_dict\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2}, {['b'] = 2}), {['a'] = 1, ['b'] = 2})\n    lu.assertEquals(candidate({['x'] = 1, ['y'] = 2, ['z'] = 3}, {['x'] = 1, ['y'] = 2, ['z'] = 3}), {['x'] = 1, ['y'] = 2, ['z'] = 3})\n    lu.assertEquals(candidate(dict(), {}), {})\n    lu.assertEquals(candidate({}, {['a'] = 1, ['b'] = 2, ['c'] = 3}), {['a'] = 1, ['b'] = 2, ['c'] = 3})\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2}, {['a'] = 2, ['b'] = 2}), {['a'] = 2, ['b'] = 2})\n    lu.assertEquals(candidate({['x'] = 1, ['y'] = 2, ['z'] = 3}, {}), {['x'] = 1, ['y'] = 2, ['z'] = 3})\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2}, {['b'] = 2, ['c'] = 3, ['d'] = 4}), {['a'] = 1, ['b'] = 2, ['c'] = 3, ['d'] = 4})\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2}, {['a'] = 2, ['b'] = 2, ['c'] = 3, ['d'] = 4}), {['a'] = 2, ['b'] = 2, ['c'] = 3, ['d'] = 4})\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2}, {['b'] = 2, ['c'] = 3}), {['a'] = 1, ['b'] = 2, ['c'] = 3})\n    lu.assertEquals(candidate({['x'] = 1, ['y'] = 2, ['z'] = 3}, {['a'] = 1, ['b'] = 2, ['c'] = 3}), {['a'] = 1, ['b'] = 2, ['c'] = 3, ['x'] = 1, ['y'] = 2, ['z'] = 3})\n    lu.assertEquals(candidate({['x'] = 1, ['y'] = 2, ['z'] = 3}, {['w'] = 7, ['x'] = 8}), {['w'] = 7, ['x'] = 8, ['y'] = 2, ['z'] = 3})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_149266_ascii_replace", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # text = text.replace(\"'\", \"&#39;\")\n-- # text = text.replace(\"[&#39;\", \"['\")\n-- # text = text.replace(\"&#39;]\", \"']\")\n-- # return text\n-- \n-- replace quotes with their ASCII character representation\n-- Args:\n--     text (str): text to replace in\n-- Returns:\n--     str: replaced text\nlocal function ascii_replace(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_149266_ascii_replace.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = ascii_replace\n    lu.assertEquals(candidate('[This] [is a] [test].'), '[This] [is a] [test].')\n    lu.assertEquals(candidate('This is [a] test'), 'This is [a] test')\n    lu.assertEquals(candidate('This [is] a test.'), 'This [is] a test.')\n    lu.assertEquals(candidate(\"Bob's favorite language is [&#39;]Python\"), \"Bob&#39;s favorite language is [']Python\")\n    lu.assertEquals(candidate('I love to ride my bike.'), 'I love to ride my bike.')\n    lu.assertEquals(candidate('This is [a] [test]'), 'This is [a] [test]')\n    lu.assertEquals(candidate('[This] is [a] [test].'), '[This] is [a] [test].')\n    lu.assertEquals(candidate('This is a test]'), 'This is a test]')\n    lu.assertEquals(candidate(\"''\"), '&#39;&#39;')\n    lu.assertEquals(candidate('This is a test.'), 'This is a test.')\n    lu.assertEquals(candidate('&#39;'), '&#39;')\n    lu.assertEquals(candidate('[This is a test].'), '[This is a test].')\n    lu.assertEquals(candidate(\"There's no place like home, home is where the heart is.\"), 'There&#39;s no place like home, home is where the heart is.')\n    lu.assertEquals(candidate('[This] is a test.'), '[This] is a test.')\n    lu.assertEquals(candidate(\"Hi, my name is 'Bob'\"), 'Hi, my name is &#39;Bob&#39;')\n    lu.assertEquals(candidate('This is [a test'), 'This is [a test')\n    lu.assertEquals(candidate('This is a [test]'), 'This is a [test]')\n    lu.assertEquals(candidate('I&#39;m a programmer'), candidate(\"I'm a programmer\"))\n    lu.assertEquals(candidate(\"this should be a simple quote: '&#39;\"), 'this should be a simple quote: &#39;&#39;')\n    lu.assertEquals(candidate(\"this should be a simple quote: '\"), 'this should be a simple quote: &#39;')\n    lu.assertEquals(candidate('This is [a] test.'), 'This is [a] test.')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_149402_obtenerBinario", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return bin(numero)[2:].zfill(8)\n-- \n-- bin(numero) obtiene el valor binario de numero\n-- [2:] obtiene los elementos de del binario anterior excepto los primeros 2, por ejemplo 11000000[2:] regresa 000000\n-- zfill(8) rellena con ceros a la izquiera el valor anterior hasta que este tenga longitud 8, por ejemplo 111111 regresa 00111111\nlocal function obtenerBinario(numero)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_149402_obtenerBinario.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = obtenerBinario\n    lu.assertEquals(candidate(12), '00001100')\n    lu.assertEquals(candidate(0), '00000000')\n    lu.assertEquals(candidate(2), '00000010')\n    lu.assertEquals(candidate(128), '10000000')\n    lu.assertEquals(candidate(2048), '100000000000')\n    lu.assertEquals(candidate(21), '00010101')\n    lu.assertEquals(candidate(64), '01000000')\n    lu.assertEquals(candidate(19), '00010011')\n    lu.assertEquals(candidate(15), '00001111')\n    lu.assertEquals(candidate(8), '00001000')\n    lu.assertEquals(candidate(7), '00000111')\n    lu.assertEquals(candidate(19), '00010011')\n    lu.assertEquals(candidate(128), '10000000')\n    lu.assertEquals(candidate(6), '00000110')\n    lu.assertEquals(candidate(0), '00000000')\n    lu.assertEquals(candidate(9), '00001001')\n    lu.assertEquals(candidate(18), '00010010')\n    lu.assertEquals(candidate(9), '00001001')\n    lu.assertEquals(candidate(187), '10111011')\n    lu.assertEquals(candidate(3), '00000011')\n    lu.assertEquals(candidate(1024), '10000000000')\n    lu.assertEquals(candidate(14), '00001110')\n    lu.assertEquals(candidate(2), '00000010')\n    lu.assertEquals(candidate(12), '00001100')\n    lu.assertEquals(candidate(13), '00001101')\n    lu.assertEquals(candidate(1), '00000001')\n    lu.assertEquals(candidate(0), '00000000')\n    lu.assertEquals(candidate(17), '00010001')\n    lu.assertEquals(candidate(6), '00000110')\n    lu.assertEquals(candidate(7), '00000111')\n    lu.assertEquals(candidate(17), '00010001')\n    lu.assertEquals(candidate(18), '00010010')\n    lu.assertEquals(candidate(16), '00010000')\n    lu.assertEquals(candidate(64), '01000000')\n    lu.assertEquals(candidate(5), '00000101')\n    lu.assertEquals(candidate(10), '00001010')\n    lu.assertEquals(candidate(2), '00000010')\n    lu.assertEquals(candidate(16), '00010000')\n    lu.assertEquals(candidate(15), '00001111')\n    lu.assertEquals(candidate(4), '00000100')\n    lu.assertEquals(candidate(4), '00000100')\n    lu.assertEquals(candidate(14), '00001110')\n    lu.assertEquals(candidate(1), '00000001')\n    lu.assertEquals(candidate(32), '00100000')\n    lu.assertEquals(candidate(4), '00000100')\n    lu.assertEquals(candidate(5), '00000101')\n    lu.assertEquals(candidate(8), '00001000')\n    lu.assertEquals(candidate(128), '10000000')\n    lu.assertEquals(candidate(3), '00000011')\n    lu.assertEquals(candidate(1), '00000001')\n    lu.assertEquals(candidate(10), '00001010')\n    lu.assertEquals(candidate(20), '00010100')\n    lu.assertEquals(candidate(170), '10101010')\n    lu.assertEquals(candidate(8), '00001000')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_149428_squared_loss", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return (x1 - x2) ** 2\n-- \n-- Returns the squared difference between two numbers\nlocal function squared_loss(x1, x2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_149428_squared_loss.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = squared_loss\n    lu.assertEquals(candidate(0, 0), 0)\n    lu.assertEquals(candidate(-1, -2), 1)\n    lu.assertEquals(candidate(3, 3), 0)\n    lu.assertEquals(candidate(5, 2), 9)\n    lu.assertEquals(candidate(5, 5), 0)\n    lu.assertEquals(candidate(1, 1), 0)\n    lu.assertEquals(candidate(1, 2), 1)\n    lu.assertEquals(candidate(5, 3), 4)\n    lu.assertEquals(candidate(4, 3), 1)\n    lu.assertEquals(candidate(10, 5), 25)\n    lu.assertEquals(candidate(9, 0), 81)\n    lu.assertEquals(candidate(3, 5), 4)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_149915_next_enum_variation", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for i in range(len(enums)):\n-- #     current = enum_indices[i]\n-- #     # if current digit has room, increment it.\n-- #     if current + 1 < len(enums[i][1]):\n-- #         enum_indices[i] = current + 1\n-- #         return True\n-- #     # otherwise reset it to 0 and carry to the next digit.\n-- #     enum_indices[i] = 0\n-- # # if this is reached, the number has overflowed and the loop is finished.\n-- # return False\n-- \n-- Loop through indices from [0, 0, ...] to [L0-1, L1-1, ...]\n-- where Li is len(enums[i]).  The list can be thought of as a number with many\n-- digits, where each digit is in [0, Li), and this function effectively implements\n-- the increment operation, with the least-significant digit being the first item.\nlocal function next_enum_variation(enums, enum_indices)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_149915_next_enum_variation.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = next_enum_variation\n    lu.assertEquals(candidate({{'a', {'a', 'b', 'c'}}, {'b', {'x', 'y', 'z'}}}, {0, 0}), true)\n    lu.assertEquals(candidate({{'a', {'a', 'b', 'c'}}, {'b', {'x', 'y', 'z'}}}, {2, 1}), true)\n    lu.assertEquals(candidate({{'a', {'a', 'b', 'c'}}, {'b', {'x', 'y', 'z'}}}, {2, 0}), true)\n    lu.assertEquals(candidate({{'A', {'a', 'b'}}, {'B', {'c', 'd'}}}, {1, 0}), true)\n    lu.assertEquals(candidate({{'A', {'a', 'b'}}, {'B', {'c'}}}, {0, 0}), true)\n    lu.assertEquals(candidate({{'a', {'a', 'b', 'c'}}, {'b', {'x', 'y', 'z'}}}, {1, 1}), true)\n    lu.assertEquals(candidate({{'a', {'b', 'c'}}, {'d', {'e', 'f'}}}, {1, 2}), false)\n    lu.assertEquals(candidate({{'a', {'b', 'c'}}, {'d', {'e', 'f'}}}, {0, 0}), true)\n    lu.assertEquals(candidate({{'A', {'a', 'b'}}, {'B', {'c'}}}, {1, 0}), false)\n    lu.assertEquals(candidate({{'a', {'a', 'b', 'c'}}, {'b', {'x', 'y', 'z'}}}, {0, 1}), true)\n    lu.assertEquals(candidate({{'a', {'b', 'c'}}, {'d', {'e', 'f'}}}, {1, 0}), true)\n    lu.assertEquals(candidate({{'a', {'a', 'b', 'c'}}, {'b', {'x', 'y', 'z'}}}, {1, 0}), true)\n    lu.assertEquals(candidate({{'a', {'b', 'c'}}, {'d', {'e', 'f'}}}, {2, 1}), false)\n    lu.assertEquals(candidate({{'a', {'b', 'c'}}, {'d', {'e', 'f'}}}, {0, 1}), true)\n    lu.assertEquals(candidate({{'A', {'a', 'b'}}, {'B', {'c', 'd'}}}, {0, 0}), true)\n    lu.assertEquals(candidate({{'a', {'b', 'c'}}, {'d', {'e', 'f'}}}, {1, 1}), false)\n    lu.assertEquals(candidate({{'A', {'a', 'b'}}, {'B', {'c', 'd'}}, {'C', {'e', 'f'}}}, {0, 0, 0}), true)\n    lu.assertEquals(candidate({{'A', {'a', 'b'}}, {'B', {'c', 'd'}}, {'C', {'e', 'f'}}}, {1, 0, 0}), true)\n    lu.assertEquals(candidate({{'a', {'b', 'c'}}, {'d', {'e', 'f'}}}, {2, 2}), false)\n    lu.assertEquals(candidate({{'a', {'a', 'b', 'c'}}, {'b', {'x', 'y', 'z'}}}, {1, 2}), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_149954_to_lutron_level", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return int((level * 100) // 255)\n-- \n-- Convert the given Home Assistant light level (0-255) to Lutron (0-100).\nlocal function to_lutron_level(level)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_149954_to_lutron_level.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = to_lutron_level\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(254), 99)\n    lu.assertEquals(candidate(255), 100)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_150881_position", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # value, tile = 0, -1\n-- # # Retrieve the largest value.\n-- # for row in b:\n-- #     for cell in row:\n-- #         if cell > tile:\n-- #             tile = cell\n-- # # Check if the tile is in the desired location.\n-- # if b[0][0] == tile:\n-- #     value += (1024 * tile)\n-- # return value\n-- \n-- Return a heuristic value based on the position of the largest value on the board.\nlocal function position(b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_150881_position.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = position\n    lu.assertEquals(candidate({{0, 0, 0, 0, 0}, {0, 0, 0, 0, 0}, {0, 0, 0, 0, 0}, {0, 0, 0, 0, 0}, {0, 0, 0, 0, 0}}), 0)\n    lu.assertEquals(candidate({{3, 5, 7}, {3, 5, 7}}), 0)\n    lu.assertEquals(candidate({{1, 0, 3}, {4, 5, 6}, {7, 8, 9}}), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_152130_parse_list", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # val = (\n-- #     val\n-- #     .replace(\"[\", \"\").replace(\"]\", \"\")\n-- #     .replace(\"(\", \"\").replace(\")\", \"\")\n-- #     .replace(\"{\", \"\").replace(\"}\", \"\")\n-- #     .strip()\n-- #     .lower())\n-- # if \",\" not in val:\n-- #     val = \",\".join(val.split())\n-- # val = \"[{0}]\".format(val)\n-- # return val\n-- \n--  Parse a list of input strings \nlocal function parse_list(val)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_152130_parse_list.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_list\n    lu.assertEquals(candidate(' 1, 2, 3'), '[1, 2, 3]')\n    lu.assertEquals(candidate('  1, 2, 3  '), '[1, 2, 3]')\n    lu.assertEquals(candidate(''), '[]')\n    lu.assertEquals(candidate('    '), '[]')\n    lu.assertEquals(candidate('1, 2, 3'), '[1, 2, 3]')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_154389_roman_to_int", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # expr = expr.upper()\n-- # nums = {'M': 1000, 'D': 500, 'C': 100, 'L': 50, 'X': 10, 'V': 5, 'I': 1}\n-- # sum = 0\n-- # for i in range(len(expr)):\n-- #     try:\n-- #         value = nums[expr[i]]\n-- #         # If the next place holds a larger number, this value is negative\n-- #         if i+1 < len(expr) and nums[expr[i+1]] > value:\n-- #             sum -= value\n-- #         else:\n-- #             sum += value\n-- #     except KeyError:\n-- #         raise ValueError(f'expr is not a valid Roman numeral:{expr}')\n-- # return sum\n-- \n--  Convert a Roman numeral to an integer.\n-- Adopted from https://www.oreilly.com/library/view/python-cookbook/0596001673/ch03s24.html\nlocal function roman_to_int(expr)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_154389_roman_to_int.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = roman_to_int\n    lu.assertEquals(candidate('MMXXIV'), 2024)\n    lu.assertEquals(candidate('L'), 50)\n    lu.assertEquals(candidate('MCMXCIV'), 1994)\n    lu.assertEquals(candidate('MCMLXXXIX'), 1989)\n    lu.assertEquals(candidate('IIII'), 4)\n    lu.assertEquals(candidate('IX'), 9)\n    lu.assertEquals(candidate('MCMLXXXIV'), 1984)\n    lu.assertEquals(candidate('V'), 5)\n    lu.assertEquals(candidate('I'), 1)\n    lu.assertEquals(candidate('XXXV'), 35)\n    lu.assertEquals(candidate('C'), 100)\n    lu.assertEquals(candidate('X'), 10)\n    lu.assertEquals(candidate('MMMM'), 4000)\n    lu.assertEquals(candidate('LVIII'), 58)\n    lu.assertEquals(candidate('D'), 500)\n    lu.assertEquals(candidate('II'), 2)\n    lu.assertEquals(candidate('CLXVI'), 166)\n    lu.assertEquals(candidate('M'), 1000)\n    lu.assertEquals(candidate('IV'), 4)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_154778__clamp_transpose", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if transpose_amount < 0:\n-- #     transpose_amount = -min(ns_min_pitch - min_allowed_pitch,\n-- #                             abs(transpose_amount))\n-- # else:\n-- #     transpose_amount = min(max_allowed_pitch - ns_max_pitch, transpose_amount)\n-- # return transpose_amount\n-- \n-- Clamps the specified transpose amount to keep a ns in the desired bounds.\n-- Args:\n--   transpose_amount: Number of steps to transpose up or down.\n--   ns_min_pitch: The lowest pitch in the target note sequence.\n--   ns_max_pitch: The highest pitch in the target note sequence.\n--   min_allowed_pitch: The lowest pitch that should be allowed in the transposed\n--     note sequence.\n--   max_allowed_pitch: The highest pitch that should be allowed in the\n--     transposed note sequence.\n-- Returns:\n--   A new transpose amount that, if applied to the target note sequence, will\n--   keep all notes within the range [MIN_PITCH, MAX_PITCH]\nlocal function _clamp_transpose(transpose_amount, ns_min_pitch, ns_max_pitch, min_allowed_pitch, max_allowed_pitch)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_154778__clamp_transpose.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _clamp_transpose\n    lu.assertEquals(candidate(1, 24, 25, 24, 26), 1)\n    lu.assertEquals(candidate(1, 24, 25, 21, 26), 1)\n    lu.assertEquals(candidate(1, 24, 25, 23, 26), 1)\n    lu.assertEquals(candidate(0, 21, 108, 0, 127), 0)\n    lu.assertEquals(candidate(0, 21, 108, 0, 108), 0)\n    lu.assertEquals(candidate(1, 24, 25, 22, 26), 1)\n    lu.assertEquals(candidate(1, 24, 25, 20, 26), 1)\n    lu.assertEquals(candidate(1, 24, 25, 25, 26), 1)\n    lu.assertEquals(candidate(1, 24, 25, 20, 24), -1)\n    lu.assertEquals(candidate(1, 24, 25, 25, 24), -1)\n    lu.assertEquals(candidate(1, 21, 108, 0, 127), 1)\n    lu.assertEquals(candidate(1, 24, 25, 21, 24), -1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_155131_align_up", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return (v + unit_size - 1) // unit_size * unit_size\n-- \n-- Align the input variable with unit of sizes. The aligned data will always\n-- be larger than the inputs.\n-- Args:\n--     v: the variable to be aligned.\n--     unit_size: the block size of the aligned data.\n-- Return:\n--     aligned variable.\nlocal function align_up(v, unit_size)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_155131_align_up.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = align_up\n    lu.assertEquals(candidate(5, 4), 8)\n    lu.assertEquals(candidate(8, 8), 8)\n    lu.assertEquals(candidate(1024), 1024)\n    lu.assertEquals(candidate(1, 4), 4)\n    lu.assertEquals(candidate(3), 4)\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(1, 2), 2)\n    lu.assertEquals(candidate(1023), 1024)\n    lu.assertEquals(candidate(2, 3), 3)\n    lu.assertEquals(candidate(8), 8)\n    lu.assertEquals(candidate(7, 4), 8)\n    lu.assertEquals(candidate(3, 8), 8)\n    lu.assertEquals(candidate(1023, 512), 1024)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(2, 2), 2)\n    lu.assertEquals(candidate(3, 4), 4)\n    lu.assertEquals(candidate(-1, 512), 0)\n    lu.assertEquals(candidate(1024, 512), 1024)\n    lu.assertEquals(candidate(6), 6)\n    lu.assertEquals(candidate(-1), 0)\n    lu.assertEquals(candidate(1), 2)\n    lu.assertEquals(candidate(4, 4), 4)\n    lu.assertEquals(candidate(5, 16), 16)\n    lu.assertEquals(candidate(7), 8)\n    lu.assertEquals(candidate(11), 12)\n    lu.assertEquals(candidate(0, 512), 0)\n    lu.assertEquals(candidate(6, 4), 8)\n    lu.assertEquals(candidate(8, 16), 16)\n    lu.assertEquals(candidate(5), 6)\n    lu.assertEquals(candidate(4), 4)\n    lu.assertEquals(candidate(7, 128), 128)\n    lu.assertEquals(candidate(9), 10)\n    lu.assertEquals(candidate(10), 10)\n    lu.assertEquals(candidate(1, 3), 3)\n    lu.assertEquals(candidate(3, 2), 4)\n    lu.assertEquals(candidate(2, 4), 4)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_155421_get_bandwidth", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return data * 8/duration\n-- \n--  Module to determine the bandwidth for a segment\n-- download\nlocal function get_bandwidth(data, duration)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_155421_get_bandwidth.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_bandwidth\n    lu.assertEquals(candidate(0, 1), 0)\n    lu.assertEquals(candidate(0, 2), 0)\n    lu.assertEquals(candidate(0, 3), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_155975_get_false_positive", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if false_p:\n-- #     return True\n-- # else:\n-- #     return False\n-- \n--     Returns True, False for false positive as per DefectDojo standards.\n-- :param false_p:\n-- :return:\nlocal function get_false_positive(false_p)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_155975_get_false_positive.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_false_positive\n    lu.assertEquals(candidate(true), true)\n    lu.assertEquals(candidate(false), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_15619_is_anagram_0", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if s1 is None or s2 is None:\n-- #     return False\n-- # if len(s1) != len(s2):\n-- #     return False\n-- # s1_list = list(s1)\n-- # s2_list = list(s2)\n-- # for i in range((len(s1_list))):\n-- #     # print(\"{0}, {1}\".format(s1_list[i], s2_list[-i]))\n-- #     if s1_list[i] != s2_list[len(s2_list)-1-i]:\n-- #         return False\n-- # return True\n-- \n--     This is my first solution, and it's incorrect because this method checks palindrome, not anagram.\nlocal function is_anagram_0(s1, s2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_15619_is_anagram_0.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_anagram_0\n    lu.assertEquals(candidate('dog', 'dog'), false)\n    lu.assertEquals(candidate('', ' '), false)\n    lu.assertEquals(candidate(' ', ''), false)\n    lu.assertEquals(candidate('ab', 'ba'), true)\n    lu.assertEquals(candidate('a', 'ab'), false)\n    lu.assertEquals(candidate('aba', 'aba'), true)\n    lu.assertEquals(candidate('listen', 'silent'), false)\n    lu.assertEquals(candidate('abc', ''), false)\n    lu.assertEquals(candidate('a', None), false)\n    lu.assertEquals(candidate('abcd', 'badc'), false)\n    lu.assertEquals(candidate('a', 'a'), true)\n    lu.assertEquals(candidate(None, None), false)\n    lu.assertEquals(candidate('ab', None), false)\n    lu.assertEquals(candidate('ab', 'a'), false)\n    lu.assertEquals(candidate('', 'a'), false)\n    lu.assertEquals(candidate('abc', 'def'), false)\n    lu.assertEquals(candidate('abcd', 'dbac'), false)\n    lu.assertEquals(candidate('ab', 'ac'), false)\n    lu.assertEquals(candidate('  d', 'd  '), true)\n    lu.assertEquals(candidate(None, 'ab'), false)\n    lu.assertEquals(candidate('dog', 'god'), true)\n    lu.assertEquals(candidate('aabbcc', 'bbcca'), false)\n    lu.assertEquals(candidate('dog', ''), false)\n    lu.assertEquals(candidate('ab', 'bb'), false)\n    lu.assertEquals(candidate('a', ''), false)\n    lu.assertEquals(candidate('aabb', 'bbaa'), true)\n    lu.assertEquals(candidate('a b', 'b a'), true)\n    lu.assertEquals(candidate('a', 'b'), false)\n    lu.assertEquals(candidate(None, 'a'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_156273__extract_param", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if arg.startswith(name):\n-- #     param = arg[len(name):]\n-- #     if param == '':\n-- #         return ''\n-- #     if param[0] != '=':\n-- #         return None\n-- #     return param[1:]\n-- # return None\n-- \n-- Extract a parameter in the style of \"key=value\".\n-- Return `''` if the `arg == name`,\n--        :obj:`None` if the key does not match name,\n--        value otherwise (might be `''`).\nlocal function _extract_param(arg, name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_156273__extract_param.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _extract_param\n    lu.assertEquals(candidate('notkey=value', 'key'), None)\n    lu.assertEquals(candidate('key=value', 'notkey'), None)\n    lu.assertEquals(candidate('', 'a'), None)\n    lu.assertEquals(candidate('a=', 'a'), '')\n    lu.assertEquals(candidate('someother=', 'someother'), '')\n    lu.assertEquals(candidate('a=b', 'a=c'), None)\n    lu.assertEquals(candidate('a=b', 'c='), None)\n    lu.assertEquals(candidate('some=value', 'some'), 'value')\n    lu.assertEquals(candidate('key=value', 'key'), 'value')\n    lu.assertEquals(candidate('a=b', ''), None)\n    lu.assertEquals(candidate('=a', 'a'), None)\n    lu.assertEquals(candidate('a', ''), None)\n    lu.assertEquals(candidate('key=value=more', 'key'), 'value=more')\n    lu.assertEquals(candidate('key', 'key'), '')\n    lu.assertEquals(candidate('a=b', 'c'), None)\n    lu.assertEquals(candidate('=a', '=a'), '')\n    lu.assertEquals(candidate('a=b', 'a'), 'b')\n    lu.assertEquals(candidate('a=b', 'a='), None)\n    lu.assertEquals(candidate('a=', 'a='), '')\n    lu.assertEquals(candidate('', 'key'), None)\n    lu.assertEquals(candidate('some=', 'some'), '')\n    lu.assertEquals(candidate('key=', 'key'), '')\n    lu.assertEquals(candidate('someother=value', 'someother'), 'value')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_156491_numbits", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if num_values == 0:\n-- #     return 0\n-- # if num_values == 1:\n-- #     return 1\n-- # return (num_values - 1).bit_length()\n-- \n-- Gets the minimum number of bits required to encode given number of different values.\n-- This method implements zserio built-in operator numBits.\n-- :param num_values: The number of different values from which to calculate number of bits.\n-- :returns: Number of bits required to encode num_values different values.\nlocal function numbits(num_values)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_156491_numbits.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = numbits\n    lu.assertEquals(candidate(26), 5)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(12), 4)\n    lu.assertEquals(candidate(25), 5)\n    lu.assertEquals(candidate(23), 5)\n    lu.assertEquals(candidate(33), 6)\n    lu.assertEquals(candidate(9), 4)\n    lu.assertEquals(candidate(31), 5)\n    lu.assertEquals(candidate(14), 4)\n    lu.assertEquals(candidate(10), 4)\n    lu.assertEquals(candidate(7), 3)\n    lu.assertEquals(candidate(15), 4)\n    lu.assertEquals(candidate(35), 6)\n    lu.assertEquals(candidate(19), 5)\n    lu.assertEquals(candidate(21), 5)\n    lu.assertEquals(candidate(8), 3)\n    lu.assertEquals(candidate(3), 2)\n    lu.assertEquals(candidate(24), 5)\n    lu.assertEquals(candidate(22), 5)\n    lu.assertEquals(candidate(11), 4)\n    lu.assertEquals(candidate(30), 5)\n    lu.assertEquals(candidate(34), 6)\n    lu.assertEquals(candidate(20), 5)\n    lu.assertEquals(candidate(36), 6)\n    lu.assertEquals(candidate(4), 2)\n    lu.assertEquals(candidate(18), 5)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(27), 5)\n    lu.assertEquals(candidate(2), 1)\n    lu.assertEquals(candidate(28), 5)\n    lu.assertEquals(candidate(29), 5)\n    lu.assertEquals(candidate(6), 3)\n    lu.assertEquals(candidate(17), 5)\n    lu.assertEquals(candidate(5), 3)\n    lu.assertEquals(candidate(13), 4)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_157050_takemod", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # mod = int(int(nvols) % 2)\n-- # if mod == 1:\n-- #     return 0\n-- # else:\n-- #     return 1\n-- \n-- Determine if the input is odd or even values and \n-- return a of 0 and 1 depending on the truth value\n-- Parameters\n-- ----------\n-- nvols : int\n-- Returns\n-- -------\n-- decisions : int\nlocal function takemod(nvols)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_157050_takemod.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = takemod\n    lu.assertEquals(candidate(1), 0)\n    lu.assertEquals(candidate(2), 1)\n    lu.assertEquals(candidate(3), 0)\n    lu.assertEquals(candidate(5), 0)\n    lu.assertEquals(candidate(8), 1)\n    lu.assertEquals(candidate(6), 1)\n    lu.assertEquals(candidate(4), 1)\n    lu.assertEquals(candidate(7), 0)\n    lu.assertEquals(candidate(200), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_157096_unquote", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return tag.split('}').pop()\n-- \n-- Remove namespace from prefixed tag.\n-- See: [Python issue 18304](https://bugs.python.org/issue18304)\n-- Arguments:\n--     tag {str} -- (possibly-)namespaced tag\n-- Returns:\n--     str -- tag name without namespace\nlocal function unquote(tag)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_157096_unquote.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = unquote\n    lu.assertEquals(candidate('{http://www.w3.org/1999/02/22-rdf-syntax-ns#}Description'), 'Description')\n    lu.assertEquals(candidate('{http://www.w3.org/1999/xhtml}html'), 'html')\n    lu.assertEquals(candidate('{http://www.w3.org/2000/01/rdf-schema#}subClassOf'), 'subClassOf')\n    lu.assertEquals(candidate('{http://www.w3.org/2000/01/rdf-schema#}domain'), 'domain')\n    lu.assertEquals(candidate('{http://www.w3.org/1999/02/22-rdf-syntax-ns#}li'), 'li')\n    lu.assertEquals(candidate('{http://purl.org/dc/elements/1.1/}rights'), 'rights')\n    lu.assertEquals(candidate('bar'), 'bar')\n    lu.assertEquals(candidate('html'), 'html')\n    lu.assertEquals(candidate('{http://www.w3.org/2000/01/rdf-schema#}label'), 'label')\n    lu.assertEquals(candidate('{http://www.example.com}bar'), 'bar')\n    lu.assertEquals(candidate('{http://www.w3.org/1999/xhtml}p'), 'p')\n    lu.assertEquals(candidate('{http://www.w3.org/1999/02/22-rdf-syntax-ns#}Bag'), 'Bag')\n    lu.assertEquals(candidate('{http://www.w3.org/2000/01/rdf-schema#}subPropertyOf'), 'subPropertyOf')\n    lu.assertEquals(candidate('p'), 'p')\n    lu.assertEquals(candidate('{http://schemas.opengis.net/kml/2.2}MultiGeometry'), 'MultiGeometry')\n    lu.assertEquals(candidate('{http://www.w3.org/XML/1998/namespace}lang'), 'lang')\n    lu.assertEquals(candidate('{http://www.w3.org/1999/02/22-rdf-syntax-ns#}RDF'), 'RDF')\n    lu.assertEquals(candidate('foo bar'), 'foo bar')\n    lu.assertEquals(candidate('{http://www.w3.org/2000/01/rdf-schema#}seeAlso'), 'seeAlso')\n    lu.assertEquals(candidate('{http://www.w3.org/2000/01/rdf-schema#}isDefinedBy'), 'isDefinedBy')\n    lu.assertEquals(candidate('{http://www.w3.org/2000/01/rdf-schema#}range'), 'range')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_157569_queen_constraint", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return A == B or (a != b and A + a != B + b and A - a != B - b)\n-- \n-- Constraint is satisfied (true) if A, B are really the same variable,\n-- or if they are not in the same row, down diagonal, or up diagonal.\nlocal function queen_constraint(A, a, B, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_157569_queen_constraint.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = queen_constraint\n    lu.assertEquals(candidate(1, 1, 3, 1), false)\n    lu.assertEquals(candidate(1, 1, 2, 2), false)\n    lu.assertEquals(candidate(1, 2, 2, 3), false)\n    lu.assertEquals(candidate(0, 3, 1, 3), false)\n    lu.assertEquals(candidate(1, 3, 2, 2), false)\n    lu.assertEquals(candidate(0, 1, 1, 3), true)\n    lu.assertEquals(candidate(1, 2, 4, 2), false)\n    lu.assertEquals(candidate(1, 2, 2, 1), false)\n    lu.assertEquals(candidate(4, 4, 2, 4), false)\n    lu.assertEquals(candidate(1, 3, 3, 1), false)\n    lu.assertEquals(candidate(0, 1, 2, 3), false)\n    lu.assertEquals(candidate(1, 2, 2, 2), false)\n    lu.assertEquals(candidate(1, 1, 1, 1), true)\n    lu.assertEquals(candidate(1, 3, 3, 3), false)\n    lu.assertEquals(candidate(0, 1, 2, 1), false)\n    lu.assertEquals(candidate(1, 1, 2, 1), false)\n    lu.assertEquals(candidate(1, 3, 2, 3), false)\n    lu.assertEquals(candidate(1, 2, 3, 2), false)\n    lu.assertEquals(candidate(1, 3, 3, 2), true)\n    lu.assertEquals(candidate(1, 2, 1, 2), true)\n    lu.assertEquals(candidate(1, 2, 3, 3), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_158362_fix_typo", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return 'jednostkaAdministracyjna' if name == 'jednostkaAdmnistracyjna' else name\n-- \n-- Helper function. Fix typo in one of the tags\nlocal function fix_typo(name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_158362_fix_typo.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = fix_typo\n    lu.assertEquals(candidate('jednostkaAdmnistracyjna'), 'jednostkaAdministracyjna')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_158852_frames", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # seconds = minutes * 60\n-- # return seconds * fps\n-- \n-- Find the number of frames shown.\nlocal function frames(minutes, fps)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_158852_frames.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = frames\n    lu.assertEquals(candidate(1, 1), 60)\n    lu.assertEquals(candidate(1, 30), 1800)\n    lu.assertEquals(candidate(1, 5), 300)\n    lu.assertEquals(candidate(0, 60), 0)\n    lu.assertEquals(candidate(1, 60), 3600)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_158961_parse_custom_data", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # pair_list = []\n-- # for pair in custom_str.split(\",\"):\n-- #     pair_list.append(pair.split(\"|\"))\n-- # return pair_list\n-- \n-- Parse SCOUT_CUSTOM info field\n-- Input: \"key1|val1,key2|val2\"\n-- Output: [ [\"key1\",\"val1\"], [\"key2\", \"val2\"] ]\nlocal function parse_custom_data(custom_str)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_158961_parse_custom_data.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_custom_data\n    lu.assertEquals(candidate('key1|val1,key2|val2'), {{'key1', 'val1'}, {'key2', 'val2'}})\n    lu.assertEquals(candidate('key1|val1,key2|val2'), {{'key1', 'val1'}, {'key2', 'val2'}})\n    lu.assertEquals(candidate('key1|val1,key2|val2'), {{'key1', 'val1'}, {'key2', 'val2'}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_159043_rational_to_cfrac", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # out = []\n-- # while d != 0:\n-- #     i = n//d\n-- #     out.append(i)\n-- #     n, d = d, n-(d*i)\n-- # return out\n-- \n--     Terms of the simple continued fraction representation of n/d\nlocal function rational_to_cfrac(n, d)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_159043_rational_to_cfrac.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = rational_to_cfrac\n    lu.assertEquals(candidate(6, 4), {1, 2})\n    lu.assertEquals(candidate(2, 1), {2})\n    lu.assertEquals(candidate(0, 3), {0})\n    lu.assertEquals(candidate(1234567890, 1), {1234567890})\n    lu.assertEquals(candidate(3, 1), {3})\n    lu.assertEquals(candidate(1, 1), {1})\n    lu.assertEquals(candidate(2, 4), {0, 2})\n    lu.assertEquals(candidate(0, 1), {0})\n    lu.assertEquals(candidate(6, 2), {3})\n    lu.assertEquals(candidate(5, 5), {1})\n    lu.assertEquals(candidate(0, 1), {0})\n    lu.assertEquals(candidate(4, 4), {1})\n    lu.assertEquals(candidate(4, 3), {1, 3})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_159200_linear", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return a*x + b\n-- \n-- linear\n-- Parameters\n-- ----------\n-- x : int\n-- a : float\n-- b : float\n-- Returns\n-- -------\n-- float\n--     a*x + b\nlocal function linear(x, a, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_159200_linear.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = linear\n    lu.assertEquals(candidate(3, 2, 1), 7)\n    lu.assertEquals(candidate(5, 3, 1), 16)\n    lu.assertEquals(candidate(1, 1, 0), 1)\n    lu.assertEquals(candidate(0, 1, 1), 1)\n    lu.assertEquals(candidate(0, 0, -1), -1)\n    lu.assertEquals(candidate(6, 1, 0), 6)\n    lu.assertEquals(candidate(0, 1, -1), -1)\n    lu.assertEquals(candidate(0, 1, 0), 0)\n    lu.assertEquals(candidate(4, 3, 1), 13)\n    lu.assertEquals(candidate(0, 0, 0), 0)\n    lu.assertEquals(candidate(3, 3, 1), 10)\n    lu.assertEquals(candidate(1, 2, 3), 5)\n    lu.assertEquals(candidate(1, 3, 1), 4)\n    lu.assertEquals(candidate(3, 1, 0), 3)\n    lu.assertEquals(candidate(6, 3, 1), 19)\n    lu.assertEquals(candidate(3, 5, 6), 21)\n    lu.assertEquals(candidate(0, 2, 2), 2)\n    lu.assertEquals(candidate(5, 1, 0), 5)\n    lu.assertEquals(candidate(10, 0, 0), 0)\n    lu.assertEquals(candidate(2, 1, 0), 2)\n    lu.assertEquals(candidate(4, 1, 0), 4)\n    lu.assertEquals(candidate(0, 0, 1), 1)\n    lu.assertEquals(candidate(2, 3, 1), 7)\n    lu.assertEquals(candidate(1, 0, 0), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_159279__kamb_radius", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # a = sigma ** 2 / (float(n) + sigma ** 2)\n-- # return 1 - a\n-- \n-- Radius of kernel for Kamb-style smoothing.\nlocal function _kamb_radius(n, sigma)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_159279__kamb_radius.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _kamb_radius\n    lu.assertEquals(candidate(10, 0), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_159762_contains_sublist", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # n = len(sublst)\n-- # return any((sublst == lst[i:i + n]) for i in range(len(lst) - n + 1))\n-- \n-- Check if one list contains the items from another list (in the same order).\n-- :param lst: The main list.\n-- :param sublist: The sublist to check for.\n-- :returns: :data:`True` if the main list contains the items from the\n--           sublist in the same order, :data:`False` otherwise.\n-- Based on `this StackOverflow answer <http://stackoverflow.com/a/3314913>`_.\nlocal function contains_sublist(lst, sublst)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_159762_contains_sublist.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = contains_sublist\n    lu.assertEquals(candidate({1, 2, 3, 4, 5, 6}, {2, 3}), true)\n    lu.assertEquals(candidate({1, 2, 3, 4, 5, 6}, {1, 2, 3, 4, 5, 6}), true)\n    lu.assertEquals(candidate({1, 2, 3, 4, 5, 6}, {4, 5, 6}), true)\n    lu.assertEquals(candidate({1, 2, 3, 4, 5, 6}, {5, 6}), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_159828_make_key", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return dir + \"|\" + cmdline\n-- \n-- Make a single string, combining multiple fields.\nlocal function make_key(dir, cmdline)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_159828_make_key.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = make_key\n    lu.assertEquals(candidate('/home/user/src/project1', 'command line argument 2'), '/home/user/src/project1|command line argument 2')\n    lu.assertEquals(candidate('foo|bar', 'ls | grep'), 'foo|bar|ls | grep')\n    lu.assertEquals(candidate('dir', 'other_cmdline'), 'dir|other_cmdline')\n    lu.assertEquals(candidate('foo|bar', 'ls | grep && touch'), 'foo|bar|ls | grep && touch')\n    lu.assertEquals(candidate('foo|bar', 'ls | grep && touch'), 'foo|bar|ls | grep && touch')\n    lu.assertEquals(candidate('foobar', 'echo'), 'foobar|echo')\n    lu.assertEquals(candidate('/home/user/src/project2', 'command line argument 2'), '/home/user/src/project2|command line argument 2')\n    lu.assertEquals(candidate('foobar', 'ls'), 'foobar|ls')\n    lu.assertEquals(candidate('/home/user/src/project2', 'command line argument 1'), '/home/user/src/project2|command line argument 1')\n    lu.assertEquals(candidate('dir', 'cmdline'), 'dir|cmdline')\n    lu.assertEquals(candidate('foo bar', 'ls'), 'foo bar|ls')\n    lu.assertEquals(candidate('/usr', '-h'), '/usr|-h')\n    lu.assertEquals(candidate('foo|bar', 'ls'), 'foo|bar|ls')\n    lu.assertEquals(candidate('/home/user/src/project1', 'command line argument 1'), '/home/user/src/project1|command line argument 1')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_160355_life_counter", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # hype = 0\n-- # for y in range(len(field)):\n-- #     for x in range(len(field[y])):\n-- #         if field[y][x] == 'o':\n-- #             hype += 1\n-- # return hype\n-- \n--     returns quanity of living squares\nlocal function life_counter(field)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_160355_life_counter.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = life_counter\n    lu.assertEquals(candidate({{None, None, None, None, None, None, None, None, None, None}, {None, None, None, None, None, None, None, None, None, None}, {None, None, None, None, None, None, None, None, None, None}, {None, None, None, None, None, None, None, None, None, None}, {None, None, None, None, None, None, None, None, None, None}, {None, None, None, None, None, None, None, None, None, None}, {None, None, None, None, None, None, None, None, None, None}, {None, None, None, None, None, None, None, None, None, None}, {None, None, None, None, None, None, None, None, None, None}, {None, None, None, None, None, None, None, None, None, None}}), 0)\n    lu.assertEquals(candidate({{'o', 'o', 'o', 'o', 'o'}, {'o', 'o', 'x', 'o', 'o'}, {'o', 'o', 'o', 'o', 'o'}, {'o', 'o', 'o', 'o', 'o'}, {'o', 'o', 'o', 'o', 'o'}}), 24)\n    lu.assertEquals(candidate({{'o', 'o', 'o', 'o', 'o'}, {'o', 'o', 'o', 'o', 'o'}, {'o', 'o', 'o', 'o', 'o'}, {'o', 'o', 'o', 'o', 'o'}, {'o', 'o', 'o', 'o', 'o'}}), 25)\n    lu.assertEquals(candidate({{'.', '.', '.'}, {'o', 'o', 'o'}, {'o', 'o', 'o'}}), 6)\n    lu.assertEquals(candidate({{'.', '.', '.', '.', '.', '.'}, {'.', 'o', '.', '.', '.', '.'}, {'.', '.', '.', '.', '.', '.'}, {'.', '.', '.', '.', '.', '.'}, {'.', '.', '.', '.', '.', '.'}, {'.', '.', '.', '.', '.', '.'}}), 1)\n    lu.assertEquals(candidate({{'o', 'o', 'o', 'o', 'o'}, {'o', 'o', 'o', 'o', 'o'}, {'o', 'o', 'o', 'o', 'o'}, {'o', 'o', 'o', 'o', 'o'}, {'o', 'o', 'o', 'o', 'o'}}), 25)\n    lu.assertEquals(candidate({{'o', 'o', 'o', 'o'}, {'o', 'o', 'o', 'o'}, {'o', 'o', 'o', 'o'}, {'o', 'o', 'o', 'o'}}), 16)\n    lu.assertEquals(candidate({{'.', '.', '.'}, {'.', '.', '.'}, {'.', '.', '.'}}), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_161014_compute_returns", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # returns = [rewards[-1]]\n-- # for i, reward in enumerate(rewards[-2::-1]):\n-- #     return_ = gamma * returns[i] + reward\n-- #     returns.append(return_)\n-- # return returns[::-1]\n-- \n--     Compute returns for each time step, given the rewards\n-- @param rewards: list of floats, where rewards[t] is the reward\n--                 obtained at time step t\n-- @param gamma: the discount factor\n-- @returns list of floats representing the episode's returns\n--     G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... \n--     >>> compute_returns([0,0,0,1], 1.0)\n--     [1.0, 1.0, 1.0, 1.0]\n--     >>> compute_returns([0,0,0,1], 0.9)\n--     [0.7290000000000001, 0.81, 0.9, 1.0]\n--     >>> compute_returns([0,-0.5,5,0.5,-10], 0.9)\n--     [-2.5965000000000003, -2.8850000000000002, -2.6500000000000004, -8.5, -10.0]\nlocal function compute_returns(rewards, gamma)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_161014_compute_returns.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = compute_returns\n    lu.assertEquals(candidate({0, 0, 0, 1}, 1.0), {1.0, 1.0, 1.0, 1.0})\n    lu.assertEquals(candidate({0, 0, 0, 1}, 0.9), {0.7290000000000001, 0.81, 0.9, 1.0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_161910_get_F1score", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # hit = 0\n-- # precision_set = 0\n-- # recall_set = 0\n-- # for i in range(len(correct)):\n-- #     if correct[i] == predict[i] and predict[i] == 0:\n-- #         hit += 1\n-- #     if correct[i] == 0:\n-- #         precision_set += 1\n-- #     if predict[i] == 0:\n-- #         recall_set += 1\n-- # return 2*hit/(precision_set + recall_set)\n-- \n-- correct like [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n-- predict like [0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n-- :return: F1\nlocal function get_F1score(correct, predict)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_161910_get_F1score.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_F1score\n    lu.assertEquals(candidate({1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}, {1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_162923_tricks_to_result", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return tricks - (level + 6)\n-- \n-- Convert tricks made to a result, e.g. 8 tricks\n-- in a 4-level contract becomes -2\nlocal function tricks_to_result(tricks, level)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_162923_tricks_to_result.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = tricks_to_result\n    lu.assertEquals(candidate(8, 4), -2)\n    lu.assertEquals(candidate(3, 1), -4)\n    lu.assertEquals(candidate(1, 1), -6)\n    lu.assertEquals(candidate(5, 1), -2)\n    lu.assertEquals(candidate(9, 1), 2)\n    lu.assertEquals(candidate(6, 1), -1)\n    lu.assertEquals(candidate(8, 1), 1)\n    lu.assertEquals(candidate(7, 1), 0)\n    lu.assertEquals(candidate(2, 1), -5)\n    lu.assertEquals(candidate(4, 1), -3)\n    lu.assertEquals(candidate(10, 1), 3)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_164096_denoise", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # global _denoise\n-- # if val is not None:\n-- #     _denoise = val\n-- # return _denoise\n-- \n--  Set or get denoise \nlocal function denoise(val)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_164096_denoise.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = denoise\n    lu.assertEquals(candidate(true), true)\n    lu.assertEquals(candidate(false), false)\n    lu.assertEquals(candidate(true), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_164767_get_competitive_tier_mi18n", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"bbs/\" + (\"area1\", \"area2\", \"area3\", \"area4\")[tier - 1]\n-- \n-- Turn the tier returned by the API into the respective tier name displayed in-game.\nlocal function get_competitive_tier_mi18n(tier)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_164767_get_competitive_tier_mi18n.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_competitive_tier_mi18n\n    lu.assertEquals(candidate(1), 'bbs/area1')\n    lu.assertEquals(candidate(3), 'bbs/area3')\n    lu.assertEquals(candidate(2), 'bbs/area2')\n    lu.assertEquals(candidate(4), 'bbs/area4')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_16478_batting_average", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return round(hits / at_bats, 3)\n-- \n-- Calculates the batting average to 3 decimal places using number of at bats and hits\nlocal function batting_average(at_bats, hits)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_16478_batting_average.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = batting_average\n    lu.assertEquals(candidate(1, 1), 1.0)\n    lu.assertEquals(candidate(10, 0), 0.0)\n    lu.assertEquals(candidate(1, 0), 0)\n    lu.assertEquals(candidate(20, 10), 0.5)\n    lu.assertEquals(candidate(20, 20), 1.0)\n    lu.assertEquals(candidate(100, 0), 0.0)\n    lu.assertEquals(candidate(100, 30), 0.3)\n    lu.assertEquals(candidate(100, 20), 0.2)\n    lu.assertEquals(candidate(3, 2), 0.667)\n    lu.assertEquals(candidate(5, 2), 0.4)\n    lu.assertEquals(candidate(2, 2), 1)\n    lu.assertEquals(candidate(10, 10), 1.0)\n    lu.assertEquals(candidate(10, 5), 0.5)\n    lu.assertEquals(candidate(100, 10), 0.1)\n    lu.assertEquals(candidate(4, 2), 0.5)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_165045__test", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # do the Fermat test\n-- # b = pow(base, t, n)\n-- # if b == 1 or b == n - 1:\n-- #     return True\n-- # else:\n-- #     for j in range(1, s):\n-- #         b = pow(b, 2, n)\n-- #         if b == n - 1:\n-- #             return True\n-- #         # see I. Niven et al. \"An Introduction to Theory of Numbers\", page 78\n-- #         if b == 1:\n-- #             return False\n-- # return False\n-- \n-- Miller-Rabin strong pseudoprime test for one base.\n-- Return False if n is definitely composite, True if n is\n-- probably prime, with a probability greater than 3/4.\nlocal function _test(n, base, s, t)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_165045__test.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _test\n    lu.assertEquals(candidate(33, 7, 1, 1), false)\n    lu.assertEquals(candidate(341, 5, 10, 3), false)\n    lu.assertEquals(candidate(15, 2, 1, 1), false)\n    lu.assertEquals(candidate(1341, 3, 2, 3), false)\n    lu.assertEquals(candidate(2, 3, 1, 1), true)\n    lu.assertEquals(candidate(41, 2, 1, 4), false)\n    lu.assertEquals(candidate(341, 3, 2, 4), false)\n    lu.assertEquals(candidate(65, 2, 1, 1), false)\n    lu.assertEquals(candidate(41, 2, 5, 3), false)\n    lu.assertEquals(candidate(341, 3, 2, 3), false)\n    lu.assertEquals(candidate(130, 2, 1, 1), false)\n    lu.assertEquals(candidate(3, 19, 1, 2), true)\n    lu.assertEquals(candidate(3, 11, 1, 2), true)\n    lu.assertEquals(candidate(64, 2, 1, 1), false)\n    lu.assertEquals(candidate(5, 3, 1, 3), false)\n    lu.assertEquals(candidate(6, 3, 1, 3), false)\n    lu.assertEquals(candidate(341, 3, 10, 3), false)\n    lu.assertEquals(candidate(129, 2, 1, 1), false)\n    lu.assertEquals(candidate(15, 7, 1, 1), false)\n    lu.assertEquals(candidate(5, 2, 1, 1), false)\n    lu.assertEquals(candidate(9, 7, 1, 1), false)\n    lu.assertEquals(candidate(1341, 5, 10, 3), false)\n    lu.assertEquals(candidate(33, 2, 1, 1), false)\n    lu.assertEquals(candidate(341, 2, 10, 3), false)\n    lu.assertEquals(candidate(1341, 2, 10, 3), false)\n    lu.assertEquals(candidate(128, 2, 1, 1), false)\n    lu.assertEquals(candidate(11, 2, 1, 1), false)\n    lu.assertEquals(candidate(2, 2, 1, 1), false)\n    lu.assertEquals(candidate(7, 3, 1, 3), true)\n    lu.assertEquals(candidate(102, 2, 1, 1), false)\n    lu.assertEquals(candidate(1341, 3, 10, 3), false)\n    lu.assertEquals(candidate(6, 2, 1, 1), false)\n    lu.assertEquals(candidate(41, 2, 1, 3), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_165051__convert_from_F", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return round((temp - 32) * 5/9, 1)\n-- \n-- Convert F temp to C\n-- param temp: temp in F to convert\n-- return: float\nlocal function _convert_from_F(temp)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_165051__convert_from_F.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _convert_from_F\n    lu.assertEquals(candidate(90.0), 32.2)\n    lu.assertEquals(candidate(212), 100.0)\n    lu.assertEquals(candidate(32), 0.0)\n    lu.assertEquals(candidate(212), 100)\n    lu.assertEquals(candidate(70), 21.1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_165396_rgb_to_hex", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Components need to be integers for hex to make sense\n-- # rgb = [int(x) for x in rgb]\n-- # hex_rgb = \"\".join([f\"0{v:x}\" if v < 16 else f\"{v:x}\" for v in rgb])\n-- # return f\"#{hex_rgb}\"\n-- \n--  [255,255,255] -> \"#FFFFFF\" \nlocal function rgb_to_hex(rgb)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_165396_rgb_to_hex.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = rgb_to_hex\n    lu.assertEquals(candidate({0, 0, 0}), '#000000')\n    lu.assertEquals(candidate({1, 2, 3}), '#010203')\n    lu.assertEquals(candidate({0, 0, 0}), '#000000')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_165487_get_uce_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return header.lstrip('>').split(' ')[0].split('_')[0]\n-- \n-- use own function vs. import from match_contigs_to_probes - we don't want lowercase\nlocal function get_uce_name(header)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_165487_get_uce_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_uce_name\n    lu.assertEquals(candidate('>uce-1_1202_10807_10836_2401_2430_0_0_0'), 'uce-1')\n    lu.assertEquals(candidate('>uce-1 1202 10807 10836 2401 2430 0 0 0.000000 0.000000'), 'uce-1')\n    lu.assertEquals(candidate('>uce-1 1202 10807 10836 2401 2430 0 0 0'), 'uce-1')\n    lu.assertEquals(candidate('>uce-1_1202_10807_10836_2401_2430_0_0_0 1213 1389'), 'uce-1')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_16588_rst_heading", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return ''.join([value, '\\n', arg*len(value)])\n-- \n-- Provides an underline for restructured text heading.\n-- Syntax::\n--     {{ value|rst_heading:\"=\" }}\n-- Results in:\n-- ``value``\n-- ``=====``\nlocal function rst_heading(value, arg)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_16588_rst_heading.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = rst_heading\n    lu.assertEquals(candidate('value', '='), candidate('value', '='))\n    lu.assertEquals(candidate('value', '='), 'value\\n=====')\n    lu.assertEquals(candidate('foo bar', '*'), 'foo bar\\n*******')\n    lu.assertEquals(candidate('value', '='), candidate('value', '='))\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_16590_format_time", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # US_IN_SECOND = 1000.0 * 1000.0\n-- # US_IN_MS = 1000.0\n-- # if time_us >= US_IN_SECOND:\n-- #     return '{:.3f}s'.format(time_us / US_IN_SECOND)\n-- # if time_us >= US_IN_MS:\n-- #     return '{:.3f}ms'.format(time_us / US_IN_MS)\n-- # return '{:.3f}us'.format(time_us)\n-- \n-- Defines how to format time in FunctionEvent\nlocal function format_time(time_us)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_16590_format_time.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = format_time\n    lu.assertEquals(candidate(0), '0.000us')\n    lu.assertEquals(candidate(10), '10.000us')\n    lu.assertEquals(candidate(1), '1.000us')\n    lu.assertEquals(candidate(100), '100.000us')\n    lu.assertEquals(candidate(123456), '123.456ms')\n    lu.assertEquals(candidate(123), '123.000us')\n    lu.assertEquals(candidate(123456789), '123.457s')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_167250_det_matriz", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # lin = len(matriz)\n-- # col = len(matriz[0])\n-- # if lin == 1:\n-- #     determinante = matriz[0][0]\n-- #     return determinante\n-- # elif lin == 2:\n-- #     principal, secundaria = 1, 1\n-- #     for i in range(lin):\n-- #         for j in range(col):\n-- #             if i == j:\n-- #                 principal *= matriz[i][j]\n-- #             else:\n-- #                 secundaria *= matriz[i][j]\n-- #     determinante = principal - secundaria\n-- #     return determinante\n-- # elif lin == 3:\n-- #     principais = 0\n-- #     for i in range(lin):\n-- #         soma = 1\n-- #         for j in range(col):\n-- #             k = (i + j) % col\n-- #             soma *= matriz[j][k]\n-- #         principais += soma\n-- #     secundarias = 0\n-- #     for i in range(lin):\n-- #         subtracao = 1\n-- #         for j in range(col):\n-- #             k = (i - j) % col\n-- #             subtracao *= matriz[j][k]\n-- #         secundarias -= subtracao\n-- #     determinante = secundarias + principais\n-- #     return determinante\n-- # else:\n-- #     sub_matriz = []\n-- #     for i in range(lin - 1):\n-- #         l = []\n-- #         for j in range(col - 1):\n-- #             l.append(0)\n-- #         sub_matriz.append(l)\n-- #     aux, determinante = 0, 0\n-- #     for i in range(lin):\n-- #         l = -1\n-- #         for j in range(col):\n-- #             c = 0\n-- #             for k in range(col):\n-- #                 if aux != j and k != i:\n-- #                     sub_matriz[l][c] = matriz[j][k]\n-- #                     c += 1\n-- #             l += 1\n-- #         determinante += (-1) ** (aux + i) * \\\n-- #             matriz[aux][i] * det_matriz(sub_matriz)\n-- #     return determinante\n-- \n-- Calcula a determinante de uma matriz\nlocal function det_matriz(matriz)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_167250_det_matriz.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = det_matriz\n    lu.assertEquals(candidate({{1, 2, 3, 4}, {5, 6, 7, 8}, {9, 10, 11, 12}, {13, 14, 15, 16}}), 0)\n    lu.assertEquals(candidate({{0, 2, 3}, {0, 4, 5}, {0, 0, 6}}), 0)\n    lu.assertEquals(candidate({{1, 2}, {3, 4}}), -2)\n    lu.assertEquals(candidate({{1, 2, 3, 4}, {3, 4, 5, 6}, {5, 6, 7, 8}, {7, 8, 9, 10}}), 0)\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}}), 0)\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}}), 0)\n    lu.assertEquals(candidate({{2, 2, 2, 2}, {2, 2, 2, 2}, {2, 2, 2, 2}, {2, 2, 2, 2}}), 0)\n    lu.assertEquals(candidate({{1, 2, 3, 4}, {5, 6, 7, 8}, {9, 10, 11, 12}, {13, 14, 15, 16}}), 0)\n    lu.assertEquals(candidate({{1, 2, 3}, {1, 2, 3}, {1, 2, 3}}), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_167319_build_years_list", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # years = []\n-- # for i in range(0, 10):\n-- #     years.append(start - i)\n-- # return years\n-- \n-- create a list of 10 years counting backward from the start year\nlocal function build_years_list(start)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_167319_build_years_list.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = build_years_list\n    lu.assertEquals(candidate(2017), {2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008})\n    lu.assertEquals(candidate(2012), {2012, 2011, 2010, 2009, 2008, 2007, 2006, 2005, 2004, 2003})\n    lu.assertEquals(candidate(1995), {1995, 1994, 1993, 1992, 1991, 1990, 1989, 1988, 1987, 1986})\n    lu.assertEquals(candidate(2021), {2021, 2020, 2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_167707_validate_ticket_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # errors = []\n-- # # name cannot be empty\n-- # if len(name) <= 0:\n-- #     errors.append(\"Ticket name must contain at least (1) character\")\n-- # # name must be alphanumeric\n-- # if not all(n.isalnum() or n.isspace() for n in name):\n-- #     errors.append(\"Ticket name must be alphanumeric only\")\n-- # # cannot have leading/trailing spaces\n-- # if len(name) != 0 and (name[0] == ' ' or name[-1] == ' '):\n-- #     errors.append(\"Ticket name cannot begin/end with a space\")\n-- # # name has a max of 60 characters\n-- # if len(name) > 60:\n-- #     errors.append(\"Ticket name exceeds character limit (60)\")\n-- # return errors\n-- \n-- validate that the ticket name is valid\n-- :param name: the name of the ticket\n-- :return: an error message (if any) or nothing if the ticket name is in the correct format\nlocal function validate_ticket_name(name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_167707_validate_ticket_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = validate_ticket_name\n    lu.assertEquals(candidate(' '), {'Ticket name cannot begin/end with a space'})\n    lu.assertEquals(candidate('hello@world'), {'Ticket name must be alphanumeric only'})\n    lu.assertEquals(candidate('Hello '), {'Ticket name cannot begin/end with a space'})\n    lu.assertEquals(candidate('Hello World'), {})\n    lu.assertEquals(candidate('hello world'), {})\n    lu.assertEquals(candidate('0123456789012345678901234567890123456789012345678901234567890'), {'Ticket name exceeds character limit (60)'})\n    lu.assertEquals(candidate(''), {'Ticket name must contain at least (1) character'})\n    lu.assertEquals(candidate('012345678901234567890123456789012345678901234567890123456789'), {})\n    lu.assertEquals(candidate(' Hello'), {'Ticket name cannot begin/end with a space'})\n    lu.assertEquals(candidate('Ticket$100'), {'Ticket name must be alphanumeric only'})\n    lu.assertEquals(candidate(''), {'Ticket name must contain at least (1) character'})\n    lu.assertEquals(candidate(''), {'Ticket name must contain at least (1) character'})\n    lu.assertEquals(candidate('123456789012345678901234567890'), {})\n    lu.assertEquals(candidate('Hello, World'), {'Ticket name must be alphanumeric only'})\n    lu.assertEquals(candidate('Hello World!'), {'Ticket name must be alphanumeric only'})\n    lu.assertEquals(candidate('hello world '), {'Ticket name cannot begin/end with a space'})\n    lu.assertEquals(candidate('hello+world'), {'Ticket name must be alphanumeric only'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_168621_create_new_id", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # new_id = region + str(last_id_number + 1)\n-- # return new_id\n-- \n-- Create a new unique id for the record to be added.\nlocal function create_new_id(region, last_id_number)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_168621_create_new_id.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = create_new_id\n    lu.assertEquals(candidate('North Carolina', 2499), 'North Carolina2500')\n    lu.assertEquals(candidate('New York', 5999), 'New York6000')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_169552_is_group", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return type(group).__name__ == \"Group\"\n-- \n-- Return ``True`` if passed object is Group and ``False`` otherwise.\nlocal function is_group(group)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_169552_is_group.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_group\n    lu.assertEquals(candidate(10), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_169608_address_fixup", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # d = {\n-- #     \"2901 Silent Ave Suite 201, Bakersfield, CA 93308\": \"2901 Sillect Ave Suite 201, Bakersfield, CA 93308\",\n-- #     \"3300 BUENA VISTA RD A, Bakersfield, CA 93311\": \"3300 Buena Vista Rd Bldg A, Bakersfield, CA 93311\",\n-- #     \"8000 WHITE LANE, Bakersfield, CA 93301\": \"8000 WHITE LANE, BAKERSFIELD, CA 93309\",\n-- #     \"Rite Aid Store 06303, Bakersfield, CA 93313\": \"3225 PANAMA LANE, BAKERSFIELD, CA 93313\",\n-- #     \"3500 Stine Rd Bakersfield, Bakersfield, CA 93309\": \"3500 Stine Rd, Bakersfield, CA 93309\",\n-- # }\n-- # return d.get(a, a)\n-- \n--  Some Kern Co. addresses have typos. \nlocal function address_fixup(a)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_169608_address_fixup.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = address_fixup\n    lu.assertEquals(candidate(candidate('3225 PANAMA LANE, BAKERSFIELD, CA 93313')), '3225 PANAMA LANE, BAKERSFIELD, CA 93313')\n    lu.assertEquals(candidate(candidate('3500 Stine Rd, Bakersfield, CA 93309')), '3500 Stine Rd, Bakersfield, CA 93309')\n    lu.assertEquals(candidate('2901 Silent Ave Suite 201, Bakersfield, CA 93308'), '2901 Sillect Ave Suite 201, Bakersfield, CA 93308')\n    lu.assertEquals(candidate(candidate('2901 Sillect Ave Suite 201, Bakersfield, CA 93308')), '2901 Sillect Ave Suite 201, Bakersfield, CA 93308')\n    lu.assertEquals(candidate(candidate('3300 Buena Vista Rd Bldg A, Bakersfield, CA 93311')), '3300 Buena Vista Rd Bldg A, Bakersfield, CA 93311')\n    lu.assertEquals(candidate('3500 Stine Rd Bakersfield, Bakersfield, CA 93309'), '3500 Stine Rd, Bakersfield, CA 93309')\n    lu.assertEquals(candidate('3300 BUENA VISTA RD A, Bakersfield, CA 93311'), '3300 Buena Vista Rd Bldg A, Bakersfield, CA 93311')\n    lu.assertEquals(candidate(candidate('8000 WHITE LANE, BAKERSFIELD, CA 93309')), '8000 WHITE LANE, BAKERSFIELD, CA 93309')\n    lu.assertEquals(candidate('8000 WHITE LANE, Bakersfield, CA 93301'), '8000 WHITE LANE, BAKERSFIELD, CA 93309')\n    lu.assertEquals(candidate('3500 Stine Rd Bakersfield, Bakersfield, CA 93309'), '3500 Stine Rd, Bakersfield, CA 93309')\n    lu.assertEquals(candidate('Rite Aid Store 06303, Bakersfield, CA 93313'), '3225 PANAMA LANE, BAKERSFIELD, CA 93313')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_170197_mask_to_cidr", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return sum(bin(int(x)).count('1') for x in netmask.split('.'))\n-- \n-- Convert netmask in dot-notation to decimal CIDR notation\nlocal function mask_to_cidr(netmask)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_170197_mask_to_cidr.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = mask_to_cidr\n    lu.assertEquals(candidate('255.255.248.128'), 22)\n    lu.assertEquals(candidate('255.255.255.0'), 24)\n    lu.assertEquals(candidate('255.255.192.128'), 19)\n    lu.assertEquals(candidate('255.255.255.252'), 30)\n    lu.assertEquals(candidate('255.255.240.128'), 21)\n    lu.assertEquals(candidate('255.255.0.0'), 16)\n    lu.assertEquals(candidate('255.255.240.0'), 20)\n    lu.assertEquals(candidate('255.255.252.128'), 23)\n    lu.assertEquals(candidate('255.255.192.0'), 18)\n    lu.assertEquals(candidate('255.255.248.0'), 21)\n    lu.assertEquals(candidate('255.255.252.0'), 22)\n    lu.assertEquals(candidate('255.255.255.0'), 24)\n    lu.assertEquals(candidate('255.255.255.255'), 32)\n    lu.assertEquals(candidate('255.255.255.128'), 25)\n    lu.assertEquals(candidate('255.0.0.0'), 8)\n    lu.assertEquals(candidate('255.255.254.128'), 24)\n    lu.assertEquals(candidate('255.255.254.0'), 23)\n    lu.assertEquals(candidate('255.255.224.0'), 19)\n    lu.assertEquals(candidate('255.255.224.128'), 20)\n    lu.assertEquals(candidate('255.255.255.0'), 24)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_170431_prelogin_url", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # urlname_to_url = {\n-- #     'go_to_pricing': 'https://dimagi.com/commcare/pricing/',\n-- #     'public_pricing': 'https://dimagi.com/commcare/pricing/',\n-- # }\n-- # return urlname_to_url.get(urlname, 'https://dimagi.com/commcare/')\n-- \n--     Fetches the correct dimagi.com url for a \"prelogin\" view.\nlocal function prelogin_url(urlname)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_170431_prelogin_url.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = prelogin_url\n    lu.assertEquals(candidate('public_pricing'), 'https://dimagi.com/commcare/pricing/')\n    lu.assertEquals(candidate('go_to_pricing'), 'https://dimagi.com/commcare/pricing/')\n    lu.assertEquals(candidate('https://dimagi.com/commcare/'), 'https://dimagi.com/commcare/')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_170606_gcd_modulus", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # while b != 0:\n-- #     a, b = b, a % b\n-- # return a\n-- \n-- finds the GCD of a and b\n-- Args:\n--  a, b: non-negative integers\n-- Returns:\n--  int: the GCD of a and b\nlocal function gcd_modulus(a, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_170606_gcd_modulus.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = gcd_modulus\n    lu.assertEquals(candidate(1, 1), 1)\n    lu.assertEquals(candidate(92, 14), 2)\n    lu.assertEquals(candidate(69, 19), 1)\n    lu.assertEquals(candidate(0, 0), 0)\n    lu.assertEquals(candidate(3, 2), 1)\n    lu.assertEquals(candidate(2, 9), 1)\n    lu.assertEquals(candidate(1, 0), 1)\n    lu.assertEquals(candidate(1000, 10), 10)\n    lu.assertEquals(candidate(29, 51), 1)\n    lu.assertEquals(candidate(4, 19), 1)\n    lu.assertEquals(candidate(3, 7), 1)\n    lu.assertEquals(candidate(89, 46), 1)\n    lu.assertEquals(candidate(3, 4), 1)\n    lu.assertEquals(candidate(12, 10), 2)\n    lu.assertEquals(candidate(0, 10), 10)\n    lu.assertEquals(candidate(1000, 0), 1000)\n    lu.assertEquals(candidate(37, 53), 1)\n    lu.assertEquals(candidate(1000000000, 1000000001), 1)\n    lu.assertEquals(candidate(5, 81), 1)\n    lu.assertEquals(candidate(82, 90), 2)\n    lu.assertEquals(candidate(2, 1), 1)\n    lu.assertEquals(candidate(89, 77), 1)\n    lu.assertEquals(candidate(70, 97), 1)\n    lu.assertEquals(candidate(12, 6), 6)\n    lu.assertEquals(candidate(42, 56), 14)\n    lu.assertEquals(candidate(42, 6), 6)\n    lu.assertEquals(candidate(93, 22), 1)\n    lu.assertEquals(candidate(2, 8), 2)\n    lu.assertEquals(candidate(48, 18), 6)\n    lu.assertEquals(candidate(10, 5), 5)\n    lu.assertEquals(candidate(10, 100), 10)\n    lu.assertEquals(candidate(4, 2), 2)\n    lu.assertEquals(candidate(96, 22), 2)\n    lu.assertEquals(candidate(2, 4), 2)\n    lu.assertEquals(candidate(2, 6), 2)\n    lu.assertEquals(candidate(6, 24), 6)\n    lu.assertEquals(candidate(100, 5), 5)\n    lu.assertEquals(candidate(38, 17), 1)\n    lu.assertEquals(candidate(2, 2), 2)\n    lu.assertEquals(candidate(1, 2), 1)\n    lu.assertEquals(candidate(29, 90), 1)\n    lu.assertEquals(candidate(73, 88), 1)\n    lu.assertEquals(candidate(5, 6), 1)\n    lu.assertEquals(candidate(4, 3), 1)\n    lu.assertEquals(candidate(1000000000, 1000000000), 1000000000)\n    lu.assertEquals(candidate(11, 49), 1)\n    lu.assertEquals(candidate(0, 1), 1)\n    lu.assertEquals(candidate(15, 20), 5)\n    lu.assertEquals(candidate(1000000001, 1000000000), 1)\n    lu.assertEquals(candidate(4, 8), 4)\n    lu.assertEquals(candidate(5, 2), 1)\n    lu.assertEquals(candidate(10, 0), 10)\n    lu.assertEquals(candidate(10, 1000), 10)\n    lu.assertEquals(candidate(100, 10), 10)\n    lu.assertEquals(candidate(4, 9), 1)\n    lu.assertEquals(candidate(12, 18), 6)\n    lu.assertEquals(candidate(2, 5), 1)\n    lu.assertEquals(candidate(3, 6), 3)\n    lu.assertEquals(candidate(3, 9), 3)\n    lu.assertEquals(candidate(3, 8), 1)\n    lu.assertEquals(candidate(4, 5), 1)\n    lu.assertEquals(candidate(5, 4), 1)\n    lu.assertEquals(candidate(4, 6), 2)\n    lu.assertEquals(candidate(2, 3), 1)\n    lu.assertEquals(candidate(20, 12), 4)\n    lu.assertEquals(candidate(41, 69), 1)\n    lu.assertEquals(candidate(5, 10), 5)\n    lu.assertEquals(candidate(12, 30), 6)\n    lu.assertEquals(candidate(2, 7), 1)\n    lu.assertEquals(candidate(100, 12), 4)\n    lu.assertEquals(candidate(4, 7), 1)\n    lu.assertEquals(candidate(8, 4), 4)\n    lu.assertEquals(candidate(97, 7), 1)\n    lu.assertEquals(candidate(0, 1000), 1000)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_171423_greatest_common_divisor", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # while a != b:\n-- #     if a < b:\n-- #         b = b - a\n-- #     else:\n-- #         a = a - b\n-- # return a\n-- \n-- Function to calculate the greatest common divisor\nlocal function greatest_common_divisor(a, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_171423_greatest_common_divisor.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = greatest_common_divisor\n    lu.assertEquals(candidate(30, 10), 10)\n    lu.assertEquals(candidate(54, 24), 6)\n    lu.assertEquals(candidate(2, 6), 2)\n    lu.assertEquals(candidate(1, 10), 1)\n    lu.assertEquals(candidate(10, 15), 5)\n    lu.assertEquals(candidate(20, 10), 10)\n    lu.assertEquals(candidate(123, 45), 3)\n    lu.assertEquals(candidate(100, 10), 10)\n    lu.assertEquals(candidate(1000000, 4000000000), 1000000)\n    lu.assertEquals(candidate(10, 1), 1)\n    lu.assertEquals(candidate(5, 10), 5)\n    lu.assertEquals(candidate(3, 15), 3)\n    lu.assertEquals(candidate(20, 30), 10)\n    lu.assertEquals(candidate(100, 100), 100)\n    lu.assertEquals(candidate(100, 25), 25)\n    lu.assertEquals(candidate(6, 15), 3)\n    lu.assertEquals(candidate(15, 5), 5)\n    lu.assertEquals(candidate(2, 3), 1)\n    lu.assertEquals(candidate(25, 100), 25)\n    lu.assertEquals(candidate(2, 4), 2)\n    lu.assertEquals(candidate(7, 3), 1)\n    lu.assertEquals(candidate(25, 25), 25)\n    lu.assertEquals(candidate(5, 2), 1)\n    lu.assertEquals(candidate(4000000000, 1000000), 1000000)\n    lu.assertEquals(candidate(4, 6), 2)\n    lu.assertEquals(candidate(30, 20), 10)\n    lu.assertEquals(candidate(10, 30), 10)\n    lu.assertEquals(candidate(234567, 89), 1)\n    lu.assertEquals(candidate(3, 5), 1)\n    lu.assertEquals(candidate(10, 5), 5)\n    lu.assertEquals(candidate(1, 1), 1)\n    lu.assertEquals(candidate(15, 20), 5)\n    lu.assertEquals(candidate(20, 15), 5)\n    lu.assertEquals(candidate(10, 100), 10)\n    lu.assertEquals(candidate(2, 1), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_171452_normalize", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \" \".join(text.strip().split())\n-- \n-- Normalizes whitespace in a specified string of text.\nlocal function normalize(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_171452_normalize.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = normalize\n    lu.assertEquals(candidate('I   \\t\\t\\n love\\n\\n\\nthis course!'), 'I love this course!')\n    lu.assertEquals(candidate('This is a test!'), 'This is a test!')\n    lu.assertEquals(candidate('I love this        course!'), 'I love this course!')\n    lu.assertEquals(candidate('The  quick   brown      fox    jumps over the lazy dog.'), 'The quick brown fox jumps over the lazy dog.')\n    lu.assertEquals(candidate('I love\\n\\n\\n\\n        this\\n\\n\\n\\n\\n        course!'), 'I love this course!')\n    lu.assertEquals(candidate('I love this\\n        course!'), 'I love this course!')\n    lu.assertEquals(candidate('I love this course!'), 'I love this course!')\n    lu.assertEquals(candidate('I   \\t\\t\\n love this course!'), 'I love this course!')\n    lu.assertEquals(candidate('This is a test?'), 'This is a test?')\n    lu.assertEquals(candidate('This is a test.'), 'This is a test.')\n    lu.assertEquals(candidate('  I love this course!  '), 'I love this course!')\n    lu.assertEquals(candidate(' I love this course!'), 'I love this course!')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_171829_obter_pos_c", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if pos[0] in (1, 4, 7):\n-- #     return 'a'\n-- # elif pos[0] in (2, 5, 8):\n-- #     return 'b'\n-- # elif pos[0] in (3, 6, 9):\n-- #     return 'c'\n-- \n-- obter_pos_c: posicao -> str\n-- Esta funcao devolve a componente coluna da posicao.\nlocal function obter_pos_c(pos)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_171829_obter_pos_c.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = obter_pos_c\n    lu.assertEquals(candidate({3, 3}), 'c')\n    lu.assertEquals(candidate({9, 9}), 'c')\n    lu.assertEquals(candidate({2, 2}), 'b')\n    lu.assertEquals(candidate({6, 6}), 'c')\n    lu.assertEquals(candidate({1, 1}), 'a')\n    lu.assertEquals(candidate({5, 5}), 'b')\n    lu.assertEquals(candidate({7, 7}), 'a')\n    lu.assertEquals(candidate({4, 4}), 'a')\n    lu.assertEquals(candidate({8, 8}), 'b')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_1724_map", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return int((x-in_min) * (out_max-out_min) / (in_max-in_min) + out_min)\n-- \n--     Map a value from one range to another\n-- :param in_min: minimum of input range\n-- :param in_max: maximum of input range\n-- :param out_min: minimum of output range\n-- :param out_max: maximum of output range\n-- :return: The value scaled to the new range\n-- :rtype: int\nlocal function map(x, in_min, in_max, out_min, out_max)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_1724_map.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = map\n    lu.assertEquals(candidate(100, 100, 0, 1, 10), 1)\n    lu.assertEquals(candidate(100, 0, 100, 0, 100), 100)\n    lu.assertEquals(candidate(-100, -100, 100, 0, 100), 0)\n    lu.assertEquals(candidate(0, 0, 100, 0, 100), 0)\n    lu.assertEquals(candidate(99, 0, 99, 0, 100), 100)\n    lu.assertEquals(candidate(10, 0, 10, 0, 10), 10)\n    lu.assertEquals(candidate(1, 0, 10, 1, 10), 1)\n    lu.assertEquals(candidate(8, 0, 10, 0, 10), 8)\n    lu.assertEquals(candidate(50, 0, 100, -100, 100), 0)\n    lu.assertEquals(candidate(50, 0, 100, 0, 200), 100)\n    lu.assertEquals(candidate(9, 0, 10, 0, 10), 9)\n    lu.assertEquals(candidate(3, 0, 10, 0, 10), 3)\n    lu.assertEquals(candidate(1, 0, 10, 0, 10), 1)\n    lu.assertEquals(candidate(-100, -100, 100, 0, 200), 0)\n    lu.assertEquals(candidate(1, 0, 10, 1, 20), 2)\n    lu.assertEquals(candidate(2, 0, 10, 0, 10), 2)\n    lu.assertEquals(candidate(0, 0, 10, 0, 10), 0)\n    lu.assertEquals(candidate(1, 0, 10, 10, 0), 9)\n    lu.assertEquals(candidate(3, 0, 10, 1, 20), 6)\n    lu.assertEquals(candidate(4, 0, 10, 1, 20), 8)\n    lu.assertEquals(candidate(2, 0, 10, 1, 20), 4)\n    lu.assertEquals(candidate(10, 10, 0, 1, 10), 1)\n    lu.assertEquals(candidate(42, 0, 100, 0, 100), 42)\n    lu.assertEquals(candidate(50, 0, 100, 0, 100), 50)\n    lu.assertEquals(candidate(4, 0, 10, 0, 10), 4)\n    lu.assertEquals(candidate(5, 0, 10, 1, 20), 10)\n    lu.assertEquals(candidate(1, 0, 1, 0, 100), 100)\n    lu.assertEquals(candidate(100, 0, 100, 1, 10), 10)\n    lu.assertEquals(candidate(5, 0, 10, 0, 10), 5)\n    lu.assertEquals(candidate(6, 0, 10, 0, 10), 6)\n    lu.assertEquals(candidate(7, 0, 10, 0, 10), 7)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_172573_replace_word_choice", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # better_sentence = sentence.replace(old_word, new_word)\n-- # return better_sentence\n-- \n-- Replace a word in the provided sentence with a new one.\n-- :param sentence: str - a sentence to replace words in.\n-- :param old_word: str - word to replace.\n-- :param new_word: str - replacement word.\n-- :return: str - input sentence with new words in place of old words.\nlocal function replace_word_choice(sentence, old_word, new_word)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_172573_replace_word_choice.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = replace_word_choice\n    lu.assertEquals(candidate('The rain in Spain falls mainly on the plain.', 'plain', 'lake'), 'The rain in Spain falls mainly on the lake.')\n    lu.assertEquals(candidate('There was a girl with a telescope', 'robot', 'girl'), 'There was a girl with a telescope')\n    lu.assertEquals(candidate(\"I'm so happy to have you here.\", 'happy', 'excited'), \"I'm so excited to have you here.\")\n    lu.assertEquals(candidate('I am so happy', 'happy', 'glad'), 'I am so glad')\n    lu.assertEquals(candidate('There was a girl with a telescope', 'girl', 'robot'), 'There was a robot with a telescope')\n    lu.assertEquals(candidate('There was a girl with a telescope', 'telescope', 'robot'), 'There was a girl with a robot')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_172804_to_rtl", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return '_'.join(path.split('.'))\n-- \n-- Modifies a path to look like the RTL net/register names\nlocal function to_rtl(path)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_172804_to_rtl.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = to_rtl\n    lu.assertEquals(candidate('net_a.field_b.field_c'), 'net_a_field_b_field_c')\n    lu.assertEquals(candidate('net_a.field_b.field_c.field_d.field_e.field_f'), 'net_a_field_b_field_c_field_d_field_e_field_f')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_172880_get_key", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return list(dict.keys())[list(dict.values()).index(value)]\n-- \n--  Return the first key in the dictionary \"dict\" that contains the\n-- received value \"value\".\n-- Parameters\n-- ==========\n-- dict: Dict[Any, Any]\n--     Dictionary to be used.\n-- value: Any\n--     Value to be found in the dictionary.\nlocal function get_key(dict, value)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_172880_get_key.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_key\n    lu.assertEquals(candidate({['a'] = 0, ['b'] = 1, ['c'] = 2}, 2), 'c')\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 0, ['c'] = 0}, 0), 'b')\n    lu.assertEquals(candidate({['a'] = 0, ['b'] = 1, ['c'] = 0}, 0), 'a')\n    lu.assertEquals(candidate({['1'] = 1}, 1), '1')\n    lu.assertEquals(candidate({['1'] = 1, ['2'] = 2}, 1), '1')\n    lu.assertEquals(candidate({['a'] = 0, ['b'] = 0, ['c'] = 1}, 1), 'c')\n    lu.assertEquals(candidate({['1'] = 1, ['2'] = 2}, 2), '2')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_173041__get_authority_url", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Some Azure Stack (bellevue)'s metadata returns\n-- # #   \"loginEndpoint\": \"https://login.microsoftonline.com/\"\n-- # # Normalize it by removing the trailing /, so that authority_url won't become\n-- # # \"https://login.microsoftonline.com//tenant_id\".\n-- # authority_endpoint = authority_endpoint.rstrip('/').lower()\n-- # is_adfs = authority_endpoint.endswith('adfs')\n-- # if is_adfs:\n-- #     authority_url = authority_endpoint\n-- # else:\n-- #     authority_url = '{}/{}'.format(authority_endpoint, tenant or \"organizations\")\n-- # return authority_url, is_adfs\n-- \n-- Convert authority endpoint (active_directory) to MSAL authority:\n-- - AAD: https://login.microsoftonline.com/your_tenant\n-- - ADFS: https://adfs.redmond.azurestack.corp.microsoft.com/adfs\n--     For ADFS, tenant is discarded.\nlocal function _get_authority_url(authority_endpoint, tenant)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_173041__get_authority_url.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _get_authority_url\n    lu.assertEquals(candidate('https://login.microsoftonline.com', None), {'https://login.microsoftonline.com/organizations', false})\n    lu.assertEquals(candidate('https://login.microsoftonline.com/', None), {'https://login.microsoftonline.com/organizations', false})\n    lu.assertEquals(candidate('https://login.microsoftonline.com/', 'common'), {'https://login.microsoftonline.com/common', false})\n    lu.assertEquals(candidate('https://login.microsoftonline.com', 'common'), {'https://login.microsoftonline.com/common', false})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_173554_part_one", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # result = 0\n-- # for i in range(1, len(data)):\n-- #     first = int(data[i - 1])\n-- #     second = int(data[i])\n-- #     if first == second:\n-- #         result += first\n-- #     while first == second and i < len(data) - 1:\n-- #         i += 1\n-- #         first = int(data[i - 1])\n-- #         second = int(data[i])\n-- # if data[0] == data[len(data) - 1]:\n-- #     result += int(data[0])\n-- # return result\n-- \n-- Part one\nlocal function part_one(data)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_173554_part_one.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = part_one\n    lu.assertEquals(candidate('1122'), 3)\n    lu.assertEquals(candidate('91212129'), 9)\n    lu.assertEquals(candidate('1234'), 0)\n    lu.assertEquals(candidate('1111'), 4)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_17433_get_max", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if current_max != 0 and current_max > input_score:\n-- #     return current_max\n-- # return input_score\n-- \n-- compare two input numbers, and return bigger one.\n-- :param current_max: int, the current max score.\n-- :param input_score: int, the score just input.\n-- :return: int, compare two numbers and return bigger one.\nlocal function get_max(current_max, input_score)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_17433_get_max.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_max\n    lu.assertEquals(candidate(10, 1), 10)\n    lu.assertEquals(candidate(1, 2), 2)\n    lu.assertEquals(candidate(15, 15), 15)\n    lu.assertEquals(candidate(0, 1), 1)\n    lu.assertEquals(candidate(10, 0), 10)\n    lu.assertEquals(candidate(0, 0), 0)\n    lu.assertEquals(candidate(10, 15), 15)\n    lu.assertEquals(candidate(10, 10), 10)\n    lu.assertEquals(candidate(1, 1), 1)\n    lu.assertEquals(candidate(3, 5), 5)\n    lu.assertEquals(candidate(-1, 2), 2)\n    lu.assertEquals(candidate(2, -1), 2)\n    lu.assertEquals(candidate(15, 10), 15)\n    lu.assertEquals(candidate(1, 0), 1)\n    lu.assertEquals(candidate(2, 1), 2)\n    lu.assertEquals(candidate(0, 10), 10)\n    lu.assertEquals(candidate(5, 10), 10)\n    lu.assertEquals(candidate(1, 10), 10)\n    lu.assertEquals(candidate(10, 5), 10)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_174589_ngrams", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # text_len = len(seq)\n-- # res = []\n-- # for n in range(min_n, min(max_n + 1, text_len + 1)):\n-- #     for i in range(text_len - n + 1):\n-- #         res.append(seq[i: i + n])\n-- # return res\n-- \n--     Return min_n to max_n n-grams of elements from a given sequence.\nlocal function ngrams(seq, min_n, max_n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_174589_ngrams.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = ngrams\n    lu.assertEquals(candidate('Hello', 6, 6), {})\n    lu.assertEquals(candidate('abcde', 1, 1), {'a', 'b', 'c', 'd', 'e'})\n    lu.assertEquals(candidate('abcd', 1, 1), {'a', 'b', 'c', 'd'})\n    lu.assertEquals(candidate('abcd', 1, 2), {'a', 'b', 'c', 'd', 'ab', 'bc', 'cd'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_175069__link_environment", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # alias = alias.upper()\n-- # base = u'%s_PORT_%d_%s' % (alias, local_port, protocol.upper())\n-- # return {\n-- #     base: u'%s://%s:%d' % (protocol, hostname, remote_port),\n-- #     base + u'_ADDR': hostname,\n-- #     base + u'_PORT': u'%d' % (remote_port,),\n-- #     base + u'_PROTO': protocol,\n-- # }\n-- \n-- Generate the environment variables used for defining a docker link.\n-- Docker containers expect an enviroment variable\n-- `<alias>_PORT_<local_port>_TCP`` which contains the URL of the remote end\n-- of a link, as well as parsed variants ``_ADDR``, ``_PORT``, ``_PROTO``.\n-- :param unicode protocol: The protocol used for the link.\n-- :param unicode alias: The name of the link.\n-- :param int local_port: The port the local application expects to access.\n-- :param unicode hostname: The remote hostname to connect to.\n-- :param int remote_port: The remote port to connect to.\nlocal function _link_environment(protocol, alias, local_port, hostname, remote_port)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_175069__link_environment.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _link_environment\n    lu.assertEquals(candidate('udp', 'ALIAS', 123, 'HOST', 456), {['ALIAS_PORT_123_UDP'] = 'udp://HOST:456', ['ALIAS_PORT_123_UDP_ADDR'] = 'HOST', ['ALIAS_PORT_123_UDP_PORT'] = '456', ['ALIAS_PORT_123_UDP_PROTO'] = 'udp'})\n    lu.assertEquals(candidate('tcp', 'ALIAS', 123, '10.0.0.1', 456), {['ALIAS_PORT_123_TCP'] = 'tcp://10.0.0.1:456', ['ALIAS_PORT_123_TCP_ADDR'] = '10.0.0.1', ['ALIAS_PORT_123_TCP_PORT'] = '456', ['ALIAS_PORT_123_TCP_PROTO'] = 'tcp'})\n    lu.assertEquals(candidate('tcp', 'ALIAS', 123, 'HOST', 456), {['ALIAS_PORT_123_TCP'] = 'tcp://HOST:456', ['ALIAS_PORT_123_TCP_ADDR'] = 'HOST', ['ALIAS_PORT_123_TCP_PORT'] = '456', ['ALIAS_PORT_123_TCP_PROTO'] = 'tcp'})\n    lu.assertEquals(candidate('tcp', 'bar', 8080, 'foo', 5000), {['BAR_PORT_8080_TCP'] = 'tcp://foo:5000', ['BAR_PORT_8080_TCP_ADDR'] = 'foo', ['BAR_PORT_8080_TCP_PORT'] = '5000', ['BAR_PORT_8080_TCP_PROTO'] = 'tcp'})\n    lu.assertEquals(candidate('tcp', 'name', 5000, 'hostname', 5001), {['NAME_PORT_5000_TCP'] = 'tcp://hostname:5001', ['NAME_PORT_5000_TCP_ADDR'] = 'hostname', ['NAME_PORT_5000_TCP_PORT'] = '5001', ['NAME_PORT_5000_TCP_PROTO'] = 'tcp'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_176136_array_reverse_order_transform_next_index_to_current_index", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # check if position is altered by the move\n-- # if (position >= move[0]) & (position <= move[1]):\n-- #     # alter the position\n-- #     offset = position - move[0]\n-- #     position = move[1] - offset\n-- # return position\n-- \n-- Transforms the position depending on the move.\n-- Works with the array_swap move type.\n-- This function transforms the position so that it can be used as the indice\n-- in the unaltered array, yet return the value it would have had if the move\n-- was actually performed and the position was used as indice.\n-- Parameters\n-- ----------\n-- position : int\n--     The index that one wants to use in the array if the move was performed.\n-- move : tuple of int\n--     A tuple with that represents a single, unique move.\n-- Returns\n-- -------\n-- int\n--     The index in the unaltered array that has the same value as the\n--     location in an array where the move was performed.\n-- Examples\n-- --------\n-- Some simple examples, the move remains the same, but the position changes:\n-- .. doctest::\n--     >>> from lclpy.evaluation.deltaeval.delta_qap \\\n--     ...     import array_reverse_order_transform_next_index_to_current_index \\\n--     ...         as transform_next_index_to_current_index\n--     ... # tests\n--     >>> transform_next_index_to_current_index(0, (1, 4))\n--     0\n--     >>> transform_next_index_to_current_index(1, (1, 4))\n--     4\n--     >>> transform_next_index_to_current_index(2, (1, 4))\n--     3\n--     >>> transform_next_index_to_current_index(3, (1, 4))\n--     2\n--     >>> transform_next_index_to_current_index(4, (1, 4))\n--     1\n--     >>> transform_next_index_to_current_index(5, (1, 4))\n--     5\nlocal function array_reverse_order_transform_next_index_to_current_index(position, move)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_176136_array_reverse_order_transform_next_index_to_current_index.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = array_reverse_order_transform_next_index_to_current_index\n    lu.assertEquals(candidate(5, {1, 4}), 5)\n    lu.assertEquals(candidate(2, {1, 4}), 3)\n    lu.assertEquals(candidate(0, {1, 4}), 0)\n    lu.assertEquals(candidate(4, {1, 4}), 1)\n    lu.assertEquals(candidate(1, {1, 4}), 4)\n    lu.assertEquals(candidate(3, {1, 4}), 2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_176194_other_classes", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if class_ind < 0 or class_ind >= nb_classes:\n-- #     error_str = \"class_ind must be within the range (0, nb_classes - 1)\"\n-- #     raise ValueError(error_str)\n-- # other_classes_list = list(range(nb_classes))\n-- # other_classes_list.remove(class_ind)\n-- # return other_classes_list\n-- \n-- Returns a list of class indices excluding the class indexed by class_ind\n-- :param nb_classes: number of classes in the task\n-- :param class_ind: the class index to be omitted\n-- :return: list of class indices excluding the class indexed by class_ind\nlocal function other_classes(nb_classes, class_ind)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_176194_other_classes.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = other_classes\n    lu.assertEquals(candidate(10, 3), {0, 1, 2, 4, 5, 6, 7, 8, 9})\n    lu.assertEquals(candidate(5, 2), {0, 1, 3, 4})\n    lu.assertEquals(candidate(6, 2), {0, 1, 3, 4, 5})\n    lu.assertEquals(candidate(5, 1), {0, 2, 3, 4})\n    lu.assertEquals(candidate(5, 4), {0, 1, 2, 3})\n    lu.assertEquals(candidate(3, 2), {0, 1})\n    lu.assertEquals(candidate(2, 1), {0})\n    lu.assertEquals(candidate(3, 0), {1, 2})\n    lu.assertEquals(candidate(2, 0), {1})\n    lu.assertEquals(candidate(5, 3), {0, 1, 2, 4})\n    lu.assertEquals(candidate(8, 4), {0, 1, 2, 3, 5, 6, 7})\n    lu.assertEquals(candidate(4, 1), {0, 2, 3})\n    lu.assertEquals(candidate(3, 1), {0, 2})\n    lu.assertEquals(candidate(5, 0), {1, 2, 3, 4})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_17889_clean_query_string", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # string.replace(' ', '%20')\n-- # string.replace(',', '')\n-- # return string\n-- \n--  Cleans string of ' 's and 's \nlocal function clean_query_string(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_17889_clean_query_string.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = clean_query_string\n    lu.assertEquals(candidate('Hello%20World'), 'Hello%20World')\n    lu.assertEquals(candidate('Hello%2C+World'), 'Hello%2C+World')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_179175_partition_2", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # swap the first and last\n-- # temp = a[l]\n-- # a[l] = a[r]\n-- # a[r] = temp\n-- # p = a[l]\n-- # i = l+1\n-- # j = l+1\n-- # while j <= r:\n-- #     if a[j] <= p:\n-- #         temp = a[j]\n-- #         a[j] = a[i]\n-- #         a[i] = temp\n-- #         i += 1\n-- #         j += 1\n-- #     else:\n-- #         j += 1\n-- # temp = a[l]\n-- # a[l] = a[i-1]\n-- # a[i-1] = temp\n-- # return i-1\n-- \n-- partition array a[l..r]\n-- Pivot: Use the last element of the array. Swap with the first element.\nlocal function partition_2(a, l, r)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_179175_partition_2.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = partition_2\n    lu.assertEquals(candidate({1, 2, 3, 4, 5}, 0, 4), 4)\n    lu.assertEquals(candidate({3, 1, 2, 5, 4, 6}, 0, 3), 3)\n    lu.assertEquals(candidate({1, 4, 2, 3}, 0, 3), 2)\n    lu.assertEquals(candidate({3, 1, 2, 5, 4, 6}, 0, 4), 3)\n    lu.assertEquals(candidate(list(range(20)), 0, 19), 19)\n    lu.assertEquals(candidate({1, 2, 3, 4, 5}, 0, 3), 3)\n    lu.assertEquals(candidate({4, 3, 2, 1}, 0, 3), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_180440_dir_filter", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return not item.startswith(\"_\")\n-- \n-- Accept each item which doesn't start with _\n-- :type item: str\n-- :param item: a string item to filter\n-- :return: true if item doesn't start with _\nlocal function dir_filter(item)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_180440_dir_filter.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = dir_filter\n    lu.assertEquals(candidate('1'), true)\n    lu.assertEquals(candidate('foo/_bar.py'), true)\n    lu.assertEquals(candidate('___'), false)\n    lu.assertEquals(candidate('_____'), false)\n    lu.assertEquals(candidate('_foo.py'), false)\n    lu.assertEquals(candidate('a'), true)\n    lu.assertEquals(candidate('____'), false)\n    lu.assertEquals(candidate('a_1'), true)\n    lu.assertEquals(candidate('foo.py'), true)\n    lu.assertEquals(candidate('_'), false)\n    lu.assertEquals(candidate('a1'), true)\n    lu.assertEquals(candidate('_foo/_bar.py'), false)\n    lu.assertEquals(candidate('_1'), false)\n    lu.assertEquals(candidate('________'), false)\n    lu.assertEquals(candidate('__'), false)\n    lu.assertEquals(candidate('_______'), false)\n    lu.assertEquals(candidate('not_underscore'), true)\n    lu.assertEquals(candidate('_'), false)\n    lu.assertEquals(candidate('__init__.py'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_180566_factorial", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if number <= 1:\n-- #     return 1\n-- # return number * factorial(number - 1)\n-- \n-- Return factorial of number.\nlocal function factorial(number)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_180566_factorial.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = factorial\n    lu.assertEquals(candidate(8), 40320)\n    lu.assertEquals(candidate(5), 120)\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(16), 20922789888000)\n    lu.assertEquals(candidate(17), 355687428096000)\n    lu.assertEquals(candidate(20), 2432902008176640000)\n    lu.assertEquals(candidate(4), 24)\n    lu.assertEquals(candidate(11), 39916800)\n    lu.assertEquals(candidate(0), 1)\n    lu.assertEquals(candidate(12), 479001600)\n    lu.assertEquals(candidate(10), 3628800)\n    lu.assertEquals(candidate(19), 121645100408832000)\n    lu.assertEquals(candidate(15), 1307674368000)\n    lu.assertEquals(candidate(13), 6227020800)\n    lu.assertEquals(candidate(9), 362880)\n    lu.assertEquals(candidate(18), 6402373705728000)\n    lu.assertEquals(candidate(3), 6)\n    lu.assertEquals(candidate(6), 720)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(14), 87178291200)\n    lu.assertEquals(candidate(7), 5040)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_180620_compute_iou", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Compute intersection\n-- # b1_y1, b1_x1, b1_h, b1_w = box1\n-- # b2_y1, b2_x1, b2_h, b2_w = box2\n-- # b1_y2, b1_x2 = b1_y1+b1_h, b1_x1+b1_w\n-- # b2_y2, b2_x2 = b2_y1+b2_h, b2_x1+b2_w\n-- # y1 = max(b1_y1, b2_y1)\n-- # x1 = max(b1_x1, b2_x1)\n-- # y2 = min(b1_y2, b2_y2)\n-- # x2 = min(b1_x2, b2_x2)\n-- # intersection = max(x2 - x1, 0)*max(y2 - y1, 0)\n-- # # Compute unions\n-- # b1_area = (b1_y2 - b1_y1) * (b1_x2 - b1_x1)\n-- # b2_area = (b2_y2 - b2_y1) * (b2_x2 - b2_x1)\n-- # union = b1_area + b2_area - intersection\n-- # iou = intersection / union\n-- # return iou\n-- \n-- Compute IoU between two boxes.\n-- box1: [b1_y1, b1_x1, b1_y2, b1_x2]\n-- box2: [b2_y1, b2_x1, b2_y2, b2_x2]\n-- return: float\nlocal function compute_iou(box1, box2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_180620_compute_iou.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = compute_iou\n    lu.assertEquals(candidate({0, 0, 1, 1}, {0, 0, 0, 1}), 0)\n    lu.assertEquals(candidate({0, 0, 1, 1}, {0, 0, 1, 1}), 1)\n    lu.assertEquals(candidate({0, 0, 1, 1}, {0, 1, 1, 1}), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_180956__fabric_network_ipam_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return '%s-%s-network-ipam' % (fabric_name, network_type)\n-- \n-- :param fabric_name: string\n-- :param network_type: string (One of the constants defined in NetworkType)\n-- :return: string\nlocal function _fabric_network_ipam_name(fabric_name, network_type)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_180956__fabric_network_ipam_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _fabric_network_ipam_name\n    lu.assertEquals(candidate('fab9', 'management'), 'fab9-management-network-ipam')\n    lu.assertEquals(candidate('foo', 'bar'), 'foo-bar-network-ipam')\n    lu.assertEquals(candidate('fab10', 'management'), 'fab10-management-network-ipam')\n    lu.assertEquals(candidate('my-fabric-name', 'management'), 'my-fabric-name-management-network-ipam')\n    lu.assertEquals(candidate('foo-bar', 'vlan'), 'foo-bar-vlan-network-ipam')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_181012_get_msg", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # msg = \"Training a ChemProp model with \"\n-- # if feat:\n-- #     msg += \"set features \"\n-- # if feat and mpnn:\n-- #     msg += \"and \"\n-- # if mpnn:\n-- #     msg += \"an MPNN \"\n-- # msg += f\"in folder {train_folder}\\n\"\n-- # return msg\n-- \n-- Create a message telling the user what kind of model we're training.\n-- Args:\n--   feat (bool): whether this model is being trained with external features\n--   mpnn (bool): whether this model is being trained with an mpnn (vs. just with\n--     external features)\n--   train_folder (str): path to the training folder\n-- Returns:\n--   msg (str): the message\nlocal function get_msg(feat, mpnn, train_folder)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_181012_get_msg.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_msg\n    lu.assertEquals(candidate(false, true, 'test'), 'Training a ChemProp model with an MPNN in folder test\\n')\n    lu.assertEquals(candidate(true, true, 'folder1'), 'Training a ChemProp model with set features and an MPNN in folder folder1\\n')\n    lu.assertEquals(candidate(false, true, 'folder1'), 'Training a ChemProp model with an MPNN in folder folder1\\n')\n    lu.assertEquals(candidate(true, false, 'folder1'), 'Training a ChemProp model with set features in folder folder1\\n')\n    lu.assertEquals(candidate(true, true, 'test'), 'Training a ChemProp model with set features and an MPNN in folder test\\n')\n    lu.assertEquals(candidate(true, false, 'test'), 'Training a ChemProp model with set features in folder test\\n')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_181215_b_add", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # y = ''\n-- # c = 0\n-- # for i in range(len(a)):\n-- #     ia = int(a[i])\n-- #     ib = int(b[i])\n-- #     iy = ia+ib+c\n-- #     if iy > 1:\n-- #         c = 1\n-- #     else:\n-- #         c = 0\n-- #     if iy % 2 == 0:\n-- #         y += '0'\n-- #     else:\n-- #         y += '1'\n-- # return y\n-- \n-- add two bitstrings encoded as strings of '0's and '1's.\nlocal function b_add(a, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_181215_b_add.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = b_add\n    lu.assertEquals(candidate('0000', '0001'), '0001')\n    lu.assertEquals(candidate('0010', '0000'), '0010')\n    lu.assertEquals(candidate('0000', '0000'), '0000')\n    lu.assertEquals(candidate('0001', '0010'), '0011')\n    lu.assertEquals(candidate('0001', '0000'), '0001')\n    lu.assertEquals(candidate('0011', '0000'), '0011')\n    lu.assertEquals(candidate('0010', '0001'), '0011')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_181976_get_entity_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # components = entity.split('.')\n-- # return components[len(components)-1]\n-- \n-- Returns the entity name after stripping off namespace and/or version information\n-- Args:\n--     entity: The full entity name\nlocal function get_entity_name(entity)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_181976_get_entity_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_entity_name\n    lu.assertEquals(candidate('entity_name'), 'entity_name')\n    lu.assertEquals(candidate('org.sagebionetworks.repo.model.FileEntity'), 'FileEntity')\n    lu.assertEquals(candidate('org.sagebionetworks.repo.model.Folder'), 'Folder')\n    lu.assertEquals(candidate('my_namespace.my_entity'), 'my_entity')\n    lu.assertEquals(candidate('my_entity'), 'my_entity')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_182034_strip_url", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # u = str(domain)\n-- # u = u.lower()\n-- # check_for_http = \"http://\"\n-- # check_for_https = \"https://\"\n-- # result = u.replace(check_for_http, \"\")\n-- # result = result.replace(check_for_https, \"\")\n-- # if www != None:\n-- #     result = result.replace(www.lower() + \".\", \"\")\n-- # return result\n-- \n-- receive a URL Field and remove leading http:// or https://\n-- optionally remove www.\n-- :param url: eg. http://www.medyear.com\n-- :param www remove the prefix passed = \"www.\"\n-- :return:\nlocal function strip_url(domain, www)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_182034_strip_url.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = strip_url\n    lu.assertEquals(candidate('www.medyear.com', 'www'), 'medyear.com')\n    lu.assertEquals(candidate('http://www.medyear.com'), 'www.medyear.com')\n    lu.assertEquals(candidate('www.medyear.com', 'www'), 'medyear.com')\n    lu.assertEquals(candidate('https://www.medyear.com', 'www'), 'medyear.com')\n    lu.assertEquals(candidate('www.medyear.com'), 'www.medyear.com')\n    lu.assertEquals(candidate('https://www.medyear.com'), 'www.medyear.com')\n    lu.assertEquals(candidate('http://www.medyear.com', 'www'), 'medyear.com')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_182677_pddl_to_tarski_type", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # translations = {\"int\": \"Integer\", \"real\": \"Real\", \"number\": \"Real\"}\n-- # return translations.get(typename, typename)\n-- \n--  Translate a few PDDL types into their corresponding Tarski names\n-- (e.g. the FSTRIPS type \"int\" corresponds to the Tarski type \"Integer\").\nlocal function pddl_to_tarski_type(typename)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_182677_pddl_to_tarski_type.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = pddl_to_tarski_type\n    lu.assertEquals(candidate('int'), 'Integer')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_184499_get_bucket_key", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if uri.startswith(\"s3://\"):\n-- #     uri = uri[5:]\n-- # components = uri.split(\"/\")\n-- # bucket = components[0]\n-- # key = \"\"\n-- # if len(components) > 1:\n-- #     key = \"/\".join(components[1:])\n-- # return bucket, key\n-- \n-- Return bucket name and key from given S3 URI\nlocal function get_bucket_key(uri)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_184499_get_bucket_key.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_bucket_key\n    lu.assertEquals(candidate('s3://mybucket/foo/bar.json'), {'mybucket', 'foo/bar.json'})\n    lu.assertEquals(candidate('s3://mybucket'), {'mybucket', ''})\n    lu.assertEquals(candidate('s3://mybucket/foo/bar/'), {'mybucket', 'foo/bar/'})\n    lu.assertEquals(candidate('s3://foo/bar/baz'), {'foo', 'bar/baz'})\n    lu.assertEquals(candidate('s3://mybucket/'), {'mybucket', ''})\n    lu.assertEquals(candidate('s3://foo/bar'), {'foo', 'bar'})\n    lu.assertEquals(candidate('s3://mybucket/foo/bar.json'), {'mybucket', 'foo/bar.json'})\n    lu.assertEquals(candidate('s3://mybucket'), {'mybucket', ''})\n    lu.assertEquals(candidate('s3://mybucket/'), {'mybucket', ''})\n    lu.assertEquals(candidate('s3://mybucket/foo/bar/'), {'mybucket', 'foo/bar/'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_18473_find_sum_of_arithmetic_sequence", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return int((requested_terms / 2) * (2 * first_term + (requested_terms - 1) * common_difference))\n-- \n-- Finds the sum of an arithmetic sequence\n-- :param requested_terms:\n-- :param first_term:\n-- :param common_difference:\n-- :return: the sum of an arithmetic sequence\nlocal function find_sum_of_arithmetic_sequence(requested_terms, first_term, common_difference)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_18473_find_sum_of_arithmetic_sequence.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = find_sum_of_arithmetic_sequence\n    lu.assertEquals(candidate(10, 1, 1), 55)\n    lu.assertEquals(candidate(1, 1, 2), 1)\n    lu.assertEquals(candidate(1, 5, 1), 5)\n    lu.assertEquals(candidate(5, 1, 1), 15)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_185648_IRAF_image_type", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return image_type.split()[0].upper()\n-- \n-- Convert MaximDL default image type names to IRAF\n-- Parameters\n-- ----------\n-- image_type : str\n--     Value of the FITS header keyword IMAGETYP; acceptable values are\n--     below in Notes.\n-- Returns\n-- -------\n-- str\n--     IRAF image type (one of 'BIAS', 'DARK', 'FLAT' or 'LIGHT')\n-- Notes\n-- -----\n-- The MaximDL default is, e.g. 'Bias Frame', which IRAF calls\n-- 'BIAS'. Can safely be called with an IRAF-style image_type.\nlocal function IRAF_image_type(image_type)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_185648_IRAF_image_type.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = IRAF_image_type\n    lu.assertEquals(candidate(candidate('FLAT')), 'FLAT')\n    lu.assertEquals(candidate('Flat Frame'), 'FLAT')\n    lu.assertEquals(candidate(candidate('BIAS')), 'BIAS')\n    lu.assertEquals(candidate('Bias Frame'), 'BIAS')\n    lu.assertEquals(candidate(candidate('DARK')), 'DARK')\n    lu.assertEquals(candidate('Light Frame'), 'LIGHT')\n    lu.assertEquals(candidate(candidate('LIGHT')), 'LIGHT')\n    lu.assertEquals(candidate('Dark Frame'), 'DARK')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_185855_length_range_for_entropy", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # min_length = 3\n-- # max_length = min_length + int(entropy / 2)\n-- # return min_length, max_length\n-- \n-- Returns length range to sample from for given entropy.\nlocal function length_range_for_entropy(entropy)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_185855_length_range_for_entropy.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = length_range_for_entropy\n    lu.assertEquals(candidate(2), {3, 4})\n    lu.assertEquals(candidate(5), {3, 5})\n    lu.assertEquals(candidate(0), {3, 3})\n    lu.assertEquals(candidate(1234), candidate(1234))\n    lu.assertEquals(candidate(4), {3, 5})\n    lu.assertEquals(candidate(3), {3, 4})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_186735_string_to_list", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # l = []\n-- # seperate_secondaries = the_string.split()\n-- # for secondary in enumerate(seperate_secondaries):\n-- #     l.append([])\n-- #     for item in secondary[1].split(',')[:-1]:\n-- #         l[secondary[0]].append(int(item))\n-- # return l\n-- \n--  converts string to list of ints \nlocal function string_to_list(the_string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_186735_string_to_list.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = string_to_list\n    lu.assertEquals(candidate('1,'), {{1}})\n    lu.assertEquals(candidate('3, 4, 5,'), {{3}, {4}, {5}})\n    lu.assertEquals(candidate('3,'), {{3}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_186984__module_descriptor_file", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"{}.descriptor.txt\".format(module_dir)\n-- \n-- Returns the name of the file containing descriptor for the 'module_dir'.\nlocal function _module_descriptor_file(module_dir)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_186984__module_descriptor_file.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _module_descriptor_file\n    lu.assertEquals(candidate('foo/bar'), 'foo/bar.descriptor.txt')\n    lu.assertEquals(candidate('foo'), 'foo.descriptor.txt')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_188499_bond_energy", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return 0.5 * fc * (r - r0)**2\n-- \n-- Calculate the bond energy using the harmonic potential.\n-- Args:\n--   r (float):  distance between atoms [angstrom]\n--   fc (float):  force constant [kcal/mol]\n--   r0 (float):  equilibrium distance [angstrom]\n-- Returns:\n--   e_bond (float):  energy of bond [kcal/mol]\nlocal function bond_energy(r, fc, r0)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_188499_bond_energy.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = bond_energy\n    lu.assertEquals(candidate(0.0, 0.0, 0.0), 0.0)\n    lu.assertEquals(candidate(1.0, 1.0, 0.0), 0.5)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_189051_is_return", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return param_name.startswith('$return')\n-- \n-- Determine if a parameter is named as a (internal) return.\n-- :param param_name: String with a parameter name\n-- :returns: True iff the name has the form of an internal return name\nlocal function is_return(param_name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_189051_is_return.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_return\n    lu.assertEquals(candidate(' _return_ '), false)\n    lu.assertEquals(candidate('$return'), true)\n    lu.assertEquals(candidate('$return_8'), true)\n    lu.assertEquals(candidate('return.'), false)\n    lu.assertEquals(candidate('$return_values[0][1]'), true)\n    lu.assertEquals(candidate('$return_17'), true)\n    lu.assertEquals(candidate('$return_26'), true)\n    lu.assertEquals(candidate('$return_7'), true)\n    lu.assertEquals(candidate('$return_18'), true)\n    lu.assertEquals(candidate('$return_0'), true)\n    lu.assertEquals(candidate('return_'), false)\n    lu.assertEquals(candidate('$return_values[0][1][2][3]'), true)\n    lu.assertEquals(candidate('$return_28'), true)\n    lu.assertEquals(candidate('$return_25'), true)\n    lu.assertEquals(candidate('$return_values[0][1][2]'), true)\n    lu.assertEquals(candidate('$return_values'), true)\n    lu.assertEquals(candidate('$return_values[0][1][2][3][4]'), true)\n    lu.assertEquals(candidate('$return_9'), true)\n    lu.assertEquals(candidate('_return_'), false)\n    lu.assertEquals(candidate('$return_23'), true)\n    lu.assertEquals(candidate('$return_14'), true)\n    lu.assertEquals(candidate('$return_29'), true)\n    lu.assertEquals(candidate('_return '), false)\n    lu.assertEquals(candidate('$return_16'), true)\n    lu.assertEquals(candidate(' return'), false)\n    lu.assertEquals(candidate('$return_19'), true)\n    lu.assertEquals(candidate('return'), false)\n    lu.assertEquals(candidate('$return_21'), true)\n    lu.assertEquals(candidate('$return_20'), true)\n    lu.assertEquals(candidate('.return.'), false)\n    lu.assertEquals(candidate('$return'), true)\n    lu.assertEquals(candidate(' return_'), false)\n    lu.assertEquals(candidate('.return'), false)\n    lu.assertEquals(candidate('$return_15'), true)\n    lu.assertEquals(candidate('return '), false)\n    lu.assertEquals(candidate('$return_27'), true)\n    lu.assertEquals(candidate('$return_value'), true)\n    lu.assertEquals(candidate('$return_values[0]'), true)\n    lu.assertEquals(candidate('$return_24'), true)\n    lu.assertEquals(candidate('not_a_return'), false)\n    lu.assertEquals(candidate('$return_22'), true)\n    lu.assertEquals(candidate('$return_values[0][1][2][3][4][5]'), true)\n    lu.assertEquals(candidate('$return_12'), true)\n    lu.assertEquals(candidate('$return_6'), true)\n    lu.assertEquals(candidate('$return_13'), true)\n    lu.assertEquals(candidate('$return_11'), true)\n    lu.assertEquals(candidate('$return_10'), true)\n    lu.assertEquals(candidate('$return_5'), true)\n    lu.assertEquals(candidate('_return'), false)\n    lu.assertEquals(candidate(' return '), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_189531_composite_colors", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # r1, g1, b1, a1 = first\n-- # r2, g2, b2, a2 = second\n-- # y = a2 * (1.0 - a1)\n-- # ro = r1 * a1 + r2 * y\n-- # go = g1 * a1 + g2 * y\n-- # bo = b1 * a1 + b2 * y\n-- # ao = a1 + y\n-- # return (ro, go, bo, ao)\n-- \n--  Composite two colors together using their given alpha.\n-- The first color will be composited on top of the second color.\n-- Parameters\n-- ----------\n-- first : tuple\n--     The rgba tuple of the first color. All values are floats in\n--     the range 0.0 - 1.0.\n-- second : tuple\n--     The rgba tuple of the second color. The format of this tuple\n--     is the same as the first color.\n-- Returns\n-- -------\n-- result : tuple\n--     The composited rgba color tuple.\nlocal function composite_colors(first, second)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_189531_composite_colors.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = composite_colors\n    lu.assertEquals(candidate({1.0, 1.0, 1.0, 1.0}, {0.0, 0.0, 0.0, 1.0}), {1.0, 1.0, 1.0, 1.0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_189571_infer_emg_channels", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # emg = ['EMG Chin']\n-- # found = []\n-- # # find frontal channel\n-- # for ch in ch_names:\n-- #     if any([x in ch for x in emg]):\n-- #         found.append(ch)\n-- # return found\n-- \n-- This function receives a list of channel names and will return\n-- one frontal, one central and one occipital channel.    \nlocal function infer_emg_channels(ch_names)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_189571_infer_emg_channels.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = infer_emg_channels\n    lu.assertEquals(candidate({'EMG Chin'}), {'EMG Chin'})\n    lu.assertEquals(candidate({'EMG Chin'}), {'EMG Chin'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_189989_constrain", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # x, y = pos\n-- # w, h = size\n-- # if x >= w:\n-- #     x = w-1\n-- # if y >= h:\n-- #     y = h-1\n-- # if x < 0:\n-- #     x = 0\n-- # if y < 0:\n-- #     y = 0\n-- # return x, y\n-- \n-- constrain a position (pos) in the area defined by size\nlocal function constrain(pos, size)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_189989_constrain.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = constrain\n    lu.assertEquals(candidate({99, 199}, {100, 200}), {99, 199})\n    lu.assertEquals(candidate({-1, -1}, {100, 200}), {0, 0})\n    lu.assertEquals(candidate({101, 201}, {100, 200}), {99, 199})\n    lu.assertEquals(candidate({100, 200}, {100, 200}), {99, 199})\n    lu.assertEquals(candidate({9, 19}, {100, 200}), {9, 19})\n    lu.assertEquals(candidate({0, 0}, {0, 0}), {0, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_19021_diff_possible", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if k < 0:\n-- #     raise ValueError('k can not be non negative')\n-- # # Find k since as long as i is not larger than k\n-- # # we do not even need to compare\n-- # if numbers[-1] < k:\n-- #     return False\n-- # start_i = 0\n-- # while start_i < len(numbers):\n-- #     if numbers[start_i] >= k:\n-- #         break\n-- #     else:\n-- #         start_i += 1\n-- # for i in range(start_i, len(numbers)):\n-- #     needed_num = numbers[i] - k\n-- #     for j in reversed(range(0, i)):\n-- #         if numbers[j] == needed_num:\n-- #             return True\n-- #         elif numbers[j] < needed_num:\n-- #             # All hope is lost, we can never reach k again\n-- #             break\n-- # return False\n-- \n-- Given a list of sorted integers and a non negative\n-- integer k, find if there exists 2 indicies i and j\n-- such that A[i] - A[j] = k, i != j\nlocal function diff_possible(numbers, k)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_19021_diff_possible.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = diff_possible\n    lu.assertEquals(candidate(list(range(1000)), 1000), false)\n    lu.assertEquals(candidate({1, 2, 3, 5, 6, 10}, 5), true)\n    lu.assertEquals(candidate({1, 2, 3, 5, 6, 10}, 3), true)\n    lu.assertEquals(candidate(list(range(10)), 7), true)\n    lu.assertEquals(candidate(list(range(10)), 8), true)\n    lu.assertEquals(candidate(list(range(20)), 4), true)\n    lu.assertEquals(candidate(list(range(10)), 3), true)\n    lu.assertEquals(candidate(list(range(10)), 2), true)\n    lu.assertEquals(candidate(list(range(20)), 9), true)\n    lu.assertEquals(candidate(list(range(10)), 4), true)\n    lu.assertEquals(candidate(list(range(1000)), 999), true)\n    lu.assertEquals(candidate(list(range(10)), 10), false)\n    lu.assertEquals(candidate({1, 2, 3, 5, 6, 10}, 9), true)\n    lu.assertEquals(candidate(list(range(10)), 9), true)\n    lu.assertEquals(candidate({1, 2, 3, 5, 6, 10}, 6), false)\n    lu.assertEquals(candidate({1, 2, 3, 5, 6, 10}, 1), true)\n    lu.assertEquals(candidate(list(range(5)), 3), true)\n    lu.assertEquals(candidate(list(range(100)), 1000), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_190761_seg_text2listxy", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # strList = text[1:-1].split(\", \")\n-- # x = list(map(int, strList[::2]))\n-- # y = list(map(int, strList[1::2]))\n-- # return x, y\n-- \n-- Purpose: parse x, y coordinates from text of seg (segmentation) annotation in xml\n-- Args: \n--     text: text of seg (segmentation) annotation in xml, \"[x0,y0, x1,y1, x2,y2, x3,y3, ...]\"\n-- Returns:  lists of storing x y coordinates, \n--     x: [x0, x1, x2, x3, ...]\n--     y: [y0, y1, y2, y3, ...]\nlocal function seg_text2listxy(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_190761_seg_text2listxy.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = seg_text2listxy\n    lu.assertEquals(candidate('[1, 2, 3, 4, 5, 6]'), {{1, 3, 5}, {2, 4, 6}})\n    lu.assertEquals(candidate('[1, 2, 3, 4]'), {{1, 3}, {2, 4}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_190870___get_list_str", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # str_list = x.split('\\001')\n-- # s = ''\n-- # for i in range(len(str_list)):\n-- #     s += str_list[i] + ','\n-- #     if (i + 1) % 3 == 0 and i + 1 != len(str_list):\n-- #         s += '\\n'\n-- # return s[:-1]\n-- \n-- Get value of the categorical variable, and put 3 value in one line\n-- :param x: str\n-- :return: list\nlocal function __get_list_str(x)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_190870___get_list_str.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = __get_list_str\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('1,2,3'), '1,2,3')\n    lu.assertEquals(candidate('a'), 'a')\n    lu.assertEquals(candidate('a,b,c'), 'a,b,c')\n    lu.assertEquals(candidate('1\\x012\\x013'), '1,2,3')\n    lu.assertEquals(candidate('1'), '1')\n    lu.assertEquals(candidate('1,2,3'), '1,2,3')\n    lu.assertEquals(candidate('a\\x01b\\x01c'), 'a,b,c')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_191156_caesar", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # ic = ord(c)\n-- # if not 97 <= ic <= 122 and not 65 <= ic <= 90:\n-- #     return c\n-- # if 97 <= ic <= 122 and 97 <= ic + x % 26 <= 122:\n-- #     return chr(ic + x % 26)\n-- # elif 65 <= ic <= 122 and 65 <= ic + x % 26 <= 90:\n-- #     return chr(ic + x % 26)\n-- # else:\n-- #     return chr(ic + x % 26 - 26)\n-- \n-- :type c: str\n-- :type x: int\n-- :rtype: str\nlocal function caesar(c, x)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_191156_caesar.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = caesar\n    lu.assertEquals(candidate('Z', 1), 'A')\n    lu.assertEquals(candidate('N', 1), 'O')\n    lu.assertEquals(candidate('a', 3), 'd')\n    lu.assertEquals(candidate('n', 0), 'n')\n    lu.assertEquals(candidate('Z', 2), 'B')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_191557_calc_permutation", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # value = 1\n-- # return value\n-- \n--     Evaluates the permutation expression\nlocal function calc_permutation(m, mm, _accuracy)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_191557_calc_permutation.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = calc_permutation\n    lu.assertEquals(candidate(1, 1, 0.9), 1)\n    lu.assertEquals(candidate(5, 5, 0.9), 1)\n    lu.assertEquals(candidate(10, 10, 0.9), 1)\n    lu.assertEquals(candidate(9, 9, 0.9), 1)\n    lu.assertEquals(candidate(6, 6, 0.9), 1)\n    lu.assertEquals(candidate(2, 2, 0.9), 1)\n    lu.assertEquals(candidate(3, 3, 0.9), 1)\n    lu.assertEquals(candidate(4, 4, 0.9), 1)\n    lu.assertEquals(candidate(7, 7, 0.9), 1)\n    lu.assertEquals(candidate(8, 8, 0.9), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_192692_tordist", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # dx = abs(x2 - x1)\n-- # if dx > wrap_dist*0.5:\n-- #     return wrap_dist-dx\n-- # else:\n-- #     return dx\n-- \n-- Calculate the toroidial distance between two scalars\n-- Args:\n--     x1(float) : first datapoint\n--     x2(float) : second datapoint\n--     wrap_dist(float) : wrapping distance (highest value), values higher than this will wrap around to zero\n-- Returns:\n--     distance(float) : toroidial distance between x1 and x2, wrapping around wrap_dist\nlocal function tordist(x1, x2, wrap_dist)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_192692_tordist.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = tordist\n    lu.assertEquals(candidate(0.75, 1, 2), 0.25)\n    lu.assertEquals(candidate(100, 100, 10), 0)\n    lu.assertEquals(candidate(0.75, 1.25, 2), 0.5)\n    lu.assertEquals(candidate(9, 10, 10), 1)\n    lu.assertEquals(candidate(0, 1, 10), 1)\n    lu.assertEquals(candidate(0, 9, 10), 1)\n    lu.assertEquals(candidate(1.25, 0.75, 2), 0.5)\n    lu.assertEquals(candidate(1, 0.5, 1), 0.5)\n    lu.assertEquals(candidate(0.5, 1, 1), 0.5)\n    lu.assertEquals(candidate(1, 0.75, 2), 0.25)\n    lu.assertEquals(candidate(2, 11, 10), 1)\n    lu.assertEquals(candidate(1, 2, 3), 1)\n    lu.assertEquals(candidate(1, 1, 1), 0)\n    lu.assertEquals(candidate(0, 0, 10), 0)\n    lu.assertEquals(candidate(0.5, 0.75, 1), 0.25)\n    lu.assertEquals(candidate(5, 0, 10), 5)\n    lu.assertEquals(candidate(0.75, 0.5, 1), 0.25)\n    lu.assertEquals(candidate(0, 5, 10), 5)\n    lu.assertEquals(candidate(2, 1, 10), 1)\n    lu.assertEquals(candidate(0, -1, 10), 1)\n    lu.assertEquals(candidate(0, 10, 10), 0)\n    lu.assertEquals(candidate(2, 1, 2), 1)\n    lu.assertEquals(candidate(0, -9, 10), 1)\n    lu.assertEquals(candidate(0, -10, 10), 0)\n    lu.assertEquals(candidate(0, 0, 10), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_193452_g_iter", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # \"*** YOUR CODE HERE ***\"\n-- # if n <= 3:\n-- #     return n\n-- # g1, g2, g3 = 1, 2, 3\n-- # for i in range(4, n + 1):\n-- #     g1, g2, g3 = g2, g3, 3 * g1 + 2 * g2 + g3\n-- # return g3\n-- \n-- Return the value of G(n), computed iteratively.\n-- >>> g_iter(1)\n-- 1\n-- >>> g_iter(2)\n-- 2\n-- >>> g_iter(3)\n-- 3\n-- >>> g_iter(4)\n-- 10\n-- >>> g_iter(5)\n-- 22\n-- >>> from construct_check import check\n-- >>> # ban recursion\n-- >>> check(HW_SOURCE_FILE, 'g_iter', ['Recursion'])\n-- True\nlocal function g_iter(n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_193452_g_iter.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = g_iter\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(5), 22)\n    lu.assertEquals(candidate(3), 3)\n    lu.assertEquals(candidate(4), 10)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_193505_decoupParagraphEnPhrases", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # import re\n-- # finsDePhrase = re.compile(r\"\"\"\n-- #     # Split sentences on whitespace between them.\n-- #     (?:               # Group for two positive lookbehinds.\n-- #       (?<=[.!?])      # Either an end of sentence punct,\n-- #     | (?<=[.!?]['\"])  # or end of sentence punct and quote.\n-- #     )                 # End group of two positive lookbehinds.\n-- #     (?<!  Mr\\.   )    # Don't end sentence on \"Mr.\"\n-- #     (?<!  M\\.   )    # Don't end sentence on \"M.\"\n-- #     (?<!  Mme\\.   )    # Don't end sentence on \"Mme.\"\n-- #     (?<!  Mrs\\.  )    # Don't end sentence on \"Mrs.\"\n-- #     (?<!  Jr\\.   )    # Don't end sentence on \"Jr.\"\n-- #     (?<!  Dr\\.   )    # Don't end sentence on \"Dr.\"\n-- #     (?<!  Prof\\. )    # Don't end sentence on \"Prof.\"\n-- #     (?<!  Sr\\.   )    # Don't end sentence on \"Sr.\"\n-- #     \\s+               # Split on whitespace between sentences.\n-- #     \"\"\",\n-- #                           re.IGNORECASE | re.VERBOSE)\n-- # listeDePhrases = finsDePhrase.split(paragraph)\n-- # return [ph for ph in listeDePhrases if ph]\n-- \n-- returns the paragraph splited in phrases ignoring specifics titles. To be completed\nlocal function decoupParagraphEnPhrases(paragraph)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_193505_decoupParagraphEnPhrases.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = decoupParagraphEnPhrases\n    lu.assertEquals(candidate('Bonjour, comment vous allez?'), {'Bonjour, comment vous allez?'})\n    lu.assertEquals(candidate('Ceci est une phrase. Ceci est une autre. Ceci est une derni\u00e8re. Ceci est un quatri\u00e8me.'), {'Ceci est une phrase.', 'Ceci est une autre.', 'Ceci est une derni\u00e8re.', 'Ceci est un quatri\u00e8me.'})\n    lu.assertEquals(candidate('Bonjour, comment allez vous?'), {'Bonjour, comment allez vous?'})\n    lu.assertEquals(candidate('Bonjour, comment allez-vous?'), {'Bonjour, comment allez-vous?'})\n    lu.assertEquals(candidate(''), {})\n    lu.assertEquals(candidate('Ceci est une phrase. Ceci est une autre.'), {'Ceci est une phrase.', 'Ceci est une autre.'})\n    lu.assertEquals(candidate('Ceci est une phrase.'), {'Ceci est une phrase.'})\n    lu.assertEquals(candidate('Hello world!'), {'Hello world!'})\n    lu.assertEquals(candidate('Ceci est une phrase. Ceci est une autre. Ceci est une derni\u00e8re.'), {'Ceci est une phrase.', 'Ceci est une autre.', 'Ceci est une derni\u00e8re.'})\n    lu.assertEquals(candidate('Bonjour, comment allez vous?'), {'Bonjour, comment allez vous?'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_193898_to_devport", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return pipe << 7 | port\n-- \n-- Convert a (pipe, port) combination into a 9-bit (devport) number\n-- NOTE: For now this is a Tofino-specific method\nlocal function to_devport(pipe, port)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_193898_to_devport.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = to_devport\n    lu.assertEquals(candidate(0, 129), 129)\n    lu.assertEquals(candidate(1, 5), 133)\n    lu.assertEquals(candidate(1, 12), 140)\n    lu.assertEquals(candidate(2, 5), 261)\n    lu.assertEquals(candidate(0, 15), 15)\n    lu.assertEquals(candidate(0, 63), 63)\n    lu.assertEquals(candidate(2, 6), 262)\n    lu.assertEquals(candidate(1, 4), 132)\n    lu.assertEquals(candidate(1, 14), 142)\n    lu.assertEquals(candidate(1, 4), 132)\n    lu.assertEquals(candidate(1, 0), 128)\n    lu.assertEquals(candidate(1, 1), 129)\n    lu.assertEquals(candidate(2, 3), 259)\n    lu.assertEquals(candidate(0, 128), 128)\n    lu.assertEquals(candidate(1, 8), 136)\n    lu.assertEquals(candidate(1, 10), 138)\n    lu.assertEquals(candidate(1, 6), 134)\n    lu.assertEquals(candidate(1, 0), 128)\n    lu.assertEquals(candidate(2, 7), 263)\n    lu.assertEquals(candidate(1, 5), 133)\n    lu.assertEquals(candidate(1, 6), 134)\n    lu.assertEquals(candidate(2, 0), 256)\n    lu.assertEquals(candidate(2, 128), 384)\n    lu.assertEquals(candidate(1, 1), 129)\n    lu.assertEquals(candidate(0, 2), 2)\n    lu.assertEquals(candidate(0, 6), 6)\n    lu.assertEquals(candidate(1, 11), 139)\n    lu.assertEquals(candidate(2, 4), 260)\n    lu.assertEquals(candidate(0, 305419896), 305419896)\n    lu.assertEquals(candidate(0, 1311768467463790320), 1311768467463790320)\n    lu.assertEquals(candidate(0, 3), 3)\n    lu.assertEquals(candidate(1, 9), 137)\n    lu.assertEquals(candidate(0, 159), 159)\n    lu.assertEquals(candidate(0, 1), 1)\n    lu.assertEquals(candidate(0, 7), 7)\n    lu.assertEquals(candidate(1, 2), 130)\n    lu.assertEquals(candidate(0, 8), 8)\n    lu.assertEquals(candidate(0, 0), 0)\n    lu.assertEquals(candidate(2, 1), 257)\n    lu.assertEquals(candidate(0, 5), 5)\n    lu.assertEquals(candidate(0, 4), 4)\n    lu.assertEquals(candidate(1, 15), 143)\n    lu.assertEquals(candidate(1, 7), 135)\n    lu.assertEquals(candidate(1, 3), 131)\n    lu.assertEquals(candidate(2, 2), 258)\n    lu.assertEquals(candidate(1, 3), 131)\n    lu.assertEquals(candidate(1, 2), 130)\n    lu.assertEquals(candidate(0, 255), 255)\n    lu.assertEquals(candidate(0, 127), 127)\n    lu.assertEquals(candidate(1, 13), 141)\n    lu.assertEquals(candidate(1, 7), 135)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_193960__is_mgmt_url", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return path.startswith(\"/mgmt/\")\n-- \n--     small helper to test if URL is for management API.\nlocal function _is_mgmt_url(path)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_193960__is_mgmt_url.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _is_mgmt_url\n    lu.assertEquals(candidate(''), false)\n    lu.assertEquals(candidate('/mgmt/foo'), true)\n    lu.assertEquals(candidate('/mgmt/'), true)\n    lu.assertEquals(candidate('/'), false)\n    lu.assertEquals(candidate('/mgmt/not-mgmt'), true)\n    lu.assertEquals(candidate('not-mgmt'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_194153__fix_endpoint", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # idx = endpoint.find('http')\n-- # return endpoint[idx:]\n-- \n-- Remove all text before \"http\".\n-- Workaround because the endpoint is automatically prefixed with the data folder. However this does not make sense for\n-- a sparql endpoint.\n-- :param endpoint:\n-- :return:\nlocal function _fix_endpoint(endpoint)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_194153__fix_endpoint.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _fix_endpoint\n    lu.assertEquals(candidate('https://dbpedia.org/sparql'), 'https://dbpedia.org/sparql')\n    lu.assertEquals(candidate('http://dbpedia.org/sparql'), 'http://dbpedia.org/sparql')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_194405_factorial", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if n == 0:\n-- #     result = 1\n-- # else:\n-- #     result = n * factorial(n-1)\n-- # return result\n-- \n--  To Find Factorial Of n \nlocal function factorial(n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_194405_factorial.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = factorial\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(4), 24)\n    lu.assertEquals(candidate(9), 362880)\n    lu.assertEquals(candidate(7), 5040)\n    lu.assertEquals(candidate(5), 120)\n    lu.assertEquals(candidate(6), 720)\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(3), 6)\n    lu.assertEquals(candidate(8), 40320)\n    lu.assertEquals(candidate(10), 3628800)\n    lu.assertEquals(candidate(0), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_196090_max_subarray", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # max_ending_here = max_so_far = 0\n-- # for x in sequence:\n-- #     max_ending_here = max(0, max_ending_here + x)\n-- #     max_so_far = max(max_so_far, max_ending_here)\n-- # return max_so_far\n-- \n--  Maximum subarray - optimized version \nlocal function max_subarray(sequence)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_196090_max_subarray.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = max_subarray\n    lu.assertEquals(candidate({1}), 1)\n    lu.assertEquals(candidate(list()), 0)\n    lu.assertEquals(candidate(list({10})), 10)\n    lu.assertEquals(candidate(list({10, 20})), 30)\n    lu.assertEquals(candidate({-1, 1, 2, -2, 3}), 4)\n    lu.assertEquals(candidate(list({10, 20, 30, 40, 50, 60})), 210)\n    lu.assertEquals(candidate(list({10, -20})), 10)\n    lu.assertEquals(candidate({1, 2, 3, 4, 5}), 15)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_196910_is_event", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return attribute.startswith('on_')\n-- \n--     Test if a method is an event.\nlocal function is_event(attribute)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_196910_is_event.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_event\n    lu.assertEquals(candidate('on_anything'), true)\n    lu.assertEquals(candidate('on_event123'), true)\n    lu.assertEquals(candidate('_on_event'), false)\n    lu.assertEquals(candidate('on_Event123'), true)\n    lu.assertEquals(candidate('on_event1'), true)\n    lu.assertEquals(candidate('on'), false)\n    lu.assertEquals(candidate('on_event3'), true)\n    lu.assertEquals(candidate('on_my_button_2'), true)\n    lu.assertEquals(candidate('on_event5'), true)\n    lu.assertEquals(candidate('on_123'), true)\n    lu.assertEquals(candidate('On_event'), false)\n    lu.assertEquals(candidate('on_Event'), true)\n    lu.assertEquals(candidate('on_event4'), true)\n    lu.assertEquals(candidate('on_Event_with_dashes'), true)\n    lu.assertEquals(candidate('on_Event'), true)\n    lu.assertEquals(candidate('on_event'), true)\n    lu.assertEquals(candidate('on_event_with_dashes'), true)\n    lu.assertEquals(candidate('on_event2'), true)\n    lu.assertEquals(candidate('abc'), false)\n    lu.assertEquals(candidate('on_my_button'), true)\n    lu.assertEquals(candidate('1'), false)\n    lu.assertEquals(candidate('foo'), false)\n    lu.assertEquals(candidate('on_my_button_3'), true)\n    lu.assertEquals(candidate('On'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_197145_calc_average_ig", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # List = []\n-- # global average_ig\n-- # for no in path_nodes:\n-- #     List.append(node_values[no])\n-- # if len(path_nodes) != 0:\n-- #     average_ig = sum(List)/len(path_nodes)\n-- # return average_ig\n-- \n-- Helper function for SHSEL filter algorithm. It returns the average\n-- Infomation gain value of one existing path in pruning function.\n-- Args:\n--     path_nodes (list): Node in path whose node_availability is True.\n--     node_values (dict): Dictionary about every node in the directed graph\n--         and its information gain value.\n-- Returns:\n--     float: The average InfoGain value of one existing path.\nlocal function calc_average_ig(path_nodes, node_values)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_197145_calc_average_ig.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = calc_average_ig\n    lu.assertEquals(candidate({0, 1, 2}, {[0] = 1, [1] = 2, [2] = 3}), 2.0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_197672_get_search_threshs", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # counts = [count for _, count in word_counts]\n-- # max_count = max(counts)\n-- # min_count = min(counts)\n-- # if upper_thresh is None:\n-- #     upper_search = max_count\n-- # else:\n-- #     upper_search = max_count if max_count < upper_thresh else upper_thresh\n-- # if lower_thresh is None:\n-- #     lower_search = min_count\n-- # else:\n-- #     lower_search = min_count if min_count > lower_thresh else lower_thresh\n-- # return upper_search, lower_search\n-- \n-- Clips the thresholds for binary search based on current word counts.\n-- The upper threshold parameter typically has a large default value that can\n-- result in many iterations of unnecessary search. Thus we clip the upper and\n-- lower bounds of search to the maximum and the minimum wordcount values.\n-- Args:\n--   word_counts: list of (string, int) tuples\n--   upper_thresh: int, upper threshold for binary search\n--   lower_thresh: int, lower threshold for binary search\n-- Returns:\n--   upper_search: int, clipped upper threshold for binary search\n--   lower_search: int, clipped lower threshold for binary search\nlocal function get_search_threshs(word_counts, upper_thresh, lower_thresh)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_197672_get_search_threshs.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_search_threshs\n    lu.assertEquals(candidate({{'a', 2}, {'b', 3}, {'c', 4}, {'d', 5}, {'e', 6}, {'f', 7}, {'g', 8}}, None, 7), {8, 7})\n    lu.assertEquals(candidate({{'cat', 3}, {'dog', 5}, {'bird', 7}, {'fish', 9}, {'dog', 11}}, 11, 9), {11, 9})\n    lu.assertEquals(candidate({{'a', 1}, {'b', 2}, {'c', 3}}, None, 5), {3, 5})\n    lu.assertEquals(candidate({{'cat', 3}, {'dog', 5}, {'bird', 7}, {'fish', 9}, {'dog', 11}}, 11, 11), {11, 11})\n    lu.assertEquals(candidate({{'a', 10}, {'b', 20}, {'c', 30}}, 20, 30), {20, 30})\n    lu.assertEquals(candidate({{'a', 2}, {'b', 3}, {'c', 4}, {'d', 5}, {'e', 6}, {'f', 7}, {'g', 8}}, None, 6), {8, 6})\n    lu.assertEquals(candidate({{'a', 2}, {'b', 3}, {'c', 4}, {'d', 5}, {'e', 6}, {'f', 7}, {'g', 8}}, None, 3), {8, 3})\n    lu.assertEquals(candidate({{'a', 10}, {'b', 20}, {'c', 30}}, 10, 15), {10, 15})\n    lu.assertEquals(candidate({{'a', 10}, {'b', 20}, {'c', 30}}, 25, 25), {25, 25})\n    lu.assertEquals(candidate({{'cat', 3}, {'dog', 5}, {'bird', 7}, {'fish', 9}, {'dog', 11}}, 8, 8), {8, 8})\n    lu.assertEquals(candidate({{'a', 10}, {'b', 20}, {'c', 30}}, None, None), {30, 10})\n    lu.assertEquals(candidate({{'cat', 3}, {'dog', 5}, {'bird', 7}, {'fish', 9}, {'dog', 11}}, 8, 6), {8, 6})\n    lu.assertEquals(candidate({{'a', 10}, {'b', 20}, {'c', 30}}, None, 30), {30, 30})\n    lu.assertEquals(candidate(list(zip({'a', 'b', 'c'}, {5, 2, 1})), None, None), {5, 1})\n    lu.assertEquals(candidate({{'a', 1}, {'b', 1}, {'c', 1}}, None, 2), {1, 2})\n    lu.assertEquals(candidate(list(zip({'a', 'b', 'c'}, {5, 2, 1})), 100, 0), {5, 1})\n    lu.assertEquals(candidate({{'a', 2}, {'b', 3}, {'c', 4}, {'d', 5}, {'e', 6}, {'f', 7}, {'g', 8}}, None, 5), {8, 5})\n    lu.assertEquals(candidate({{'a', 10}, {'b', 20}, {'c', 30}}, 20, None), {20, 10})\n    lu.assertEquals(candidate({{'a', 10}, {'b', 20}, {'c', 30}}, 25, 15), {25, 15})\n    lu.assertEquals(candidate({{'a', 10}, {'b', 20}, {'c', 30}}, 10, 25), {10, 25})\n    lu.assertEquals(candidate({{'a', 2}, {'b', 3}, {'c', 4}, {'d', 5}, {'e', 6}, {'f', 7}, {'g', 8}}, None, 4), {8, 4})\n    lu.assertEquals(candidate({{'a', 10}, {'b', 3}, {'c', 5}, {'d', 8}, {'e', 20}, {'f', 25}}, None, 3), {25, 3})\n    lu.assertEquals(candidate({{'a', 2}, {'b', 3}, {'c', 4}, {'d', 5}, {'e', 6}, {'f', 7}, {'g', 8}}, None, 2), {8, 2})\n    lu.assertEquals(candidate({{'a', 10}, {'b', 20}, {'c', 30}}, None, None), {30, 10})\n    lu.assertEquals(candidate({{'cat', 3}, {'dog', 5}, {'bird', 7}, {'fish', 9}, {'dog', 11}}, 8, None), {8, 3})\n    lu.assertEquals(candidate({{'a', 10}, {'b', 20}, {'c', 30}}, None, 15), {30, 15})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_197814_contar_letras", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # cuenta = 0\n-- # for caracter in cadena:\n-- #     if caracter == letras:\n-- #         cuenta += 1\n-- # return cuenta\n-- \n-- Cuenta la cantidad de letras especificas en la cadena\n-- Argumentos:\n--     cadena (str) -- cadena sobre la que contar\n--     letra (str) -- letra que quiero contar\nlocal function contar_letras(cadena, letras)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_197814_contar_letras.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = contar_letras\n    lu.assertEquals(candidate('hola a todos', 'e'), 0)\n    lu.assertEquals(candidate('hola a todos', 't'), 1)\n    lu.assertEquals(candidate('hola a todos', 'a'), 2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_198063__get_docstring_default_value", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # default_val: str = var_doc.split('\\n')[0]\n-- # is_in: bool = ', default ' in default_val or '(default ' in default_val\n-- # if not is_in:\n-- #     return ''\n-- # default_val = default_val.split('default')[1]\n-- # default_val = default_val.split(',')[0]\n-- # default_val = default_val.replace(')', '')\n-- # default_val = default_val.strip()\n-- # return default_val\n-- \n-- Get the description of argument's default value from docstring.\n-- Parameters\n-- ----------\n-- var_doc : str\n--     Docstring's part of argument.\n-- Returns\n-- -------\n-- default_val : str\n--     Description of the defautl value.\nlocal function _get_docstring_default_value(var_doc)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_198063__get_docstring_default_value.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _get_docstring_default_value\n    lu.assertEquals(candidate('name : str, default None\\n   A name for the user.'), 'None')\n    lu.assertEquals(candidate('name : str, optional\\n   A name for the user.'), '')\n    lu.assertEquals(candidate('name : str, default\\n   A name for the user.'), '')\n    lu.assertEquals(candidate('name: str, optional\\n   A name for the user.'), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_198911_is_quantity_range", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if '[' in val and ']' in val:\n-- #     return True\n-- # return False\n-- \n-- Checks if [] are present in val.\nlocal function is_quantity_range(val)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_198911_is_quantity_range.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_quantity_range\n    lu.assertEquals(candidate('4[]m]'), true)\n    lu.assertEquals(candidate(''), false)\n    lu.assertEquals(candidate('   '), false)\n    lu.assertEquals(candidate('6[m]'), true)\n    lu.assertEquals(candidate('1[m]'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_199908_ats_url", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Strip '/' from url as necessary\n-- # base_url = base_url.rstrip('/')\n-- # return '/'.join([base_url,\n-- #                 'esgf-idp/saml/soap/secure/attributeService.htm'])\n-- \n--     Return the URL for the ESGF SAML AttributeService\nlocal function ats_url(base_url)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_199908_ats_url.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = ats_url\n    lu.assertEquals(candidate('http://esgf.org'), 'http://esgf.org/esgf-idp/saml/soap/secure/attributeService.htm')\n    lu.assertEquals(candidate('https://esgf-node.llnl.gov/idp/shibboleth'), 'https://esgf-node.llnl.gov/idp/shibboleth/esgf-idp/saml/soap/secure/attributeService.htm')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_200188_RPL_ENDOFWHOIS", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"<\" + sender + \">: \" + message\n-- \n--  Reply Code 318 \nlocal function RPL_ENDOFWHOIS(sender, receipient, message)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_200188_RPL_ENDOFWHOIS.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = RPL_ENDOFWHOIS\n    lu.assertEquals(candidate('foo', 'bar', 'baz'), '<foo>: baz')\n    lu.assertEquals(candidate('TestSender', 'TestReceipient', 'TestMessage'), '<TestSender>: TestMessage')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_200932_get_position_from_periods", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for i, period in enumerate(cumulative_period):\n-- #     if iteration <= period:\n-- #         return i\n-- \n-- Get the position from a period list.\n-- It will return the index of the right-closest number in the period list.\n-- For example, the cumulative_period = [100, 200, 300, 400],\n-- if iteration == 50, return 0;\n-- if iteration == 210, return 2;\n-- if iteration == 300, return 2.\n-- Args:\n--     iteration (int): Current iteration.\n--     cumulative_period (list[int]): Cumulative period list.\n-- Returns:\n--     int: The position of the right-closest number in the period list.\nlocal function get_position_from_periods(iteration, cumulative_period)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_200932_get_position_from_periods.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_position_from_periods\n    lu.assertEquals(candidate(10, {10, 20}), 0)\n    lu.assertEquals(candidate(210, {100, 200, 300, 400}), 2)\n    lu.assertEquals(candidate(201, {100, 200, 300}), 2)\n    lu.assertEquals(candidate(2, {100, 200, 300}), 0)\n    lu.assertEquals(candidate(99, {100, 200}), 0)\n    lu.assertEquals(candidate(299, {100, 200, 300}), 2)\n    lu.assertEquals(candidate(1, {100, 200, 300, 400}), 0)\n    lu.assertEquals(candidate(99, {100, 200, 300}), 0)\n    lu.assertEquals(candidate(0, {100}), 0)\n    lu.assertEquals(candidate(99, {100}), 0)\n    lu.assertEquals(candidate(101, {100, 200, 300}), 1)\n    lu.assertEquals(candidate(199, {100, 200, 300}), 1)\n    lu.assertEquals(candidate(199, {100, 200}), 1)\n    lu.assertEquals(candidate(300, {100, 200, 300, 400}), 2)\n    lu.assertEquals(candidate(50, {100, 200, 300, 400}), 0)\n    lu.assertEquals(candidate(100, {100}), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_202385_removeallspaces", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return ''.join([c for c in s if c != ' '])\n-- \n-- Remove all spaces.\nlocal function removeallspaces(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_202385_removeallspaces.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = removeallspaces\n    lu.assertEquals(candidate(' this   is a     mess of   spaces   '), 'thisisamessofspaces')\n    lu.assertEquals(candidate('Hello   World  '), 'HelloWorld')\n    lu.assertEquals(candidate('Hello   World'), 'HelloWorld')\n    lu.assertEquals(candidate('     a b c     '), 'abc')\n    lu.assertEquals(candidate(' Hello   World  '), 'HelloWorld')\n    lu.assertEquals(candidate('abc'), 'abc')\n    lu.assertEquals(candidate('     This is a mess of spaces'), 'Thisisamessofspaces')\n    lu.assertEquals(candidate('This is a mess of spaces'), 'Thisisamessofspaces')\n    lu.assertEquals(candidate('     '), '')\n    lu.assertEquals(candidate('      '), '')\n    lu.assertEquals(candidate('Hello World '), 'HelloWorld')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('This is a mess of spaces     '), 'Thisisamessofspaces')\n    lu.assertEquals(candidate('      a b c      '), 'abc')\n    lu.assertEquals(candidate('Hello World  '), 'HelloWorld')\n    lu.assertEquals(candidate('  This is a mess of spaces  '), 'Thisisamessofspaces')\n    lu.assertEquals(candidate(' Hello World  '), 'HelloWorld')\n    lu.assertEquals(candidate('Hello World'), 'HelloWorld')\n    lu.assertEquals(candidate('     This is a mess of spaces     '), 'Thisisamessofspaces')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_203080_format", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"@{name}\\n{seq}\\n+\\n{qual}\\n\".format(\n-- #     name=name,\n-- #     seq=sequence,\n-- #     qual=quality)\n-- \n-- Format a FastQ entry.\n-- :param name: the read name\n-- :param sequence: the read sequence\n-- :param quality: the read quality\n-- :return: a formatted fastq entry\nlocal function format(name, sequence, quality)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_203080_format.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = format\n    lu.assertEquals(candidate('name', 'ACGT', '####'), '@name\\nACGT\\n+\\n####\\n')\n    lu.assertEquals(candidate('name', 'ACGT', '####'), '@name\\nACGT\\n+\\n####\\n')\n    lu.assertEquals(candidate('foo', 'bar', 'baz'), '@foo\\nbar\\n+\\nbaz\\n')\n    lu.assertEquals(candidate('name', 'ATCGATCG', '#####'), '@name\\nATCGATCG\\n+\\n#####\\n')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_204072_make_subtype", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not biotype:\n-- #     return type_\n-- # return '{} {}'.format(type_, biotype)\n-- \n-- Make subtype from type and biotype.\nlocal function make_subtype(type_, biotype)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_204072_make_subtype.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = make_subtype\n    lu.assertEquals(candidate('gene', 'macro_lncRNA antisense'), 'gene macro_lncRNA antisense')\n    lu.assertEquals(candidate(None, None), None)\n    lu.assertEquals(candidate('gene', 'sense_overlapping'), 'gene sense_overlapping')\n    lu.assertEquals(candidate('TF_binding', None), 'TF_binding')\n    lu.assertEquals(candidate('gene', 'antisense lincRNA'), 'gene antisense lincRNA')\n    lu.assertEquals(candidate('TF_binding', 'protein_coding'), 'TF_binding protein_coding')\n    lu.assertEquals(candidate('gene', 'tRNA'), 'gene tRNA')\n    lu.assertEquals(candidate('TF_binding', 'protein_coding TF_binding'), 'TF_binding protein_coding TF_binding')\n    lu.assertEquals(candidate('gene', 'snRNA'), 'gene snRNA')\n    lu.assertEquals(candidate('gene', 'tRNA-pseudogene'), 'gene tRNA-pseudogene')\n    lu.assertEquals(candidate('gene', 'antisense'), 'gene antisense')\n    lu.assertEquals(candidate('gene', 'snoRNA'), 'gene snoRNA')\n    lu.assertEquals(candidate('gene', 'miRNA'), 'gene miRNA')\n    lu.assertEquals(candidate('gene', 'lincRNA'), 'gene lincRNA')\n    lu.assertEquals(candidate('gene', 'macro_lncRNA'), 'gene macro_lncRNA')\n    lu.assertEquals(candidate('gene', 'rRNA'), 'gene rRNA')\n    lu.assertEquals(candidate('TF_binding', 'protein_coding TF_binding TF_binding'), 'TF_binding protein_coding TF_binding TF_binding')\n    lu.assertEquals(candidate('gene', None), 'gene')\n    lu.assertEquals(candidate('gene', 'pseudogene'), 'gene pseudogene')\n    lu.assertEquals(candidate('gene', 'sense_intronic'), 'gene sense_intronic')\n    lu.assertEquals(candidate('gene', 'protein_coding'), 'gene protein_coding')\n    lu.assertEquals(candidate('gene', 'rRNA-pseudogene'), 'gene rRNA-pseudogene')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_20424_bbox_to_pixel_offsets", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # originX = gt[0]\n-- # originY = gt[3]\n-- # pixel_width = gt[1]\n-- # pixel_height = gt[5]\n-- # x1 = int((bbox[0] - originX) / pixel_width)\n-- # x2 = int((bbox[1] - originX) / pixel_width) + 1\n-- # y1 = int((bbox[3] - originY) / pixel_height)\n-- # y2 = int((bbox[2] - originY) / pixel_height) + 1\n-- # xsize = x2 - x1\n-- # ysize = y2 - y1\n-- # return (x1, y1, xsize, ysize)\n-- \n-- Helper function for zonal_stats(). Modified from:\n-- https://gist.github.com/perrygeo/5667173\n-- Original code copyright 2013 Matthew Perry\nlocal function bbox_to_pixel_offsets(gt, bbox)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_20424_bbox_to_pixel_offsets.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = bbox_to_pixel_offsets\n    lu.assertEquals(candidate({0, 1, 2, 3, 4, 5}, {-0.1, 0.1, 0.3, 0.4}), {0, 0, 1, 1})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_204699_prepare_summary_line", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return (f'{cost} {currency}')\n-- \n-- Prepares a summary line by formatting it.\n-- Args:\n--     cost: The cost of the summary line item.\n--     currency: The currency to append to the summary line item.\n-- Returns:\n--     The formatted summary line.\nlocal function prepare_summary_line(cost, currency)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_204699_prepare_summary_line.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = prepare_summary_line\n    lu.assertEquals(candidate(20000.12, 'JPY'), '20000.12 JPY')\n    lu.assertEquals(candidate(20.14, 'USD'), '20.14 USD')\n    lu.assertEquals(candidate(456, 'EUR'), '456 EUR')\n    lu.assertEquals(candidate(789, 'GBP'), '789 GBP')\n    lu.assertEquals(candidate(10.55, 'JPY'), '10.55 JPY')\n    lu.assertEquals(candidate(123, 'USD'), '123 USD')\n    lu.assertEquals(candidate(0.05, 'CAD'), '0.05 CAD')\n    lu.assertEquals(candidate(200.1, 'EUR'), '200.1 EUR')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_205474_get_chain_length", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # length = 0\n-- # for gen_idx, gen in enumerate(chains[start_gen_idx:]):\n-- #     bn = gen[chain_idx]\n-- #     if len(bn) == 0:\n-- #         break\n-- #     length += 1\n-- #     # print(\"\\nbn: \" + str([x+1 for x in bn]))\n-- # return length\n-- \n-- Get length of the chain with index \"chain_idx\", starting from (and including)\n-- generation \"start_gen_idx\" to end of chain, or until first\n-- empty bin (while excluding empty bin).\nlocal function get_chain_length(chains, chain_idx, start_gen_idx)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_205474_get_chain_length.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_chain_length\n    lu.assertEquals(candidate({{{0, 0}, {0, 0}, {0, 0}}, {{0, 0}, {0, 0}, {0, 0}}, {{0, 0}, {0, 0}, {0, 0}}}, 1, 0), 3)\n    lu.assertEquals(candidate({{{0, 0}, {0, 0}}, {{0, 0}, {0, 0}}, {{0, 0}, {0, 0}}, {{0, 0}, {0, 0}}}, 1, 0), 4)\n    lu.assertEquals(candidate({{{0, 0}, {0, 0}, {0, 0}}, {{0, 0}, {0, 0}}, {{0, 0}, {0, 0}, {0, 0}}}, 1, 0), 3)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_206622_iso_date2", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if type(adate) != str or len(str(adate)) != 10:\n-- #     return (0, 0, 0)\n-- # alist = adate.split('-')\n-- # if len(alist) != 3:\n-- #     return (0, 0, 0)\n-- # y, m, d = alist\n-- # return (int(y), int(m), int(d)) if (\n-- #     y.isdigit() and len(y) == 4 and\n-- #     m.isdigit() and len(m) == 2 and\n-- #     d.isdigit() and len(d) == 2) \\\n-- #     else (0, 0, 0)\n-- \n-- Returns int tuple (yyyy,mm,dd) if adate is in form of 'yyyy-dd-mm' else (0,0,0)\nlocal function iso_date2(adate)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_206622_iso_date2.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = iso_date2\n    lu.assertEquals(candidate(''), {0, 0, 0})\n    lu.assertEquals(candidate('201-1-00'), {0, 0, 0})\n    lu.assertEquals(candidate('201-1-1'), {0, 0, 0})\n    lu.assertEquals(candidate('2018-1-1'), {0, 0, 0})\n    lu.assertEquals(candidate('20-1-01'), {0, 0, 0})\n    lu.assertEquals(candidate(None), {0, 0, 0})\n    lu.assertEquals(candidate('201-01-1'), {0, 0, 0})\n    lu.assertEquals(candidate('201-00-1'), {0, 0, 0})\n    lu.assertEquals(candidate('2022-1-3'), {0, 0, 0})\n    lu.assertEquals(candidate('20-01-01'), {0, 0, 0})\n    lu.assertEquals(candidate('1987-02-03'), {1987, 2, 3})\n    lu.assertEquals(candidate('2022-11-1'), {0, 0, 0})\n    lu.assertEquals(candidate('2022-1-32'), {0, 0, 0})\n    lu.assertEquals(candidate('201-1-01'), {0, 0, 0})\n    lu.assertEquals(candidate('2018-01-1'), {0, 0, 0})\n    lu.assertEquals(candidate('1987-02-100'), {0, 0, 0})\n    lu.assertEquals(candidate('2018-00-1'), {0, 0, 0})\n    lu.assertEquals(candidate('2018-01-21'), {2018, 1, 21})\n    lu.assertEquals(candidate('2020-01-02'), {2020, 1, 2})\n    lu.assertEquals(candidate('2018-01-01'), {2018, 1, 1})\n    lu.assertEquals(candidate('1987-3-03'), {0, 0, 0})\n    lu.assertEquals(candidate('20-1-1'), {0, 0, 0})\n    lu.assertEquals(candidate('20-01-1'), {0, 0, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_207245_get_binary_category", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if score < 0.5:\n-- #     return 0\n-- # else:\n-- #     return 1\n-- \n-- Get an integer binary classification label from a score between 0 and 1.\nlocal function get_binary_category(score)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_207245_get_binary_category.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_binary_category\n    lu.assertEquals(candidate(0.75), 1)\n    lu.assertEquals(candidate(0.99), 1)\n    lu.assertEquals(candidate(0.01), 0)\n    lu.assertEquals(candidate(0.25), 0)\n    lu.assertEquals(candidate(0.6), 1)\n    lu.assertEquals(candidate(0.2), 0)\n    lu.assertEquals(candidate(0.9), 1)\n    lu.assertEquals(candidate(0.4), 0)\n    lu.assertEquals(candidate(0.8), 1)\n    lu.assertEquals(candidate(1.0), 1)\n    lu.assertEquals(candidate(0.0), 0)\n    lu.assertEquals(candidate(0.3), 0)\n    lu.assertEquals(candidate(0.9999), 1)\n    lu.assertEquals(candidate(0.0001), 0)\n    lu.assertEquals(candidate(0.1), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_207726_get_stride_sequence", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # sequence = []\n-- # for sd in stride_desc:\n-- #     sequence.append(sd[1])\n-- # return sequence\n-- \n-- Return the sequence of the secondary structure.\n-- Parameters\n-- ----------\n-- stride_desc : list of (str, str, float, float, float)\n--     The Stride description.\n-- Returns\n-- -------\n-- sequence : list of str\n--     The secondary structure sequence.\nlocal function get_stride_sequence(stride_desc)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_207726_get_stride_sequence.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_stride_sequence\n    lu.assertEquals(candidate({{'H', 'H', 1, 2, 1}, {'E', 'E', 3, 4, 3}, {'L', 'L', 5, 6, 5}, {'H', 'H', 7, 8, 7}, {'E', 'E', 9, 10, 9}}), {'H', 'E', 'L', 'H', 'E'})\n    lu.assertEquals(candidate({{'H', 'H', 1, 2, 1}, {'E', 'E', 3, 4, 3}, {'L', 'L', 5, 6, 5}, {'H', 'H', 7, 8, 7}, {'E', 'E', 9, 10, 9}, {'L', 'L', 11, 12, 11}}), {'H', 'E', 'L', 'H', 'E', 'L'})\n    lu.assertEquals(candidate({{'H', 'H', 1, 2, 1}, {'E', 'E', 3, 4, 3}, {'L', 'L', 5, 6, 5}, {'H', 'H', 7, 8, 7}}), {'H', 'E', 'L', 'H'})\n    lu.assertEquals(candidate({{'H', 'H', 1, 2, 1}, {'E', 'E', 3, 4, 3}, {'L', 'L', 5, 6, 5}}), {'H', 'E', 'L'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_209252_parse_size", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not s:\n-- #     return None\n-- # mult = None\n-- # if s[-1].lower() == \"k\":\n-- #     mult = 1024**1\n-- # elif s[-1].lower() == \"m\":\n-- #     mult = 1024**2\n-- # elif s[-1].lower() == \"g\":\n-- #     mult = 1024**3\n-- # if mult:\n-- #     s = s[:-1]\n-- # else:\n-- #     mult = 1\n-- # try:\n-- #     return int(s) * mult\n-- # except ValueError:\n-- #     raise ValueError(\"Invalid size specification: %s\" % s)\n-- \n--         Parses a size specification. Valid specifications are:\n-- 123: bytes\n-- 123k: kilobytes\n-- 123m: megabytes\n-- 123g: gigabytes\nlocal function parse_size(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_209252_parse_size.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_size\n    lu.assertEquals(candidate('1k'), 1024)\n    lu.assertEquals(candidate('1'), 1)\n    lu.assertEquals(candidate('123'), 123)\n    lu.assertEquals(candidate('1000'), 1000)\n    lu.assertEquals(candidate(None), None)\n    lu.assertEquals(candidate(''), None)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_209453_contrasting_text_color", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # r, g, b = (hex_str[1:3], hex_str[3:5], hex_str[5:])\n-- # luminance = (int(r, 16) * 0.299 + int(g, 16) * 0.587 + int(b, 16) * 0.114) / 255\n-- # if luminance > 0.5:\n-- #     return \"#000\"\n-- # else:\n-- #     return \"#FFF\"\n-- \n-- Get a contrasting foreground text color for specified background hex color\n-- :param hext_str: A hex string color ('#XXXXXX') for which to determine a black-or-white\n--     foreground color.\n-- :return: '#FFF' or '#000'.\nlocal function contrasting_text_color(hex_str)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_209453_contrasting_text_color.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = contrasting_text_color\n    lu.assertEquals(candidate('#FFFFFF'), '#000')\n    lu.assertEquals(candidate('#012345'), '#FFF')\n    lu.assertEquals(candidate('#000000'), '#FFF')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_209455_is_glob_mask", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # symbols = [\"*\", \"?\"]\n-- # return any(s in text for s in symbols)\n-- \n--     Checks whether text contains mathing symbols usable with glob.glob()\nlocal function is_glob_mask(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_209455_is_glob_mask.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_glob_mask\n    lu.assertEquals(candidate('a/b/c/**.py'), true)\n    lu.assertEquals(candidate('foo\\\\?'), true)\n    lu.assertEquals(candidate('*'), true)\n    lu.assertEquals(candidate('foo*'), true)\n    lu.assertEquals(candidate('a/b/*.py'), true)\n    lu.assertEquals(candidate('foo?'), true)\n    lu.assertEquals(candidate('a/b/**/*.py'), true)\n    lu.assertEquals(candidate('foo\\\\*'), true)\n    lu.assertEquals(candidate('a/b/c'), false)\n    lu.assertEquals(candidate('a/b/**.py'), true)\n    lu.assertEquals(candidate('foo'), false)\n    lu.assertEquals(candidate('*.py'), true)\n    lu.assertEquals(candidate('some_text'), false)\n    lu.assertEquals(candidate('a/b/c/**/*.py'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_209668_to_yx", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return int(point % 64), int((point - (point % 64)) / 64)\n-- \n-- transform a scalar from [0;4095] to a (y,x) coordinate in [0:63,0:63]\nlocal function to_yx(point)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_209668_to_yx.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = to_yx\n    lu.assertEquals(candidate(4095), {63, 63})\n    lu.assertEquals(candidate(1), {1, 0})\n    lu.assertEquals(candidate(0), {0, 0})\n    lu.assertEquals(candidate(65), {1, 1})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_210558_is_matched", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # opening = tuple('({[')\n-- # closing = tuple(')}]')\n-- # mapping = dict(zip(opening, closing))\n-- # queue = []\n-- # for letter in expression:\n-- #     if letter in opening:\n-- #         queue.append(mapping[letter])\n-- #     elif letter in closing:\n-- #         if not queue or letter != queue.pop():\n-- #             return False\n-- # return not queue\n-- \n-- Finds out how balanced an expression is.\n-- With a string containing only brackets.\n-- >>> is_matched('[]()()(((([])))')\n-- False\n-- >>> is_matched('[](){{{[]}}}')\n-- True\nlocal function is_matched(expression)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_210558_is_matched.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_matched\n    lu.assertEquals(candidate('[](){{{[]}}}'), true)\n    lu.assertEquals(candidate('[]()()()'), true)\n    lu.assertEquals(candidate('[]()()()(((([])))'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_211884_simple_atmo_opstring", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # gamma_b = 1 - haze\n-- # gamma_g = 1 - (haze / 3.0)\n-- # ops = (\n-- #     \"gamma g {gamma_g}, \" \"gamma b {gamma_b}, \" \"sigmoidal rgb {contrast} {bias}\"\n-- # ).format(gamma_g=gamma_g, gamma_b=gamma_b, contrast=contrast, bias=bias)\n-- # return ops\n-- \n-- Make a simple atmospheric correction formula.\nlocal function simple_atmo_opstring(haze, contrast, bias)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_211884_simple_atmo_opstring.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = simple_atmo_opstring\n    lu.assertEquals(candidate(0.0, 1.0, 0.0), 'gamma g 1.0, gamma b 1.0, sigmoidal rgb 1.0 0.0')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_212029_fix_url_path", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return url if url.endswith(\"/\") else url + \"/\"\n-- \n-- Add \"/\" to end of URL, if URL has path and doesn't end with \"/\"\n-- the path will be removed by urljoin.\nlocal function fix_url_path(url)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_212029_fix_url_path.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = fix_url_path\n    lu.assertEquals(candidate('https://www.example.com'), 'https://www.example.com/')\n    lu.assertEquals(candidate('https://www.example.com/'), 'https://www.example.com/')\n    lu.assertEquals(candidate('https://www.example.com/test'), 'https://www.example.com/test/')\n    lu.assertEquals(candidate('https://www.example.com/test/'), 'https://www.example.com/test/')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_212643_binary_to_integer", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return int(binary, 2)\n-- \n-- Convert R or G or B pixel values from binary to integer.\n-- INPUT: A string tuple (e.g. (\"00101010\"))\n-- OUTPUT: Return an int tuple (e.g. (220))\nlocal function binary_to_integer(binary)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_212643_binary_to_integer.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = binary_to_integer\n    lu.assertEquals(candidate('011011101'), 221)\n    lu.assertEquals(candidate('10101010'), 170)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_212796_is_abs", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if field[:4] == 'ABS(' and field[-1] == ')':\n-- #     return field[4:-1], True\n-- # else:\n-- #     return field, False\n-- \n-- Check if field is absolute value.\n-- Parameters\n-- ----------\n-- field : str\n--     Field name.\n-- Returns\n-- -------\n-- (bool, str)\n--     Whether the field is absolute or not along with the basic field itself.\nlocal function is_abs(field)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_212796_is_abs.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_abs\n    lu.assertEquals(candidate('hello'), {'hello', false})\n    lu.assertEquals(candidate('hello)world'), {'hello)world', false})\n    lu.assertEquals(candidate('ABS(ABS(hello))'), {'ABS(hello)', true})\n    lu.assertEquals(candidate('ABS(hello)'), {'hello', true})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_212940_path2ParentPath", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return '/'.join(path.split('/')[:-1])\n-- \n-- >>> path2ParentPath('/xxx/yyy/zzz/')\n-- '/xxx/yyy/zzz'\n-- >>> path2ParentPath('/xxx/yyy/zzz')\n-- '/xxx/yyy'\n-- >>> path2ParentPath('/xxx/yyy/zzz.gif')\n-- '/xxx/yyy'\nlocal function path2ParentPath(path)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_212940_path2ParentPath.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = path2ParentPath\n    lu.assertEquals(candidate('/xxx/yyy/zzz'), '/xxx/yyy')\n    lu.assertEquals(candidate('/xxx/yyy/zzz/'), '/xxx/yyy/zzz')\n    lu.assertEquals(candidate('/xxx/yyy/zzz.gif'), '/xxx/yyy')\n    lu.assertEquals(candidate('/xxx/yyy/zzz'), '/xxx/yyy')\n    lu.assertEquals(candidate('/xxx/yyy/zzz/'), '/xxx/yyy/zzz')\n    lu.assertEquals(candidate('/xxx/yyy/zzz.gif'), '/xxx/yyy')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_213348__mask_to_shift", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # shift = 0\n-- # while mask & 0x1 == 0:\n-- #     shift += 1\n-- #     mask >>= 1\n-- # return shift\n-- \n--  Return the index of the least significant bit in the mask \nlocal function _mask_to_shift(mask)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_213348__mask_to_shift.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _mask_to_shift\n    lu.assertEquals(candidate(1), 0)\n    lu.assertEquals(candidate(15), 0)\n    lu.assertEquals(candidate(64), 6)\n    lu.assertEquals(candidate(2), 1)\n    lu.assertEquals(candidate(251658240), 24)\n    lu.assertEquals(candidate(1), 0)\n    lu.assertEquals(candidate(128), 7)\n    lu.assertEquals(candidate(8), 3)\n    lu.assertEquals(candidate(65536), 16)\n    lu.assertEquals(candidate(3), 0)\n    lu.assertEquals(candidate(12), 2)\n    lu.assertEquals(candidate(2), 1)\n    lu.assertEquals(candidate(2), 1)\n    lu.assertEquals(candidate(4), 2)\n    lu.assertEquals(candidate(1), 0)\n    lu.assertEquals(candidate(32), 5)\n    lu.assertEquals(candidate(4), 2)\n    lu.assertEquals(candidate(16), 4)\n    lu.assertEquals(candidate(8), 3)\n    lu.assertEquals(candidate(8), 3)\n    lu.assertEquals(candidate(4), 2)\n    lu.assertEquals(candidate(6), 1)\n    lu.assertEquals(candidate(14), 1)\n    lu.assertEquals(candidate(256), 8)\n    lu.assertEquals(candidate(7), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_21334_azimu_half", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if degrees >= 180:\n-- #     degrees = degrees - 180\n-- # return degrees\n-- \n-- Transform azimuth from 180-360 range to range 0-180.\n-- :param degrees: Degrees in range 0 - 360\n-- :return: Degrees in range 0 - 180\nlocal function azimu_half(degrees)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_21334_azimu_half.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = azimu_half\n    lu.assertEquals(candidate(360), 180)\n    lu.assertEquals(candidate(180), 0)\n    lu.assertEquals(candidate(210), 30)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(60), 60)\n    lu.assertEquals(candidate(135), 135)\n    lu.assertEquals(candidate(350), 170)\n    lu.assertEquals(candidate(270), 90)\n    lu.assertEquals(candidate(90), 90)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_213491_calculate_a", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return cl_i * ((int_ * (1 + int_) ** n_term) / ((1 + int_) ** n_term - 1))\n-- \n--  Private function \nlocal function calculate_a(cl_i, int_, n_term)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_213491_calculate_a.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = calculate_a\n    lu.assertEquals(candidate(0, 0.08, 10), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_213836_decodeModifiers", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # result = []\n-- # bincode = '{:032b}'.format(modifier)\n-- # if bincode[11] == '1':\n-- #     result.append('Cmd')\n-- # if bincode[12] == '1':\n-- #     result.append('Alt')\n-- # if bincode[13] == '1':\n-- #     result.append('Ctrl')\n-- # if bincode[14] == '1':\n-- #     result.append('Shift')\n-- # if bincode[15] == '1':\n-- #     result.append('Caps')\n-- # return '+'.join(result)\n-- \n-- {:032b}\n-- 0 0 0 0 0 0 0 0 0 0 0 1   1   1   1   1   0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1\n-- 0                     11  12  13  14  15                                31\n-- cmd  00000000000100000000000100001000 b11\n-- alt  00000000000010000000000100100000 b12\n-- ctrl 00000000000001000000000100000001 b13\n-- shft 00000000000000100000000100000010 b14\n-- caps 00000000000000010000000100000000 b15\nlocal function decodeModifiers(modifier)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_213836_decodeModifiers.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = decodeModifiers\n    lu.assertEquals(candidate(65536), 'Caps')\n    lu.assertEquals(candidate(0), '')\n    lu.assertEquals(candidate(1), '')\n    lu.assertEquals(candidate(0), '')\n    lu.assertEquals(candidate(0), '')\n    lu.assertEquals(candidate(0), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_213973_choice_verification", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if choice.upper() == \"O\":\n-- #     return True\n-- \n-- Check choice and return True according.\nlocal function choice_verification(choice)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_213973_choice_verification.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = choice_verification\n    lu.assertEquals(candidate('O'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_214021_joinargs", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return join_str.join([\"%s\" % a for a in args if a])\n-- \n-- Joins given args (if valid) using join_str.\n-- The result is stored in a context variable.\n-- Example usage: {% join '/' str1 str2 str3 ... as var %}\nlocal function joinargs(join_str)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_214021_joinargs.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = joinargs\n    lu.assertEquals(candidate('/', None, None, None, None, None), '')\n    lu.assertEquals(candidate('/', None, None, None), '')\n    lu.assertEquals(candidate('/', 'hello'), 'hello')\n    lu.assertEquals(candidate('/', None, None), '')\n    lu.assertEquals(candidate('/', 'foo', '', 'bar'), 'foo/bar')\n    lu.assertEquals(candidate('/', 'hello', 'world', '!', ''), 'hello/world/!')\n    lu.assertEquals(candidate('/', 1, 2, 3, 4, 5), '1/2/3/4/5')\n    lu.assertEquals(candidate('', None, None, None, None, None), '')\n    lu.assertEquals(candidate('/', None, None, None, None), '')\n    lu.assertEquals(candidate('', None, None, None, None), '')\n    lu.assertEquals(candidate('', None, None, None), '')\n    lu.assertEquals(candidate('/'), '')\n    lu.assertEquals(candidate('/', 'foo', 'bar'), 'foo/bar')\n    lu.assertEquals(candidate('/', 1, None, 2, None, 3, None, 4, None, 5), '1/2/3/4/5')\n    lu.assertEquals(candidate('/', 'foo'), 'foo')\n    lu.assertEquals(candidate('/', 'hello', 'world'), 'hello/world')\n    lu.assertEquals(candidate('/', 1), '1')\n    lu.assertEquals(candidate('', None, None), '')\n    lu.assertEquals(candidate('/', 'hello', 'world', '!', '', ''), 'hello/world/!')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_214624_metacyc_url", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"http://metacyc.org/META/NEW-IMAGE?type=NIL&object=\"+pathway\n-- \n--  Return the url for the pathway on the MetaCyc website \n-- Args:\n--     pathway (string): The MetaCyc pathway\n-- Returns\n--    (string): The url to the website for the pathway\nlocal function metacyc_url(pathway)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_214624_metacyc_url.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = metacyc_url\n    lu.assertEquals(candidate('PFK'), 'http://metacyc.org/META/NEW-IMAGE?type=NIL&object=PFK')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_215353_sum_2_dictionaries", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # dict_out = dicta.copy()\n-- # for key in dictb.keys():\n-- #     if 'key' in dict_out:\n-- #         # Add the sum for key in dictb to that of dict_out:\n-- #         dict_out[key] += dictb[key]\n-- #     else:\n-- #         # the key is not in the first dictionary - add it directly:\n-- #         dict_out[key] = dictb[key]\n-- # return dict_out\n-- \n-- Given two dictionaries of totals, where each total refers to a key\n-- in the dictionary, add the totals.\n-- E.g.:  dicta = { 'a' : 3, 'b' : 1 }\n--        dictb = { 'a' : 1, 'c' : 5 }\n--        dicta + dictb = { 'a' : 4, 'b' : 1, 'c' : 5 }\n-- @param dicta: (dictionary)\n-- @param dictb: (dictionary)\n-- @return: (dictionary) - the sum of the 2 dictionaries\nlocal function sum_2_dictionaries(dicta, dictb)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_215353_sum_2_dictionaries.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sum_2_dictionaries\n    lu.assertEquals(candidate({['a'] = 1}, {}), {['a'] = 1})\n    lu.assertEquals(candidate({['a'] = 0, ['b'] = 0}, {['a'] = 0, ['b'] = 0}), {['a'] = 0, ['b'] = 0})\n    lu.assertEquals(candidate({['a'] = 0}, {['a'] = 0}), {['a'] = 0})\n    lu.assertEquals(candidate({['a'] = 0, ['b'] = 0}, {['a'] = 0, ['b'] = 0}), {['a'] = 0, ['b'] = 0})\n    lu.assertEquals(candidate({}, {['a'] = 3, ['b'] = 1, ['c'] = 5}), {['a'] = 3, ['b'] = 1, ['c'] = 5})\n    lu.assertEquals(candidate({}, {}), {})\n    lu.assertEquals(candidate({['a'] = 3}, {['b'] = 1, ['c'] = 5}), {['a'] = 3, ['b'] = 1, ['c'] = 5})\n    lu.assertEquals(candidate({['a'] = 3, ['b'] = 1, ['c'] = 5}, {}), {['a'] = 3, ['b'] = 1, ['c'] = 5})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_215837_verify", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if \"www.youtube.com\" in url or \"youtu.be\" in url:\n-- #     return True\n-- # return False\n-- \n-- To check whether `url` belongs to YouTube, if yes returns True\n-- else False\nlocal function verify(url)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_215837_verify.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = verify\n    lu.assertEquals(candidate('https://www.youtube.com/watch?v=vZ19cI4k-oI&index=12'), true)\n    lu.assertEquals(candidate('https://www.youtube.com'), true)\n    lu.assertEquals(candidate('https://www.youttube.com'), false)\n    lu.assertEquals(candidate(''), false)\n    lu.assertEquals(candidate('https://youtu.be/dQw4w9WgXcQ'), true)\n    lu.assertEquals(candidate('https://www.youtube.com/watch?v=vZ19cI4k-oI'), true)\n    lu.assertEquals(candidate('https://www.youtube.com/watch?v=vZ19cI4k-oI&list=PLhTjy8cBISEqY-J1w0J6iC613vudN2Y4B'), true)\n    lu.assertEquals(candidate('https://www.youtube.com/watch?v=vZ19cI4k-oI&list=PLhTjy8cBISEqY-J1w0J6iC613vudN2Y4B&index=12&t=454s'), true)\n    lu.assertEquals(candidate('https://www.youtube.com/watch?v=dQw4w9WgXcQ'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_215878_is_duplicate", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return 'Possible Duplicate:' in s\n-- \n-- Return True if the string 'Possible Duplicate is in string s or False, otherwise.'\n-- :param s:\n-- :return:\nlocal function is_duplicate(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_215878_is_duplicate.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_duplicate\n    lu.assertEquals(candidate(str(16)), false)\n    lu.assertEquals(candidate('Possible Duplicate: 6221711006451504961, 6221711006451504962, 6221711006451504963, 6221711006451504964, 6221711006451504965'), true)\n    lu.assertEquals(candidate('Possible Duplicate: 13.24 13.24 13.24 13.24'), true)\n    lu.assertEquals(candidate('4'), false)\n    lu.assertEquals(candidate('0'), false)\n    lu.assertEquals(candidate('Possible Duplicate: 13.24 13.24 13.24 13.24 13.24'), true)\n    lu.assertEquals(candidate(str(4)), false)\n    lu.assertEquals(candidate(str(8)), false)\n    lu.assertEquals(candidate('Possible Duplicate: 0.00014341297035841606 0.00014341297035841606'), true)\n    lu.assertEquals(candidate('Possible Duplicate: 13.24'), true)\n    lu.assertEquals(candidate('12'), false)\n    lu.assertEquals(candidate('\\nPossible Duplicate:\\nFirst name: John\\nLast name: Doe\\nEmail: <EMAIL>\\nFirst name: John\\nLast name: Doe\\nEmail: <EMAIL>\\n'), true)\n    lu.assertEquals(candidate(str(0)), false)\n    lu.assertEquals(candidate(str(2)), false)\n    lu.assertEquals(candidate(str(6)), false)\n    lu.assertEquals(candidate('8'), false)\n    lu.assertEquals(candidate('Possible Duplicate: 13.24 '), true)\n    lu.assertEquals(candidate('Possible Duplicate: 6221711006451504961, 6221711006451504962, 6221711006451504963, 6221711006451504964'), true)\n    lu.assertEquals(candidate(str(10)), false)\n    lu.assertEquals(candidate('10'), false)\n    lu.assertEquals(candidate(str(14)), false)\n    lu.assertEquals(candidate('2'), false)\n    lu.assertEquals(candidate(str(12)), false)\n    lu.assertEquals(candidate('6'), false)\n    lu.assertEquals(candidate(str(18)), false)\n    lu.assertEquals(candidate('\\nPossible Duplicate:\\nFirst name: John\\nLast name: Doe\\nEmail: <EMAIL>\\nFirst name: John\\nLast name: Doe\\nEmail: <EMAIL>\\n\\nPossible Duplicate:\\nFirst name: John\\nLast name: Doe\\nEmail: <EMAIL>\\nFirst name: Mary\\nLast name: Smith\\nEmail: <EMAIL>\\n'), true)\n    lu.assertEquals(candidate('14'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_216479_find_common_left", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # length = min([len(s) for s in strings])\n-- # result = ''\n-- # for i in range(length):\n-- #     if all([strings[0][i] == s[i] for s in strings[1:]]):\n-- #         result += strings[0][i]\n-- #     else:\n-- #         break\n-- # return result\n-- \n-- :param list[str] strings: list of strings we want to find a common left part in\n-- :rtype: str\nlocal function find_common_left(strings)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_216479_find_common_left.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = find_common_left\n    lu.assertEquals(candidate({'a', 'b'}), '')\n    lu.assertEquals(candidate({'abc', 'def'}), '')\n    lu.assertEquals(candidate({'aa', 'ab', 'a'}), 'a')\n    lu.assertEquals(candidate({'aba', 'cdc', 'eae'}), '')\n    lu.assertEquals(candidate({'ab', 'a', 'ac'}), 'a')\n    lu.assertEquals(candidate({'abcd', 'abab', 'ababd', 'abcdx'}), 'ab')\n    lu.assertEquals(candidate({'abc', 'a', 'acb'}), 'a')\n    lu.assertEquals(candidate({'aaa', 'aab', 'abb', 'bbb', 'ccc'}), '')\n    lu.assertEquals(candidate({'abc', 'd', 'cba'}), '')\n    lu.assertEquals(candidate({'', 'c', 'c'}), '')\n    lu.assertEquals(candidate({'a', 'b', 'c', 'd'}), '')\n    lu.assertEquals(candidate({'', 'T', 'a', 't', 'a', 'a', 'T', 'T', ''}), '')\n    lu.assertEquals(candidate({'aa', 'a'}), 'a')\n    lu.assertEquals(candidate({'', ''}), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_216526_center_position", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # size = high_bound - low_bound\n-- # extra = space - size\n-- # if extra > 0:\n-- #     if low_bound == low_limit:\n-- #         return 0, size\n-- #     elif high_bound == high_limit:\n-- #         return space - size, space\n-- #     else:\n-- #         leading_pad = extra // 2\n-- #         adjusted_low_bound = low_bound - leading_pad\n-- #         if adjusted_low_bound < low_limit:\n-- #             leading_pad = low_bound - low_limit\n-- #         adjusted_high_bound = low_bound - leading_pad + space\n-- #         if adjusted_high_bound > high_limit:\n-- #             leading_pad = space - size - (high_limit - high_bound)\n-- #         return leading_pad, leading_pad + size\n-- # else:\n-- #     return 0, size\n-- \n-- Center bounds within available space.\n-- :param low_bound: current lower bound\n-- :param high_bound: current upper bound\n-- :param low_limit: minimum allowed bound\n-- :param high_limit: maximum allowed bound\n-- :param space: available space\n-- :return: centered low bound, centered high bound\nlocal function center_position(low_bound, high_bound, low_limit, high_limit, space)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_216526_center_position.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = center_position\n    lu.assertEquals(candidate(0, 0, 0, 0, 10), {0, 0})\n    lu.assertEquals(candidate(1, 3, 0, 4, 4), {1, 3})\n    lu.assertEquals(candidate(0, 3, 0, 4, 2), {0, 3})\n    lu.assertEquals(candidate(0, 5, 0, 0, 10), {0, 5})\n    lu.assertEquals(candidate(0, 1, 0, 2, 2), {0, 1})\n    lu.assertEquals(candidate(0, 10, 0, 0, 10), {0, 10})\n    lu.assertEquals(candidate(0, 1, 0, 0, 10), {0, 1})\n    lu.assertEquals(candidate(0, 2, 0, 4, 2), {0, 2})\n    lu.assertEquals(candidate(0, 4, 0, 6, 10), {0, 4})\n    lu.assertEquals(candidate(0, 4, 0, 8, 10), {0, 4})\n    lu.assertEquals(candidate(0, 1, 1, 2, 2), {0, 1})\n    lu.assertEquals(candidate(0, 2, 0, 3, 2), {0, 2})\n    lu.assertEquals(candidate(1, 3, 0, 4, 2), {0, 2})\n    lu.assertEquals(candidate(0, 2, 0, 4, 4), {0, 2})\n    lu.assertEquals(candidate(0, 1, 1, 2, 3), {1, 2})\n    lu.assertEquals(candidate(0, 4, 0, 0, 10), {0, 4})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_217312_and_is_true", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # truth_tbl = {\"Yes\": True, \"No\": False}\n-- # return truth_tbl[itemx] and truth_tbl[itemy]\n-- \n-- Function:  and_is_true\n-- Description:  Uses a truth table to do an AND check between two values of\n--     {Yes (True) / No (False)}.\n-- Arguments:\n--     (input) itemx -> Yes or No value.\n--     (input) itemy -> Yes or No value.\n--     (output) Return True | False based on AND comparsion.\nlocal function and_is_true(itemx, itemy)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_217312_and_is_true.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = and_is_true\n    lu.assertEquals(candidate('No', 'Yes'), false)\n    lu.assertEquals(candidate('Yes', 'No'), false)\n    lu.assertEquals(candidate('No', 'No'), false)\n    lu.assertEquals(candidate('Yes', 'Yes'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_217479__number_channels", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if rgb:\n-- #     return 3\n-- # else:\n-- #     return 1\n-- \n-- Determines the number of channels corresponding to a RGB flag.\nlocal function _number_channels(rgb)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_217479__number_channels.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _number_channels\n    lu.assertEquals(candidate(true), 3)\n    lu.assertEquals(candidate(false), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_217549_is_valid_git_sha1", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if len(hash) != 40:\n-- #     return False\n-- # try:\n-- #     value = int(hash, 16)\n-- # except ValueError:\n-- #     return False\n-- # return True\n-- \n-- check if a string is a valid git sha1 string\n-- Input:\n-- hash: string to validate\n-- Output:\n-- True if the string has 40 characters and is an hexadecimal number, False\n-- otherwise.\nlocal function is_valid_git_sha1(hash)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_217549_is_valid_git_sha1.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_valid_git_sha1\n    lu.assertEquals(candidate('0123456789012345678901234567890123456789x'), false)\n    lu.assertEquals(candidate('012345678901234567890123456789012345678x'), false)\n    lu.assertEquals(candidate('a'), false)\n    lu.assertEquals(candidate(''), false)\n    lu.assertEquals(candidate('01234567890'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_217944_upsample", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return data | (data >> bits)\n-- \n-- Stretch bits worth of data to fill the byte.\n-- This is done by duplicating the MSB to fill the remaining space.\nlocal function upsample(bits, data)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_217944_upsample.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = upsample\n    lu.assertEquals(candidate(9, 0), 0)\n    lu.assertEquals(candidate(0, 0), 0)\n    lu.assertEquals(candidate(0, 1), 1)\n    lu.assertEquals(candidate(1, 0), 0)\n    lu.assertEquals(candidate(8, 0), 0)\n    lu.assertEquals(candidate(0, 255), 255)\n    lu.assertEquals(candidate(3, 0), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_218685_capitalized", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not s:\n-- #     return s\n-- # return s[0].upper() + s[1:]\n-- \n-- Return a string with its first character capitalized.\nlocal function capitalized(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_218685_capitalized.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = capitalized\n    lu.assertEquals(candidate('Word'), 'Word')\n    lu.assertEquals(candidate('sentence'), 'Sentence')\n    lu.assertEquals(candidate('abc'), 'Abc')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('a'), 'A')\n    lu.assertEquals(candidate(None), None)\n    lu.assertEquals(candidate('A'), 'A')\n    lu.assertEquals(candidate('word'), 'Word')\n    lu.assertEquals(candidate('Sentence'), 'Sentence')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_218914_problem_26", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # def div_cycle(n, print_cyc=False):\n-- #     \"\"\" helper function to determine the length of the cycle in 1/n.\n-- #     Essentially, step through long division and once we reach a remainder\n-- #     that has already been seen, we have found the cycle (between the\n-- #     two remainders) \"\"\"\n-- #     base = 10\n-- #     remainder = 1\n-- #     # for debugging\n-- #     cyc = \"\"\n-- #     # track remainders and length of cycle before then\n-- #     rems = {}\n-- #     cycle_len = 0\n-- #     while remainder:\n-- #         # for debugging\n-- #         dec = remainder * base / n\n-- #         # long-division \"carry\"\n-- #         remainder *= base\n-- #         # already seen remainder is the start of next cycle\n-- #         if remainder in rems:\n-- #             if print_cyc:\n-- #                 print(\"cycle found: \", cyc[rems[remainder]:])\n-- #             return cycle_len - rems[remainder]\n-- #         # store the index at occurence of remainder, because there can be\n-- #         # digits before the cycle begins, need to subtract out\n-- #         rems[remainder] = cycle_len\n-- #         cycle_len += 1\n-- #         remainder %= n\n-- #         cyc += str(dec)\n-- #     return -1\n-- # max_len = 0\n-- # max_n = 0\n-- # # loop over all numbers and find the n with highest corresponding cycle len.\n-- # for num in range(1, lim):\n-- #     val = div_cycle(num)\n-- #     if val > max_len:\n-- #         max_len = val\n-- #         max_n = num\n-- # return max_n\n-- \n--  longest recurring cycle in decimal fractions\nlocal function problem_26(lim)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_218914_problem_26.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = problem_26\n    lu.assertEquals(candidate(1000), 983)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_220758_arabic_to_roman", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if number > 3999:\n-- #     raise ValueError(f\"Number can not be represented with roman numerals: \"\n-- #                      f\"{number}\")\n-- # roman_number = \"\"\n-- # for index, digit in enumerate(reversed(str(number))):\n-- #     if index == 0:\n-- #         first = \"I\"\n-- #         fifth = \"V\"\n-- #         tenth = \"X\"\n-- #     elif index == 1:\n-- #         first = \"X\"\n-- #         fifth = \"L\"\n-- #         tenth = \"C\"\n-- #     elif index == 2:\n-- #         first = \"C\"\n-- #         fifth = \"D\"\n-- #         tenth = \"M\"\n-- #     elif index == 3:\n-- #         first = \"M\"\n-- #         fifth = \"\"\n-- #         tenth = \"\"\n-- #     else:\n-- #         raise ValueError(f\"Invalid input: {number}\")\n-- #     if digit == \"0\":\n-- #         continue\n-- #     elif digit == \"1\":\n-- #         roman_number = first + roman_number\n-- #     elif digit == \"2\":\n-- #         roman_number = first * 2 + roman_number\n-- #     elif digit == \"3\":\n-- #         roman_number = first * 3 + roman_number\n-- #     elif digit == \"4\":\n-- #         roman_number = first + fifth + roman_number\n-- #     elif digit == \"5\":\n-- #         roman_number = fifth + roman_number\n-- #     elif digit == \"6\":\n-- #         roman_number = fifth + first + roman_number\n-- #     elif digit == \"7\":\n-- #         roman_number = fifth + first * 2 + roman_number\n-- #     elif digit == \"8\":\n-- #         roman_number = fifth + first * 3 + roman_number\n-- #     elif digit == \"9\":\n-- #         roman_number = first + tenth + roman_number\n-- # return roman_number\n-- \n-- Return roman version of the specified arabic number.\nlocal function arabic_to_roman(number)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_220758_arabic_to_roman.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = arabic_to_roman\n    lu.assertEquals(candidate(891), 'DCCCXCI')\n    lu.assertEquals(candidate(25), 'XXV')\n    lu.assertEquals(candidate(16), 'XVI')\n    lu.assertEquals(candidate(80), 'LXXX')\n    lu.assertEquals(candidate(501), 'DI')\n    lu.assertEquals(candidate(990), 'CMXC')\n    lu.assertEquals(candidate(14), 'XIV')\n    lu.assertEquals(candidate(15), 'XV')\n    lu.assertEquals(candidate(6), 'VI')\n    lu.assertEquals(candidate(649), 'DCXLIX')\n    lu.assertEquals(candidate(93), 'XCIII')\n    lu.assertEquals(candidate(500), 'D')\n    lu.assertEquals(candidate(88), 'LXXXVIII')\n    lu.assertEquals(candidate(9), 'IX')\n    lu.assertEquals(candidate(99), 'XCIX')\n    lu.assertEquals(candidate(12), 'XII')\n    lu.assertEquals(candidate(10), 'X')\n    lu.assertEquals(candidate(1), 'I')\n    lu.assertEquals(candidate(7), 'VII')\n    lu.assertEquals(candidate(8), 'VIII')\n    lu.assertEquals(candidate(19), 'XIX')\n    lu.assertEquals(candidate(3), 'III')\n    lu.assertEquals(candidate(2), 'II')\n    lu.assertEquals(candidate(600), 'DC')\n    lu.assertEquals(candidate(17), 'XVII')\n    lu.assertEquals(candidate(400), 'CD')\n    lu.assertEquals(candidate(23), 'XXIII')\n    lu.assertEquals(candidate(27), 'XXVII')\n    lu.assertEquals(candidate(20), 'XX')\n    lu.assertEquals(candidate(100), 'C')\n    lu.assertEquals(candidate(40), 'XL')\n    lu.assertEquals(candidate(21), 'XXI')\n    lu.assertEquals(candidate(0), '')\n    lu.assertEquals(candidate(30), 'XXX')\n    lu.assertEquals(candidate(798), 'DCCXCVIII')\n    lu.assertEquals(candidate(22), 'XXII')\n    lu.assertEquals(candidate(300), 'CCC')\n    lu.assertEquals(candidate(13), 'XIII')\n    lu.assertEquals(candidate(70), 'LXX')\n    lu.assertEquals(candidate(200), 'CC')\n    lu.assertEquals(candidate(11), 'XI')\n    lu.assertEquals(candidate(24), 'XXIV')\n    lu.assertEquals(candidate(5), 'V')\n    lu.assertEquals(candidate(60), 'LX')\n    lu.assertEquals(candidate(90), 'XC')\n    lu.assertEquals(candidate(18), 'XVIII')\n    lu.assertEquals(candidate(50), 'L')\n    lu.assertEquals(candidate(4), 'IV')\n    lu.assertEquals(candidate(700), 'DCC')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_221003_create_nonlocal_service_cluster_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"remote-{0}-{1}-{2}-{3}\".format(namespace, service, color, index)\n-- \n-- Create the cluster name for the non-local namespace, service, color.\nlocal function create_nonlocal_service_cluster_name(namespace, service, color, index)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_221003_create_nonlocal_service_cluster_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = create_nonlocal_service_cluster_name\n    lu.assertEquals(candidate('default', 'test-service', 'blue', 1), 'remote-default-test-service-blue-1')\n    lu.assertEquals(candidate('default', 'test-service', 'blue', 2), 'remote-default-test-service-blue-2')\n    lu.assertEquals(candidate('default', 'test-service', 'blue', 2), 'remote-default-test-service-blue-2')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_221888_FindMaximumSubarray", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # max_left = 0\n-- # max_right = 0\n-- # suma = A[0]\n-- # p = 0\n-- # suma_p = A[0]\n-- # for i in range(1, len(A)):\n-- #     suma_p += A[i]\n-- #     if suma_p <= 0:\n-- #         p = i\n-- #         suma_p = A[i]\n-- #     if suma_p > suma:\n-- #         max_left = p\n-- #         max_right = i\n-- #         suma = suma_p\n-- # return (max_left, max_right, suma)\n-- \n-- Encuentra la mayor subsecuencia dentro de la lista A.\n-- Si se recibe una lista vacia se lanza una excepcion de tipo IndexError.\n-- Retorna:\n-- max_left -- El indice izquierdo del subarreglo\n-- max_right -- El indice derecho del subarreglo\n-- suma -- La suma total del subarreglo\nlocal function FindMaximumSubarray(A)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_221888_FindMaximumSubarray.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = FindMaximumSubarray\n    lu.assertEquals(candidate({1, 2, 3}), {0, 2, 6})\n    lu.assertEquals(candidate({-1, -2, -3, -4}), {0, 0, -1})\n    lu.assertEquals(candidate({1}), {0, 0, 1})\n    lu.assertEquals(candidate({-1, -2, 0, 1}), {2, 3, 1})\n    lu.assertEquals(candidate({1, 2, 3}), {0, 2, 6})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_22246_get_chunk_slices", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # chunks = list(range(0, ds_dim, chunk_size))\n-- # if chunks[-1] < ds_dim:\n-- #     chunks.append(ds_dim)\n-- # else:\n-- #     chunks[-1] = ds_dim\n-- # chunks = list(zip(chunks[:-1], chunks[1:]))\n-- # return chunks\n-- \n-- Create list of chunk slices [(s_i, e_i), ...]\n-- Parameters\n-- ----------\n-- ds_len : 'int'\n--     Length of dataset axis to chunk\n-- chunk_size : 'int'\n--     Size of chunks\n-- Returns\n-- -------\n-- chunks : 'list'\n--     List of chunk start and end positions\n--     [(s_i, e_i), (s_i+1, e_i+1), ...]\nlocal function get_chunk_slices(ds_dim, chunk_size)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_22246_get_chunk_slices.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_chunk_slices\n    lu.assertEquals(candidate(10, 15), {{0, 10}})\n    lu.assertEquals(candidate(2, 1), {{0, 1}, {1, 2}})\n    lu.assertEquals(candidate(6, 6), {{0, 6}})\n    lu.assertEquals(candidate(10, 13), {{0, 10}})\n    lu.assertEquals(candidate(10, 14), {{0, 10}})\n    lu.assertEquals(candidate(10, 12), {{0, 10}})\n    lu.assertEquals(candidate(9, 5), {{0, 5}, {5, 9}})\n    lu.assertEquals(candidate(10, 10), {{0, 10}})\n    lu.assertEquals(candidate(3, 2), {{0, 2}, {2, 3}})\n    lu.assertEquals(candidate(6, 2), {{0, 2}, {2, 4}, {4, 6}})\n    lu.assertEquals(candidate(10, 17), {{0, 10}})\n    lu.assertEquals(candidate(6, 1), {{0, 1}, {1, 2}, {2, 3}, {3, 4}, {4, 5}, {5, 6}})\n    lu.assertEquals(candidate(1, 1), {{0, 1}})\n    lu.assertEquals(candidate(2, 2), {{0, 2}})\n    lu.assertEquals(candidate(6, 4), {{0, 4}, {4, 6}})\n    lu.assertEquals(candidate(7, 2), {{0, 2}, {2, 4}, {4, 6}, {6, 7}})\n    lu.assertEquals(candidate(5, 1), {{0, 1}, {1, 2}, {2, 3}, {3, 4}, {4, 5}})\n    lu.assertEquals(candidate(3, 1), {{0, 1}, {1, 2}, {2, 3}})\n    lu.assertEquals(candidate(10, 5), {{0, 5}, {5, 10}})\n    lu.assertEquals(candidate(10, 16), {{0, 10}})\n    lu.assertEquals(candidate(6, 7), {{0, 6}})\n    lu.assertEquals(candidate(10, 18), {{0, 10}})\n    lu.assertEquals(candidate(5, 2), {{0, 2}, {2, 4}, {4, 5}})\n    lu.assertEquals(candidate(8, 4), {{0, 4}, {4, 8}})\n    lu.assertEquals(candidate(9, 2), {{0, 2}, {2, 4}, {4, 6}, {6, 8}, {8, 9}})\n    lu.assertEquals(candidate(7, 4), {{0, 4}, {4, 7}})\n    lu.assertEquals(candidate(4, 1), {{0, 1}, {1, 2}, {2, 3}, {3, 4}})\n    lu.assertEquals(candidate(4, 2), {{0, 2}, {2, 4}})\n    lu.assertEquals(candidate(5, 3), {{0, 3}, {3, 5}})\n    lu.assertEquals(candidate(5, 5), {{0, 5}})\n    lu.assertEquals(candidate(5, 6), {{0, 5}})\n    lu.assertEquals(candidate(6, 3), {{0, 3}, {3, 6}})\n    lu.assertEquals(candidate(10, 11), {{0, 10}})\n    lu.assertEquals(candidate(8, 2), {{0, 2}, {2, 4}, {4, 6}, {6, 8}})\n    lu.assertEquals(candidate(6, 5), {{0, 5}, {5, 6}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_222628_indent", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # indentation = \" \" * 2\n-- # return \"\\n\".join([indentation + s for s in block.split('\\n')])\n-- \n-- Indent each row of the given string block with ``n*2`` spaces.\nlocal function indent(block)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_222628_indent.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = indent\n    lu.assertEquals(candidate('foo\\nbar'), '  foo\\n  bar')\n    lu.assertEquals(candidate('hello\\n  world'), '  hello\\n    world')\n    lu.assertEquals(candidate('hello\\nworld'), '  hello\\n  world')\n    lu.assertEquals(candidate('hello'), '  hello')\n    lu.assertEquals(candidate('foo'), '  foo')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_223007_get_unlabled_last_index", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # ratio_unlabeled = num_unlabeled_samples / len_dataset\n-- # last_unlabeled_index = int(ratio_unlabeled * len_class)\n-- # return last_unlabeled_index\n-- \n-- For example, for CIFAR100 we have len_dataset 10000 for test data.\n-- The number of samples per class is 1000.\n-- If we want 9000 unlabeled samples then the ratio_unlabeled is 9/10.\n-- The number of samples per class for the unlabeled dataset is 9/10*100=90.\n-- If the number of samples for the final test is 1000 samples and we have 100\n-- classes, then the number of samples per class will be 10 (only).\n-- :param num_unlabeled_samples: number of unlabeled samples from the test set\n-- :param len_dataset: the total number of samples in the intial test set\n-- :param len_class: the number of samples for a given class\n-- :return: for the array of sample indices for the class, the last index for\n-- the unlabeled part\n-- >>> num_unlabeled_samples = 9000\n-- >>> len_dataset = 10000\n-- >>> len_class = 100\n-- >>> result = get_unlabled_last_index(num_unlabeled_samples=num_unlabeled_samples, len_dataset=len_dataset, len_class=len_class)\n-- >>> assert result == 90\n-- >>> # print('result: ', result)\nlocal function get_unlabled_last_index(num_unlabeled_samples, len_dataset, len_class)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_223007_get_unlabled_last_index.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_unlabled_last_index\n    lu.assertEquals(candidate(9000, 10000, 10000), 9000)\n    lu.assertEquals(candidate(2000, 1000, 100), 200)\n    lu.assertEquals(candidate(1000, 1000, 100), 100)\n    lu.assertEquals(candidate(10, 100, 100), 10)\n    lu.assertEquals(candidate(1, 100, 100), 1)\n    lu.assertEquals(candidate(9000, 10000, 100), 90)\n    lu.assertEquals(candidate(100, 1000, 100), 10)\n    lu.assertEquals(candidate(0, 100, 100), 0)\n    lu.assertEquals(candidate(100, 100, 30), 30)\n    lu.assertEquals(candidate(100, 100, 50), 50)\n    lu.assertEquals(candidate(10000, 1000, 100), 1000)\n    lu.assertEquals(candidate(200, 100, 10), 20)\n    lu.assertEquals(candidate(100, 100, 10), 10)\n    lu.assertEquals(candidate(9000, 10000, 1000), 900)\n    lu.assertEquals(candidate(50, 100, 10), 5)\n    lu.assertEquals(candidate(100, 100, 1), 1)\n    lu.assertEquals(candidate(9000, 10000, 500), 450)\n    lu.assertEquals(candidate(9000, 10000, 150), 135)\n    lu.assertEquals(candidate(9000, 10000, 200), 180)\n    lu.assertEquals(candidate(100, 100, 40), 40)\n    lu.assertEquals(candidate(100, 100, 60), 60)\n    lu.assertEquals(candidate(100, 100, 20), 20)\n    lu.assertEquals(candidate(900, 1000, 100), 90)\n    lu.assertEquals(candidate(9000, 10000, 100), 90)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_223709_one_to_three", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return ((1, 0, 0), (0, 1, 0), (0, 0, 1))[one+1]\n-- \n--  Take a score -1, 0, or 1, and return the three vector of labels \nlocal function one_to_three(one)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_223709_one_to_three.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = one_to_three\n    lu.assertEquals(candidate(0), {0, 1, 0})\n    lu.assertEquals(candidate(1), {0, 0, 1})\n    lu.assertEquals(candidate(-1), {1, 0, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_224204_glue_template_and_params", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # (template, params) = template_and_params\n-- # text = u''\n-- # for item in params:\n-- #     text += u'|%s=%s\\n' % (item, params[item])\n-- # return u'{{%s\\n%s}}' % (template, text)\n-- \n-- Return wiki text of template glued from params.\n-- You can use items from extract_templates_and_params here to get\n-- an equivalent template wiki text (it may happen that the order\n-- of the params changes).\nlocal function glue_template_and_params(template_and_params)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_224204_glue_template_and_params.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = glue_template_and_params\n    lu.assertEquals(candidate({'Template', {['param1'] = 'value1', ['param2'] = 'value2', ['param3'] = 'value3', ['param4'] = 'value4'}}), '{{Template\\n|param1=value1\\n|param2=value2\\n|param3=value3\\n|param4=value4\\n}}')\n    lu.assertEquals(candidate({'Template', {['param1'] = 'value1', ['param2'] = 'value2', ['param3'] = 'value3', ['param4'] = 'value4', ['param5'] = 'value5'}}), '{{Template\\n|param1=value1\\n|param2=value2\\n|param3=value3\\n|param4=value4\\n|param5=value5\\n}}')\n    lu.assertEquals(candidate({'Template', {['param1'] = 'value1', ['param2'] = 'value2', ['param3'] = 'value3'}}), '{{Template\\n|param1=value1\\n|param2=value2\\n|param3=value3\\n}}')\n    lu.assertEquals(candidate({'Template', {['param1'] = 'value1', ['param2'] = 'value2'}}), '{{Template\\n|param1=value1\\n|param2=value2\\n}}')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_224251_export_shard_uuid", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # import os\n-- # return os.path.join(uuid[:2], uuid[2:4], uuid[4:])\n-- \n-- Sharding of the UUID for the import/export\n-- :param uuid: UUID to be sharded (v4)\n-- :type uuid: str\n-- :return: Sharded UUID as a subfolder path\n-- :rtype: str\nlocal function export_shard_uuid(uuid)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_224251_export_shard_uuid.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = export_shard_uuid\n    lu.assertEquals(candidate('412c7920-220e-45d8-b1a1-8704b04144b7'), '41/2c/7920-220e-45d8-b1a1-8704b04144b7')\n    lu.assertEquals(candidate('a83f28ec-9b98-4036-923a-84b758b43790'), 'a8/3f/28ec-9b98-4036-923a-84b758b43790')\n    lu.assertEquals(candidate('9d241b76-b780-4a0f-98e1-400088a41f92'), '9d/24/1b76-b780-4a0f-98e1-400088a41f92')\n    lu.assertEquals(candidate('61516d6c-3190-4434-998f-4e9179a86b61'), '61/51/6d6c-3190-4434-998f-4e9179a86b61')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_224338_check_header", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # header_id = line.replace(' ', '-')\n-- # if line[:7] == '###### ':\n-- #     line = '<h6 id=\"' + header_id[7:] + '\">' + line[7:] + '</h6>'\n-- # elif line[:6] == '##### ':\n-- #     line = '<h5 id=\"' + header_id[6:] + '\">' + line[6:] + '</h5>'\n-- # elif line[:5] == '#### ':\n-- #     line = '<h4 id=\"' + header_id[5:] + '\">' + line[5:] + '</h4>'\n-- # elif line[:4] == '### ':\n-- #     line = '<h3 id=\"' + header_id[4:] + '\">' + line[4:] + '</h3>'\n-- # elif line[:3] == '## ':\n-- #     line = '<h2 id=\"' + header_id[3:] + '\">' + line[3:] + '</h2>'\n-- # elif line[:2] == '# ':\n-- #     line = '<h1 id=\"' + header_id[2:] + '\">' + line[2:] + '</h1>'\n-- # else:\n-- #     return False, ''\n-- # return True, line\n-- \n--  Check whether a line is header, if it is, change it into html format\n-- :param line: str, a line in markdown file\n-- :return: boolean, whether a line is header\n--          str, the line in html format\nlocal function check_header(line)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_224338_check_header.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = check_header\n    lu.assertEquals(candidate('This is not a header.'), {false, ''})\n    lu.assertEquals(candidate('#Heading 1'), {false, ''})\n    lu.assertEquals(candidate('##### Heading 5'), {true, '<h5 id=\"Heading-5\">Heading 5</h5>'})\n    lu.assertEquals(candidate('##Heading 2'), {false, ''})\n    lu.assertEquals(candidate('# foo'), {true, '<h1 id=\"foo\">foo</h1>'})\n    lu.assertEquals(candidate('This is not a header'), {false, ''})\n    lu.assertEquals(candidate('##### test'), {true, '<h5 id=\"test\">test</h5>'})\n    lu.assertEquals(candidate('###'), {false, ''})\n    lu.assertEquals(candidate('#####Heading 5'), {false, ''})\n    lu.assertEquals(candidate('test'), {false, ''})\n    lu.assertEquals(candidate('foo'), {false, ''})\n    lu.assertEquals(candidate('A Header'), {false, ''})\n    lu.assertEquals(candidate('##### foo'), {true, '<h5 id=\"foo\">foo</h5>'})\n    lu.assertEquals(candidate('###### Heading 6'), {true, '<h6 id=\"Heading-6\">Heading 6</h6>'})\n    lu.assertEquals(candidate('# Heading 1'), {true, '<h1 id=\"Heading-1\">Heading 1</h1>'})\n    lu.assertEquals(candidate('#### Heading 4'), {true, '<h4 id=\"Heading-4\">Heading 4</h4>'})\n    lu.assertEquals(candidate('### foo'), {true, '<h3 id=\"foo\">foo</h3>'})\n    lu.assertEquals(candidate('## foo'), {true, '<h2 id=\"foo\">foo</h2>'})\n    lu.assertEquals(candidate('####### An Un-Countable Level Header'), {false, ''})\n    lu.assertEquals(candidate('###### test'), {true, '<h6 id=\"test\">test</h6>'})\n    lu.assertEquals(candidate('# test'), {true, '<h1 id=\"test\">test</h1>'})\n    lu.assertEquals(candidate('### test'), {true, '<h3 id=\"test\">test</h3>'})\n    lu.assertEquals(candidate('#### test'), {true, '<h4 id=\"test\">test</h4>'})\n    lu.assertEquals(candidate('###### foo'), {true, '<h6 id=\"foo\">foo</h6>'})\n    lu.assertEquals(candidate('## Heading 2'), {true, '<h2 id=\"Heading-2\">Heading 2</h2>'})\n    lu.assertEquals(candidate('####Heading 4'), {false, ''})\n    lu.assertEquals(candidate('#### foo'), {true, '<h4 id=\"foo\">foo</h4>'})\n    lu.assertEquals(candidate('##'), {false, ''})\n    lu.assertEquals(candidate('######Heading 6'), {false, ''})\n    lu.assertEquals(candidate('This is not a header\\n\\n'), {false, ''})\n    lu.assertEquals(candidate('### Heading 3'), {true, '<h3 id=\"Heading-3\">Heading 3</h3>'})\n    lu.assertEquals(candidate('## test'), {true, '<h2 id=\"test\">test</h2>'})\n    lu.assertEquals(candidate('###Heading 3'), {false, ''})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_225363_minify_html", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return html.strip().replace(\"    \", \"\").replace(\"\\n\", \"\")\n-- \n-- Perform a template-specific, rudimentary HTML minification for displaCy.\n-- Disclaimer: NOT a general-purpose solution, only removes indentation and\n-- newlines.\n-- html (unicode): Markup to minify.\n-- RETURNS (unicode): \"Minified\" HTML.\nlocal function minify_html(html)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_225363_minify_html.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = minify_html\n    lu.assertEquals(candidate(' \\n  <p> Hello, world! </p> \\n '), '<p> Hello, world! </p>')\n    lu.assertEquals(candidate('\\n    <h1>Hello, World!</h1>\\n    <p>This is a paragraph.</p>\\n    '), '<h1>Hello, World!</h1><p>This is a paragraph.</p>')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('\\n    <h1>Hello, World!</h1>\\n    '), '<h1>Hello, World!</h1>')\n    lu.assertEquals(candidate('\\n    <h1>Hello, World!</h1>\\n\\n    <p>This is a paragraph.</p>\\n\\n    <p>This is another paragraph.</p>\\n    '), '<h1>Hello, World!</h1><p>This is a paragraph.</p><p>This is another paragraph.</p>')\n    lu.assertEquals(candidate('   '), '')\n    lu.assertEquals(candidate('\\n'), '')\n    lu.assertEquals(candidate('        <p>Hello, world.</p>        '), '<p>Hello, world.</p>')\n    lu.assertEquals(candidate('   <p>Hello, world!</p> \\n \\n\\n '), '<p>Hello, world!</p>')\n    lu.assertEquals(candidate('    <p>Hello, world.</p>    '), '<p>Hello, world.</p>')\n    lu.assertEquals(candidate('\\n        <p>Hello, world.</p>        \\n'), '<p>Hello, world.</p>')\n    lu.assertEquals(candidate('    <p>A paragraph.</p>'), '<p>A paragraph.</p>')\n    lu.assertEquals(candidate('<html><head><title>Test</title></head><body><p>A paragraph.</p></body></html>'), '<html><head><title>Test</title></head><body><p>A paragraph.</p></body></html>')\n    lu.assertEquals(candidate('\\n    <h1>Hello, World!</h1>\\n    <p>This is a paragraph.</p>\\n    <p>This is another paragraph.</p>\\n    '), '<h1>Hello, World!</h1><p>This is a paragraph.</p><p>This is another paragraph.</p>')\n    lu.assertEquals(candidate('\\n    <h1>Hello, World!</h1>\\n\\n    <p>This is a paragraph.</p>\\n    '), '<h1>Hello, World!</h1><p>This is a paragraph.</p>')\n    lu.assertEquals(candidate('<p>A paragraph.</p>'), '<p>A paragraph.</p>')\n    lu.assertEquals(candidate('<p>Hello, world.</p>'), '<p>Hello, world.</p>')\n    lu.assertEquals(candidate('\\n    <p>Hello, world.</p>    \\n'), '<p>Hello, world.</p>')\n    lu.assertEquals(candidate(' \\n\\n\\n\\n  \\n'), '')\n    lu.assertEquals(candidate('Hello, world!'), 'Hello, world!')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_226063_conjugatePartition", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # p = sorted(filter(lambda x: x > 0, p), reverse=True)\n-- # result = []\n-- # j = len(p)\n-- # if j <= 0:\n-- #     return result\n-- # while True:\n-- #     result.append(j)\n-- #     while len(result) >= p[j-1]:\n-- #         j -= 1\n-- #         if j == 0:\n-- #             return result\n-- \n-- Find the conjugate of a partition.\n-- E.g. len(p) = max(conjugate(p)) and vice versa.\n-- D. Eppstein, August 2005.\nlocal function conjugatePartition(p)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_226063_conjugatePartition.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = conjugatePartition\n    lu.assertEquals(candidate({1, 1, 1}), {3})\n    lu.assertEquals(candidate({3, 3, 3}), {3, 3, 3})\n    lu.assertEquals(candidate({1}), {1})\n    lu.assertEquals(candidate({1, 2, 3}), {3, 2, 1})\n    lu.assertEquals(candidate({1}), {1})\n    lu.assertEquals(candidate(candidate({2, 2, 2, 2})), {2, 2, 2, 2})\n    lu.assertEquals(candidate({1, 1, 1, 1, 1, 1, 1, 1}), {8})\n    lu.assertEquals(candidate(candidate({10, 5, 3, 2, 1})), {10, 5, 3, 2, 1})\n    lu.assertEquals(candidate({}), {})\n    lu.assertEquals(candidate(candidate({2})), {2})\n    lu.assertEquals(candidate({1, 1}), {2})\n    lu.assertEquals(candidate(candidate({2, 2, 2, 2, 2, 2, 1})), {2, 2, 2, 2, 2, 2, 1})\n    lu.assertEquals(candidate(candidate({2, 2, 2, 2, 2, 2})), {2, 2, 2, 2, 2, 2})\n    lu.assertEquals(candidate({1, 2, 1}), {3, 1})\n    lu.assertEquals(candidate(candidate({2, 1, 1})), {2, 1, 1})\n    lu.assertEquals(candidate({}), {})\n    lu.assertEquals(candidate(candidate({})), {})\n    lu.assertEquals(candidate({1, 2}), {2, 1})\n    lu.assertEquals(candidate(candidate({2, 2, 2, 2, 2})), {2, 2, 2, 2, 2})\n    lu.assertEquals(candidate({1, 2, 3, 4, 5, 6, 7, 8}), {8, 7, 6, 5, 4, 3, 2, 1})\n    lu.assertEquals(candidate(candidate({2, 2})), {2, 2})\n    lu.assertEquals(candidate(candidate({5, 4, 3, 2, 1})), {5, 4, 3, 2, 1})\n    lu.assertEquals(candidate({1, 1, 1, 1}), {4})\n    lu.assertEquals(candidate({2, 1}), {2, 1})\n    lu.assertEquals(candidate({1, 1, 1, 1, 1, 1, 1}), {7})\n    lu.assertEquals(candidate(candidate({2, 2, 2})), {2, 2, 2})\n    lu.assertEquals(candidate({1, 1}), {2})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_22607_float_str", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if a < 0:\n-- #     # Use lots of decimal places for negative numbers because argparse\n-- #     # has problems parsing option args that are negative numbers in\n-- #     # exponential form.\n-- #     # https://github.com/popsim-consortium/demes-python/issues/325\n-- #     # https://bugs.python.org/issue9334\n-- #     return format(a, \".10f\")\n-- # else:\n-- #     return str(a)\n-- \n--     Convert float to string, for use in command line arguments.\nlocal function float_str(a)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_22607_float_str.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = float_str\n    lu.assertEquals(candidate(1e-100), '1e-100')\n    lu.assertEquals(candidate(1e-08), '1e-08')\n    lu.assertEquals(candidate(float('inf')), 'inf')\n    lu.assertEquals(candidate(1.0), '1.0')\n    lu.assertEquals(candidate(10000000000.0), '10000000000.0')\n    lu.assertEquals(candidate(1000.0), '1000.0')\n    lu.assertEquals(candidate(1e-10), '1e-10')\n    lu.assertEquals(candidate(0.0), '0.0')\n    lu.assertEquals(candidate(float('-inf')), '-inf')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_226474_get_version_substitute", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # version_str = version_str.lower()\n-- # # substitute edit and edition with version\n-- # if \"edition\" in version_str or \"edit\" in version_str:\n-- #     version_str = version_str.replace(\" edition\", \" version\")\n-- #     version_str = version_str.replace(\" edit \", \" version\")\n-- # if version_str.startswith(\"the \"):\n-- #     version_str = version_str.split(\"the \")[1]\n-- # if \"radio mix\" in version_str:\n-- #     version_str = \"radio version\"\n-- # elif \"video mix\" in version_str:\n-- #     version_str = \"video version\"\n-- # elif \"spanglish\" in version_str or \"spanish\" in version_str:\n-- #     version_str = \"spanish version\"\n-- # elif version_str.endswith(\"remaster\"):\n-- #     version_str = \"remaster\"\n-- # return version_str.strip()\n-- \n-- Transform provider version str to universal version type.\nlocal function get_version_substitute(version_str)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_226474_get_version_substitute.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_version_substitute\n    lu.assertEquals(candidate('The 2009 video mix'), 'video version')\n    lu.assertEquals(candidate('the 2009 spanglish version'), 'spanish version')\n    lu.assertEquals(candidate('The 2009 radio mix'), 'radio version')\n    lu.assertEquals(candidate('the remaster'), 'remaster')\n    lu.assertEquals(candidate('some other stuff'), 'some other stuff')\n    lu.assertEquals(candidate('The 2009 spanglish version'), 'spanish version')\n    lu.assertEquals(candidate('the 2009 spanish version'), 'spanish version')\n    lu.assertEquals(candidate('The 2009 spanish version'), 'spanish version')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_226709__LJ_ab_to_ab", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return {'A': coeffs['A'], 'B': coeffs['B']}\n-- \n--     Convert AB representation to AB representation of the LJ potential\nlocal function _LJ_ab_to_ab(coeffs)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_226709__LJ_ab_to_ab.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _LJ_ab_to_ab\n    lu.assertEquals(candidate({['A'] = 2, ['B'] = 3}), {['A'] = 2, ['B'] = 3})\n    lu.assertEquals(candidate({['A'] = 1, ['B'] = 2}), {['A'] = 1, ['B'] = 2})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_22682_remove_quotes", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if string.startswith('\"'):\n-- #     string = string[1:]\n-- # if string.endswith('\"'):\n-- #     string = string[:-1]\n-- # return string\n-- \n-- This function is used here to remove quotes from\n-- paths used in this script.\n-- :param string: Path with quotes.\n-- :return: Path without quotes.\nlocal function remove_quotes(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_22682_remove_quotes.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = remove_quotes\n    lu.assertEquals(candidate('C:\\\\Users\\\\Test User\\\\AppData\\\\Local\\\\Programs'), 'C:\\\\Users\\\\Test User\\\\AppData\\\\Local\\\\Programs')\n    lu.assertEquals(candidate('\"C:\\\\\\\\Users\\\\\\\\Markus\\\\\\\\Desktop\"'), 'C:\\\\\\\\Users\\\\\\\\Markus\\\\\\\\Desktop')\n    lu.assertEquals(candidate('\"C:\\\\Users\\\\Markus\\\\Desktop\"'), 'C:\\\\Users\\\\Markus\\\\Desktop')\n    lu.assertEquals(candidate('C:\\\\Users\\\\Batman\\\\Desktop\\\\not_so_important_file.txt'), 'C:\\\\Users\\\\Batman\\\\Desktop\\\\not_so_important_file.txt')\n    lu.assertEquals(candidate('\"C:\\\\Users\\\\Batman\\\\Desktop\\\\super_important_file.txt\"'), 'C:\\\\Users\\\\Batman\\\\Desktop\\\\super_important_file.txt')\n    lu.assertEquals(candidate('\"C:\\\\Users\\\\Batman\\\\Desktop\\\\not_so_important_file.txt\"'), 'C:\\\\Users\\\\Batman\\\\Desktop\\\\not_so_important_file.txt')\n    lu.assertEquals(candidate('\"/path/with/quotes/in/it.txt\"'), '/path/with/quotes/in/it.txt')\n    lu.assertEquals(candidate('C:\\\\Users\\\\Batman\\\\Desktop\\\\super_important_file.txt'), 'C:\\\\Users\\\\Batman\\\\Desktop\\\\super_important_file.txt')\n    lu.assertEquals(candidate('\"C:\\\\my path\\\\file.txt\"'), 'C:\\\\my path\\\\file.txt')\n    lu.assertEquals(candidate('C:\\\\Users\\\\Test User\\\\AppData\\\\Local\\\\Programs'), 'C:\\\\Users\\\\Test User\\\\AppData\\\\Local\\\\Programs')\n    lu.assertEquals(candidate('\"C:\\\\Users\\\\Markus\"'), 'C:\\\\Users\\\\Markus')\n    lu.assertEquals(candidate('\"/path/with/quotes/in/it.txt\"'), '/path/with/quotes/in/it.txt')\n    lu.assertEquals(candidate('\"C:\\\\Users\\\\Test User\\\\AppData\\\\Local\\\\Programs\"'), 'C:\\\\Users\\\\Test User\\\\AppData\\\\Local\\\\Programs')\n    lu.assertEquals(candidate('C:\\\\Users\\\\Batman\\\\Desktop\\\\super_important_file.txt'), 'C:\\\\Users\\\\Batman\\\\Desktop\\\\super_important_file.txt')\n    lu.assertEquals(candidate('C:\\\\Users\\\\Test User\\\\AppData\\\\Local\\\\Programs'), 'C:\\\\Users\\\\Test User\\\\AppData\\\\Local\\\\Programs')\n    lu.assertEquals(candidate('C:\\\\Users\\\\Batman\\\\Desktop\\\\not_so_important_file.txt'), 'C:\\\\Users\\\\Batman\\\\Desktop\\\\not_so_important_file.txt')\n    lu.assertEquals(candidate('\"C:\\\\Users\\\\Batman\\\\Desktop\\\\super_important_file.txt\"'), 'C:\\\\Users\\\\Batman\\\\Desktop\\\\super_important_file.txt')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_227166_partition", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # pivot = v[0]\n-- # smaller = [elem for elem in v[1:] if elem < pivot]\n-- # larger = [elem for elem in v[1:] if elem >= pivot]\n-- # return len(smaller), smaller + [pivot] + larger\n-- \n-- Partitions a given array using the first element as the pivot.\nlocal function partition(v)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_227166_partition.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = partition\n    lu.assertEquals(candidate(list(range(10))), {0, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}})\n    lu.assertEquals(candidate({1}), {0, {1}})\n    lu.assertEquals(candidate({0, 2, 4, 6, 8}), {0, {0, 2, 4, 6, 8}})\n    lu.assertEquals(candidate({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}), {0, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_227167_parse_reg_02h_byte", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # assert 0 <= byte_val < 256\n-- # return byte_val\n-- \n-- Net ID\nlocal function parse_reg_02h_byte(byte_val)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_227167_parse_reg_02h_byte.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_reg_02h_byte\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(127), 127)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(255), 255)\n    lu.assertEquals(candidate(255), 255)\n    lu.assertEquals(candidate(0), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_227348__find_minmax_indices", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # vec = [abs(i) for i in invec]\n-- # this_min = vec.index(min([i for i in vec if i > 0]))\n-- # rvec = [i for i in reversed(vec)]\n-- # this_max = 2 - rvec.index(max(rvec))\n-- # return (this_min, this_max)\n-- \n-- Finds the indices corresponding to the minimum and maximum values\n-- in an integer vector.\n-- :args invec: The input integer array.\nlocal function _find_minmax_indices(invec)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_227348__find_minmax_indices.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _find_minmax_indices\n    lu.assertEquals(candidate({1, 2, 3}), {0, 2})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_22782_extract_instance_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return url.rsplit('/', 1)[-1]\n-- \n-- Given instance URL returns instance name.\nlocal function extract_instance_name(url)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_22782_extract_instance_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = extract_instance_name\n    lu.assertEquals(candidate('http://localhost:8000/instances/project_id:region:instance_id'), 'project_id:region:instance_id')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_229260_get_status_code_value", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # status_code_dict = {\n-- #     \"100\": \"Continue\", \"101\": \"Switching Protocols\", \"200\": \"OK\", \"201\": \"Created\",\n-- #     \"202\": \"Accepted\", \"203\": \"Non-authoritative Information\", \"204\": \"No Content\",\n-- #     \"205\": \"Reset Content\", \"206\": \"Partial Content\", \"300\": \"Multiple Choices\",\n-- #     \"301\": \"Moved Permanently\", \"302\": \"Found\", \"303\": \"See Other\", \"304\": \"Not Modified\",\n-- #     \"305\": \"Use Proxy\", \"306\": \"Unused\", \"307\": \"Temporary Redirect\", \"400\": \"Bad Request\",\n-- #     \"401\": \"Unauthorized\", \"402\": \"Payment Required\", \"403\": \"Forbidden\", \"404\": \"Not Found\",\n-- #     \"405\": \"Method Not Allowed\", \"406\": \"Not Acceptable\", \"407\": \"Proxy Authentication Required\",\n-- #     \"408\": \"Request Timeout\", \"409\": \"Conflict\", \"410\": \"Gone\", \"411\": \"Length Required\",\n-- #     \"412\": \"Precondition Failed\", \"413\": \"Request Entity Too Large\", \"414\": \"Request-url Too Long\",\n-- #     \"415\": \"Unsupported Media Type\", \"416\": \"Requested Range Not Satisfiable\",\n-- #     \"417\": \"Expectation Failed\", \"500\": \"Internal Server Error\", \"501\": \"Not Implemented\",\n-- #     \"502\": \"Bad Gateway\", \"503\": \"Service Unavailable\", \"504\": \"Gateway Timeout\",\n-- #     \"505\": \"HTTP Version Not Supported\", \"No Response\": \"\"\n-- # }\n-- # return status_code_dict[status_code]\n-- \n-- function to return appropriate status code value from the dictionary\nlocal function get_status_code_value(status_code)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_229260_get_status_code_value.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_status_code_value\n    lu.assertEquals(candidate('203'), 'Non-authoritative Information')\n    lu.assertEquals(candidate('415'), 'Unsupported Media Type')\n    lu.assertEquals(candidate('204'), 'No Content')\n    lu.assertEquals(candidate('302'), 'Found')\n    lu.assertEquals(candidate('303'), 'See Other')\n    lu.assertEquals(candidate('403'), 'Forbidden')\n    lu.assertEquals(candidate('304'), 'Not Modified')\n    lu.assertEquals(candidate('501'), 'Not Implemented')\n    lu.assertEquals(candidate('201'), 'Created')\n    lu.assertEquals(candidate('206'), 'Partial Content')\n    lu.assertEquals(candidate('400'), 'Bad Request')\n    lu.assertEquals(candidate('100'), 'Continue')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_229673_u16leListToByteList", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # byteData = []\n-- # for h in data:\n-- #     byteData.extend([h & 0xff, (h >> 8) & 0xff])\n-- # return byteData\n-- \n-- Convert a halfword array into a byte array\nlocal function u16leListToByteList(data)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_229673_u16leListToByteList.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = u16leListToByteList\n    lu.assertEquals(candidate({4660, 22136}), {52, 18, 120, 86})\n    lu.assertEquals(candidate(list()), list())\n    lu.assertEquals(candidate({0}), {0, 0})\n    lu.assertEquals(candidate({4660, 22136}), {52, 18, 120, 86})\n    lu.assertEquals(candidate({65535}), {255, 255})\n    lu.assertEquals(candidate({}), {})\n    lu.assertEquals(candidate({1, 2, 3, 4}), {1, 0, 2, 0, 3, 0, 4, 0})\n    lu.assertEquals(candidate({4660, 22136, 39612, 57087}), {52, 18, 120, 86, 188, 154, 255, 222})\n    lu.assertEquals(candidate({4660, 22136, 39612}), {52, 18, 120, 86, 188, 154})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_229855_parse_youtube_title", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # assert full_title.count('-') <= 1\n-- # first_dash_index = full_title.find('-')\n-- # if first_dash_index == -1:  # No dashes exist\n-- #     return (None, full_title)\n-- # else:\n-- #     author = full_title[:first_dash_index].strip()\n-- #     song_title = full_title[first_dash_index + 1:].strip()\n-- #     return (author, song_title)\n-- \n-- Parse song tags from youtube title.\n-- Current logic is to assumme the youtube title is always in format 'author-song title'.\n-- If there exists dashes in the title, up to the first dash is the author and the rest is the song title.\n-- If there are no dashes, return original title as song title.\n-- There will never be more than one dash because of the way we clean youtube titles.\n-- Return tuple (author_title, song_title)\nlocal function parse_youtube_title(full_title)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_229855_parse_youtube_title.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_youtube_title\n    lu.assertEquals(candidate('author-song title'), {'author', 'song title'})\n    lu.assertEquals(candidate('author - song title'), {'author', 'song title'})\n    lu.assertEquals(candidate(\"The Beatles - Sgt. Pepper's Lonely Hearts Club Band\"), {'The Beatles', \"Sgt. Pepper's Lonely Hearts Club Band\"})\n    lu.assertEquals(candidate(\"Sgt. Pepper's Lonely Hearts Club Band\"), {None, \"Sgt. Pepper's Lonely Hearts Club Band\"})\n    lu.assertEquals(candidate('Someone-awesome song title'), {'Someone', 'awesome song title'})\n    lu.assertEquals(candidate('Sgt. Pepper Lonely Hearts Club Band'), {None, 'Sgt. Pepper Lonely Hearts Club Band'})\n    lu.assertEquals(candidate('Someone-awesome song title'), {'Someone', 'awesome song title'})\n    lu.assertEquals(candidate('The Beatles - Sgt. Pepper Lonely Hearts Club Band'), {'The Beatles', 'Sgt. Pepper Lonely Hearts Club Band'})\n    lu.assertEquals(candidate('awesome song title'), {None, 'awesome song title'})\n    lu.assertEquals(candidate('awesome song title'), {None, 'awesome song title'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_23033_is_basic_type", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # basic_types = ('b', 'd', 'g', 'i', 'n', 'o', 'q', 's', 't', 'u', 'x', 'y')\n-- # return signature in basic_types\n-- \n-- Returns True if the signature is a basic type\n-- 'a', '(', '{', and 'v' are not considered basic types because they usually\n-- cannot be handled the same as other types.\nlocal function is_basic_type(signature)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_23033_is_basic_type.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_basic_type\n    lu.assertEquals(candidate('q'), true)\n    lu.assertEquals(candidate('d'), true)\n    lu.assertEquals(candidate('y'), true)\n    lu.assertEquals(candidate('g'), true)\n    lu.assertEquals(candidate('i'), true)\n    lu.assertEquals(candidate('s'), true)\n    lu.assertEquals(candidate('x'), true)\n    lu.assertEquals(candidate('('), false)\n    lu.assertEquals(candidate('b'), true)\n    lu.assertEquals(candidate('n'), true)\n    lu.assertEquals(candidate('t'), true)\n    lu.assertEquals(candidate('v'), false)\n    lu.assertEquals(candidate('a'), false)\n    lu.assertEquals(candidate('{'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_230829_echo", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return 'ECHO: %s' % message\n-- \n--     Very simple endpoint that just echos your message back to you\n-- :param message:  str of the message to echo\n-- :return:         str of the message echoed\nlocal function echo(message)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_230829_echo.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = echo\n    lu.assertEquals(candidate('world'), 'ECHO: world')\n    lu.assertEquals(candidate('hello'), 'ECHO: hello')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_231491_truncate", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if len(text) > max_length:\n-- #     return text[: max_length - 3] + \"...\"\n-- # return text\n-- \n-- Return text truncated to the max_length character if needed.\nlocal function truncate(text, max_length)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_231491_truncate.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = truncate\n    lu.assertEquals(candidate('This is a long string.', 10), 'This is...')\n    lu.assertEquals(candidate('Hello World', 1000), 'Hello World')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_231656_get_unique_tokens", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # tokens = geneset_name.split(\"; \")\n-- # uniq_tokens = []\n-- # for t in tokens:\n-- #     if t not in uniq_tokens:\n-- #         uniq_tokens.append(t)\n-- # return \"; \".join(uniq_tokens)\n-- \n-- Delimit the input `geneset_name` by \"; \", and return a new string\n-- that includes only unique tokens delimited by \"; \".\nlocal function get_unique_tokens(geneset_name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_231656_get_unique_tokens.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_unique_tokens\n    lu.assertEquals(candidate('A; A; B; C'), 'A; B; C')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_232067_truncpad", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # truncate\n-- # if len(srcline) > length:\n-- #     if ellipsis and length > 4:\n-- #         ret = srcline[0:(length-1)] + u'\\u2026'  # Ellipsis\n-- #     else:\n-- #         ret = srcline[0:length]\n-- # else:\n-- #     # pad\n-- #     if len(srcline) < length:\n-- #         if align == u'l':\n-- #             ret = srcline.ljust(length)\n-- #         elif align == u'r':\n-- #             ret = srcline.rjust(length)\n-- #         else:\n-- #             ret = srcline.center(length)\n-- #     else:\n-- #         ret = srcline\n-- # return ret\n-- \n-- Return srcline truncated and padded to length, aligned as requested.\nlocal function truncpad(srcline, length, align, ellipsis)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_232067_truncpad.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = truncpad\n    lu.assertEquals(candidate('1234', 3, 'l', false), '123')\n    lu.assertEquals(candidate('1234', 6, 'r'), '  1234')\n    lu.assertEquals(candidate('this is a test', 4), 'this')\n    lu.assertEquals(candidate('1234', 5, 'r'), ' 1234')\n    lu.assertEquals(candidate('1234', 3, 'c', false), '123')\n    lu.assertEquals(candidate('12345', 5), '12345')\n    lu.assertEquals(candidate('hello', 6, 'r'), ' hello')\n    lu.assertEquals(candidate('1234', 6, 'l'), '1234  ')\n    lu.assertEquals(candidate('this is a test', 4, 'r', false), 'this')\n    lu.assertEquals(candidate('1234', 5, 'c'), ' 1234')\n    lu.assertEquals(candidate('hello', 6, 'l'), 'hello ')\n    lu.assertEquals(candidate('1234567', 10), '1234567   ')\n    lu.assertEquals(candidate('hello', 4), 'hell')\n    lu.assertEquals(candidate('hello', 6), 'hello ')\n    lu.assertEquals(candidate('1234', 3, 'r', false), '123')\n    lu.assertEquals(candidate('hello', 4, 'l'), 'hell')\n    lu.assertEquals(candidate('1234', 3, 'l'), '123')\n    lu.assertEquals(candidate('hello', 5), 'hello')\n    lu.assertEquals(candidate('1234', 5, 'l'), '1234 ')\n    lu.assertEquals(candidate('1234', 3, 'r'), '123')\n    lu.assertEquals(candidate('12345', 10), '12345     ')\n    lu.assertEquals(candidate('123456', 10), '123456    ')\n    lu.assertEquals(candidate('1234', 3, 'c'), '123')\n    lu.assertEquals(candidate('hello', 4, 'c'), 'hell')\n    lu.assertEquals(candidate('hello', 4, 'r'), 'hell')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_232208_linearize", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # force_zero_a = \"- %s + %s >= 0\" % (y, a)\n-- # force_zero_b = \"- %s + %s >= 0\" % (y, b)\n-- # force_one = \"- %s + %s + %s <= 1\" % (y, a, b)\n-- # return [force_zero_a, force_zero_b, force_one]\n-- \n-- Generates linearization constraints for the product y = ab.\n-- Parameters\n-- ----------\n-- a : str\n--     First factor.\n-- b : str\n--     Second factor.\n-- y : Product.\n-- Returns\n-- -------\n-- List[str]\n--     A list holding the three linearization constraints.\nlocal function linearize(a, b, y)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_232208_linearize.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = linearize\n    lu.assertEquals(candidate('a1', 'b1', 'y1'), {'- y1 + a1 >= 0', '- y1 + b1 >= 0', '- y1 + a1 + b1 <= 1'})\n    lu.assertEquals(candidate('a', 'b', 'y1'), {'- y1 + a >= 0', '- y1 + b >= 0', '- y1 + a + b <= 1'})\n    lu.assertEquals(candidate('1', '2', '3'), {'- 3 + 1 >= 0', '- 3 + 2 >= 0', '- 3 + 1 + 2 <= 1'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_232442_padding", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for i in range(len(sample)):\n-- #     if len(sample[i]) < seq_max_len:\n-- #         sample[i] += [0 for _ in range(seq_max_len - len(sample[i]))]\n-- # return sample\n-- \n-- use '0' to padding the sentence\nlocal function padding(sample, seq_max_len)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_232442_padding.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = padding\n    lu.assertEquals(candidate({{1, 2}, {3, 4}, {5}}, 4), {{1, 2, 0, 0}, {3, 4, 0, 0}, {5, 0, 0, 0}})\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}}, 5), {{1, 2, 3, 0, 0}, {4, 5, 6, 0, 0}, {7, 8, 9, 0, 0}})\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5}, {6, 7, 8, 9}}, 5), {{1, 2, 3, 0, 0}, {4, 5, 0, 0, 0}, {6, 7, 8, 9, 0}})\n    lu.assertEquals(candidate({{1, 2, 3}, {2, 3}}, 5), {{1, 2, 3, 0, 0}, {2, 3, 0, 0, 0}})\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5, 6, 7}}, 4), {{1, 2, 3, 0}, {4, 5, 6, 7}})\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5, 6}}, 5), {{1, 2, 3, 0, 0}, {4, 5, 6, 0, 0}})\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5, 6}, {7}}, 5), {{1, 2, 3, 0, 0}, {4, 5, 6, 0, 0}, {7, 0, 0, 0, 0}})\n    lu.assertEquals(candidate({{1, 2}, {3, 4}, {5}}, 5), {{1, 2, 0, 0, 0}, {3, 4, 0, 0, 0}, {5, 0, 0, 0, 0}})\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5, 6}}, 4), {{1, 2, 3, 0}, {4, 5, 6, 0}})\n    lu.assertEquals(candidate({{1}, {2, 3}}, 3), {{1, 0, 0}, {2, 3, 0}})\n    lu.assertEquals(candidate({{1, 2, 3}, {2, 3}}, 4), {{1, 2, 3, 0}, {2, 3, 0, 0}})\n    lu.assertEquals(candidate({}, 3), {})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_233100_coding_problem_22", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # result = []\n-- # while the_string:\n-- #     found = False\n-- #     for word in dictionary:\n-- #         if the_string.startswith(word):\n-- #             the_string = the_string[len(word):]\n-- #             result += [word]\n-- #             found = True\n-- #             break\n-- #     if not found:\n-- #         return None\n-- # return result\n-- \n-- Given a dictionary of words and a string made up of those words (no spaces), return the original sentence in a\n-- list. If there is more than one possible reconstruction, return any of them. If there is no possible\n-- reconstruction, then return null.\n-- Examples:\n-- >>> coding_problem_22(['Riccardo', 'Brigittie', 'and', 'lollipop'], 'RiccardoandBrigittie')\n-- ['Riccardo', 'and', 'Brigittie']\n-- >>> coding_problem_22(['quick', 'brown', 'the', 'fox'], 'thequickbrownfox')\n-- ['the', 'quick', 'brown', 'fox']\n-- >>> coding_problem_22(['bed', 'bath', 'bedbath', 'and', 'beyond'], 'bedbathandbeyond')\n-- ['bed', 'bath', 'and', 'beyond']\nlocal function coding_problem_22(dictionary, the_string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_233100_coding_problem_22.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = coding_problem_22\n    lu.assertEquals(candidate({'bed', 'bath', 'bedbath', 'and', 'beyond'}, 'bedbathandbeyond'), {'bed', 'bath', 'and', 'beyond'})\n    lu.assertEquals(candidate({'quick', 'brown', 'the', 'fox'}, 'thequickbrownfox'), {'the', 'quick', 'brown', 'fox'})\n    lu.assertEquals(candidate({'Riccardo', 'Brigittie', 'and', 'lollipop'}, 'RiccardoandBrigittie'), {'Riccardo', 'and', 'Brigittie'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_234706_is_valid", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # success = False\n-- # for i, num in enumerate(sequence):\n-- #     if (number - num) in sequence[i:]:\n-- #         success = True\n-- #         break\n-- # return success\n-- \n--  Returns True if the number is a sum of two discrete numbers in \n-- the preceding sequence.\nlocal function is_valid(sequence, number)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_234706_is_valid.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_valid\n    lu.assertEquals(candidate(list(range(100)), 200), false)\n    lu.assertEquals(candidate({1}, 1), false)\n    lu.assertEquals(candidate(list(range(10)), 16), true)\n    lu.assertEquals(candidate({}, 10), false)\n    lu.assertEquals(candidate({1, 2, 3, 4, 5}, 6), true)\n    lu.assertEquals(candidate(list(range(10)), 1), true)\n    lu.assertEquals(candidate(list(range(10)), 15), true)\n    lu.assertEquals(candidate({1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, 12), true)\n    lu.assertEquals(candidate(list(range(10)), 19), false)\n    lu.assertEquals(candidate(list(range(100)), 201), false)\n    lu.assertEquals(candidate({1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, 16), true)\n    lu.assertEquals(candidate(list(range(10)), 6), true)\n    lu.assertEquals(candidate(list(range(10)), 9), true)\n    lu.assertEquals(candidate(list(range(10)), 4), true)\n    lu.assertEquals(candidate(list(range(100)), 10), true)\n    lu.assertEquals(candidate({1, 2, 3, 4, 5}, 7), true)\n    lu.assertEquals(candidate(list(range(10)), 18), true)\n    lu.assertEquals(candidate(list(range(10)), 21), false)\n    lu.assertEquals(candidate(list(range(10)), 2), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_23499_get_years", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # years = []\n-- # start_year = start_time.split(\"-\")[0]\n-- # finish_year = stop_time.split(\"-\")[0]\n-- # year = int(start_year)\n-- # while year <= int(finish_year):\n-- #     years.append(str(year))\n-- #     year += 1\n-- # return years\n-- \n-- Get years contained in a time period.\n-- Returns the list of years contained in between the provided\n-- start time and the stop time.\n-- :param start_time: Start time to determine list of years\n-- :type start_time: str\n-- :param stop_time: Stop time to determine list of years\n-- :type stop_time: str\n-- :return: Creation date\n-- :rtype: list of str\nlocal function get_years(start_time, stop_time)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_23499_get_years.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_years\n    lu.assertEquals(candidate('1968-06-19T13:46:29.000Z', '2007-04-26T20:36:19.000Z'), {'1968', '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007'})\n    lu.assertEquals(candidate('2017-05-05', '2017-05-15'), {'2017'})\n    lu.assertEquals(candidate('2017-05-05', '2017-05-05'), {'2017'})\n    lu.assertEquals(candidate('2010-09-16T04:04:36.000Z', '2012-04-26T20:36:19.000Z'), {'2010', '2011', '2012'})\n    lu.assertEquals(candidate('2017-05-05', '2016-05-05'), {})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_235080_gen_color", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # red_tone = num_elems / float(max_elems)\n-- # color = \"0.000 %.3f 1.000\" % (red_tone)\n-- #  print color\n-- # return color.lower()\n-- \n-- Generates a red color in 'dot' format, which tone is based in the number of elements of a cluster (more elements, more intense).\n-- @param num_elems: \n-- @param max_elems: \n-- @return: \nlocal function gen_color(num_elems, max_elems)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_235080_gen_color.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = gen_color\n    lu.assertEquals(candidate(3, 5), '0.000 0.600 1.000')\n    lu.assertEquals(candidate(5, 5), '0.000 1.000 1.000')\n    lu.assertEquals(candidate(200, 200), '0.000 1.000 1.000')\n    lu.assertEquals(candidate(0, 100), '0.000 0.000 1.000')\n    lu.assertEquals(candidate(50, 100), '0.000 0.500 1.000')\n    lu.assertEquals(candidate(50, 50), '0.000 1.000 1.000')\n    lu.assertEquals(candidate(1, 10), '0.000 0.100 1.000')\n    lu.assertEquals(candidate(0, 100), '0.000 0.000 1.000')\n    lu.assertEquals(candidate(1, 1), '0.000 1.000 1.000')\n    lu.assertEquals(candidate(0, 10), '0.000 0.000 1.000')\n    lu.assertEquals(candidate(100, 100), '0.000 1.000 1.000')\n    lu.assertEquals(candidate(3, 3), '0.000 1.000 1.000')\n    lu.assertEquals(candidate(0, 5), '0.000 0.000 1.000')\n    lu.assertEquals(candidate(5, 5), '0.000 1.000 1.000')\n    lu.assertEquals(candidate(1, 1), '0.000 1.000 1.000')\n    lu.assertEquals(candidate(0, 1), '0.000 0.000 1.000')\n    lu.assertEquals(candidate(10, 20), '0.000 0.500 1.000')\n    lu.assertEquals(candidate(10, 30), '0.000 0.333 1.000')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_236346__is_compliant_with_reference_json", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return reference_problem_name != json[\"problem_name\"] or reference_graph_type != json[\n-- #     \"graph_type\"] or reference_p != json[\"p\"]\n-- \n-- Validates if a json file contains the same metadata as provided in arguments.\nlocal function _is_compliant_with_reference_json(reference_problem_name, reference_graph_type, reference_p, json)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_236346__is_compliant_with_reference_json.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _is_compliant_with_reference_json\n    lu.assertEquals(candidate('reference_problem_name', 'reference_graph_type', 2, {['problem_name'] = 'reference_problem_name', ['graph_type'] = 'reference_graph_type', ['p'] = 2}), false)\n    lu.assertEquals(candidate('reference_problem_name', 'reference_graph_type', 2, {['problem_name'] = 'reference_problem_name', ['graph_type'] = 'reference_graph_type', ['p'] = 2, ['foo'] = 'bar'}), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_236919_egcd", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if a == 0:\n-- #     return b, 0, 1\n-- # else:\n-- #     g, x, y = egcd(b % a, a)\n-- #     return g, y - (b // a) * x, x\n-- \n-- SRC: https://en.wikibooks.org/wiki/Algorithm_Implementation/Mathematics/Extended_Euclidean_algorithm\n-- return (g, x, y) such that a*x + b*y = g = gcd(a, b)\nlocal function egcd(a, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_236919_egcd.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = egcd\n    lu.assertEquals(candidate(1, 0), {1, 1, 0})\n    lu.assertEquals(candidate(0, 20), {20, 0, 1})\n    lu.assertEquals(candidate(4, 5), {1, -1, 1})\n    lu.assertEquals(candidate(10, 0), {10, 1, 0})\n    lu.assertEquals(candidate(0, 1), {1, 0, 1})\n    lu.assertEquals(candidate(0, 0), {0, 0, 1})\n    lu.assertEquals(candidate(30, 30), {30, 1, 0})\n    lu.assertEquals(candidate(5, 4), {1, 1, -1})\n    lu.assertEquals(candidate(200, 200), {200, 1, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_237220_setbit", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if nth_bit < 0:\n-- #     raise ValueError('position of bit cannot be negative')\n-- # mask = 1 << nth_bit\n-- # return x | mask\n-- \n-- set n-th bit (i.e. set to 1) in an integer or array of integers\n-- Args:\n--     x: integer or :class:`numpy.ndarray` of integers\n--     nth_bit: position of bit to be set (0, 1, 2, ..)\n-- Returns:\n--     integer or array of integers where n-th bit is set while all other bits are kept as in input x\n-- Examples:\n--     >>> setbit(0, 1)\n--         2\n--     >>> setbit(3, 2)\n--         7\nlocal function setbit(x, nth_bit)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_237220_setbit.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = setbit\n    lu.assertEquals(candidate(0, 6), 64)\n    lu.assertEquals(candidate(2, 1), 2)\n    lu.assertEquals(candidate(1, 4), 17)\n    lu.assertEquals(candidate(7, 0), 7)\n    lu.assertEquals(candidate(3, 2), 7)\n    lu.assertEquals(candidate(0, 5), 32)\n    lu.assertEquals(candidate(0, 2), 4)\n    lu.assertEquals(candidate(0, 1), 2)\n    lu.assertEquals(candidate(0, 3), 8)\n    lu.assertEquals(candidate(0, 0), 1)\n    lu.assertEquals(candidate(0, 4), 16)\n    lu.assertEquals(candidate(0, 8), 256)\n    lu.assertEquals(candidate(0, 7), 128)\n    lu.assertEquals(candidate(1, 0), 1)\n    lu.assertEquals(candidate(1, 2), 5)\n    lu.assertEquals(candidate(1, 1), 3)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_237237_fix_indentation", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return text.replace('\\t', ' '*4)\n-- \n-- Replace tabs by spaces\nlocal function fix_indentation(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_237237_fix_indentation.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = fix_indentation\n    lu.assertEquals(candidate(\"if True:\\n    print('The indentation is wrong!')\"), \"if True:\\n    print('The indentation is wrong!')\")\n    lu.assertEquals(candidate('\\nif x:\\n    return 0\\nelse:\\n    return 1\\n'), '\\nif x:\\n    return 0\\nelse:\\n    return 1\\n')\n    lu.assertEquals(candidate('\\ndef func():\\n    if x:\\n        return 0\\n    else:\\n        return 1\\n'), '\\ndef func():\\n    if x:\\n        return 0\\n    else:\\n        return 1\\n')\n    lu.assertEquals(candidate('\\nif x:\\n    return 0\\nelif y:\\n    return 1\\nelse:\\n    return 2\\n'), '\\nif x:\\n    return 0\\nelif y:\\n    return 1\\nelse:\\n    return 2\\n')\n    lu.assertEquals(candidate('\\ndef func():\\n    if x:\\n        return 0\\n    elif y:\\n        return 1\\n    elif z:\\n        return 2\\n    else:\\n        return 3\\n'), '\\ndef func():\\n    if x:\\n        return 0\\n    elif y:\\n        return 1\\n    elif z:\\n        return 2\\n    else:\\n        return 3\\n')\n    lu.assertEquals(candidate('    I am indented by four spaces, or four tabs.'), '    I am indented by four spaces, or four tabs.')\n    lu.assertEquals(candidate(\"if True:\\n\\tprint('The indentation is wrong!')\"), \"if True:\\n    print('The indentation is wrong!')\")\n    lu.assertEquals(candidate('\\ndef func():\\n    if x:\\n        return 0\\n    elif y:\\n        return 1\\n    else:\\n        return 2\\n'), '\\ndef func():\\n    if x:\\n        return 0\\n    elif y:\\n        return 1\\n    else:\\n        return 2\\n')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_237242_normalize_archive_entry_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return name.replace('\\\\', '/')\n-- \n-- Get the normalized name of an archive file entry.\n-- Args:\n--     name (str): Name of the archive file entry.\n-- Returns:\n--     str: The normalized name.\nlocal function normalize_archive_entry_name(name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_237242_normalize_archive_entry_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = normalize_archive_entry_name\n    lu.assertEquals(candidate('foo/bar/'), 'foo/bar/')\n    lu.assertEquals(candidate('foo\\\\bar'), 'foo/bar')\n    lu.assertEquals(candidate('..'), '..')\n    lu.assertEquals(candidate('a/b'), 'a/b')\n    lu.assertEquals(candidate('../'), '../')\n    lu.assertEquals(candidate('.'), '.')\n    lu.assertEquals(candidate('a'), 'a')\n    lu.assertEquals(candidate('foo/bar'), 'foo/bar')\n    lu.assertEquals(candidate('foo\\\\bar\\\\baz'), 'foo/bar/baz')\n    lu.assertEquals(candidate('/'), '/')\n    lu.assertEquals(candidate('a/b/'), 'a/b/')\n    lu.assertEquals(candidate('./'), './')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('a/'), 'a/')\n    lu.assertEquals(candidate('foo'), 'foo')\n    lu.assertEquals(candidate('/a/'), '/a/')\n    lu.assertEquals(candidate('/a'), '/a')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_237520_extended_gcd", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # r = gcd(a,b) i = multiplicitive inverse of a mod b\n-- # #      or      j = multiplicitive inverse of b mod a\n-- # # Neg return values for i or j are made positive mod b or a respectively\n-- # # Iterateive Version is faster and uses much less stack space\n-- # x = 0\n-- # y = 1\n-- # lx = 1\n-- # ly = 0\n-- # oa = a  # Remember original a/b to remove\n-- # ob = b  # negative values from return results\n-- # while b != 0:\n-- #     q = a // b\n-- #     (a, b) = (b, a % b)\n-- #     (x, lx) = ((lx - (q * x)), x)\n-- #     (y, ly) = ((ly - (q * y)), y)\n-- # if lx < 0:\n-- #     lx += ob  # If neg wrap modulo orignal b\n-- # if ly < 0:\n-- #     ly += oa  # If neg wrap modulo orignal a\n-- # return a, lx, ly\n-- \n-- Returns a tuple (r, i, j) such that r = gcd(a, b) = ia + jb\nlocal function extended_gcd(a, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_237520_extended_gcd.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = extended_gcd\n    lu.assertEquals(candidate(12, 12), {12, 0, 1})\n    lu.assertEquals(candidate(1, 12), {1, 1, 0})\n    lu.assertEquals(candidate(100, 100), {100, 0, 1})\n    lu.assertEquals(candidate(12, 1), {1, 0, 1})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_238527_get_cluster", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # rst = 0\n-- # # BEGIN wxPirs\n-- # while startclust >= 170:\n-- #     startclust //= 170\n-- #     rst += (startclust + 1) * offset\n-- # # END wxPirs\n-- # return rst\n-- \n-- get the real starting cluster\nlocal function get_cluster(startclust, offset)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_238527_get_cluster.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_cluster\n    lu.assertEquals(candidate(0, 2), 0)\n    lu.assertEquals(candidate(181, 1), 2)\n    lu.assertEquals(candidate(180, 1), 2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_238578_get_library_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # name = library_name\n-- # for sign in ['!=', '==', '>=', '~=']:\n-- #     name = name.split(sign)[0]\n-- # return name.strip()\n-- \n--     Utility to split between the library name and version number when needed\nlocal function get_library_name(library_name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_238578_get_library_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_library_name\n    lu.assertEquals(candidate('django == 2.2.2'), 'django')\n    lu.assertEquals(candidate('Django >= 1.11'), 'Django')\n    lu.assertEquals(candidate('django'), 'django')\n    lu.assertEquals(candidate('os!=1.0'), 'os')\n    lu.assertEquals(candidate('os'), 'os')\n    lu.assertEquals(candidate('django ==1.11.6,>=1.8,>=1.4,<1.9'), 'django')\n    lu.assertEquals(candidate('django.utils'), 'django.utils')\n    lu.assertEquals(candidate('django ~=1.11,!=1.11.6,>=1.8,>=1.4,<1.9'), 'django')\n    lu.assertEquals(candidate('django'), 'django')\n    lu.assertEquals(candidate('django!=1.11,!=1.11.6'), 'django')\n    lu.assertEquals(candidate('django ==1.11.6,==1.11'), 'django')\n    lu.assertEquals(candidate('Django == 1.2.3,!=1.3.4.5, >=2.3.4'), 'Django')\n    lu.assertEquals(candidate('os==1.0!=1.0'), 'os')\n    lu.assertEquals(candidate('django.utils.log'), 'django.utils.log')\n    lu.assertEquals(candidate('django.utils.log.AdminEmailHandler'), 'django.utils.log.AdminEmailHandler')\n    lu.assertEquals(candidate('Django >= 1.11, == 2.3.4'), 'Django')\n    lu.assertEquals(candidate('Django == 2.2.2'), 'Django')\n    lu.assertEquals(candidate('os==1.0'), 'os')\n    lu.assertEquals(candidate('django!=1.11,!=1.11.6,>=1.8,>=1.4,<1.9'), 'django')\n    lu.assertEquals(candidate('os==1.0!=1.0>0.0'), 'os')\n    lu.assertEquals(candidate('os~=1.0'), 'os')\n    lu.assertEquals(candidate('django ==1.11.6'), 'django')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_239032_remove_path_segments", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # [''] means a '/', which is properly represented by ['', ''].\n-- # if segments == ['']:\n-- #     segments.append('')\n-- # if remove == ['']:\n-- #     remove.append('')\n-- # ret = None\n-- # if remove == segments:\n-- #     ret = []\n-- # elif len(remove) > len(segments):\n-- #     ret = segments\n-- # else:\n-- #     toremove = list(remove)\n-- #     if len(remove) > 1 and remove[0] == '':\n-- #         toremove.pop(0)\n-- #     if toremove and toremove == segments[-1 * len(toremove):]:\n-- #         ret = segments[:len(segments) - len(toremove)]\n-- #         if remove[0] != '' and ret:\n-- #             ret.append('')\n-- #     else:\n-- #         ret = segments\n-- # return ret\n-- \n-- Removes the path segments of <remove> from the end of the path\n-- segments <segments>.\n-- Examples:\n--   # ('/a/b/c', 'b/c') -> '/a/'\n--   remove_path_segments(['','a','b','c'], ['b','c']) == ['','a','']\n--   # ('/a/b/c', '/b/c') -> '/a'\n--   remove_path_segments(['','a','b','c'], ['','b','c']) == ['','a']\n-- Returns: The list of all remaining path segments after the segments\n-- in <remove> have been removed from the end of <segments>. If no\n-- segments from <remove> were removed from <segments>, <segments> is\n-- returned unmodified.\nlocal function remove_path_segments(segments, remove)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_239032_remove_path_segments.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = remove_path_segments\n    lu.assertEquals(candidate({'', 'a'}, {'', 'a', 'b'}), {'', 'a'})\n    lu.assertEquals(candidate({'', 'a'}, {'', 'a'}), {})\n    lu.assertEquals(candidate({'', 'a'}, {'', 'b', 'c', 'd', 'e'}), {'', 'a'})\n    lu.assertEquals(candidate({'', 'a', 'b', 'c'}, {'', 'b', 'c'}), {'', 'a'})\n    lu.assertEquals(candidate({'', 'a'}, {'', 'b', 'a', 'c'}), {'', 'a'})\n    lu.assertEquals(candidate({'', 'a'}, {'', 'b', 'c'}), {'', 'a'})\n    lu.assertEquals(candidate({''}, {''}), {})\n    lu.assertEquals(candidate({'', 'a', 'b', 'c'}, {'a', 'b', 'd'}), {'', 'a', 'b', 'c'})\n    lu.assertEquals(candidate({'', 'a', 'b', 'c', 'd'}, {'', 'b', 'c', 'd'}), {'', 'a'})\n    lu.assertEquals(candidate({'', 'a'}, {'', 'b', 'c', 'a'}), {'', 'a'})\n    lu.assertEquals(candidate({'', 'a', 'b', 'c'}, {'b', 'c'}), {'', 'a', ''})\n    lu.assertEquals(candidate({'', 'a', 'b', 'c'}, {'', 'b', 'c'}), {'', 'a'})\n    lu.assertEquals(candidate({'', 'a'}, {'', 'b', 'a'}), {'', 'a'})\n    lu.assertEquals(candidate({'', 'a'}, {}), {'', 'a'})\n    lu.assertEquals(candidate({'', 'a', 'b', 'c'}, {'c', 'd'}), {'', 'a', 'b', 'c'})\n    lu.assertEquals(candidate({'', 'a', 'b', 'c'}, {'b', 'd'}), {'', 'a', 'b', 'c'})\n    lu.assertEquals(candidate({'', 'a'}, {'', 'b', 'c', 'd'}), {'', 'a'})\n    lu.assertEquals(candidate({'', 'a', 'b', 'c'}, {'b', 'c'}), {'', 'a', ''})\n    lu.assertEquals(candidate({'', 'a'}, {''}), {'', 'a'})\n    lu.assertEquals(candidate({'', 'a'}, {'', 'b'}), {'', 'a'})\n    lu.assertEquals(candidate({'', 'a'}, {'', 'a', 'b', 'c'}), {'', 'a'})\n    lu.assertEquals(candidate({'', 'a', 'b', 'c'}, {'d'}), {'', 'a', 'b', 'c'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_23945_pre_text", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # s = \"```{}```\"\n-- # if lang is not None:\n-- #     s = s.format(lang+\"\\n{}\")\n-- # return s.format(string.rstrip().strip(\"\\n\").replace(\"\\t\", \"\"))\n-- \n--     Encapsulate a string inside a Markdown <pre> container\nlocal function pre_text(string, lang)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_23945_pre_text.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = pre_text\n    lu.assertEquals(candidate('test\\ntest', 'cpp'), '```cpp\\ntest\\ntest```')\n    lu.assertEquals(candidate('test', 'java'), '```java\\ntest```')\n    lu.assertEquals(candidate('test', 'cpp'), '```cpp\\ntest```')\n    lu.assertEquals(candidate('test'), '```test```')\n    lu.assertEquals(candidate('test\\ntest'), '```test\\ntest```')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_239754__extract_language", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return locale_string.split(\"_\")[0].lower()\n-- \n-- Extracts language from locale string.\n-- :param locale_string: Something like language_COUNTRY.encoding\n-- :return: language\nlocal function _extract_language(locale_string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_239754__extract_language.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _extract_language\n    lu.assertEquals(candidate('en_GB'), 'en')\n    lu.assertEquals(candidate('sr_RS@latin'), 'sr')\n    lu.assertEquals(candidate('sr_RS@cyrillic'), 'sr')\n    lu.assertEquals(candidate('sr_ME'), 'sr')\n    lu.assertEquals(candidate('fr_FR'), 'fr')\n    lu.assertEquals(candidate('sr_RS'), 'sr')\n    lu.assertEquals(candidate('fr_CA'), 'fr')\n    lu.assertEquals(candidate('en_US'), 'en')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_23979_hash_to_dir", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return hash[:2]+'/'+hash[2:]\n-- \n-- Transforms a given hash to a relative path and filename\n-- ex: '002badb952000339cdcf1b61a3205b221766bf49' ->\n--     '00/2badb952000339cdcf1b61a3205b221766bf49'\n-- :param hash: the hash to split\n-- :rtype: string\nlocal function hash_to_dir(hash)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_23979_hash_to_dir.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = hash_to_dir\n    lu.assertEquals(candidate('00'), '00/')\n    lu.assertEquals(candidate('006061c77988c862085fc6c29c5357e3810822d3'), '00/6061c77988c862085fc6c29c5357e3810822d3')\n    lu.assertEquals(candidate('007a90c0e00899f0596419a627d76f4256705188'), '00/7a90c0e00899f0596419a627d76f4256705188')\n    lu.assertEquals(candidate('002badb952000339cdcf1b61a3205b221766bf49'), '00/2badb952000339cdcf1b61a3205b221766bf49')\n    lu.assertEquals(candidate('0'), '0/')\n    lu.assertEquals(candidate('002badb952000339cdcf1b61a3205b221766bf49'), '00/2badb952000339cdcf1b61a3205b221766bf49')\n    lu.assertEquals(candidate('001426260e00059b5e5a401954d6071f19c74031'), '00/1426260e00059b5e5a401954d6071f19c74031')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_239937_convert2relative", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # x, y, w, h = bbox\n-- # _height = darknet_height\n-- # _width = darknet_width\n-- # return x/_width, y/_height, w/_width, h/_height\n-- \n--     YOLO format use relative coordinates for annotation\nlocal function convert2relative(bbox, darknet_height, darknet_width)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_239937_convert2relative.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = convert2relative\n    lu.assertEquals(candidate({0, 0, 10, 10}, 10, 10), {0, 0, 1, 1})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_240045_caterpillar_sub_sequence", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # sub_sequence = []\n-- # for index, _ in enumerate(frame):\n-- #     current = frame.copy()\n-- #     current[index] = new\n-- #     for cell in range(cat_length):\n-- #         tail_index = index - cell\n-- #         if tail_index >= 0:\n-- #             current[tail_index] = new\n-- #     sub_sequence.append(current)\n-- #     if all([item == new for item in current[index:]]):\n-- #         return sub_sequence\n-- \n-- Generate the steps for a given frame.\nlocal function caterpillar_sub_sequence(frame, new, cat_length)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_240045_caterpillar_sub_sequence.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = caterpillar_sub_sequence\n    lu.assertEquals(candidate({0, 0, 0, 0, 0, 0, 0}, 0, 5), {{0, 0, 0, 0, 0, 0, 0}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_240081_compose_username", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if nick:\n-- #     return '{0}%{1}'.format(nick, provider)\n-- # else:\n-- #     if email:\n-- #         return '{0}%{1}'.format(email, provider)\n-- # return None\n-- \n-- @brief Compose the username\n-- The username policy via openid is as follow:\n-- * Try to use the nick name\n-- * If not available fall back to email\n-- * Append the the openid provider as domain. Delimiter is % in order to \n--   diferentiate from @ (if email is used)\n-- @param nick The nick name or None\n-- @param email The email or None\n-- @param provider the Authentication provider\n-- @return Non in case no user name was derived, otherwise the username\nlocal function compose_username(nick, email, provider)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_240081_compose_username.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = compose_username\n    lu.assertEquals(candidate(), '<EMAIL>%provider')\n    lu.assertEquals(candidate(None, '<EMAIL>', 'google'), '<EMAIL>%google')\n    lu.assertEquals(candidate('Peter', None, 'yahoo'), 'Peter%yahoo')\n    lu.assertEquals(candidate(), 'Nick%provider')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_241238_concatixnames", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # ixnamesource = '_'.join([ixname, source_suffix])\n-- # ixnametarget = '_'.join([ixname, target_suffix])\n-- # ixnamepairs = [ixnamesource, ixnametarget]\n-- # return ixnamesource, ixnametarget, ixnamepairs\n-- \n--     Args:\n-- ixname (str): 'ix'\n-- source_suffix (str): 'left'\n-- target_suffix (str): 'right'\n--     Returns:\n-- str, str, list(): 'ix_source', 'ix_target', ['ix_source', 'ix_target']\nlocal function concatixnames(ixname, source_suffix, target_suffix)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_241238_concatixnames.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = concatixnames\n    lu.assertEquals(candidate('ix', 'source', 'target'), {'ix_source', 'ix_target', {'ix_source', 'ix_target'}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_24135_temp_commentary", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # temperature = int(current_temp)\n-- # temperature_level = {\n-- #     0: \"It's scorching hot right now. Stay inside and be cool!\",\n-- #     1: \"It's hot and sunny right now. Don't forget that sunscreen!\",\n-- #     2: \"It's nice and warm right now. Time to flex those flip-flops!\",\n-- #     3: \"It's nice and cool right now. Go play outside in this great weather!\",\n-- #     4: \"It's cold right now. Make sure you keep yourself warm!\",\n-- #     5: \"Brrrrrrr!!! Remember to wear your protective gear so you don't freeze!\",\n-- #     6: \"It's Freezing Cold. Staying inside and a cup of Hot chocolate would be nice!\"\n-- # }\n-- # if temperature >= 95:\n-- #     return temperature_level[0]\n-- # elif 80 <= temperature < 95:\n-- #     return temperature_level[1]\n-- # elif 69 <= temperature < 80:\n-- #     return temperature_level[2]\n-- # elif 59 <= temperature < 69:\n-- #     return temperature_level[3]\n-- # elif 40 <= temperature < 59:\n-- #     return temperature_level[4]\n-- # elif 25 <= temperature < 40:\n-- #     return temperature_level[5]\n-- # elif temperature < 25:\n-- #     return temperature_level[6]\n-- \n-- Gives temperature advice to the end user.\nlocal function temp_commentary(current_temp)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_24135_temp_commentary.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = temp_commentary\n    lu.assertEquals(candidate(5), \"It's Freezing Cold. Staying inside and a cup of Hot chocolate would be nice!\")\n    lu.assertEquals(candidate(55), \"It's cold right now. Make sure you keep yourself warm!\")\n    lu.assertEquals(candidate(40), \"It's cold right now. Make sure you keep yourself warm!\")\n    lu.assertEquals(candidate(85), \"It's hot and sunny right now. Don't forget that sunscreen!\")\n    lu.assertEquals(candidate(69), \"It's nice and warm right now. Time to flex those flip-flops!\")\n    lu.assertEquals(candidate(25), \"Brrrrrrr!!! Remember to wear your protective gear so you don't freeze!\")\n    lu.assertEquals(candidate(75), \"It's nice and warm right now. Time to flex those flip-flops!\")\n    lu.assertEquals(candidate(59), \"It's nice and cool right now. Go play outside in this great weather!\")\n    lu.assertEquals(candidate(95), \"It's scorching hot right now. Stay inside and be cool!\")\n    lu.assertEquals(candidate(100), \"It's scorching hot right now. Stay inside and be cool!\")\n    lu.assertEquals(candidate(-15), \"It's Freezing Cold. Staying inside and a cup of Hot chocolate would be nice!\")\n    lu.assertEquals(candidate(65), \"It's nice and cool right now. Go play outside in this great weather!\")\n    lu.assertEquals(candidate(6), \"It's Freezing Cold. Staying inside and a cup of Hot chocolate would be nice!\")\n    lu.assertEquals(candidate(80), \"It's hot and sunny right now. Don't forget that sunscreen!\")\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_241380_sum_digits", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # i = 0\n-- # def pow_count(n):\n-- #     return len(str(n))-1\n-- # def digit_count(n):\n-- #     return n // pow(10, pow_count(n))\n-- # while n > 1:\n-- #     i, n = i + digit_count(n), n - digit_count(n) * pow(10, pow_count(n))\n-- # return i\n-- \n-- Sum all the digits of n.\n-- >>> sum_digits(10) # 1 + 0 = 1\n-- 1\n-- >>> sum_digits(4224) # 4 + 2 + 2 + 4 = 12\n-- 12\n-- >>> sum_digits(1234567890)\n-- 45\n-- >>> x = sum_digits(123) # make sure that you are using return rather than print\n-- >>> x\n-- 6\nlocal function sum_digits(n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_241380_sum_digits.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sum_digits\n    lu.assertEquals(candidate(4224), 12)\n    lu.assertEquals(candidate(42), 6)\n    lu.assertEquals(candidate(123), 6)\n    lu.assertEquals(candidate(10), 1)\n    lu.assertEquals(candidate(1234567890), 45)\n    lu.assertEquals(candidate(1000), 1)\n    lu.assertEquals(candidate(10000), 1)\n    lu.assertEquals(candidate(10000000), 1)\n    lu.assertEquals(candidate(123456789), 45)\n    lu.assertEquals(candidate(0), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_241518_str_tags_to_list", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # tags = tags.split(\",\")\n-- # tags = [tag.strip() for tag in tags if tag.strip()]\n-- # return tags\n-- \n-- Convert string of comma separated tags to list, stripped of empty tags and whitespace.\nlocal function str_tags_to_list(tags)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_241518_str_tags_to_list.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = str_tags_to_list\n    lu.assertEquals(candidate('  hello ,   world   '), {'hello', 'world'})\n    lu.assertEquals(candidate('hello, world'), {'hello', 'world'})\n    lu.assertEquals(candidate(',hello,,, world'), {'hello', 'world'})\n    lu.assertEquals(candidate('hello'), {'hello'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_241903_datenum", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"datenum(\" + name + \",\" + format + \")\"\n-- \n-- Return a date category with format\nlocal function datenum(name, format)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_241903_datenum.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = datenum\n    lu.assertEquals(candidate('date', \"'%Y-%m-%d'\"), \"candidate(date,'%Y-%m-%d')\")\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_242285_str_width", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # from unicodedata import east_asian_width\n-- # return sum(1+(east_asian_width(c) in \"WF\") for c in unicode_text)\n-- \n-- calc string width, support cjk characters.\nlocal function str_width(unicode_text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_242285_str_width.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = str_width\n    lu.assertEquals(candidate('      '), 6)\n    lu.assertEquals(candidate('abc     '), 8)\n    lu.assertEquals(candidate('abc    '), 7)\n    lu.assertEquals(candidate('A\\u2000B'), 3)\n    lu.assertEquals(candidate('1234567890'), 10)\n    lu.assertEquals(candidate('abc  '), 5)\n    lu.assertEquals(candidate('hello\\nworld'), 11)\n    lu.assertEquals(candidate('       '), 7)\n    lu.assertEquals(candidate('abc '), 4)\n    lu.assertEquals(candidate('ABC'), 3)\n    lu.assertEquals(candidate('   '), 3)\n    lu.assertEquals(candidate('abcdef'), 6)\n    lu.assertEquals(candidate('A\u0301B'), 3)\n    lu.assertEquals(candidate('abc'), 3)\n    lu.assertEquals(candidate('  '), 2)\n    lu.assertEquals(candidate('\uff41\uff42\uff43'), 6)\n    lu.assertEquals(candidate('hello world'), 11)\n    lu.assertEquals(candidate('\uff21\uff22\uff23'), 6)\n    lu.assertEquals(candidate('abc       '), 10)\n    lu.assertEquals(candidate('abcdefg'), 7)\n    lu.assertEquals(candidate(' '), 1)\n    lu.assertEquals(candidate('abc   '), 6)\n    lu.assertEquals(candidate('hello\\tworld'), 11)\n    lu.assertEquals(candidate('\uff71\uff71\uff71\uff71'), 4)\n    lu.assertEquals(candidate('abc      '), 9)\n    lu.assertEquals(candidate('abcde'), 5)\n    lu.assertEquals(candidate('\uff71\uff71'), 2)\n    lu.assertEquals(candidate('\uff71\uff71\uff71'), 3)\n    lu.assertEquals(candidate('        '), 8)\n    lu.assertEquals(candidate('     '), 5)\n    lu.assertEquals(candidate('    '), 4)\n    lu.assertEquals(candidate('abc        '), 11)\n    lu.assertEquals(candidate('A'), 1)\n    lu.assertEquals(candidate(''), 0)\n    lu.assertEquals(candidate('\uff71'), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_242482_get_interval_unit", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"seconds\" if interval > 1 else \"seconds\"\n-- \n-- Get interval unit.\n-- :param interval:\n-- :return:\nlocal function get_interval_unit(interval)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_242482_get_interval_unit.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_interval_unit\n    lu.assertEquals(candidate(1000), 'seconds')\n    lu.assertEquals(candidate(-1.5), 'seconds')\n    lu.assertEquals(candidate(10), 'seconds')\n    lu.assertEquals(candidate(59), 'seconds')\n    lu.assertEquals(candidate(0.5), 'seconds')\n    lu.assertEquals(candidate(3), 'seconds')\n    lu.assertEquals(candidate(1), 'seconds')\n    lu.assertEquals(candidate(-0.5), 'seconds')\n    lu.assertEquals(candidate(61), 'seconds')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_242504_LJ", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return (4*epsilon*(pow(sigma/v, 12) - pow(sigma/v, 6)))\n-- \n--  Lennard-Jones potential \nlocal function LJ(v, epsilon, sigma)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_242504_LJ.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = LJ\n    lu.assertEquals(candidate(20, 0, 2), 0)\n    lu.assertEquals(candidate(20, 0, 1), 0)\n    lu.assertEquals(candidate(20, 0, 0), 0)\n    lu.assertEquals(candidate(1, 1, 1), 0)\n    lu.assertEquals(candidate(20, 1, 0), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_243078_check_origin", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if origin == ('X', 'X'):\n-- #     return 'X'\n-- # if origin == ('Y', 'Y'):\n-- #     return 'Y'\n-- # elif '' in origin or 'XY' in origin:\n-- #     return None\n-- # else:\n-- #     return False\n-- \n--  Sorting function for origin \nlocal function check_origin(origin)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_243078_check_origin.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = check_origin\n    lu.assertEquals(candidate({'XY', 'Y'}), None)\n    lu.assertEquals(candidate({'X', 'X'}), 'X')\n    lu.assertEquals(candidate({'X', 'X'}), 'X')\n    lu.assertEquals(candidate({'XY', ''}), None)\n    lu.assertEquals(candidate({'Y', 'XY'}), None)\n    lu.assertEquals(candidate({'', 'XX'}), None)\n    lu.assertEquals(candidate({'XY', 'XY'}), None)\n    lu.assertEquals(candidate({'XX', ''}), None)\n    lu.assertEquals(candidate({'Y', 'Y'}), 'Y')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_243138_prepend_slash", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if url.startswith(\"/\"):\n-- #     if prepend:\n-- #         return url\n-- #     else:\n-- #         return url[1:]\n-- # else:\n-- #     if prepend:\n-- #         return \"/\" + url\n-- #     else:\n-- #         return url\n-- \n-- Prepend a slash to a URL fragment, checking if it already has one.\nlocal function prepend_slash(url, prepend)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_243138_prepend_slash.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = prepend_slash\n    lu.assertEquals(candidate('https://example.com', false), 'https://example.com')\n    lu.assertEquals(candidate('https://example.com', true), '/https://example.com')\n    lu.assertEquals(candidate('/https://example.com/', true), '/https://example.com/')\n    lu.assertEquals(candidate('https://example.com/', true), '/https://example.com/')\n    lu.assertEquals(candidate('https://example.com/', false), 'https://example.com/')\n    lu.assertEquals(candidate('/https://example.com', true), '/https://example.com')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_243650__nf", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return None if s == \"None\" else s\n-- \n-- None Filter\nlocal function _nf(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_243650__nf.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _nf\n    lu.assertEquals(candidate('foo'), 'foo')\n    lu.assertEquals(candidate('Hello World!'), 'Hello World!')\n    lu.assertEquals(candidate('test'), 'test')\n    lu.assertEquals(candidate('None'), None)\n    lu.assertEquals(candidate(None), None)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_24406_rearange_base_link_list", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # value = table[base_link_index]\n-- # del table[base_link_index]\n-- # table.insert(0, value)\n-- # return table\n-- \n-- Rarange base link to beginning of table\nlocal function rearange_base_link_list(table, base_link_index)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_24406_rearange_base_link_list.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = rearange_base_link_list\n    lu.assertEquals(candidate({1}, 0), {1})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_244262_get_shard_range", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # remainder = dataset_size % world_size\n-- # shard_len = dataset_size // world_size\n-- # if remainder == 0:\n-- #     shard_offset = rank * shard_len\n-- # else:\n-- #     # take one extra when dataset_size is not evenly divided by world_size\n-- #     shard_len += 1\n-- #     shard_offset = rank * shard_len - max(0, rank + 1 - remainder)\n-- # shard_end = shard_offset + shard_len - 1\n-- # return (shard_offset, shard_end)\n-- \n-- In case dataset_size is not evenly divided by world_size, we need to pad\n-- one extra example in each shard\n-- shard_len = dataset_size // world_size + 1\n-- Case 1 rank < remainder: each shard start position is rank * shard_len\n-- Case 2 rank >= remainder: without padding, each shard start position is\n-- rank * (shard_len - 1) + remainder = rank * shard_len - (rank - remainder)\n-- But to make sure all shard have same size, we need to pad one extra example\n-- when rank >= remainder, so start_position = start_position - 1\n-- For example, dataset_size = 21, world_size = 8\n-- rank 0 to 4: [0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14]\n-- rank 5 to 7: [14, 15, 16], [16, 17, 18], [18, 19, 20]\nlocal function get_shard_range(dataset_size, rank, world_size)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_244262_get_shard_range.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_shard_range\n    lu.assertEquals(candidate(10, 0, 3), {0, 3})\n    lu.assertEquals(candidate(11, 1, 3), {4, 7})\n    lu.assertEquals(candidate(3, 0, 2), {0, 1})\n    lu.assertEquals(candidate(3, 1, 2), {1, 2})\n    lu.assertEquals(candidate(11, 0, 4), {0, 2})\n    lu.assertEquals(candidate(11, 2, 3), {7, 10})\n    lu.assertEquals(candidate(4, 1, 2), {2, 3})\n    lu.assertEquals(candidate(10, 0, 1), {0, 9})\n    lu.assertEquals(candidate(7, 0, 3), {0, 2})\n    lu.assertEquals(candidate(10, 0, 4), {0, 2})\n    lu.assertEquals(candidate(3, 0, 1), {0, 2})\n    lu.assertEquals(candidate(4, 0, 1), {0, 3})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_244748_get_3x3_translation", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # a = [x, 0, 0]\n-- # b = [0, y, 0]\n-- # c = [0, 0, z]\n-- # return [a, b, c]\n-- \n-- return a matrix 3x3 for translation\nlocal function get_3x3_translation(x, y, z)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_244748_get_3x3_translation.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_3x3_translation\n    lu.assertEquals(candidate(2, 3, 4), {{2, 0, 0}, {0, 3, 0}, {0, 0, 4}})\n    lu.assertEquals(candidate(1, 2, 3), {{1, 0, 0}, {0, 2, 0}, {0, 0, 3}})\n    lu.assertEquals(candidate(1, 1, 1), {{1, 0, 0}, {0, 1, 0}, {0, 0, 1}})\n    lu.assertEquals(candidate(1, 2, 3), candidate(1, 2, 3))\n    lu.assertEquals(candidate(3, 1, 2), {{3, 0, 0}, {0, 1, 0}, {0, 0, 2}})\n    lu.assertEquals(candidate(1, 2, 3), {{1, 0, 0}, {0, 2, 0}, {0, 0, 3}})\n    lu.assertEquals(candidate(0, 0, 0), {{0, 0, 0}, {0, 0, 0}, {0, 0, 0}})\n    lu.assertEquals(candidate(4, 4, 4), {{4, 0, 0}, {0, 4, 0}, {0, 0, 4}})\n    lu.assertEquals(candidate(-3, -2, -1), {{-3, 0, 0}, {0, -2, 0}, {0, 0, -1}})\n    lu.assertEquals(candidate(-3, -6, -9), {{-3, 0, 0}, {0, -6, 0}, {0, 0, -9}})\n    lu.assertEquals(candidate(1, 2, 3), {{1, 0, 0}, {0, 2, 0}, {0, 0, 3}})\n    lu.assertEquals(candidate(0, 0, 0), {{0, 0, 0}, {0, 0, 0}, {0, 0, 0}})\n    lu.assertEquals(candidate(3, 2, 1), {{3, 0, 0}, {0, 2, 0}, {0, 0, 1}})\n    lu.assertEquals(candidate(-1, 3, 1), {{-1, 0, 0}, {0, 3, 0}, {0, 0, 1}})\n    lu.assertEquals(candidate(5, 10, 15), {{5, 0, 0}, {0, 10, 0}, {0, 0, 15}})\n    lu.assertEquals(candidate(3, -1, 2), {{3, 0, 0}, {0, -1, 0}, {0, 0, 2}})\n    lu.assertEquals(candidate(-2, -3, -4), {{-2, 0, 0}, {0, -3, 0}, {0, 0, -4}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_245061_decode_bert", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # return \"\".join(text.split()).replace(\"##\", \" \").strip()\n-- # return text.replace(\" ##\", \"\").strip()\n-- \n-- Decodes text that uses https://github.com/google/sentencepiece encoding.\n-- Assumes that pieces are separated by a space\nlocal function decode_bert(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_245061_decode_bert.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = decode_bert\n    lu.assertEquals(candidate('A long time ago in a galaxy far, far away'), 'A long time ago in a galaxy far, far away')\n    lu.assertEquals(candidate('I like cats and ## dogs '), 'I like cats and dogs')\n    lu.assertEquals(candidate('I like to go to the mall'), 'I like to go to the mall')\n    lu.assertEquals(candidate('long time ago in a galaxy far, far away'), 'long time ago in a galaxy far, far away')\n    lu.assertEquals(candidate('this is a test'), 'this is a test')\n    lu.assertEquals(candidate('I like to go to the m all'), 'I like to go to the m all')\n    lu.assertEquals(candidate('I like cats and dogs '), 'I like cats and dogs')\n    lu.assertEquals(candidate('I like cats and ## dogs'), 'I like cats and dogs')\n    lu.assertEquals(candidate('long time ago in a galaxy far ##, far away'), 'long time ago in a galaxy far, far away')\n    lu.assertEquals(candidate('I like cats and dogs  '), 'I like cats and dogs')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_245144_get_dist_sq", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return (point_a[0] - point_b[0])**2 + (point_a[1] - point_b[1])**2\n-- \n-- returns the distance squared between two points. Faster than the true euclidean dist\nlocal function get_dist_sq(point_a, point_b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_245144_get_dist_sq.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_dist_sq\n    lu.assertEquals(candidate({0, 0}, {0, 0}), 0)\n    lu.assertEquals(candidate({0, 0}, {3, 4}), 25)\n    lu.assertEquals(candidate({1, 1}, {1, 1}), 0)\n    lu.assertEquals(candidate(), 1)\n    lu.assertEquals(candidate({0, 0}, {0, 0}), 0)\n    lu.assertEquals(candidate({-3, -4}, {-3, -4}), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_246231_parse_env", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # env = {}\n-- # good_stuff = ['USER', 'PASS', '_DB']\n-- # for item in Env:\n-- #     (keyval, value) = item.split('=')\n-- #     for target in good_stuff:\n-- #         if keyval.find(target) != -1:\n-- #             env[keyval] = value\n-- # return env\n-- \n-- Convert list of strings into dict object.\n-- Docker Inspect ENV is a list of strings. Env strings are\n-- in the form of KEY=VALUE. Split strings into key=value pairs\n-- and return dict object. Only return keys with \"good_stuff\".\nlocal function parse_env(Env)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_246231_parse_env.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_env\n    lu.assertEquals(candidate({'USER=db_user', 'PASS=<PASSWORD>', 'EXTRA=1'}), {['USER'] = 'db_user', ['PASS'] = '<PASSWORD>'})\n    lu.assertEquals(candidate({}), {})\n    lu.assertEquals(candidate({'USER=db_user', 'PASS=<PASSWORD>', '_DB=db_name', 'EXTRA=1'}), {['USER'] = 'db_user', ['PASS'] = '<PASSWORD>', ['_DB'] = 'db_name'})\n    lu.assertEquals(candidate({'_DB=foo', 'USER=bar'}), {['_DB'] = 'foo', ['USER'] = 'bar'})\n    lu.assertEquals(candidate({'USER=foo', '_DB=bar'}), {['USER'] = 'foo', ['_DB'] = 'bar'})\n    lu.assertEquals(candidate({'USER=foo', 'PASS=<PASSWORD>', 'BAD_THING=bar'}), {['USER'] = 'foo', ['PASS'] = '<PASSWORD>'})\n    lu.assertEquals(candidate({'USER=foo', 'USER=bar'}), {['USER'] = 'bar'})\n    lu.assertEquals(candidate({'USER=db_user', 'PASS=<PASSWORD>', '_DB=db_name'}), {['USER'] = 'db_user', ['PASS'] = '<PASSWORD>', ['_DB'] = 'db_name'})\n    lu.assertEquals(candidate({'USER=foo', 'PASS=<PASSWORD>', '_DB=bar'}), {['USER'] = 'foo', ['PASS'] = '<PASSWORD>', ['_DB'] = 'bar'})\n    lu.assertEquals(candidate({'USER=foo', 'PASS=bar', '_DB=baz', 'USER=foo', 'PASS=bar', '_DB=baz'}), {['USER'] = 'foo', ['PASS'] = 'bar', ['_DB'] = 'baz'})\n    lu.assertEquals(candidate({'USER=foo', 'PASS=<PASSWORD>'}), {['USER'] = 'foo', ['PASS'] = '<PASSWORD>'})\n    lu.assertEquals(candidate({'USER=foo', 'PASS=bar', '_DB=baz'}), {['USER'] = 'foo', ['PASS'] = 'bar', ['_DB'] = 'baz'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_246717_version_get", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return [int(i) for i in v1.split('.') if i.isdigit()] > [int(i) for i in v2.split('.') if i.isdigit()]\n-- \n-- Check if version v1 is great or equal then version v2.\nlocal function version_get(v1, v2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_246717_version_get.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = version_get\n    lu.assertEquals(candidate('1.2.3.3', '1.2.3.4'), false)\n    lu.assertEquals(candidate('1.2.3a', '1.2.3'), false)\n    lu.assertEquals(candidate('1.1.0', '1.1.1'), false)\n    lu.assertEquals(candidate('1.1.3', '1.2'), false)\n    lu.assertEquals(candidate('1.2.3a', '1.2.3b'), false)\n    lu.assertEquals(candidate('1.2', '1.2.4'), false)\n    lu.assertEquals(candidate('1', '1.2'), false)\n    lu.assertEquals(candidate('1', '1.1.0'), false)\n    lu.assertEquals(candidate('1.2.4', '1.2.3'), true)\n    lu.assertEquals(candidate('1.1.1', '1.1.2'), false)\n    lu.assertEquals(candidate('1.1.1.0', '1.1.1.1'), false)\n    lu.assertEquals(candidate('1.2.3', '1.3.0'), false)\n    lu.assertEquals(candidate('1.2.3', '1.2.4'), false)\n    lu.assertEquals(candidate('1.1', '1.2'), false)\n    lu.assertEquals(candidate('1.2.2', '1.2'), true)\n    lu.assertEquals(candidate('1', '1.1.1'), false)\n    lu.assertEquals(candidate('1.1.1', '1.2.1'), false)\n    lu.assertEquals(candidate('1', '1.1'), false)\n    lu.assertEquals(candidate('1.1.0', '1.2.0'), false)\n    lu.assertEquals(candidate('1.2.3', '1.2'), true)\n    lu.assertEquals(candidate('1', '2.1.0'), false)\n    lu.assertEquals(candidate('1.1.1', '1.1.0'), true)\n    lu.assertEquals(candidate('1.2.3', '1.2.4'), false)\n    lu.assertEquals(candidate('1.3.0', '1.2.4'), true)\n    lu.assertEquals(candidate('1.2.3b', '1.2.3a'), false)\n    lu.assertEquals(candidate('1', '1.0'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_247246_is_prefix", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # pre_path = pre_path.strip('.')\n-- # path = path.strip('.')\n-- # return not pre_path or path.startswith(pre_path + '.')\n-- \n-- Return True if pre_path is a path-prefix of path.\nlocal function is_prefix(pre_path, path)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_247246_is_prefix.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_prefix\n    lu.assertEquals(candidate('', ''), true)\n    lu.assertEquals(candidate('', 'foo'), true)\n    lu.assertEquals(candidate('foo', ''), false)\n    lu.assertEquals(candidate('a.b.c', 'a.b.c.d.e'), true)\n    lu.assertEquals(candidate('foo.bar', 'foo'), false)\n    lu.assertEquals(candidate('', 'foo.bar'), true)\n    lu.assertEquals(candidate('a.b.c.d', 'a.b.c.d.e'), true)\n    lu.assertEquals(candidate('', 'a.b.c.d.e'), true)\n    lu.assertEquals(candidate('a', 'a.b.c.d.e'), true)\n    lu.assertEquals(candidate('foo.bar', 'baz.foo.bar'), false)\n    lu.assertEquals(candidate('foo.bar', ''), false)\n    lu.assertEquals(candidate('foo.bar', 'foo.bar.baz'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_247808_get_next_coin", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if coin == 1:\n-- #     return 5\n-- # elif coin == 5:\n-- #     return 10\n-- # elif coin == 10:\n-- #     return 25\n-- \n-- Return the next coin. \n-- >>> get_next_coin(1)\n-- 5\n-- >>> get_next_coin(5)\n-- 10\n-- >>> get_next_coin(10)\n-- 25\n-- >>> get_next_coin(2) # Other values return None\nlocal function get_next_coin(coin)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_247808_get_next_coin.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_next_coin\n    lu.assertEquals(candidate(17), None)\n    lu.assertEquals(candidate(3), None)\n    lu.assertEquals(candidate(10), 25)\n    lu.assertEquals(candidate(1), 5)\n    lu.assertEquals(candidate(117), None)\n    lu.assertEquals(candidate(2), None)\n    lu.assertEquals(candidate(5), 10)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_247936__full_license", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # license_name = image_info['license'].upper()\n-- # license_version = image_info['license_version'].upper()\n-- # return '{prefix}{name} {version}'.format(\n-- #     prefix='' if license_name == 'CC0' else 'CC ',\n-- #     name=license_name,\n-- #     version=license_version\n-- # )\n-- \n-- Get the full license from the image info\n-- :param image_info: the information about a particular image\n-- :return: the full license text for the image\nlocal function _full_license(image_info)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_247936__full_license.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _full_license\n    lu.assertEquals(candidate({['license'] = 'CC0', ['license_version'] = '1.0'}), 'CC0 1.0')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_24798_p_a2", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # p = 1. / ninputs\n-- # return npsyns * p * ((1 - p) ** (npsyns - 1))\n-- \n-- Probability of selecting one input given ninputs and npsyns attempts. This\n-- uses a binomial distribution.\n-- @param npsyns: The number of proximal synapses.\n-- @param ninputs: The number of inputs.\n-- @return: The computed probability.\nlocal function p_a2(npsyns, ninputs)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_24798_p_a2.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = p_a2\n    lu.assertEquals(candidate(1, 1), 1.0)\n    lu.assertEquals(candidate(1, 4), 0.25)\n    lu.assertEquals(candidate(1, 2), 0.5)\n    lu.assertEquals(candidate(1, 1), 1.0)\n    lu.assertEquals(candidate(1, 3), 0.3333333333333333)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_248015_irshift", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # a >>= b\n-- # return a\n-- \n-- Same as a >>= b.\nlocal function irshift(a, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_248015_irshift.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = irshift\n    lu.assertEquals(candidate(1, 3), 0)\n    lu.assertEquals(candidate(-1, 8), -1)\n    lu.assertEquals(candidate(2, 8), 0)\n    lu.assertEquals(candidate(0, 2), 0)\n    lu.assertEquals(candidate(10, 5), 0)\n    lu.assertEquals(candidate(3, 3), 0)\n    lu.assertEquals(candidate(-1, 1), -1)\n    lu.assertEquals(candidate(-1, 9), -1)\n    lu.assertEquals(candidate(2, 0), 2)\n    lu.assertEquals(candidate(10, 7), 0)\n    lu.assertEquals(candidate(0, 1), 0)\n    lu.assertEquals(candidate(1, 5), 0)\n    lu.assertEquals(candidate(2, 3), 0)\n    lu.assertEquals(candidate(-1, 23), -1)\n    lu.assertEquals(candidate(-2, 1), -1)\n    lu.assertEquals(candidate(-1, 24), -1)\n    lu.assertEquals(candidate(2, 4), 0)\n    lu.assertEquals(candidate(-1, 17), -1)\n    lu.assertEquals(candidate(-1, 3), -1)\n    lu.assertEquals(candidate(-1, 14), -1)\n    lu.assertEquals(candidate(-1, 18), -1)\n    lu.assertEquals(candidate(3, 2), 0)\n    lu.assertEquals(candidate(-1, 22), -1)\n    lu.assertEquals(candidate(-1, 21), -1)\n    lu.assertEquals(candidate(1, 1), 0)\n    lu.assertEquals(candidate(-1, 4), -1)\n    lu.assertEquals(candidate(1, 6), 0)\n    lu.assertEquals(candidate(-1, 25), -1)\n    lu.assertEquals(candidate(1, 2), 0)\n    lu.assertEquals(candidate(-1, 10), -1)\n    lu.assertEquals(candidate(-1, 16), -1)\n    lu.assertEquals(candidate(0, 10), 0)\n    lu.assertEquals(candidate(1, 0), 1)\n    lu.assertEquals(candidate(1, 8), 0)\n    lu.assertEquals(candidate(-1, 5), -1)\n    lu.assertEquals(candidate(-1, 6), -1)\n    lu.assertEquals(candidate(10, 4), 0)\n    lu.assertEquals(candidate(2, 6), 0)\n    lu.assertEquals(candidate(2, 7), 0)\n    lu.assertEquals(candidate(1, 4), 0)\n    lu.assertEquals(candidate(10, 6), 0)\n    lu.assertEquals(candidate(-1, 2), -1)\n    lu.assertEquals(candidate(10, 3), 1)\n    lu.assertEquals(candidate(-1, 12), -1)\n    lu.assertEquals(candidate(-1, 15), -1)\n    lu.assertEquals(candidate(10, 2), 2)\n    lu.assertEquals(candidate(-1, 7), -1)\n    lu.assertEquals(candidate(-1, 20), -1)\n    lu.assertEquals(candidate(1, 7), 0)\n    lu.assertEquals(candidate(-1, 13), -1)\n    lu.assertEquals(candidate(10, 0), 10)\n    lu.assertEquals(candidate(2, 5), 0)\n    lu.assertEquals(candidate(0, 0), 0)\n    lu.assertEquals(candidate(10, 1), 5)\n    lu.assertEquals(candidate(2, 1), 1)\n    lu.assertEquals(candidate(-1, 11), -1)\n    lu.assertEquals(candidate(-1, 19), -1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_248555_is_text_all_capital", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return text == text.upper()\n-- \n-- Returns true of the entire text is written with capitals. False otherwise.\nlocal function is_text_all_capital(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_248555_is_text_all_capital.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_text_all_capital\n    lu.assertEquals(candidate('abc'), false)\n    lu.assertEquals(candidate(''), true)\n    lu.assertEquals(candidate('ABC'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_249778_dist_modulus_to_distance", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return 10.**(dm / 5. + 1.)\n-- \n--  Convert distance modulus -> distance (pc)\n-- : dm : distance modulus\nlocal function dist_modulus_to_distance(dm)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_249778_dist_modulus_to_distance.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = dist_modulus_to_distance\n    lu.assertEquals(candidate(-10), 0.1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_25003_stringquote", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if \"'\" in text:\n-- #     if '\"' in text:\n-- #         return '\"' + text.replace('\"', '\"\"') + '\"'\n-- #     else:\n-- #         return '\"' + text + '\"'\n-- # else:\n-- #     return \"'\" + text + \"'\"\n-- \n-- escapes quotes as neccessary and returns a string representing\n-- the text\nlocal function stringquote(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_25003_stringquote.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = stringquote\n    lu.assertEquals(candidate('\"one\"'), '\\'\"one\"\\'')\n    lu.assertEquals(candidate(\"ab'c\"), '\"ab\\'c\"')\n    lu.assertEquals(candidate(\"'a\\\\nb\"), '\"\\'a\\\\nb\"')\n    lu.assertEquals(candidate(\"'\"), '\"\\'\"')\n    lu.assertEquals(candidate(\"a'b\"), '\"a\\'b\"')\n    lu.assertEquals(candidate(\"'one'\"), '\"\\'one\\'\"')\n    lu.assertEquals(candidate('ab\"c\\''), '\"ab\"\"c\\'\"')\n    lu.assertEquals(candidate(\"a\\\\nb'c\"), '\"a\\\\nb\\'c\"')\n    lu.assertEquals(candidate('one'), \"'one'\")\n    lu.assertEquals(candidate(\"a\\\\\\\\'b\"), '\"a\\\\\\\\\\'b\"')\n    lu.assertEquals(candidate(''), \"''\")\n    lu.assertEquals(candidate('abc'), \"'abc'\")\n    lu.assertEquals(candidate('one'), \"'one'\")\n    lu.assertEquals(candidate('ab\"c\\'d'), '\"ab\"\"c\\'d\"')\n    lu.assertEquals(candidate(\"one' \"), '\"one\\' \"')\n    lu.assertEquals(candidate('a'), \"'a'\")\n    lu.assertEquals(candidate(\"a\\\\'b\"), '\"a\\\\\\'b\"')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_250429_BitGet", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return (n >> (N-1-pos) & 1)\n-- \n-- Gets bit value at position pos from the left of the length-N bit-representation of n\nlocal function BitGet(n, N, pos)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_250429_BitGet.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = BitGet\n    lu.assertEquals(candidate(0, 2, 0), 0)\n    lu.assertEquals(candidate(170, 8, 7), 0)\n    lu.assertEquals(candidate(0, 1, 0), 0)\n    lu.assertEquals(candidate(7, 2, 0), 1)\n    lu.assertEquals(candidate(2, 16, 1), 0)\n    lu.assertEquals(candidate(31, 8, 1), 0)\n    lu.assertEquals(candidate(170, 8, 3), 0)\n    lu.assertEquals(candidate(7, 5, 2), 1)\n    lu.assertEquals(candidate(170, 8, 6), 1)\n    lu.assertEquals(candidate(170, 8, 1), 0)\n    lu.assertEquals(candidate(4, 2, 1), 0)\n    lu.assertEquals(candidate(3, 2, 0), 1)\n    lu.assertEquals(candidate(31, 8, 2), 0)\n    lu.assertEquals(candidate(682, 8, 0), 1)\n    lu.assertEquals(candidate(15, 8, 6), 1)\n    lu.assertEquals(candidate(682, 8, 1), 0)\n    lu.assertEquals(candidate(2, 2, 1), 0)\n    lu.assertEquals(candidate(8, 8, 3), 0)\n    lu.assertEquals(candidate(2, 8, 1), 0)\n    lu.assertEquals(candidate(2147483648, 32, 0), 1)\n    lu.assertEquals(candidate(64, 16, 6), 0)\n    lu.assertEquals(candidate(15, 8, 5), 1)\n    lu.assertEquals(candidate(170, 8, 5), 0)\n    lu.assertEquals(candidate(8, 16, 3), 0)\n    lu.assertEquals(candidate(128, 16, 7), 0)\n    lu.assertEquals(candidate(682, 8, 2), 1)\n    lu.assertEquals(candidate(3, 2, 1), 1)\n    lu.assertEquals(candidate(0, 2, 1), 0)\n    lu.assertEquals(candidate(31, 8, 3), 1)\n    lu.assertEquals(candidate(15, 8, 7), 1)\n    lu.assertEquals(candidate(4, 2, 0), 0)\n    lu.assertEquals(candidate(170, 8, 4), 1)\n    lu.assertEquals(candidate(31, 8, 0), 0)\n    lu.assertEquals(candidate(7, 5, 0), 0)\n    lu.assertEquals(candidate(0, 3, 0), 0)\n    lu.assertEquals(candidate(1, 2, 0), 0)\n    lu.assertEquals(candidate(170, 8, 2), 1)\n    lu.assertEquals(candidate(4, 16, 2), 0)\n    lu.assertEquals(candidate(682, 8, 3), 0)\n    lu.assertEquals(candidate(15, 8, 4), 1)\n    lu.assertEquals(candidate(170, 8, 0), 1)\n    lu.assertEquals(candidate(32, 16, 5), 0)\n    lu.assertEquals(candidate(7, 2, 1), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_250588_excel_col_name2int", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # d = 0\n-- # for ch in s:\n-- #     d = d * 26 + (ord(ch) - 64)\n-- # return d\n-- \n-- >>> excel_col_name2int('A')\n-- 1\n-- >>> excel_col_name2int('AA')\n-- 27\n-- >>> excel_col_name2int('AB')\n-- 28\nlocal function excel_col_name2int(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_250588_excel_col_name2int.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = excel_col_name2int\n    lu.assertEquals(candidate('A'), 1)\n    lu.assertEquals(candidate('AA'), 27)\n    lu.assertEquals(candidate('B'), 2)\n    lu.assertEquals(candidate('J'), 10)\n    lu.assertEquals(candidate('H'), 8)\n    lu.assertEquals(candidate('Q'), 17)\n    lu.assertEquals(candidate('W'), 23)\n    lu.assertEquals(candidate('K'), 11)\n    lu.assertEquals(candidate('M'), 13)\n    lu.assertEquals(candidate('A'), 1)\n    lu.assertEquals(candidate('G'), 7)\n    lu.assertEquals(candidate('O'), 15)\n    lu.assertEquals(candidate('R'), 18)\n    lu.assertEquals(candidate('F'), 6)\n    lu.assertEquals(candidate('AA'), 27)\n    lu.assertEquals(candidate('T'), 20)\n    lu.assertEquals(candidate('X'), 24)\n    lu.assertEquals(candidate('I'), 9)\n    lu.assertEquals(candidate('C'), 3)\n    lu.assertEquals(candidate('S'), 19)\n    lu.assertEquals(candidate('V'), 22)\n    lu.assertEquals(candidate('E'), 5)\n    lu.assertEquals(candidate('P'), 16)\n    lu.assertEquals(candidate('N'), 14)\n    lu.assertEquals(candidate('L'), 12)\n    lu.assertEquals(candidate('D'), 4)\n    lu.assertEquals(candidate('U'), 21)\n    lu.assertEquals(candidate('AB'), 28)\n    lu.assertEquals(candidate('AB'), 28)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_251103_hamming_with_n", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # distance = 0\n-- # for c1, c2 in zip(s1, s2):\n-- #     if c1 != c2:\n-- #         if c1 == 'N' or c2 == 'N':\n-- #             continue\n-- #         else:\n-- #             distance += 1\n-- # return distance\n-- \n-- Hamming Distance without counting wildcard smbols.\n-- Args:\n--     s1: the first sequence for comparison.\n--     s2: the second sequence for comparison.\n-- Returns:\n--     the distance without accounting unrestricted sites.\nlocal function hamming_with_n(s1, s2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_251103_hamming_with_n.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = hamming_with_n\n    lu.assertEquals(candidate('AGT', 'CCC'), 3)\n    lu.assertEquals(candidate('AGT', 'AGC'), 1)\n    lu.assertEquals(candidate('AA', 'AAA'), 0)\n    lu.assertEquals(candidate('AAA', 'AA'), 0)\n    lu.assertEquals(candidate('AGT', 'CCT'), 2)\n    lu.assertEquals(candidate('AGT', 'AGT'), 0)\n    lu.assertEquals(candidate('AA', 'AA'), 0)\n    lu.assertEquals(candidate('AGT', 'AGG'), 1)\n    lu.assertEquals(candidate('A', 'A'), 0)\n    lu.assertEquals(candidate('AGT', 'GAT'), 2)\n    lu.assertEquals(candidate('', ''), 0)\n    lu.assertEquals(candidate('AGT', 'GTT'), 2)\n    lu.assertEquals(candidate('AAA', 'AAA'), 0)\n    lu.assertEquals(candidate('AGT', 'TGG'), 2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_251439_prefix_dict", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return {prefix_s + k: v for k, v in di_.items()}\n-- \n-- Add prefix_s to every key in dict\n-- :param di_:\n-- :param prefix_s:\n-- :return:\nlocal function prefix_dict(di_, prefix_s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_251439_prefix_dict.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = prefix_dict\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2}, 'some_prefix_'), {['some_prefix_a'] = 1, ['some_prefix_b'] = 2})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_251602_match_link", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # keys = links.keys()\n-- # for k in keys:\n-- #     if k.lower() in name.lower():\n-- #         return links[k]\n-- # return \"\"\n-- \n--  Diagnoses in app have substrings of how diagnosis named in desease list \nlocal function match_link(links, name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_251602_match_link.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = match_link\n    lu.assertEquals(candidate({['pain'] = 'Pain'}, 'pain in the joints'), 'Pain')\n    lu.assertEquals(candidate({['pain'] = 'Pain'}, 'pain in the hands and joints'), 'Pain')\n    lu.assertEquals(candidate({['pain'] = 'Pain'}, 'pain is located at the knee'), 'Pain')\n    lu.assertEquals(candidate({['pain'] = 'Pain'}, 'pain in the right shoulder'), 'Pain')\n    lu.assertEquals(candidate({['pain'] = 'Pain'}, 'pain in the right hip'), 'Pain')\n    lu.assertEquals(candidate({['pain'] = 'Pain'}, 'pain in the left knee'), 'Pain')\n    lu.assertEquals(candidate({['pain'] = 'Pain'}, 'pain in the hips'), 'Pain')\n    lu.assertEquals(candidate({['pain'] = 'Pain'}, 'pain in the leg'), 'Pain')\n    lu.assertEquals(candidate({['pain'] = 'Pain'}, 'pain in the shins'), 'Pain')\n    lu.assertEquals(candidate({['pain'] = 'Pain'}, 'pain in the legs'), 'Pain')\n    lu.assertEquals(candidate({['pain'] = 'Pain'}, 'pain in the right knee'), 'Pain')\n    lu.assertEquals(candidate({['pain'] = 'Pain'}, 'pain in the left hip'), 'Pain')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_252412_is_up", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if \"UP\" in line:\n-- #     return True\n-- # if \"DOWN\" in line:\n-- #     return False\n-- # return None\n-- \n-- Returns True if the interface is up, False if it's down, and None if there\n-- is not enuough information present to determine whether it's up or down\nlocal function is_up(line)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_252412_is_up.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_up\n    lu.assertEquals(candidate('    eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500'), true)\n    lu.assertEquals(candidate('UP'), true)\n    lu.assertEquals(candidate('state UP group default qlen 1000'), true)\n    lu.assertEquals(candidate('     UP    inet6 2001:db8::1/64  scope global  '), true)\n    lu.assertEquals(candidate('    lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536'), true)\n    lu.assertEquals(candidate('    UP      inet 172.16.0.1/16  brd 172.16.0.255  mtu 1500'), true)\n    lu.assertEquals(candidate('state UP    group default qlen   1000'), true)\n    lu.assertEquals(candidate('state UNKNOWN group default qlen 1000'), None)\n    lu.assertEquals(candidate('state DOWN  group default qlen 1000'), false)\n    lu.assertEquals(candidate('    UP      inet 172.16.0.1/16  brd 172.16.0.255  mtu 1500'), true)\n    lu.assertEquals(candidate('eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500'), true)\n    lu.assertEquals(candidate('state DOWN group default qlen 1000'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_252535_last_blank", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not src:\n-- #     return False\n-- # ll = src.splitlines()[-1]\n-- # return (ll == '') or ll.isspace()\n-- \n-- Determine if the input source ends in a blank.\n-- A blank is either a newline or a line consisting of whitespace.\n-- Parameters\n-- ----------\n-- src : string\n--   A single or multiline string.\nlocal function last_blank(src)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_252535_last_blank.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = last_blank\n    lu.assertEquals(candidate('  \\\\n  '), false)\n    lu.assertEquals(candidate(' '), true)\n    lu.assertEquals(candidate(' \\n'), true)\n    lu.assertEquals(candidate('a\\\\nb'), false)\n    lu.assertEquals(candidate('   \\n   foo\\nbar\\n   '), true)\n    lu.assertEquals(candidate('a\\\\n b\\\\nc'), false)\n    lu.assertEquals(candidate(None), false)\n    lu.assertEquals(candidate(' \\n '), true)\n    lu.assertEquals(candidate('foo'), false)\n    lu.assertEquals(candidate('  \\\\n  \\\\n  '), false)\n    lu.assertEquals(candidate('\\n'), true)\n    lu.assertEquals(candidate('a\\\\n b'), false)\n    lu.assertEquals(candidate(''), false)\n    lu.assertEquals(candidate('a\\n'), false)\n    lu.assertEquals(candidate('   foo   '), false)\n    lu.assertEquals(candidate('   foo'), false)\n    lu.assertEquals(candidate('a'), false)\n    lu.assertEquals(candidate(' a\\n b'), false)\n    lu.assertEquals(candidate(' a\\n'), false)\n    lu.assertEquals(candidate(' a\\n b\\n '), true)\n    lu.assertEquals(candidate('foo\\nbar\\n   '), true)\n    lu.assertEquals(candidate('foo   '), false)\n    lu.assertEquals(candidate(''), false)\n    lu.assertEquals(candidate('\\n'), true)\n    lu.assertEquals(candidate('   \\n'), true)\n    lu.assertEquals(candidate('a'), false)\n    lu.assertEquals(candidate('a\\n\\n'), true)\n    lu.assertEquals(candidate('foo\\nbar'), false)\n    lu.assertEquals(candidate(''), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_253299_replace_slash", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # DIVISION_SLASH = '\\u2215'\n-- # return (name or '').replace('/', DIVISION_SLASH)\n-- \n-- Replaces slash with division slash symbol for CheckStyle Jenkins plugin\nlocal function replace_slash(name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_253299_replace_slash.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = replace_slash\n    lu.assertEquals(candidate('x'), 'x')\n    lu.assertEquals(candidate('/foo/bar/'), '\u2215foo\u2215bar\u2215')\n    lu.assertEquals(candidate('1'), '1')\n    lu.assertEquals(candidate('hello/world'), 'hello\u2215world')\n    lu.assertEquals(candidate('hello/world/12345'), 'hello\u2215world\u221512345')\n    lu.assertEquals(candidate('/'), '\u2215')\n    lu.assertEquals(candidate('123'), '123')\n    lu.assertEquals(candidate('//hello'), '\u2215\u2215hello')\n    lu.assertEquals(candidate('foo/bar//'), 'foo\u2215bar\u2215\u2215')\n    lu.assertEquals(candidate('foo//bar/baz'), 'foo\u2215\u2215bar\u2215baz')\n    lu.assertEquals(candidate('A/B'), 'A\u2215B')\n    lu.assertEquals(candidate('abc123'), 'abc123')\n    lu.assertEquals(candidate('//foo'), '\u2215\u2215foo')\n    lu.assertEquals(candidate('hello'), 'hello')\n    lu.assertEquals(candidate('foo/bar/baz'), 'foo\u2215bar\u2215baz')\n    lu.assertEquals(candidate('A/B/C'), 'A\u2215B\u2215C')\n    lu.assertEquals(candidate('/A/B/C/'), '\u2215A\u2215B\u2215C\u2215')\n    lu.assertEquals(candidate('//'), '\u2215\u2215')\n    lu.assertEquals(candidate('foo//bar'), 'foo\u2215\u2215bar')\n    lu.assertEquals(candidate('bar'), 'bar')\n    lu.assertEquals(candidate('hello/world/12345/54321'), 'hello\u2215world\u221512345\u221554321')\n    lu.assertEquals(candidate('/A/B/'), '\u2215A\u2215B\u2215')\n    lu.assertEquals(candidate('/A/B/C/D/'), '\u2215A\u2215B\u2215C\u2215D\u2215')\n    lu.assertEquals(candidate('abc'), 'abc')\n    lu.assertEquals(candidate('foo/bar'), 'foo\u2215bar')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('A/B/C/D'), 'A\u2215B\u2215C\u2215D')\n    lu.assertEquals(candidate('foo'), 'foo')\n    lu.assertEquals(candidate(None), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_253524_remove_block_hashtags", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # caption = caption.split('\\n', 1)[0]\n-- # clean_caption = caption.split('\\u2022', 1)[0]\n-- # return clean_caption.strip()\n-- \n-- attempt to remove hidden hashtags at the bottom of captions\nlocal function remove_block_hashtags(caption)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_253524_remove_block_hashtags.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = remove_block_hashtags\n    lu.assertEquals(candidate('a #simple #caption\\n\\n\u2022 #a hidden hashtag\\n\\n\u2022 another hidden hashtag\\n\\n\u2022 a third hidden hashtag'), 'a #simple #caption')\n    lu.assertEquals(candidate('Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.'), 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.')\n    lu.assertEquals(candidate('Hey check out this #python #code\\n\u2022 #programming #code #python #fun'), 'Hey check out this #python #code')\n    lu.assertEquals(candidate('#dogcatdog'), '#dogcatdog')\n    lu.assertEquals(candidate('Hey check out this #python #code\\n\u2022 #programming #code #python #fun\\n\u2022 #python #coding #fun\\n\u2022 #python #fun #code\\n\u2022 #python #fun #code\\n\u2022 #python #fun #code'), 'Hey check out this #python #code')\n    lu.assertEquals(candidate('Hey check out this #python #code\\n\u2022 #programming #code #python #fun\\n\u2022 #python #coding #fun\\n\u2022 #python #fun #code'), 'Hey check out this #python #code')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('a #simple #caption\\n\\n\u2022 #a hidden hashtag\\n\\n\u2022 #another hidden hashtag'), 'a #simple #caption')\n    lu.assertEquals(candidate('This is a caption with hashtags.\\n\\n###test ###hashtag ###hashtag'), 'This is a caption with hashtags.')\n    lu.assertEquals(candidate(\"Hello, Twitter! This is <NAME>, and today we're excited to introduce a brand new way to discover and share on Twitter. With the new Discover tab, you can find the content you love and follow people who love it!\"), \"Hello, Twitter! This is <NAME>, and today we're excited to introduce a brand new way to discover and share on Twitter. With the new Discover tab, you can find the content you love and follow people who love it!\")\n    lu.assertEquals(candidate('This is a caption with hashtags.\\n\\n#hello\\n\\n###test ###hashtag ###hashtag\\n\\n'), 'This is a caption with hashtags.')\n    lu.assertEquals(candidate('Hey check out this #python #code\\n\u2022 #programming #code #python #fun\\n\u2022 #python #coding #fun'), 'Hey check out this #python #code')\n    lu.assertEquals(candidate('A few days ago I had a very good feeling about the project because of how the community was and how active we were. I was also happy to see that the project was gaining momentum!'), 'A few days ago I had a very good feeling about the project because of how the community was and how active we were. I was also happy to see that the project was gaining momentum!')\n    lu.assertEquals(candidate(\"Today we're announcing a major new feature, which allows you to view who has liked your photos! You can now filter the photo grid by your liked photos to see who has liked your photos!\"), \"Today we're announcing a major new feature, which allows you to view who has liked your photos! You can now filter the photo grid by your liked photos to see who has liked your photos!\")\n    lu.assertEquals(candidate('a #simple #caption\\n\\n\u2022 #a hidden hashtag\\n\\n\u2022 another hidden hashtag\\n\\n\u2022 a third hidden hashtag\\n\\n\u2022 a fourth hidden hashtag'), 'a #simple #caption')\n    lu.assertEquals(candidate('a #simple #caption\\n\\n\u2022 #a hidden hashtag'), 'a #simple #caption')\n    lu.assertEquals(candidate('dogcatdogcatdogcat'), 'dogcatdogcatdogcat')\n    lu.assertEquals(candidate('hello world'), 'hello world')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_253671__gen_bia_img_url", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # assert isinstance(imgname, str)\n-- # return 'http://www.istartedsomething.com/bingimages/cache/' + imgname\n-- \n-- Genarate BIA Image URL\nlocal function _gen_bia_img_url(imgname)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_253671__gen_bia_img_url.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _gen_bia_img_url\n    lu.assertEquals(candidate('4011225839_82f37d3a43_b.jpg'), 'http://www.istartedsomething.com/bingimages/cache/4011225839_82f37d3a43_b.jpg')\n    lu.assertEquals(candidate('4096.jpg'), 'http://www.istartedsomething.com/bingimages/cache/4096.jpg')\n    lu.assertEquals(candidate('1844c585d7706905996f70d34e6e88f4.jpg'), 'http://www.istartedsomething.com/bingimages/cache/1844c585d7706905996f70d34e6e88f4.jpg')\n    lu.assertEquals(candidate('16526562929_4a36810594_b.jpg'), 'http://www.istartedsomething.com/bingimages/cache/16526562929_4a36810594_b.jpg')\n    lu.assertEquals(candidate('test.jpg'), 'http://www.istartedsomething.com/bingimages/cache/test.jpg')\n    lu.assertEquals(candidate(), 'http://www.istartedsomething.com/bingimages/cache/mybiaimage.jpg')\n    lu.assertEquals(candidate('13190.jpg'), 'http://www.istartedsomething.com/bingimages/cache/13190.jpg')\n    lu.assertEquals(candidate('542048039_d5e364672a_b.jpg'), 'http://www.istartedsomething.com/bingimages/cache/542048039_d5e364672a_b.jpg')\n    lu.assertEquals(candidate('12299423875_7e6178659f_b.jpg'), 'http://www.istartedsomething.com/bingimages/cache/12299423875_7e6178659f_b.jpg')\n    lu.assertEquals(candidate('25805.jpg'), 'http://www.istartedsomething.com/bingimages/cache/25805.jpg')\n    lu.assertEquals(candidate('test'), 'http://www.istartedsomething.com/bingimages/cache/test')\n    lu.assertEquals(candidate('foo.png'), 'http://www.istartedsomething.com/bingimages/cache/foo.png')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_254264_sorted_items", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return sorted(params.items())\n-- \n-- Return an iterator of the dict's items sorted by its keys.\nlocal function sorted_items(params)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_254264_sorted_items.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sorted_items\n    lu.assertEquals(candidate({['a'] = 1, ['c'] = 3, ['b'] = 2}), {{'a', 1}, {'b', 2}, {'c', 3}})\n    lu.assertEquals(candidate(dict()), {})\n    lu.assertEquals(candidate({['a'] = 1, ['c'] = 3, ['b'] = 2, ['d'] = 4}), {{'a', 1}, {'b', 2}, {'c', 3}, {'d', 4}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_254385_parse_seat_to_binary", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # replaces = {\n-- #     'B': '1',\n-- #     'F': '0',\n-- #     'R': '1',\n-- #     'L': '0',\n-- # }\n-- # out_str = seat\n-- # for old, new in replaces.items():\n-- #     out_str = out_str.replace(old, new)\n-- # return out_str\n-- \n-- Take a seat identifier BFFFBBFRRR and determine it's binary\n-- number\nlocal function parse_seat_to_binary(seat)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_254385_parse_seat_to_binary.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_seat_to_binary\n    lu.assertEquals(candidate(''), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_254436__get_int_type_index", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if i > 2147483647:\n-- #     return 2\n-- # else:\n-- #     return 3\n-- \n--     Returns the index into the types array corresponding to this int\nlocal function _get_int_type_index(i)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_254436__get_int_type_index.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _get_int_type_index\n    lu.assertEquals(candidate(12345), 3)\n    lu.assertEquals(candidate(4294967295), 2)\n    lu.assertEquals(candidate(2147483646), 3)\n    lu.assertEquals(candidate(2147483648), 2)\n    lu.assertEquals(candidate(10), 3)\n    lu.assertEquals(candidate(1024), 3)\n    lu.assertEquals(candidate(4), 3)\n    lu.assertEquals(candidate(-2147483648), 3)\n    lu.assertEquals(candidate(1), 3)\n    lu.assertEquals(candidate(-1), 3)\n    lu.assertEquals(candidate(-42), 3)\n    lu.assertEquals(candidate(42), 3)\n    lu.assertEquals(candidate(2147483647), 3)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_254770_key_int_to_str", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # switcher = {\n-- #     \"0\": \"C\",\n-- #     \"1\": \"C#\",\n-- #     \"2\": \"D\",\n-- #     \"3\": \"D#\",\n-- #     \"4\": \"E\",\n-- #     \"5\": \"F\",\n-- #     \"6\": \"F#\",\n-- #     \"7\": \"G\",\n-- #     \"8\": \"G#\",\n-- #     \"9\": \"A\",\n-- #     \"10\": \"A#\",\n-- #     \"11\": \"B\"\n-- # }\n-- # return switcher.get(str(key), \"No key\")\n-- \n-- Convert spotify's 'pitch class notation' to\n-- and actual key value\nlocal function key_int_to_str(key)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_254770_key_int_to_str.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = key_int_to_str\n    lu.assertEquals(candidate(8), 'G#')\n    lu.assertEquals(candidate('13'), 'No key')\n    lu.assertEquals(candidate(9), 'A')\n    lu.assertEquals(candidate(5), 'F')\n    lu.assertEquals(candidate(-5), 'No key')\n    lu.assertEquals(candidate(-8), 'No key')\n    lu.assertEquals(candidate(15), 'No key')\n    lu.assertEquals(candidate(3), 'D#')\n    lu.assertEquals(candidate(1), 'C#')\n    lu.assertEquals(candidate(4), 'E')\n    lu.assertEquals(candidate(-1), 'No key')\n    lu.assertEquals(candidate(10), 'A#')\n    lu.assertEquals(candidate(13), 'No key')\n    lu.assertEquals(candidate(-3), 'No key')\n    lu.assertEquals(candidate('12'), 'No key')\n    lu.assertEquals(candidate(-2), 'No key')\n    lu.assertEquals(candidate(11), 'B')\n    lu.assertEquals(candidate(-4), 'No key')\n    lu.assertEquals(candidate(14), 'No key')\n    lu.assertEquals(candidate(2), 'D')\n    lu.assertEquals(candidate(7), 'G')\n    lu.assertEquals(candidate(12), 'No key')\n    lu.assertEquals(candidate(0), 'C')\n    lu.assertEquals(candidate(-6), 'No key')\n    lu.assertEquals(candidate(24), 'No key')\n    lu.assertEquals(candidate(-7), 'No key')\n    lu.assertEquals(candidate(6), 'F#')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_255070_average_best_three", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # min_grade = min(grade1, grade2, grade3, grade4)\n-- # sum = grade1 + grade2 + grade3 + grade4 - min_grade\n-- # average = sum / 3\n-- # return int(average)\n-- \n-- from input of four numbers average biggest three\n-- average_best_three(1, 10, 20, 30)\n-- 20\nlocal function average_best_three(grade1, grade2, grade3, grade4)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_255070_average_best_three.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = average_best_three\n    lu.assertEquals(candidate(3, 3, 3, 3), 3)\n    lu.assertEquals(candidate(10, 10, 10, 10), 10)\n    lu.assertEquals(candidate(1, 10, 20, 30), 20)\n    lu.assertEquals(candidate(10, 1, 30, 20), 20)\n    lu.assertEquals(candidate(-5, -4, -3, -2), -3)\n    lu.assertEquals(candidate(10, 20, 10, 30), 20)\n    lu.assertEquals(candidate(-1, -2, -3, -4), -2)\n    lu.assertEquals(candidate(10, 1, 20, 30), 20)\n    lu.assertEquals(candidate(1, 1, 1, 1), 1)\n    lu.assertEquals(candidate(5, 5, 5, 5), 5)\n    lu.assertEquals(candidate(100, 100, 100, 100), 100)\n    lu.assertEquals(candidate(0, 0, 0, 0), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_255382_phex", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # num = hex(int(num))[2:].upper()\n-- # if num[-1].lower() == 'L':\n-- #     num = num[:-1]\n-- # return num.zfill(2)\n-- \n-- convert int to 2 characters hex string\n-- which not contain '0x' in begin or 'L' in end\nlocal function phex(num)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_255382_phex.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = phex\n    lu.assertEquals(candidate(32), '20')\n    lu.assertEquals(candidate(31), '1F')\n    lu.assertEquals(candidate(42), '2A')\n    lu.assertEquals(candidate(129), '81')\n    lu.assertEquals(candidate(17), '11')\n    lu.assertEquals(candidate(65535), 'FFFF')\n    lu.assertEquals(candidate(10), '0A')\n    lu.assertEquals(candidate(100), '64')\n    lu.assertEquals(candidate(4660), '1234')\n    lu.assertEquals(candidate(1), '01')\n    lu.assertEquals(candidate(256), '100')\n    lu.assertEquals(candidate(10000), '2710')\n    lu.assertEquals(candidate(255), 'FF')\n    lu.assertEquals(candidate(1), '01')\n    lu.assertEquals(candidate(3), '03')\n    lu.assertEquals(candidate(1000), '3E8')\n    lu.assertEquals(candidate(0), '00')\n    lu.assertEquals(candidate(257), '101')\n    lu.assertEquals(candidate(16), '10')\n    lu.assertEquals(candidate(31), '1F')\n    lu.assertEquals(candidate(12), '0C')\n    lu.assertEquals(candidate(0), '00')\n    lu.assertEquals(candidate(128), '80')\n    lu.assertEquals(candidate(11), '0B')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_255677__prune_instance_label", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # sep = \")\"\n-- # return label.split(sep, 1)[0] + sep\n-- \n-- Deletes everything after the year, which ends in a closed parenthesis.\nlocal function _prune_instance_label(label)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_255677__prune_instance_label.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _prune_instance_label\n    lu.assertEquals(candidate('The Einstein Journal (1879-1934) (1944-1946) (1947-1948)'), 'The Einstein Journal (1879-1934)')\n    lu.assertEquals(candidate('n.d. (1827-04-17).???.'), 'n.d. (1827-04-17)')\n    lu.assertEquals(candidate('n.d. (1827-04-17)???'), 'n.d. (1827-04-17)')\n    lu.assertEquals(candidate('n.d. (1827-04-17).'), 'n.d. (1827-04-17)')\n    lu.assertEquals(candidate('n.d. (1827-04-17)'), 'n.d. (1827-04-17)')\n    lu.assertEquals(candidate('n.d. (1827-04-17)???.'), 'n.d. (1827-04-17)')\n    lu.assertEquals(candidate('The Einstein Journal (1879-1934) (1944-1946)'), 'The Einstein Journal (1879-1934)')\n    lu.assertEquals(candidate('n.d. (1827-04-17).???'), 'n.d. (1827-04-17)')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_255721_hp_state_bit_english", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # value = raw_table[base_index]\n-- # stringvalue = \"\"\n-- # if value & (1 << 1):\n-- #     stringvalue += \"[Defrosting] \"\n-- # if value & (1 << 1):\n-- #     stringvalue += \"[Boiler Pump Backup] \"\n-- # if value & (1 << 1):\n-- #     stringvalue += \"[Boiler Backup] \"\n-- # if value & (1 << 1):\n-- #     stringvalue += \"[HP Pump] \"\n-- # if value & (1 << 1):\n-- #     stringvalue += \"[Backup 2] \"\n-- # if value & (1 << 1):\n-- #     stringvalue += \"[Backup 1] \"\n-- # if value & (1 << 1):\n-- #     stringvalue += \"[Compressor] \"\n-- # return stringvalue\n-- \n--  Convert derog bit flag to English \nlocal function hp_state_bit_english(raw_table, base_index)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_255721_hp_state_bit_english.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = hp_state_bit_english\n    lu.assertEquals(candidate({255, 255, 255, 255, 255, 255, 255, 255}, 0), '[Defrosting] [Boiler Pump Backup] [Boiler Backup] [HP Pump] [Backup 2] [Backup 1] [Compressor] ')\n    lu.assertEquals(candidate({255, 255, 255, 255, 255, 255, 255, 255}, 1), '[Defrosting] [Boiler Pump Backup] [Boiler Backup] [HP Pump] [Backup 2] [Backup 1] [Compressor] ')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_256372_is_mer", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # (0, 2) (1, 3)\n-- # def is_empty(lx, ly, ux, uy):\n-- #     # Check that all are 0s\n-- #     for x in range(lx, ux):\n-- #         for y in range(ly, uy):\n-- #             if grid[x][y]:\n-- #                 return False\n-- #     return True\n-- # # Check that no expansion can be made\n-- # return is_empty(llx, lly, urx, ury) and \\\n-- #     (llx == 0 or not is_empty(llx - 1, lly, urx, ury)) and \\\n-- #     (ury == len(grid[0]) or not is_empty(llx, lly, urx, ury + 1)) and \\\n-- #     (urx == len(grid) or not is_empty(llx, lly, urx + 1, ury)) and \\\n-- #     (lly == 0 or not is_empty(llx, lly - 1, urx, ury))\n-- \n--  Checks if the rectangle is a MER\n-- NOTE: exclusive ur \nlocal function is_mer(grid, llx, lly, urx, ury)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_256372_is_mer.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_mer\n    lu.assertEquals(candidate({{1, 1, 1, 1, 1, 1, 0, 1}, {0, 0, 0, 1, 0, 1, 1, 1}, {0, 1, 1, 1, 0, 1, 1, 1}, {0, 0, 1, 1, 1, 1, 1, 1}}, 0, 0, 7, 7), false)\n    lu.assertEquals(candidate({{1, 1, 1, 0, 1, 1, 1, 1}, {0, 0, 1, 1, 1, 1, 1, 1}, {0, 1, 1, 1, 0, 1, 1, 1}, {0, 0, 1, 1, 1, 1, 1, 1}}, 0, 0, 7, 7), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_257729_get_die_base_hazard_rate", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return 0.16 if type_id == 1 else 0.24\n-- \n-- Retrieve the base hazard rate for a VHISC/VLSI die.\n-- :param type_id: the VHISC/VLSI type identifier.\n-- :return: _lambda_bd; the base die hazard rate.\n-- :rtype: float\nlocal function get_die_base_hazard_rate(type_id)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_257729_get_die_base_hazard_rate.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_die_base_hazard_rate\n    lu.assertEquals(candidate(1), 0.16)\n    lu.assertEquals(candidate(2), 0.24)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_258216_parseMovie", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # fields = line.strip().split(\"::\")\n-- # return int(fields[0]), fields[1]\n-- \n--     Parses a movie record in MovieLens format movieId::movieTitle .\nlocal function parseMovie(line)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_258216_parseMovie.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parseMovie\n    lu.assertEquals(candidate('6::Twelve Monkeys (a.k.a. 12 Monkeys) (1995)'), {6, 'Twelve Monkeys (a.k.a. 12 Monkeys) (1995)'})\n    lu.assertEquals(candidate('1::Sully (1965)::http://us.imdb.com/M/title-exact?Sully%20(1965)'), {1, 'Sully (1965)'})\n    lu.assertEquals(candidate('3::Grumpier Old Men (1995)::Comedy::Romance::8.0'), {3, 'Grumpier Old Men (1995)'})\n    lu.assertEquals(candidate('1::Toy Story (1995)'), {1, 'Toy Story (1995)'})\n    lu.assertEquals(candidate('1::Toy Story (1995)::Animation::Comedy::8.3'), {1, 'Toy Story (1995)'})\n    lu.assertEquals(candidate('2::Jumanji (1995)::Adventure::Childhood::8.1'), {2, 'Jumanji (1995)'})\n    lu.assertEquals(candidate('6::Heat (1995)::Action::Sci-Fi::7.9'), {6, 'Heat (1995)'})\n    lu.assertEquals(candidate(\"8::Tom and Huck (1995)::Adventure::Children's::8.5\"), {8, 'Tom and Huck (1995)'})\n    lu.assertEquals(candidate('3::Three Billboards Outside Ebbing, Missouri (2017)::http://us.imdb.com/M/title-exact?Three%20Billboards%20Outside%20Ebbing,%20Missouri%20(2017)'), {3, 'Three Billboards Outside Ebbing, Missouri (2017)'})\n    lu.assertEquals(candidate('2::Toy Story (1995)::http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)'), {2, 'Toy Story (1995)'})\n    lu.assertEquals(candidate('4::Get Shorty (1995)'), {4, 'Get Shorty (1995)'})\n    lu.assertEquals(candidate('3::Four Rooms (1995)'), {3, 'Four Rooms (1995)'})\n    lu.assertEquals(candidate('5::Copycat (1995)'), {5, 'Copycat (1995)'})\n    lu.assertEquals(candidate('5::Father of the Bride Part II (1995)::Comedy::Romance::8.5'), {5, 'Father of the Bride Part II (1995)'})\n    lu.assertEquals(candidate('32::Clerks (1994)'), {32, 'Clerks (1994)'})\n    lu.assertEquals(candidate('7::Sabrina (1995)::Comedy::Romance::8.3'), {7, 'Sabrina (1995)'})\n    lu.assertEquals(candidate('4::Waiting to Exhale (1995)::Comedy::Romance::8.2'), {4, 'Waiting to Exhale (1995)'})\n    lu.assertEquals(candidate('2::GoldenEye (1995)'), {2, 'GoldenEye (1995)'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_258575_consolidate_paragraphs", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # pragmatically, both of these offsets will start at 0\n-- # cleaned_para_list = [0]\n-- # prior_para_marker = 0\n-- # header_ptr = 0\n-- # for para_marker in paragraph_offsets:\n-- #     if header_ptr >= len(header_offsets):\n-- #         # no more headers. All remaining paragraphs under last heading\n-- #         cleaned_para_list.append(para_marker[0])\n-- #     else:\n-- #         if (header_offsets[header_ptr][0] >= prior_para_marker and\n-- #                 header_offsets[header_ptr][1] <= para_marker[1]):\n-- #             # header is \"merged\" into this paragraph, so don't\n-- #             # add the next paragraph marker, just mvoe along\n-- #             header_ptr = header_ptr + 1\n-- #         else:\n-- #             # header isn't up yet, so this paragraph marker is valid\n-- #             cleaned_para_list.append(para_marker[0])\n-- #     prior_para_marker = para_marker[0]\n-- # return cleaned_para_list\n-- \n-- This takes the array of paragraphs and returns point at which paragraph begins. \n-- NOTE that  for purposes of paragraphs, headers are counted as being within the\n-- paragraph following it\nlocal function consolidate_paragraphs(paragraph_offsets, header_offsets)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_258575_consolidate_paragraphs.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = consolidate_paragraphs\n    lu.assertEquals(candidate({{0, 0}, {2, 2}, {3, 3}, {4, 4}, {8, 8}}, {{0, 0}}), {0, 2, 3, 4, 8})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_258727_cell_cube_coord", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # x = c[0]\n-- # z = c[1]\n-- # return (x, -x-z, z)\n-- \n--  Returns a tuple with the cube coordinates corresponding to the \n-- given axial coordinates.\nlocal function cell_cube_coord(c)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_258727_cell_cube_coord.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = cell_cube_coord\n    lu.assertEquals(candidate({0, 2}), {0, -2, 2})\n    lu.assertEquals(candidate({3, 0}), {3, -3, 0})\n    lu.assertEquals(candidate({1, 1}), {1, -2, 1})\n    lu.assertEquals(candidate({12, 12}), {12, -24, 12})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_258869_get_geo_url", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # total_level = len(geo_list)\n-- # url = \"\"\n-- # for i in range(total_level):\n-- #     level, val = geo_list[i]\n-- #     if i == 0:\n-- #         url += \"&for=%s:%s\" % (level, val)\n-- #     else:\n-- #         url += \"&in=%s:%s\" % (level, val)\n-- # return url\n-- \n--     :param geo_list: list of (geo, geo_id) pair (smallest first)\n-- e.g. [(block%20group, *), (state, 01), (county, 02), (track, *), ]\n--     :return: url for geo query\nlocal function get_geo_url(geo_list)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_258869_get_geo_url.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_geo_url\n    lu.assertEquals(candidate({{'county', '02'}, {'track', '*'}}), '&for=county:02&in=track:*')\n    lu.assertEquals(candidate({{'state', '01'}, {'county', '02'}, {'track', '*'}}), '&for=state:01&in=county:02&in=track:*')\n    lu.assertEquals(candidate({{'track', '*'}}), '&for=track:*')\n    lu.assertEquals(candidate({{'state', '06'}, {'county', '039'}}), '&for=state:06&in=county:039')\n    lu.assertEquals(candidate({}), '')\n    lu.assertEquals(candidate({{'state', '01'}, {'county', '*'}}), '&for=state:01&in=county:*')\n    lu.assertEquals(candidate({{'state', '01'}, {'county', '053'}, {'county', '005'}, {'county', '*'}}), '&for=state:01&in=county:053&in=county:005&in=county:*')\n    lu.assertEquals(candidate({{'state', '*'}}), '&for=state:*')\n    lu.assertEquals(candidate({{'state', '01'}}), '&for=state:01')\n    lu.assertEquals(candidate({{'state', '01'}, {'county', '001'}, {'tract', '001100'}}), '&for=state:01&in=county:001&in=tract:001100')\n    lu.assertEquals(candidate({{'state', '06'}, {'county', '031'}}), '&for=state:06&in=county:031')\n    lu.assertEquals(candidate({{'state', '01'}, {'county', '02'}}), '&for=state:01&in=county:02')\n    lu.assertEquals(candidate({{'state', '*'}, {'county', '*'}}), '&for=state:*&in=county:*')\n    lu.assertEquals(candidate({{'state', '01'}, {'county', '02'}, {'track', '*'}}), '&for=state:01&in=county:02&in=track:*')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_259430_is_pkg_url", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"pkgs\" in munki_url\n-- \n-- Check to see if the url is for a pkg file\nlocal function is_pkg_url(munki_url)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_259430_is_pkg_url.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_pkg_url\n    lu.assertEquals(candidate('https://pkgs.foo.bar.com/product/foo/1.0/i386/Foo-1.0.dmg'), true)\n    lu.assertEquals(candidate('https://pkgs.foo.bar.com/product/foo/1.0/x86_64/Foo-1.0.dmg'), true)\n    lu.assertEquals(candidate('https://pkgs.foo.bar.com/product/foo/1.0/Foo-1.0.pkg'), true)\n    lu.assertEquals(candidate('https://my.munki.example.org/pkgs/foo-bar-1.0.pkg'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_25952_coalesce_dates", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # parsed_dates = []\n-- # for date in dates:\n-- #     parsed_dates.extend([(date[0], 1), (date[1], -1)])\n-- # parsed_dates.sort(key=lambda d: d[0])\n-- # count = 0\n-- # coalesced = []\n-- # current_block = [None, None]\n-- # for date in parsed_dates:\n-- #     if count == 0:\n-- #         if not coalesced or (coalesced[-1][1] != date[0]):\n-- #             current_block = [date[0], None]\n-- #         else:\n-- #             coalesced.pop()\n-- #     count += date[1]\n-- #     if count == 0:\n-- #         current_block[1] = date[0]\n-- #         coalesced.append((current_block[0], current_block[1]))\n-- # return coalesced\n-- \n-- Coalesces all date pairs into combined date pairs that makes it easy to find free time gaps.\n-- >>> from date_collapse import coalesce_dates\n-- >>> dates = [(1,4),(2,8),(12,16),(16,21)]\n-- >>> cdates = coalesce_dates(dates)\n-- >>> print(cdates)\n-- [(1, 8), (12, 21)]\n-- >>> dates = [(1,4),(2,8),(8,10),(12,16),(16,21),(21,31)]\n-- >>> cdates = coalesce_dates(dates)\n-- >>> print(cdates)\n-- [(1, 10), (12, 31)]\nlocal function coalesce_dates(dates)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_25952_coalesce_dates.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = coalesce_dates\n    lu.assertEquals(candidate({{1, 4}, {2, 8}, {8, 10}, {12, 16}, {16, 21}, {21, 31}}), {{1, 10}, {12, 31}})\n    lu.assertEquals(candidate({{1, 4}, {2, 8}, {12, 16}, {16, 21}}), {{1, 8}, {12, 21}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_259671_conv_module_name_filter", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # filters = {\n-- #     'kernel_size': 'k',\n-- #     'stride': 's',\n-- #     'padding': 'pad',\n-- #     'bias': 'b',\n-- #     'groups': 'g',\n-- # }\n-- # for k in filters:\n-- #     name = name.replace(k, filters[k])\n-- # return name\n-- \n-- filter module name to have a short view\nlocal function conv_module_name_filter(name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_259671_conv_module_name_filter.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = conv_module_name_filter\n    lu.assertEquals(candidate('Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)'), 'Conv2d(3, 16, k=(1, 1), s=(1, 1), b=False)')\n    lu.assertEquals(candidate('Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)'), 'Conv2d(3, 32, k=(5, 5), s=(1, 1), pad=(2, 2), b=False)')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_259733_IsVersionNewer", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if cur_version == new_version:\n-- #     return False\n-- # cur = cur_version.split('.')\n-- # new = new_version.split('.')\n-- # if len(cur) != 4 or len(new) != 4:\n-- #     raise RuntimeError('One or both of the versions are invalid.')\n-- # for x in range(len(cur)):\n-- #     if int(cur[x]) > int(new[x]):\n-- #         return False\n-- # return True\n-- \n-- Determines if new Chrome version is higher than the installed one.\n-- Args:\n--   cur_version: Current version of Chrome.\n--   new_version: New version that will be installed.\n-- Returns:\n--   True, if new version is higher, otherwise False.\nlocal function IsVersionNewer(cur_version, new_version)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_259733_IsVersionNewer.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = IsVersionNewer\n    lu.assertEquals(candidate('1.2.3.4', '1.2.4.4'), true)\n    lu.assertEquals(candidate('1.2.3.4', '1.2.3.5'), true)\n    lu.assertEquals(candidate('1.2.3.4', '0.2.3.4'), false)\n    lu.assertEquals(candidate('1.2.3.4', '1.2.3.4'), false)\n    lu.assertEquals(candidate('1.2.3.4', '1.2.3.3'), false)\n    lu.assertEquals(candidate('1.2.3.4', '1.3.3.4'), true)\n    lu.assertEquals(candidate('1.2.3.4', '1.2.2.4'), false)\n    lu.assertEquals(candidate('1.2.3.4', '2.2.3.4'), true)\n    lu.assertEquals(candidate('1.2.3.4', '1.2.3.4'), false)\n    lu.assertEquals(candidate('1.0.0.0', '1.0.0.0'), false)\n    lu.assertEquals(candidate('1.2.3.4', '1.1.3.4'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_259839_interval_to_milliseconds", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # ms = None\n-- # seconds_per_unit = {\n-- #     \"m\": 60,\n-- #     \"h\": 60 * 60,\n-- #     \"d\": 24 * 60 * 60,\n-- #     \"w\": 7 * 24 * 60 * 60\n-- # }\n-- # unit = interval[-1]\n-- # if unit in seconds_per_unit:\n-- #     try:\n-- #         ms = int(interval[:-1]) * seconds_per_unit[unit] * 1000\n-- #     except ValueError:\n-- #         pass\n-- # return ms\n-- \n-- Convert a Binance interval string to milliseconds\n-- For clarification see document or mail d3dileep@gmail.com\n-- :param interval: Binance interval string 1m, 3m, 5m, 15m, 30m, 1h, 2h, 4h, 6h, 8h, 12h, 1d, 3d, 1w\n-- :type interval: str\n-- :return:\n--          None if unit not one of m, h, d or w\n--          None if string not in correct format\n--          int value of interval in milliseconds\nlocal function interval_to_milliseconds(interval)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_259839_interval_to_milliseconds.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = interval_to_milliseconds\n    lu.assertEquals(candidate('1h'), 3600000)\n    lu.assertEquals(candidate('1M'), None)\n    lu.assertEquals(candidate('1m'), 60000)\n    lu.assertEquals(candidate('30m'), 1800000)\n    lu.assertEquals(candidate('4h'), 14400000)\n    lu.assertEquals(candidate('3d'), 259200000)\n    lu.assertEquals(candidate('2h'), 7200000)\n    lu.assertEquals(candidate('6h'), 21600000)\n    lu.assertEquals(candidate('1d'), 86400000)\n    lu.assertEquals(candidate('1W'), None)\n    lu.assertEquals(candidate('1d'), 86400000)\n    lu.assertEquals(candidate('1w'), 604800000)\n    lu.assertEquals(candidate('15m'), 900000)\n    lu.assertEquals(candidate('1w'), 604800000)\n    lu.assertEquals(candidate('8h'), 28800000)\n    lu.assertEquals(candidate('15m'), 900000)\n    lu.assertEquals(candidate('4h'), 14400000)\n    lu.assertEquals(candidate('1m'), 60000)\n    lu.assertEquals(candidate('12h'), 43200000)\n    lu.assertEquals(candidate('8h'), 28800000)\n    lu.assertEquals(candidate('6h'), 21600000)\n    lu.assertEquals(candidate('5m'), 300000)\n    lu.assertEquals(candidate('2h'), 7200000)\n    lu.assertEquals(candidate('3m'), 180000)\n    lu.assertEquals(candidate('3m'), 180000)\n    lu.assertEquals(candidate('12h'), 43200000)\n    lu.assertEquals(candidate('1h'), 3600000)\n    lu.assertEquals(candidate('3d'), 259200000)\n    lu.assertEquals(candidate('5m'), 300000)\n    lu.assertEquals(candidate('30m'), 1800000)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_260814_str2bool", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return string in ['true', 'True', '1', 'yes'] if string else None\n-- \n--  Convert string to boolean. \nlocal function str2bool(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_260814_str2bool.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = str2bool\n    lu.assertEquals(candidate('1'), true)\n    lu.assertEquals(candidate('True'), true)\n    lu.assertEquals(candidate('True'), true)\n    lu.assertEquals(candidate(None), None)\n    lu.assertEquals(candidate('false'), false)\n    lu.assertEquals(candidate('false'), false)\n    lu.assertEquals(candidate('False'), false)\n    lu.assertEquals(candidate('yes'), true)\n    lu.assertEquals(candidate('0'), false)\n    lu.assertEquals(candidate('False'), false)\n    lu.assertEquals(candidate('no'), false)\n    lu.assertEquals(candidate('true'), true)\n    lu.assertEquals(candidate('true'), true)\n    lu.assertEquals(candidate('0'), false)\n    lu.assertEquals(candidate('1'), true)\n    lu.assertEquals(candidate('no'), false)\n    lu.assertEquals(candidate('yes'), true)\n    lu.assertEquals(candidate(''), None)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_261308_get_eqn", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # m = (p0[1] - p1[1]) / (p0[0] - p1[0])\n-- # return (m, p0[1] - m * p0[0])\n-- \n-- Returns the equation of a line in the form mx+b as a tuple of (m, b) for two points.\n-- Does not check for vertical lines.\nlocal function get_eqn(p0, p1)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_261308_get_eqn.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_eqn\n    lu.assertEquals(candidate({0, 0}, {100, 0}), {0, 0})\n    lu.assertEquals(candidate({0, 0}, {100, 100}), {1, 0})\n    lu.assertEquals(candidate({0, 0}, {1, 2}), {2, 0})\n    lu.assertEquals(candidate({0, 0}, {1, 1}), {1, 0})\n    lu.assertEquals(candidate({0, 0}, {-100, 100}), {-1, 0})\n    lu.assertEquals(candidate({0, 0}, {-100, 0}), {0, 0})\n    lu.assertEquals(candidate({0, 0}, {-1, 1}), {-1, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_261555_write_output", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # with open('{}'.format(invcf.replace('.vcf', '.tsv')), 'w') as wt_fianl:\n-- #     wt_fianl.write(final)\n-- # return True\n-- \n--     Make tsv format output file.\nlocal function write_output(final, invcf)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_261555_write_output.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = write_output\n    lu.assertEquals(candidate('1\\t10\\tA\\tG\\n1\\t12\\tT\\tC\\n1\\t15\\tC\\tT\\n1\\t18\\tG\\tA\\n1\\t20\\tC\\tT\\n', 'my_data.vcf'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_261763_get_package_type", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if 'https://github.com/' in package_link:\n-- #     return 'git_package'\n-- # elif '/' not in package_link:\n-- #     return 'pypy_package'\n-- # elif not package_link:\n-- #     return 'pypy_package'\n-- # else:\n-- #     return 'weblink'\n-- \n-- :returns the package type [\"pypy_package\"|\"git_package\"|\"weblink\"]\n-- >>> assert get_package_type('pip') == 'pypy_package'\n-- >>> assert get_package_type('https://github.com/pypa/pip.git') == 'git_package'\n-- >>> assert get_package_type('git+https://github.com/pypa/pip.git') == 'git_package'\n-- >>> assert get_package_type('https://github.com/pypa/archive/master.zip') == 'git_package'\n-- >>> assert get_package_type('https://some_link/pypa/archive/master.zip') == 'weblink'\nlocal function get_package_type(package_link)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_261763_get_package_type.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_package_type\n    lu.assertEquals(candidate('pip'), 'pypy_package')\n    lu.assertEquals(candidate('https://github.com/pypa/pip.git'), 'git_package')\n    lu.assertEquals(candidate('https://github.com/pypa/pip.git'), 'git_package')\n    lu.assertEquals(candidate('git+https://github.com/pypa/pip.git'), 'git_package')\n    lu.assertEquals(candidate('https://some_link/pypa/archive/master.zip'), 'weblink')\n    lu.assertEquals(candidate('https://github.com/pypa/archive/master.zip'), 'git_package')\n    lu.assertEquals(candidate('https://github.com/pypa/pip'), 'git_package')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_262021_empty_columns", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # columns = []\n-- # while len(columns) != len(board[0]):\n-- #     columns.append(\"\")\n-- # return columns\n-- \n-- Returns list of \"\" with length equal to length of expected column.\n-- >>> empty_columns(['***21**', '412453*', '423145*', '*543215', '*35214*', '*41532*', '*2*1***'])\n-- ['', '', '', '', '', '', '']\nlocal function empty_columns(board)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_262021_empty_columns.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = empty_columns\n    lu.assertEquals(candidate({'***21**', '412553*', '423145*', '*543215', '*35214*', '*41232*', '*2*1***'}), {'', '', '', '', '', '', ''})\n    lu.assertEquals(candidate({'*21**', '412453*'}), {'', '', '', '', ''})\n    lu.assertEquals(candidate({'***22**', '412453*', '423145*', '*543215', '*35214*', '*41532*', '*2*1***'}), {'', '', '', '', '', '', ''})\n    lu.assertEquals(candidate({'***21**', '412453*', '423145*', '*543215', '*35214*', '*41232*', '*2*1***'}), {'', '', '', '', '', '', ''})\n    lu.assertEquals(candidate({'***22**', '412453*', '423145*', '*543215', '*35214*', '*41232*', '*2*1***'}), {'', '', '', '', '', '', ''})\n    lu.assertEquals(candidate({'***21**', '412553*', '423145*', '*543215', '*35214*', '*41532*', '*2*1***'}), {'', '', '', '', '', '', ''})\n    lu.assertEquals(candidate({'***21**', '412453*', '423145*', '*543215', '*35214*', '*41532*', '*2*1***'}), {'', '', '', '', '', '', ''})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_262051_remove_numbers", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return ''.join([i for i in s if not i.isdigit()])\n-- \n-- Remove any number in a string\n-- Args:\n--     s (str): A string that need to remove number\n-- Returns:\n--     A formatted string with no number\nlocal function remove_numbers(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_262051_remove_numbers.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = remove_numbers\n    lu.assertEquals(candidate('10101010'), '')\n    lu.assertEquals(candidate('1234567890'), '')\n    lu.assertEquals(candidate('abc12345'), 'abc')\n    lu.assertEquals(candidate('abc123abc'), 'abcabc')\n    lu.assertEquals(candidate('There are no numbers in this string'), 'There are no numbers in this string')\n    lu.assertEquals(candidate('Hello World'), 'Hello World')\n    lu.assertEquals(candidate(\"2021 isn't a year\"), \" isn't a year\")\n    lu.assertEquals(candidate('1234567890-'), '-')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('hello'), 'hello')\n    lu.assertEquals(candidate('Hello123World'), 'HelloWorld')\n    lu.assertEquals(candidate('Hi there! 23 is my favorite number.'), 'Hi there!  is my favorite number.')\n    lu.assertEquals(candidate('What is the weather like tomorrow?'), 'What is the weather like tomorrow?')\n    lu.assertEquals(candidate('The 5 boxing wizards jump quickly.'), 'The  boxing wizards jump quickly.')\n    lu.assertEquals(candidate('1234567890'), '')\n    lu.assertEquals(candidate('abc 123 abc'), 'abc  abc')\n    lu.assertEquals(candidate('1234'), '')\n    lu.assertEquals(candidate('No numbers here'), 'No numbers here')\n    lu.assertEquals(candidate('2021 is a year'), ' is a year')\n    lu.assertEquals(candidate('0123456789'), '')\n    lu.assertEquals(candidate('1'), '')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('I am 20 years old.'), 'I am  years old.')\n    lu.assertEquals(candidate('123'), '')\n    lu.assertEquals(candidate('I am 100 years old.'), 'I am  years old.')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_262360_longest_substring", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # lstr = ''\n-- # if (len(str_list) > 1) and (len(str_list[0]) > 0):\n-- #     for i in range(len(str_list[0])):\n-- #         for j in range(len(str_list[0]) - i + 1):\n-- #             if (j > len(lstr)) and all(str_list[0][i:i + j] in entry for entry in str_list):\n-- #                 lstr = str_list[0][i:i + j]\n-- # lstr = 'a' + lstr  # avoid stripping beginning of line\n-- # return lstr.strip('1234567890_- \\t\\n\\r\\'\"')[1:]\n-- \n-- Finds longest substring among list of strings\n-- :param str_list: strings to be searched\n-- :type str_list: list (of str)\n-- :rtype: str\nlocal function longest_substring(str_list)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_262360_longest_substring.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = longest_substring\n    lu.assertEquals(candidate(list('   ')), '')\n    lu.assertEquals(candidate({'  '}), '')\n    lu.assertEquals(candidate({'   '}), '')\n    lu.assertEquals(candidate({'abc', 'abc'}), 'abc')\n    lu.assertEquals(candidate({' '}), '')\n    lu.assertEquals(candidate({'a', 'a'}), 'a')\n    lu.assertEquals(candidate({'abc', 'xyz'}), '')\n    lu.assertEquals(candidate({'apple', 'apple'}), 'apple')\n    lu.assertEquals(candidate(list('  ')), '')\n    lu.assertEquals(candidate(list('')), '')\n    lu.assertEquals(candidate({''}), '')\n    lu.assertEquals(candidate({'', ''}), '')\n    lu.assertEquals(candidate({'abc', 'xyz', '123'}), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_26322_get_valid_step", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if current_step < 1:\n-- #     current_step = 1\n-- # elif current_step > max_step:\n-- #     current_step = max_step\n-- # return current_step\n-- \n-- Checks if the current step is within boundaries and returns a corrected step.\n-- :param current_step: The current step to check.\n-- :param max_step: The maximum allowed step.\n-- :return: A corrected step between 1 and the maximum step.\nlocal function get_valid_step(current_step, max_step)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_26322_get_valid_step.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_valid_step\n    lu.assertEquals(candidate(4, 1), 1)\n    lu.assertEquals(candidate(1, 5), 1)\n    lu.assertEquals(candidate(0, 3), 1)\n    lu.assertEquals(candidate(2, 3), 2)\n    lu.assertEquals(candidate(5, 1), 1)\n    lu.assertEquals(candidate(11, 10), 10)\n    lu.assertEquals(candidate(6, 5), 5)\n    lu.assertEquals(candidate(1, 2), 1)\n    lu.assertEquals(candidate(7, 2), 2)\n    lu.assertEquals(candidate(5, 3), 3)\n    lu.assertEquals(candidate(1, 3), 1)\n    lu.assertEquals(candidate(1, 1), 1)\n    lu.assertEquals(candidate(4, 2), 2)\n    lu.assertEquals(candidate(5, 5), 5)\n    lu.assertEquals(candidate(7, 3), 3)\n    lu.assertEquals(candidate(6, 1), 1)\n    lu.assertEquals(candidate(2, 2), 2)\n    lu.assertEquals(candidate(7, 1), 1)\n    lu.assertEquals(candidate(5, 10), 5)\n    lu.assertEquals(candidate(3, 3), 3)\n    lu.assertEquals(candidate(6, 3), 3)\n    lu.assertEquals(candidate(4, 3), 3)\n    lu.assertEquals(candidate(5, 2), 2)\n    lu.assertEquals(candidate(100, 10), 10)\n    lu.assertEquals(candidate(2, 1), 1)\n    lu.assertEquals(candidate(3, 2), 2)\n    lu.assertEquals(candidate(0, 10), 1)\n    lu.assertEquals(candidate(6, 2), 2)\n    lu.assertEquals(candidate(0, 5), 1)\n    lu.assertEquals(candidate(3, 1), 1)\n    lu.assertEquals(candidate(3, 5), 3)\n    lu.assertEquals(candidate(0, 2), 1)\n    lu.assertEquals(candidate(-2, 5), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_263818_digit_count", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return len(f\"{num}\")\n-- \n--     Returns the count of the digits (length) of the number\nlocal function digit_count(num)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_263818_digit_count.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = digit_count\n    lu.assertEquals(candidate(0), 1)\n    lu.assertEquals(candidate(1010), 4)\n    lu.assertEquals(candidate(999), 3)\n    lu.assertEquals(candidate(12), 2)\n    lu.assertEquals(candidate(123456), 6)\n    lu.assertEquals(candidate(25), 2)\n    lu.assertEquals(candidate(1000), 4)\n    lu.assertEquals(candidate(12345), 5)\n    lu.assertEquals(candidate(12345678), 8)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(100), 3)\n    lu.assertEquals(candidate(100000000), 9)\n    lu.assertEquals(candidate(123), 3)\n    lu.assertEquals(candidate(2), 1)\n    lu.assertEquals(candidate(1234), 4)\n    lu.assertEquals(candidate(123456789), 9)\n    lu.assertEquals(candidate(50), 2)\n    lu.assertEquals(candidate(1234567), 7)\n    lu.assertEquals(candidate(101), 3)\n    lu.assertEquals(candidate(10000), 5)\n    lu.assertEquals(candidate(10), 2)\n    lu.assertEquals(candidate(1234567890), 10)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_263912__split_kv", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # key_value = pair.split('=', 1)\n-- # return {key_value[0]: key_value[1]}\n-- \n-- Return dict for key=value.\nlocal function _split_kv(pair)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_263912__split_kv.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _split_kv\n    lu.assertEquals(candidate('foo='), {['foo'] = ''})\n    lu.assertEquals(candidate('foo=bar=baz'), {['foo'] = 'bar=baz'})\n    lu.assertEquals(candidate('a=b=c'), {['a'] = 'b=c'})\n    lu.assertEquals(candidate('foo=bar'), {['foo'] = 'bar'})\n    lu.assertEquals(candidate('a=b'), {['a'] = 'b'})\n    lu.assertEquals(candidate('=bar'), {[''] = 'bar'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_264475_get_test", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # ret = date + 'pred.csv'\n-- # return ret\n-- \n-- get the file name of the test data\nlocal function get_test(date)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_264475_get_test.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_test\n    lu.assertEquals(candidate('2012010'), '2012010pred.csv')\n    lu.assertEquals(candidate('20124010'), '20124010pred.csv')\n    lu.assertEquals(candidate('2012010100'), '2012010100pred.csv')\n    lu.assertEquals(candidate('201201020'), '201201020pred.csv')\n    lu.assertEquals(candidate(), '2013-05-27pred.csv')\n    lu.assertEquals(candidate(), '2013-05-29pred.csv')\n    lu.assertEquals(candidate(), '2013-05-28pred.csv')\n    lu.assertEquals(candidate('20190103'), '20190103pred.csv')\n    lu.assertEquals(candidate('20120103'), '20120103pred.csv')\n    lu.assertEquals(candidate('20120132'), '20120132pred.csv')\n    lu.assertEquals(candidate('20120140'), '20120140pred.csv')\n    lu.assertEquals(candidate('201201010'), '201201010pred.csv')\n    lu.assertEquals(candidate('20120100'), '20120100pred.csv')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_265233_parse_cookie_data", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # cookies = {}\n-- # for c in data.split('; '):\n-- #     try:\n-- #         (k, v) = c.split('=')\n-- #         cookies[k] = v\n-- #     except ValueError:\n-- #         pass\n-- # return cookies\n-- \n--  Parse cookie data into key-value pairs \nlocal function parse_cookie_data(data)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_265233_parse_cookie_data.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_cookie_data\n    lu.assertEquals(candidate('CUSTOMER=WILE_E_COYOTE; PART_NUMBER=ROCKET_LAUNCHER_0001'), {['CUSTOMER'] = 'WILE_E_COYOTE', ['PART_NUMBER'] = 'ROCKET_LAUNCHER_0001'})\n    lu.assertEquals(candidate('theme=light; sessionToken=<PASSWORD>; likes_python=true'), {['theme'] = 'light', ['sessionToken'] = '<PASSWORD>', ['likes_python'] = 'true'})\n    lu.assertEquals(candidate(''), {})\n    lu.assertEquals(candidate('a=b; c=d; e=f; g=h; i=j; k=l; m=n; o=p; q=r; s=t; u=v; w=x; y=z'), {['a'] = 'b', ['c'] = 'd', ['e'] = 'f', ['g'] = 'h', ['i'] = 'j', ['k'] = 'l', ['m'] = 'n', ['o'] = 'p', ['q'] = 'r', ['s'] = 't', ['u'] = 'v', ['w'] = 'x', ['y'] = 'z'})\n    lu.assertEquals(candidate('a=b; c=d; e=f'), {['a'] = 'b', ['c'] = 'd', ['e'] = 'f'})\n    lu.assertEquals(candidate('a=b'), {['a'] = 'b'})\n    lu.assertEquals(candidate('CUSTOMER=WILE_E_COYOTE; PART_NUMBER=ROCKET_LAUNCHER_0001; SHIPPING=FEDEX; CN=Ed'), {['CUSTOMER'] = 'WILE_E_COYOTE', ['PART_NUMBER'] = 'ROCKET_LAUNCHER_0001', ['SHIPPING'] = 'FEDEX', ['CN'] = 'Ed'})\n    lu.assertEquals(candidate('k1=v1; k2=v2; k3=v3'), {['k1'] = 'v1', ['k2'] = 'v2', ['k3'] = 'v3'})\n    lu.assertEquals(candidate('a=b; c=d'), {['a'] = 'b', ['c'] = 'd'})\n    lu.assertEquals(candidate('cookie1=value1'), {['cookie1'] = 'value1'})\n    lu.assertEquals(candidate('theme=light'), {['theme'] = 'light'})\n    lu.assertEquals(candidate('k1=v1; k2=v2; k3=v3; k4=v4'), {['k1'] = 'v1', ['k2'] = 'v2', ['k3'] = 'v3', ['k4'] = 'v4'})\n    lu.assertEquals(candidate('a=b; c=d; e=f; g=h; i=j; k=l; m=n; o=p; q=r; s=t; u=v; w=x; y=z'), {['a'] = 'b', ['c'] = 'd', ['e'] = 'f', ['g'] = 'h', ['i'] = 'j', ['k'] = 'l', ['m'] = 'n', ['o'] = 'p', ['q'] = 'r', ['s'] = 't', ['u'] = 'v', ['w'] = 'x', ['y'] = 'z'})\n    lu.assertEquals(candidate('cookie1=value1; cookie2=value2; cookie3=value3'), {['cookie1'] = 'value1', ['cookie2'] = 'value2', ['cookie3'] = 'value3'})\n    lu.assertEquals(candidate('theme=light; likes_python=true'), {['theme'] = 'light', ['likes_python'] = 'true'})\n    lu.assertEquals(candidate('theme=light; likes_python=true; sessionToken=<PASSWORD>'), {['theme'] = 'light', ['likes_python'] = 'true', ['sessionToken'] = '<PASSWORD>'})\n    lu.assertEquals(candidate('cookie1=value1; cookie2=value2'), {['cookie1'] = 'value1', ['cookie2'] = 'value2'})\n    lu.assertEquals(candidate('likes_python=true'), {['likes_python'] = 'true'})\n    lu.assertEquals(candidate('theme'), {})\n    lu.assertEquals(candidate('CUSTOMER=WILE_E_COYOTE'), {['CUSTOMER'] = 'WILE_E_COYOTE'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_265480_parseDbDummyFname", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # remeshRestartTagIndex = dbDummyFname.rfind(\"-s\")\n-- # if remeshRestartTagIndex == -1:\n-- #     baseId = dbDummyFname\n-- #     remeshRestartTag = ''\n-- # else:\n-- #     baseId = dbDummyFname[0:remeshRestartTagIndex]\n-- #     remeshRestartTag = dbDummyFname[remeshRestartTagIndex:]\n-- # return baseId, remeshRestartTag\n-- \n-- given user data item which is dummy database name used for remesh and\n-- restart purposes, pull out the base name (minus -s0002, -sXXXX, etc.)\n-- and return the base name and extension (extension in an empty string\n-- if appropriate)\nlocal function parseDbDummyFname(dbDummyFname)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_265480_parseDbDummyFname.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parseDbDummyFname\n    lu.assertEquals(candidate('foo-s3'), {'foo', '-s3'})\n    lu.assertEquals(candidate('foo'), {'foo', ''})\n    lu.assertEquals(candidate('foo-s00003'), {'foo', '-s00003'})\n    lu.assertEquals(candidate('foo-s0002'), {'foo', '-s0002'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_265713_addContent", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # old_html += raw_html\n-- # return old_html\n-- \n-- Add html content together\nlocal function addContent(old_html, raw_html)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_265713_addContent.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = addContent\n    lu.assertEquals(candidate('<h1>Test</h1>', '<p>Paragraph content.</p>'), '<h1>Test</h1><p>Paragraph content.</p>')\n    lu.assertEquals(candidate('<div><span>Hello</span></div>', '<div><span>World</span></div>'), '<div><span>Hello</span></div><div><span>World</span></div>')\n    lu.assertEquals(candidate('<p>The first paragraph.</p><p>The second paragraph.</p>', '<h3>This is a header.</h3>'), '<p>The first paragraph.</p><p>The second paragraph.</p><h3>This is a header.</h3>')\n    lu.assertEquals(candidate('', '<div><span>World</span></div>'), '<div><span>World</span></div>')\n    lu.assertEquals(candidate('<p>The first paragraph.</p>', '<p>The second paragraph.</p>'), '<p>The first paragraph.</p><p>The second paragraph.</p>')\n    lu.assertEquals(candidate('<h1>Test</h1><h2>Subheading</h2>', '<h2>Another subheading</h2>'), '<h1>Test</h1><h2>Subheading</h2><h2>Another subheading</h2>')\n    lu.assertEquals(candidate('<h3>This is a header.</h3>', '<p>This is a paragraph.</p>'), '<h3>This is a header.</h3><p>This is a paragraph.</p>')\n    lu.assertEquals(candidate('<div><span>Hello</span></div>', '<div><span>World</span></div>'), '<div><span>Hello</span></div><div><span>World</span></div>')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_266228_scale_ticks_params", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if tick_scale == 'linear':\n-- #     base = None\n-- #     label_scale = 'Linear Scale'\n-- # else:\n-- #     if tick_scale == 'log2':\n-- #         base = 2\n-- #         label_scale = 'Log2 Scale'\n-- #     elif tick_scale == 'log10':\n-- #         base = 10\n-- #         label_scale = 'Log10 Scale'\n-- #     else:\n-- #         raise ValueError('The specified tick scale is not supported.')\n-- # return base, label_scale\n-- \n--  Helper function for learning cureve plots.\n-- Args:\n--     tick_scale : available values are [linear, log2, log10]\nlocal function scale_ticks_params(tick_scale)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_266228_scale_ticks_params.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = scale_ticks_params\n    lu.assertEquals(candidate('log2'), {2, 'Log2 Scale'})\n    lu.assertEquals(candidate('linear'), {None, 'Linear Scale'})\n    lu.assertEquals(candidate('log10'), {10, 'Log10 Scale'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_266486_to_hex", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return '{0:0{1}x}'.format(image_id, 16)\n-- \n-- Given Image Id, return its hex value\nlocal function to_hex(image_id)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_266486_to_hex.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = to_hex\n    lu.assertEquals(candidate(4095), candidate(4095))\n    lu.assertEquals(candidate(16711680), '0000000000ff0000')\n    lu.assertEquals(candidate(4294967296), '0000000100000000')\n    lu.assertEquals(candidate(255), '00000000000000ff')\n    lu.assertEquals(candidate(15), '000000000000000f')\n    lu.assertEquals(candidate(255), candidate(255))\n    lu.assertEquals(candidate(0), '0000000000000000')\n    lu.assertEquals(candidate(256), '0000000000000100')\n    lu.assertEquals(candidate(255), '00000000000000ff')\n    lu.assertEquals(candidate(256), candidate(256))\n    lu.assertEquals(candidate(4294967295), '00000000ffffffff')\n    lu.assertEquals(candidate(16), '0000000000000010')\n    lu.assertEquals(candidate(65280), '000000000000ff00')\n    lu.assertEquals(candidate(18446744073709551615), 'ffffffffffffffff')\n    lu.assertEquals(candidate(1), '0000000000000001')\n    lu.assertEquals(candidate(1), '0000000000000001')\n    lu.assertEquals(candidate(0), '0000000000000000')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_266488_create_local_cluster_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"local-{0}-{1}-{2}\".format(service, color, index)\n-- \n-- Create the local service-color cluster name.\nlocal function create_local_cluster_name(service, color, index)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_266488_create_local_cluster_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = create_local_cluster_name\n    lu.assertEquals(candidate(), 'local-foo-bar-0')\n    lu.assertEquals(candidate('service', 'color', 1), 'local-service-color-1')\n    lu.assertEquals(candidate('service', 'color', '1'), 'local-service-color-1')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_266494_normalize_line", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # stripped = line.rstrip('\\n\\r')\n-- # if stripped != line:\n-- #     return stripped + newline\n-- # return line\n-- \n-- Return line with fixed ending, if ending was present in line.\n-- Otherwise, does nothing.\nlocal function normalize_line(line, newline)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_266494_normalize_line.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = normalize_line\n    lu.assertEquals(candidate('1234567890\\n', ''), '1234567890')\n    lu.assertEquals(candidate('1234567890\\r', ''), '1234567890')\n    lu.assertEquals(candidate('1234567890\\r\\n', '\\n'), '1234567890\\n')\n    lu.assertEquals(candidate('1234567890\\n', '\\n'), '1234567890\\n')\n    lu.assertEquals(candidate('a\\nb\\n', '\\n'), 'a\\nb\\n')\n    lu.assertEquals(candidate('1234567890\\r', '\\n'), '1234567890\\n')\n    lu.assertEquals(candidate('1234567890\\n', '\\r\\n'), '1234567890\\r\\n')\n    lu.assertEquals(candidate('1234567890\\r\\n', ''), '1234567890')\n    lu.assertEquals(candidate('1234567890', ''), '1234567890')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_2664_fibonacci", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if length < 1:\n-- #     raise ValueError(\"Sequence length must be > 0\")\n-- # sequence = [0] * (length + 2)\n-- # sequence[0] = 0\n-- # sequence[1] = 1\n-- # for i in range(2, len(sequence)):\n-- #     sequence[i] = sequence[i - 1] + sequence[i - 2]\n-- # return sequence[: -2]\n-- \n-- Get fibonacci sequence given it length.\n-- Parameters\n-- ----------\n-- length : int\n--     The length of the desired sequence.\n-- Returns\n-- -------\n-- sequence : list of int\n--     The desired Fibonacci sequence\nlocal function fibonacci(length)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_2664_fibonacci.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = fibonacci\n    lu.assertEquals(candidate(1), {0})\n    lu.assertEquals(candidate(7), {0, 1, 1, 2, 3, 5, 8})\n    lu.assertEquals(candidate(10), {0, 1, 1, 2, 3, 5, 8, 13, 21, 34})\n    lu.assertEquals(candidate(4), {0, 1, 1, 2})\n    lu.assertEquals(candidate(6), {0, 1, 1, 2, 3, 5})\n    lu.assertEquals(candidate(2), {0, 1})\n    lu.assertEquals(candidate(8), {0, 1, 1, 2, 3, 5, 8, 13})\n    lu.assertEquals(candidate(5), {0, 1, 1, 2, 3})\n    lu.assertEquals(candidate(3), {0, 1, 1})\n    lu.assertEquals(candidate(9), {0, 1, 1, 2, 3, 5, 8, 13, 21})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_266766_re_remote_url", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # s = s.lower()\n-- # if s.startswith(\"http://\"):\n-- #     return True\n-- # if s.startswith(\"https://\"):\n-- #     return True\n-- # if s.startswith(\"ftp://\"):\n-- #     return True\n-- # return False\n-- \n--  Tests if a string is a \"remote\" URL, http, https, ftp. \nlocal function re_remote_url(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_266766_re_remote_url.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = re_remote_url\n    lu.assertEquals(candidate('https://foo.bar/baz'), true)\n    lu.assertEquals(candidate('https://foo.bar/baz#a?b=c'), true)\n    lu.assertEquals(candidate('https://foo.bar/baz?a=b'), true)\n    lu.assertEquals(candidate('http://foo.bar/baz'), true)\n    lu.assertEquals(candidate('ftp://foo.bar/baz'), true)\n    lu.assertEquals(candidate('https://foo.bar/baz'), true)\n    lu.assertEquals(candidate('https://foo.bar/baz#a'), true)\n    lu.assertEquals(candidate('foo.bar/baz'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_266834_sanitizeIOC", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # newIOC = ioc.replace(\"[.]\", \".\").replace(\"hxxp\", \"http\")\n-- # return newIOC\n-- \n--             Method to sanitize IOCs\nlocal function sanitizeIOC(ioc)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_266834_sanitizeIOC.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sanitizeIOC\n    lu.assertEquals(candidate('hxxp://example.com/path/to/file'), 'http://example.com/path/to/file')\n    lu.assertEquals(candidate('hxxp://test123.example.com'), 'http://test123.example.com')\n    lu.assertEquals(candidate('hxxp://example.com/test123'), 'http://example.com/test123')\n    lu.assertEquals(candidate('[.]abc.com'), '.abc.com')\n    lu.assertEquals(candidate('hxxp://google.com'), 'http://google.com')\n    lu.assertEquals(candidate('hxxp://example.com'), 'http://example.com')\n    lu.assertEquals(candidate('http://php.net/'), 'http://php.net/')\n    lu.assertEquals(candidate('hxxp://example.com'), 'http://example.com')\n    lu.assertEquals(candidate('hxxp://test.com'), 'http://test.com')\n    lu.assertEquals(candidate('[.]'), '.')\n    lu.assertEquals(candidate(''), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_267008_get_sr", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # sr = ((1 - (rij)**2) / (n - 2))**0.5\n-- # return sr\n-- \n-- \nlocal function get_sr(rij, n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_267008_get_sr.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_sr\n    lu.assertEquals(candidate(1, 100), candidate(1, 10000))\n    lu.assertEquals(candidate(1, 100), 0.0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_267051_hook_with_extra_is_in_hooks", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for hook in hooks:\n-- #     if word.startswith('{}='.format(hook)):\n-- #         return True\n-- # return False\n-- \n-- Determine if the word given is the name of a valid hook, with extra data\n-- hanging off of it (e.g., `validhookname=extradata`).\n--    hook_with_extra_is_in_hooks(\n--      'validhookname=stuff',\n--      ['validhookname', 'other'])\n--    #=> True\n--    hook_with_extra_is_in_hooks(\n--      'invalidhookname=stuff',\n--      ['validhookname', 'other'])\n--    #=> False\n--    hook_with_extra_is_in_hooks(\n--      'validhookname',\n--      ['validhookname', 'other'])\n--    #=> False\nlocal function hook_with_extra_is_in_hooks(word, hooks)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_267051_hook_with_extra_is_in_hooks.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = hook_with_extra_is_in_hooks\n    lu.assertEquals(candidate('validhookname=other=stuff', {'validhookname', 'other', 'other=stuff'}), true)\n    lu.assertEquals(candidate('validhookname=stuff', {'validhookname', 'other'}), true)\n    lu.assertEquals(candidate('validhookname=other=', {'validhookname', 'other'}), true)\n    lu.assertEquals(candidate('validhookname=other=stuff', {'validhookname', 'other', 'other=', 'other=stuff'}), true)\n    lu.assertEquals(candidate('validhookname', {'validhookname', 'other'}), false)\n    lu.assertEquals(candidate('invalidhookname=stuff', {'validhookname', 'other'}), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_267064_format_time", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # date = time_string.split(\"T\")[0]\n-- # time = time_string.split(\"T\")[1]\n-- # # 2008-01-01 00:00:01\n-- # return date+\" \"+time.split(\".\")[0]\n-- \n--  Function to format the time according to Mysql syntax \nlocal function format_time(time_string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_267064_format_time.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = format_time\n    lu.assertEquals(candidate('2020-04-26T00:00:01.000Z'), '2020-04-26 00:00:01')\n    lu.assertEquals(candidate('2018-11-14T17:14:29.909Z'), '2018-11-14 17:14:29')\n    lu.assertEquals(candidate('2008-01-01T00:00:01.000123456'), '2008-01-01 00:00:01')\n    lu.assertEquals(candidate('2016-03-09T19:58:29.023000-05:00'), '2016-03-09 19:58:29')\n    lu.assertEquals(candidate('1900-01-01T00:00:00.000Z'), '1900-01-01 00:00:00')\n    lu.assertEquals(candidate('2008-01-01T00:00:00.000Z'), '2008-01-01 00:00:00')\n    lu.assertEquals(candidate('2016-03-10T12:06:29.023000-05:00'), '2016-03-10 12:06:29')\n    lu.assertEquals(candidate('2020-04-26T00:00:00.000Z'), '2020-04-26 00:00:00')\n    lu.assertEquals(candidate('2020-04-25T23:59:59.909Z'), '2020-04-25 23:59:59')\n    lu.assertEquals(candidate('2016-03-11T16:48:29.023000-05:00'), '2016-03-11 16:48:29')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_267473_subtract", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # x = coords1[0] - coords2[0]\n-- # y = coords1[1] - coords2[1]\n-- # z = coords1[2] - coords2[2]\n-- # return [x, y, z]\n-- \n-- Subtract one 3-dimensional point from another\n-- Parameters\n--     coords1: coordinates of form [x,y,z]\n--     coords2: coordinates of form [x,y,z]\n-- Returns\n--     list:  List of coordinates equal to coords1 - coords2 (list)\nlocal function subtract(coords1, coords2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_267473_subtract.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = subtract\n    lu.assertEquals(candidate({1, 2, 3}, {4, 5, 6}), {-3, -3, -3})\n    lu.assertEquals(candidate({20, 30, 40}, {10, 10, 10}), {10, 20, 30})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_267485_check_none", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return None if (v == 'None' or v == '') else v\n-- \n-- Return None if v is the empty string or the string 'None'.\nlocal function check_none(v)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_267485_check_none.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = check_none\n    lu.assertEquals(candidate('not none'), 'not none')\n    lu.assertEquals(candidate('None'), None)\n    lu.assertEquals(candidate('abc'), 'abc')\n    lu.assertEquals(candidate(''), None)\n    lu.assertEquals(candidate(None), None)\n    lu.assertEquals(candidate('some_string'), 'some_string')\n    lu.assertEquals(candidate('foo'), 'foo')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_268129_conflict", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return (row1 == row2 or  # same row\n-- #         col1 == col2 or  # same column\n-- #         row1 - col1 == row2 - col2 or  # same \\ diagonal\n-- #         row1 + col1 == row2 + col2)\n-- \n-- Would putting two queens in (row1, col1) and (row2, col2) conflict?\nlocal function conflict(row1, col1, row2, col2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_268129_conflict.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = conflict\n    lu.assertEquals(candidate(1, 1, 2, 1), true)\n    lu.assertEquals(candidate(0, 0, 2, 3), false)\n    lu.assertEquals(candidate(1, 2, 1, 2), true)\n    lu.assertEquals(candidate(0, 0, 8, 8), true)\n    lu.assertEquals(candidate(1, 1, 6, 6), true)\n    lu.assertEquals(candidate(0, 0, 7, 7), true)\n    lu.assertEquals(candidate(1, 1, 4, 4), true)\n    lu.assertEquals(candidate(0, 0, 9, 9), true)\n    lu.assertEquals(candidate(3, 4, 1, 2), true)\n    lu.assertEquals(candidate(0, 0, 5, 5), true)\n    lu.assertEquals(candidate(0, 0, 6, 6), true)\n    lu.assertEquals(candidate(0, 0, 0, 1), true)\n    lu.assertEquals(candidate(0, 0, 4, 4), true)\n    lu.assertEquals(candidate(0, 0, 2, 1), false)\n    lu.assertEquals(candidate(2, 2, 2, 2), true)\n    lu.assertEquals(candidate(0, 0, 3, 3), true)\n    lu.assertEquals(candidate(2, 2, 1, 2), true)\n    lu.assertEquals(candidate(2, 1, 1, 1), true)\n    lu.assertEquals(candidate(1, 1, 5, 5), true)\n    lu.assertEquals(candidate(1, 1, 3, 3), true)\n    lu.assertEquals(candidate(1, 1, 8, 8), true)\n    lu.assertEquals(candidate(2, 2, 1, 1), true)\n    lu.assertEquals(candidate(0, 0, 1, 0), true)\n    lu.assertEquals(candidate(1, 1, 2, 2), true)\n    lu.assertEquals(candidate(0, 0, 1, 2), false)\n    lu.assertEquals(candidate(0, 1, 2, 2), false)\n    lu.assertEquals(candidate(0, 1, 3, 2), false)\n    lu.assertEquals(candidate(1, 2, 2, 1), true)\n    lu.assertEquals(candidate(0, 0, 3, 2), false)\n    lu.assertEquals(candidate(1, 1, 1, 1), true)\n    lu.assertEquals(candidate(0, 0, 2, 2), true)\n    lu.assertEquals(candidate(1, 1, 7, 7), true)\n    lu.assertEquals(candidate(1, 2, 1, 1), true)\n    lu.assertEquals(candidate(2, 3, 3, 2), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_268186_csv_addition", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # chars = buf.split(\",\")\n-- # result = 0\n-- # for c in chars:\n-- #     result += int(c)\n-- # return result\n-- \n--     Convert a csv string into ints and then add them.\nlocal function csv_addition(buf)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_268186_csv_addition.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = csv_addition\n    lu.assertEquals(candidate('1,2,3'), 6)\n    lu.assertEquals(candidate('5,10'), 15)\n    lu.assertEquals(candidate('1,2'), 3)\n    lu.assertEquals(candidate('2,1'), 3)\n    lu.assertEquals(candidate('1'), 1)\n    lu.assertEquals(candidate('1,2,3'), 6)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_268347_is_before", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if one[0] < two[0] or (one[0] == two[0] and one[1] > two[1]):\n-- #     return True\n-- # return False\n-- \n-- return True if ones turn is before twos,\n-- where one, two = [time_spent, last_move_number]\nlocal function is_before(one, two)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_268347_is_before.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_before\n    lu.assertEquals(candidate({3, 3}, {3, 3}), false)\n    lu.assertEquals(candidate({1, 3}, {1, 1}), true)\n    lu.assertEquals(candidate({20, 200}, {20, 200}), false)\n    lu.assertEquals(candidate({20, 200}, {10, 100}), false)\n    lu.assertEquals(candidate({1, 2}, {1, 2}), false)\n    lu.assertEquals(candidate({3, 2}, {3, 1}), true)\n    lu.assertEquals(candidate({3, 1}, {3, 2}), false)\n    lu.assertEquals(candidate({10, 100}, {10, 100}), false)\n    lu.assertEquals(candidate({20, 200}, {10, 200}), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_268388_words", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # wordcount = {}\n-- # for word in word_statement.split():\n-- #     \"\"\"\n-- #     type cast the word to string if it is digit\n-- #     \"\"\"\n-- #     if word.isdigit():\n-- #         word = int(word)\n-- #     \"\"\"\n-- #     replace tabs an multilines with space since the are not counted\n-- #     \"\"\"\n-- #     word_statement = word_statement.replace(\"\\t\", \" \")\n-- #     word_statement = word_statement.replace(\"\\n\", \" \")\n-- #     if word not in wordcount:\n-- #         wordcount[word] = 1\n-- #     else:\n-- #         wordcount[word] += 1\n-- # return wordcount\n-- \n-- function that counts the number of word occurance in the input and return a dictionary\n-- the dictionary contains the word as the key and the total number of occurance as the value\nlocal function words(word_statement)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_268388_words.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = words\n    lu.assertEquals(candidate('hello world  hello world'), {['hello'] = 2, ['world'] = 2})\n    lu.assertEquals(candidate('Hello World!'), {['Hello'] = 1, ['World!'] = 1})\n    lu.assertEquals(candidate('This is a test'), {['This'] = 1, ['is'] = 1, ['a'] = 1, ['test'] = 1})\n    lu.assertEquals(candidate('hello world  hello world hello world'), {['hello'] = 3, ['world'] = 3})\n    lu.assertEquals(candidate('hello world'), {['hello'] = 1, ['world'] = 1})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_268775_update_sum_squares", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return (old_sum_squares + ((new_data - old_mean)*(new_data - new_mean)))\n-- \n-- Compute the update of sum of squares of differences from the current mean\n-- From the previously computed sum SUM_n-1, the new and old mean M_n and\n-- M_n-1 and the new measurement X_n, we compute an update value of the new\n-- sum of squares differences noted SUM_n using the formula:\n-- SUM_n = SUM_n-1 +(X_n - M_n)*(X_n - M_n-1)\n-- See: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n-- This SUM can be use to compute the variance and sample variance:\n-- Vn = SUM_n/n\n-- Sn = SUM_n/(n+1)\n-- This make the variance computation suffer less from floating point\n-- computation instabilities.\n-- Parameters\n-- ----------\n-- new_data: int or decimal\n--     The new measurement X_n\n-- old_sum_squares: int or decimal\n--     The sum of squares SUM_n-1 computed previously\n-- new_mean: int or decimal\n--     The mean M_n computed on the current step n\n-- old_mean: int or decimal\n--     The mean M_n-1 computed previously\n-- Returns\n-- -------\n-- float\n--     The new sum of squares SUM_n updated with X_n, SUM_n-1, M_n and M_n-1\nlocal function update_sum_squares(new_data, old_sum_squares, new_mean, old_mean)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_268775_update_sum_squares.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = update_sum_squares\n    lu.assertEquals(candidate(5, 4, 2, 2), 13)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_270511_transform_boolean", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if value.lower() in (\"true\", \"t\", \"1\"):\n-- #     return True\n-- # elif value.lower() in (\"false\", \"f\", \"0\"):\n-- #     return False\n-- # else:\n-- #     return None\n-- \n-- Transform boolean values that are blank into NULL so that they are not\n-- imported as empty strings.\nlocal function transform_boolean(value)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_270511_transform_boolean.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = transform_boolean\n    lu.assertEquals(candidate('F'), false)\n    lu.assertEquals(candidate(''), None)\n    lu.assertEquals(candidate('0   '), None)\n    lu.assertEquals(candidate('T'), true)\n    lu.assertEquals(candidate('True'), true)\n    lu.assertEquals(candidate('   '), None)\n    lu.assertEquals(candidate('0'), false)\n    lu.assertEquals(candidate('TRUE'), true)\n    lu.assertEquals(candidate('TRUE   '), None)\n    lu.assertEquals(candidate('1'), true)\n    lu.assertEquals(candidate('     '), None)\n    lu.assertEquals(candidate('f'), false)\n    lu.assertEquals(candidate('t'), true)\n    lu.assertEquals(candidate('FALSE   '), None)\n    lu.assertEquals(candidate('false'), false)\n    lu.assertEquals(candidate('False'), false)\n    lu.assertEquals(candidate(' '), None)\n    lu.assertEquals(candidate('FALSE'), false)\n    lu.assertEquals(candidate('true'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_271149_waiting_time", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # before = (timestamp // bus) * bus\n-- # return (before + bus) - timestamp\n-- \n-- Bus waiting time.\nlocal function waiting_time(timestamp, bus)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_271149_waiting_time.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = waiting_time\n    lu.assertEquals(candidate(0, 1), 1)\n    lu.assertEquals(candidate(100, 15), 5)\n    lu.assertEquals(candidate(10, 5), 5)\n    lu.assertEquals(candidate(11, 5), 4)\n    lu.assertEquals(candidate(939, 59), 5)\n    lu.assertEquals(candidate(10, 3), 2)\n    lu.assertEquals(candidate(14, 5), 1)\n    lu.assertEquals(candidate(12, 5), 3)\n    lu.assertEquals(candidate(13, 5), 2)\n    lu.assertEquals(candidate(11, 3), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_271792_make_content_dict", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # frequency_dict = dict()\n-- # for character in input_string:\n-- #     if character not in frequency_dict:\n-- #         frequency_dict[character] = 1\n-- #     else:\n-- #         frequency_dict[character] += 1\n-- # return frequency_dict\n-- \n-- Method that takes an input string and returns a dict with characters as keys and occurrences as values\nlocal function make_content_dict(input_string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_271792_make_content_dict.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = make_content_dict\n    lu.assertEquals(candidate('abc'), {['a'] = 1, ['b'] = 1, ['c'] = 1})\n    lu.assertEquals(candidate('abcde'), {['a'] = 1, ['b'] = 1, ['c'] = 1, ['d'] = 1, ['e'] = 1})\n    lu.assertEquals(candidate('abcb'), {['a'] = 1, ['b'] = 2, ['c'] = 1})\n    lu.assertEquals(candidate('a'), {['a'] = 1})\n    lu.assertEquals(candidate('aaaaaa'), {['a'] = 6})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_272497_count_saccades", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # saccade_count = 0\n-- # is_currently = False\n-- # for value in saccades:\n-- #     if value == 1 and is_currently == False:\n-- #         saccade_count += 1\n-- #         is_currently = True\n-- #     if value == 0 and is_currently == True:\n-- #         is_currently = False\n-- # return saccade_count\n-- \n-- A Function that counts the number of distinct saccades\n-- :param saccades:    a list with values which indicate if the move from the previos is a saccade.\n-- :return:            a number of indicating the amount of different saccades\nlocal function count_saccades(saccades)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_272497_count_saccades.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = count_saccades\n    lu.assertEquals(candidate({}), 0)\n    lu.assertEquals(candidate({1, 0, 0, 1, 1, 0, 0, 1, 0}), 3)\n    lu.assertEquals(candidate({0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_272652_invert_injective", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # inv = {}\n-- # for k in d:\n-- #     if d[k] in inv:\n-- #         raise RuntimeError('not an injective map')\n-- #     inv[d[k]] = k\n-- # return inv\n-- \n--  invert a one-to-one map d \nlocal function invert_injective(d)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_272652_invert_injective.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = invert_injective\n    lu.assertEquals(candidate({[1] = 1, [2] = 2}), {[1] = 1, [2] = 2})\n    lu.assertEquals(candidate(dict(zip({1, 2}, {2, 1}))), {[1] = 2, [2] = 1})\n    lu.assertEquals(candidate(candidate({[1] = 2, [2] = 1, [3] = 3})), {[1] = 2, [2] = 1, [3] = 3})\n    lu.assertEquals(candidate({[1] = 2, [2] = 1}), {[1] = 2, [2] = 1})\n    lu.assertEquals(candidate({[1] = 1, [2] = 2}), {[1] = 1, [2] = 2})\n    lu.assertEquals(candidate({}), {})\n    lu.assertEquals(candidate({[1] = 2, [3] = 4, [5] = 5, [6] = 6}), {[2] = 1, [4] = 3, [5] = 5, [6] = 6})\n    lu.assertEquals(candidate({[1] = 2, [3] = 4, [5] = 5}), {[2] = 1, [4] = 3, [5] = 5})\n    lu.assertEquals(candidate({[1] = 2, [3] = 4, [5] = 5, [6] = 6, [7] = 7}), {[2] = 1, [4] = 3, [5] = 5, [6] = 6, [7] = 7})\n    lu.assertEquals(candidate(dict()), dict())\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_273233_write_group_author", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if \"||\" in group_authors:\n-- #     groups = group_authors.split(\"||\")\n-- # else:\n-- #     groups = [group_authors]\n-- # group_authors_xml_snippet = \"\"\n-- # if group_authors != \"\":\n-- #     for one_group_author in groups:\n-- #         group_authors_xml_snippet += \"\"\"<v1:author>\n-- #         <v1:role>author</v1:role>\n-- #         <v1:groupAuthor>\"\"\" + one_group_author + \"\"\"</v1:groupAuthor>\n-- #     </v1:author>\n-- #     \"\"\"\n-- # return group_authors_xml_snippet\n-- \n-- Given a string with a variable length of group author, insert into XML snippet, and return XML snippet\n-- NOTE: Group author does not require a primary key/unique ID\n-- :param group_authors: A string containing 1+ group authors, with multiple authors separated by || double pipes\n-- :return: XML snippet with group authors\n-- >>> write_group_author(\"Beauxbatons||Durmstrang\")   #doctest: +NORMALIZE_WHITESPACE\n-- '<v1:author>\n--     <v1:role>author</v1:role>\n--     <v1:groupAuthor>Beauxbatons</v1:groupAuthor>\n-- </v1:author>\n-- <v1:author>\n--     <v1:role>author</v1:role>\n--     <v1:groupAuthor>Durmstrang</v1:groupAuthor>\n-- </v1:author>'\n-- >>> write_group_author(\"Hogwarts School\")\n-- '<v1:author>\n--         <v1:role>author</v1:role>\n--         <v1:groupAuthor>Hogwarts School</v1:groupAuthor>\n--     </v1:author>\n--     '\nlocal function write_group_author(group_authors)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_273233_write_group_author.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = write_group_author\n    lu.assertEquals(candidate('Hogwarts School'), '<v1:author>\\n            <v1:role>author</v1:role>\\n            <v1:groupAuthor>Hogwarts School</v1:groupAuthor>\\n        </v1:author>\\n        ')\n    lu.assertEquals(candidate('Durmstrang'), '<v1:author>\\n            <v1:role>author</v1:role>\\n            <v1:groupAuthor>Durmstrang</v1:groupAuthor>\\n        </v1:author>\\n        ')\n    lu.assertEquals(candidate('Hogwarts School'), '<v1:author>\\n            <v1:role>author</v1:role>\\n            <v1:groupAuthor>Hogwarts School</v1:groupAuthor>\\n        </v1:author>\\n        ')\n    lu.assertEquals(candidate('Beauxbatons||Durmstrang'), '<v1:author>\\n            <v1:role>author</v1:role>\\n            <v1:groupAuthor>Beauxbatons</v1:groupAuthor>\\n        </v1:author>\\n        <v1:author>\\n            <v1:role>author</v1:role>\\n            <v1:groupAuthor>Durmstrang</v1:groupAuthor>\\n        </v1:author>\\n        ')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_273455_calc_order", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return 2 * (gap_size * 1e-6) / (wavelength * 1e-10)\n-- \n-- Returns the FP interferential order.\n-- Parameters\n-- ----------\n-- wavelength (float):\n-- gap_size (float):\n-- Returns\n-- -------\n-- order (float)\nlocal function calc_order(wavelength, gap_size)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_273455_calc_order.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = calc_order\n    lu.assertEquals(candidate(0.8, 0.0), 0.0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_273577_calculate_polynomial", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # value = 0.0\n-- # for i in range(degree + 1):\n-- #     temp = coefficients[i]\n-- #     for j in range(i):\n-- #         temp *= (x - x_data[j])\n-- #     value += temp\n-- # return value\n-- \n-- Computes the value of the interpolating polynomial.\n-- Parameters\n-- ----------\n-- degree : int\n--     The degree of the interpolating polynomial.\n-- x_data : list\n--     The values that were used when calculating the c of the interpolating polynomial.\n-- coefficients : list\n--     The coefficients of the interpolating polynomial, constant term first.\n-- x : int\n--     The point at which the polynomial will be calculated\n-- Returns\n-- -------\n-- value : float\n--     The value of the interpolating polynomial at point x.\nlocal function calculate_polynomial(degree, x_data, coefficients, x)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_273577_calculate_polynomial.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = calculate_polynomial\n    lu.assertEquals(candidate(3, {0, 1, 2}, {1, 1, 1, 1, 1, 1}, 0), 1)\n    lu.assertEquals(candidate(3, {1, 3, 4}, {0, 1, 2, 3, 4, 5}, 1), 0)\n    lu.assertEquals(candidate(0, {1, 2, 3}, {1}, 2), 1)\n    lu.assertEquals(candidate(3, {0, 1, 2, 3}, {1, 4, 1, 1}, 0), 1)\n    lu.assertEquals(candidate(3, {0, 1, 2}, {1, 1, 1, 1, 1, 1}, 1), 2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_273749_norm_page_cnt", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if page == None or page < 1:\n-- #     page = 1\n-- # if max_number != None:\n-- #     if page > max_number:\n-- #         page = max_number\n-- # return page\n-- \n-- Normalize a integer (page).\n-- * Ensure that it is greater than Zero, and is not None.\n--     - If less than 1, or None, set it to 1\n-- * if max_number is None, then do not check for max_number\n--     * if greater than max_number, reset it to be max_number\nlocal function norm_page_cnt(page, max_number)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_273749_norm_page_cnt.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = norm_page_cnt\n    lu.assertEquals(candidate(0, 200), 1)\n    lu.assertEquals(candidate(10, 100), 10)\n    lu.assertEquals(candidate(2, 200), 2)\n    lu.assertEquals(candidate(20, 10), 10)\n    lu.assertEquals(candidate(3, 3), 3)\n    lu.assertEquals(candidate(100, 100), 100)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(-1, 10), 1)\n    lu.assertEquals(candidate(1000, 100), 100)\n    lu.assertEquals(candidate(100), 100)\n    lu.assertEquals(candidate(1, 100), 1)\n    lu.assertEquals(candidate(100000, 100), 100)\n    lu.assertEquals(candidate(-5), 1)\n    lu.assertEquals(candidate(-3), 1)\n    lu.assertEquals(candidate(0, 3), 1)\n    lu.assertEquals(candidate(200, 200), 200)\n    lu.assertEquals(candidate(100000, 99), 99)\n    lu.assertEquals(candidate(0), 1)\n    lu.assertEquals(candidate(1, 10), 1)\n    lu.assertEquals(candidate(None, 10), 1)\n    lu.assertEquals(candidate(None), 1)\n    lu.assertEquals(candidate(101, 101), 101)\n    lu.assertEquals(candidate(1, 3), 1)\n    lu.assertEquals(candidate(None, 200), 1)\n    lu.assertEquals(candidate(101, 100), 100)\n    lu.assertEquals(candidate(1, None), 1)\n    lu.assertEquals(candidate(1000, 99), 99)\n    lu.assertEquals(candidate(-3, 3), 1)\n    lu.assertEquals(candidate(-1), 1)\n    lu.assertEquals(candidate(100, 10), 10)\n    lu.assertEquals(candidate(-100), 1)\n    lu.assertEquals(candidate(2, 3), 2)\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(10, 99), 10)\n    lu.assertEquals(candidate(-5, 3), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_273807_make_node_pairs_along_route", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return list(zip(route[:-1], route[1:]))\n-- \n-- Converts a list of nodes into a list of tuples for indexing edges along a route.\nlocal function make_node_pairs_along_route(route)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_273807_make_node_pairs_along_route.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = make_node_pairs_along_route\n    lu.assertEquals(candidate(list('abcde')), {{'a', 'b'}, {'b', 'c'}, {'c', 'd'}, {'d', 'e'}})\n    lu.assertEquals(candidate(list('')), {})\n    lu.assertEquals(candidate(list('abc')), {{'a', 'b'}, {'b', 'c'}})\n    lu.assertEquals(candidate(list('abcdef')), {{'a', 'b'}, {'b', 'c'}, {'c', 'd'}, {'d', 'e'}, {'e', 'f'}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_274415_str2bool", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if val is None:\n-- #     return False\n-- # val = val.lower().strip()\n-- # if val in ['true', 't', 'yes', 'y', '1', 'on']:\n-- #     return True\n-- # elif val in ['false', 'f', 'no', 'n', '0', 'off']:\n-- #     return False\n-- \n--     Helper method to convert string to bool\nlocal function str2bool(val)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_274415_str2bool.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = str2bool\n    lu.assertEquals(candidate('False'), false)\n    lu.assertEquals(candidate('tRuE'), true)\n    lu.assertEquals(candidate('True'), true)\n    lu.assertEquals(candidate('FalSe'), false)\n    lu.assertEquals(candidate('oN'), true)\n    lu.assertEquals(candidate('y'), true)\n    lu.assertEquals(candidate(None), false)\n    lu.assertEquals(candidate('n'), false)\n    lu.assertEquals(candidate('yes'), true)\n    lu.assertEquals(candidate('oFf'), false)\n    lu.assertEquals(candidate('on'), true)\n    lu.assertEquals(candidate('false'), false)\n    lu.assertEquals(candidate('true'), true)\n    lu.assertEquals(candidate('off'), false)\n    lu.assertEquals(candidate('FALSE'), false)\n    lu.assertEquals(candidate('no'), false)\n    lu.assertEquals(candidate('1'), true)\n    lu.assertEquals(candidate('t'), true)\n    lu.assertEquals(candidate('f'), false)\n    lu.assertEquals(candidate('TRUE'), true)\n    lu.assertEquals(candidate('0'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_274793_pow", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # import builtins  # pylint: disable=import-outside-toplevel\n-- # return builtins.pow(base, exp, mod)\n-- \n-- Efficiently exponentiates an integer :math:`a^k (\\textrm{mod}\\ m)`.\n-- The algorithm is more efficient than exponentiating first and then reducing modulo :math:`m`. This\n-- is the integer equivalent of :func:`galois.poly_pow`.\n-- Note\n-- ----\n-- This function is an alias of :func:`pow` in the standard library.\n-- Parameters\n-- ----------\n-- base : int\n--     The integer base :math:`a`.\n-- exp : int\n--     The integer exponent :math:`k`.\n-- mod : int\n--     The integer modulus :math:`m`.\n-- Returns\n-- -------\n-- int\n--     The modular exponentiation :math:`a^k (\\textrm{mod}\\ m)`.\n-- Examples\n-- --------\n-- .. ipython:: python\n--     galois.pow(3, 5, 7)\n--     (3**5) % 7\nlocal function pow(base, exp, mod)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_274793_pow.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = pow\n    lu.assertEquals(candidate(0, 20, 1), 0)\n    lu.assertEquals(candidate(1, 1, 5), 1)\n    lu.assertEquals(candidate(1, 1, 3), 1)\n    lu.assertEquals(candidate(1, 1, 7), 1)\n    lu.assertEquals(candidate(0, 2, 1), 0)\n    lu.assertEquals(candidate(0, 9, 1), 0)\n    lu.assertEquals(candidate(0, 4, 7), 0)\n    lu.assertEquals(candidate(1, 0, 5), 1)\n    lu.assertEquals(candidate(0, 8, 1), 0)\n    lu.assertEquals(candidate(0, 7, 1), 0)\n    lu.assertEquals(candidate(0, 15, 1), 0)\n    lu.assertEquals(candidate(0, 1, 2), 0)\n    lu.assertEquals(candidate(0, 16, 1), 0)\n    lu.assertEquals(candidate(0, 6, 2), 0)\n    lu.assertEquals(candidate(0, 3, 2), 0)\n    lu.assertEquals(candidate(1, 1, 4), 1)\n    lu.assertEquals(candidate(1, 0, 9), 1)\n    lu.assertEquals(candidate(0, 18, 1), 0)\n    lu.assertEquals(candidate(0, 6, 1), 0)\n    lu.assertEquals(candidate(0, 14, 1), 0)\n    lu.assertEquals(candidate(1, 0, 2), 1)\n    lu.assertEquals(candidate(0, 4, 1), 0)\n    lu.assertEquals(candidate(0, 1, 1), 0)\n    lu.assertEquals(candidate(0, 12, 1), 0)\n    lu.assertEquals(candidate(0, 2, 2), 0)\n    lu.assertEquals(candidate(0, 3, 7), 0)\n    lu.assertEquals(candidate(0, 7, 7), 0)\n    lu.assertEquals(candidate(1, 0, 4), 1)\n    lu.assertEquals(candidate(1, 0, 7), 1)\n    lu.assertEquals(candidate(0, 5, 1), 0)\n    lu.assertEquals(candidate(0, 5, 7), 0)\n    lu.assertEquals(candidate(1, 1, 10), 1)\n    lu.assertEquals(candidate(0, 13, 1), 0)\n    lu.assertEquals(candidate(0, 3, 1), 0)\n    lu.assertEquals(candidate(0, 10, 1), 0)\n    lu.assertEquals(candidate(0, 32, 1), 0)\n    lu.assertEquals(candidate(0, 2, 7), 0)\n    lu.assertEquals(candidate(0, 4, 2), 0)\n    lu.assertEquals(candidate(0, 63, 1), 0)\n    lu.assertEquals(candidate(1, 0, 6), 1)\n    lu.assertEquals(candidate(0, 8, 2), 0)\n    lu.assertEquals(candidate(0, 11, 1), 0)\n    lu.assertEquals(candidate(1, 1, 8), 1)\n    lu.assertEquals(candidate(1, 0, 12), 1)\n    lu.assertEquals(candidate(1, 1, 6), 1)\n    lu.assertEquals(candidate(1, 0, 8), 1)\n    lu.assertEquals(candidate(1, 1, 2), 1)\n    lu.assertEquals(candidate(1, 0, 10), 1)\n    lu.assertEquals(candidate(0, 1, 7), 0)\n    lu.assertEquals(candidate(1, 0, 3), 1)\n    lu.assertEquals(candidate(0, 6, 7), 0)\n    lu.assertEquals(candidate(1, 1, 9), 1)\n    lu.assertEquals(candidate(1, 0, 11), 1)\n    lu.assertEquals(candidate(0, 33, 1), 0)\n    lu.assertEquals(candidate(1, 1, 11), 1)\n    lu.assertEquals(candidate(0, 5, 2), 0)\n    lu.assertEquals(candidate(0, 31, 1), 0)\n    lu.assertEquals(candidate(0, 8, 7), 0)\n    lu.assertEquals(candidate(1, 0, 13), 1)\n    lu.assertEquals(candidate(0, 19, 1), 0)\n    lu.assertEquals(candidate(1, 1, 12), 1)\n    lu.assertEquals(candidate(2, 3, 10), 8)\n    lu.assertEquals(candidate(1, 1, 13), 1)\n    lu.assertEquals(candidate(0, 7, 2), 0)\n    lu.assertEquals(candidate(0, 17, 1), 0)\n    lu.assertEquals(candidate(0, 0, 1), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_274795_column_index_to_integer", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Convert column index e.g. 'A', 'BZ' etc to\n-- # # integer equivalent\n-- # idx = 0\n-- # i = 0\n-- # for c in col[::-1]:\n-- #     idx = idx + pow(26, i)*(ord(c)-64)\n-- #     i += 1\n-- # return idx-1\n-- \n-- Convert XLS-style column index into equivalent integer\n-- Given a column index e.g. 'A', 'BZ' etc, converts it\n-- to the integer equivalent using zero-based counting\n-- system (so 'A' is equivalent to zero, 'B' to 1 etc).\nlocal function column_index_to_integer(col)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_274795_column_index_to_integer.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = column_index_to_integer\n    lu.assertEquals(candidate('A'), 0)\n    lu.assertEquals(candidate('AA'), 26)\n    lu.assertEquals(candidate('Z'), 25)\n    lu.assertEquals(candidate('BZ'), 77)\n    lu.assertEquals(candidate('AZ'), 51)\n    lu.assertEquals(candidate('BA'), 52)\n    lu.assertEquals(candidate('AB'), 27)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_274982_NormalizeTargetPath", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not target:\n-- #     return target\n-- # target = target.strip()\n-- # target = target.rstrip(\"/\")\n-- # if not target:\n-- #     return target\n-- # if target[0] != \"/\":\n-- #     target = \"/{}\".format(target)\n-- # return target\n-- \n-- Normalizes the target path.\n-- Adds leading slash if needed, strips ending slashes.\n-- Args:\n--   target: The target path (fusion db publish point).\n-- Returns:\n--   Normalized target path.\nlocal function NormalizeTargetPath(target)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_274982_NormalizeTargetPath.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = NormalizeTargetPath\n    lu.assertEquals(candidate('  test  '), '/test')\n    lu.assertEquals(candidate('/test/'), '/test')\n    lu.assertEquals(candidate('  test'), '/test')\n    lu.assertEquals(candidate(None), None)\n    lu.assertEquals(candidate(' '), '')\n    lu.assertEquals(candidate('test/'), '/test')\n    lu.assertEquals(candidate('test'), '/test')\n    lu.assertEquals(candidate('/foo'), '/foo')\n    lu.assertEquals(candidate('/foo/'), '/foo')\n    lu.assertEquals(candidate('foo'), '/foo')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('/test'), '/test')\n    lu.assertEquals(candidate('foo/'), '/foo')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_276253_sub_dir_source", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"_\".join([\n-- #     '-'.join(['sub', d['sub'], ]),\n-- #     '-'.join(['hem', d['hem'], ]),\n-- #     '-'.join(['samp', d['samp'], ]),\n-- #     '-'.join(['prob', d['prob'], ]),\n-- # ])\n-- \n--  build out the source portion of the directory structure.\n-- :param dict d: A dictionary holding BIDS terms for path-building\nlocal function sub_dir_source(d)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_276253_sub_dir_source.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sub_dir_source\n    lu.assertEquals(candidate({['sub'] = '02', ['hem'] = 'L', ['samp'] = '1', ['prob'] = '1'}), 'sub-02_hem-L_samp-1_prob-1')\n    lu.assertEquals(candidate({['sub'] = '01', ['hem'] = 'R', ['samp'] = '1', ['prob'] = '1'}), 'sub-01_hem-R_samp-1_prob-1')\n    lu.assertEquals(candidate({['sub'] = '01', ['hem'] = 'L', ['samp'] = '2', ['prob'] = '2'}), 'sub-01_hem-L_samp-2_prob-2')\n    lu.assertEquals(candidate({['sub'] = '01', ['hem'] = 'R', ['samp'] = '2', ['prob'] = '2'}), 'sub-01_hem-R_samp-2_prob-2')\n    lu.assertEquals(candidate({['sub'] = '02', ['hem'] = 'L', ['samp'] = '2', ['prob'] = '2'}), 'sub-02_hem-L_samp-2_prob-2')\n    lu.assertEquals(candidate({['sub'] = '01', ['hem'] = 'L', ['samp'] = '1', ['prob'] = '1'}), 'sub-01_hem-L_samp-1_prob-1')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_276589_make_key", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return f'{x}, {y}'\n-- \n--     function to combine two coordinates into a valid dict key\nlocal function make_key(x, y)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_276589_make_key.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = make_key\n    lu.assertEquals(candidate(10, 10), '10, 10')\n    lu.assertEquals(candidate(1, 1), '1, 1')\n    lu.assertEquals(candidate(1, 0), '1, 0')\n    lu.assertEquals(candidate(11, 12), '11, 12')\n    lu.assertEquals(candidate(3, 2), '3, 2')\n    lu.assertEquals(candidate(1, 3), '1, 3')\n    lu.assertEquals(candidate(-3, 3), '-3, 3')\n    lu.assertEquals(candidate(50, 50), '50, 50')\n    lu.assertEquals(candidate(0, 1), '0, 1')\n    lu.assertEquals(candidate(-1, -1), '-1, -1')\n    lu.assertEquals(candidate(0, 0), '0, 0')\n    lu.assertEquals(candidate(3, 3), '3, 3')\n    lu.assertEquals(candidate(1, 3), '1, 3')\n    lu.assertEquals(candidate(10, 10), '10, 10')\n    lu.assertEquals(candidate(-20, 20), '-20, 20')\n    lu.assertEquals(candidate(1, 2), '1, 2')\n    lu.assertEquals(candidate(10, 1), '10, 1')\n    lu.assertEquals(candidate(3, 4), '3, 4')\n    lu.assertEquals(candidate(0, 0), '0, 0')\n    lu.assertEquals(candidate(2, 3), '2, 3')\n    lu.assertEquals(candidate(-1, -1), '-1, -1')\n    lu.assertEquals(candidate(1, 0), '1, 0')\n    lu.assertEquals(candidate(1, 0), '1, 0')\n    lu.assertEquals(candidate(-10000, 10000), '-10000, 10000')\n    lu.assertEquals(candidate(-10000, -10000), '-10000, -10000')\n    lu.assertEquals(candidate(-3, -3), '-3, -3')\n    lu.assertEquals(candidate(1, 1), '1, 1')\n    lu.assertEquals(candidate(2, 1), '2, 1')\n    lu.assertEquals(candidate(0, 1), '0, 1')\n    lu.assertEquals(candidate(10000, -10000), '10000, -10000')\n    lu.assertEquals(candidate(0, 0), '0, 0')\n    lu.assertEquals(candidate(2, 1), '2, 1')\n    lu.assertEquals(candidate(3, -3), '3, -3')\n    lu.assertEquals(candidate(3, 4), '3, 4')\n    lu.assertEquals(candidate(10000, 10000), '10000, 10000')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_276945_floatToString5", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return f\"{f:.5f}\".rstrip(\"0\").rstrip(\".\")\n-- \n-- Return float f as a string with five decimal places without trailing zeros\n-- and dot.\n-- Intended for places where five decimals are needed, e.g. transformations.\nlocal function floatToString5(f)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_276945_floatToString5.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = floatToString5\n    lu.assertEquals(candidate(3.141592653589793), '3.14159')\n    lu.assertEquals(candidate(3.141592653589793), '3.14159')\n    lu.assertEquals(candidate(-1.12345), '-1.12345')\n    lu.assertEquals(candidate(0.000123456789), '0.00012')\n    lu.assertEquals(candidate(12.3456789), '12.34568')\n    lu.assertEquals(candidate(1.23456789), '1.23457')\n    lu.assertEquals(candidate(-1.2345e-05), '-0.00001')\n    lu.assertEquals(candidate(123.45678), '123.45678')\n    lu.assertEquals(candidate(1.12345), '1.12345')\n    lu.assertEquals(candidate(-1.12345), '-1.12345')\n    lu.assertEquals(candidate(1.2345e-05), '0.00001')\n    lu.assertEquals(candidate(1.2345e-05), '0.00001')\n    lu.assertEquals(candidate(1.2345e-05), '0.00001')\n    lu.assertEquals(candidate(0), '0')\n    lu.assertEquals(candidate(0.00012345678900001), '0.00012')\n    lu.assertEquals(candidate(1.12345), '1.12345')\n    lu.assertEquals(candidate(123.456789), '123.45679')\n    lu.assertEquals(candidate(1), '1')\n    lu.assertEquals(candidate(-1.2345e-05), '-0.00001')\n    lu.assertEquals(candidate(-1.0), '-1')\n    lu.assertEquals(candidate(-1.12345), '-1.12345')\n    lu.assertEquals(candidate(-1), '-1')\n    lu.assertEquals(candidate(1.0), '1')\n    lu.assertEquals(candidate(0.0), '0')\n    lu.assertEquals(candidate(1.12345), '1.12345')\n    lu.assertEquals(candidate(0), '0')\n    lu.assertEquals(candidate(1234.56789), '1234.56789')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_277039_urlify", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not length:\n-- #     return None\n-- # chars = []\n-- # last_char = None\n-- # for index, char in enumerate(string):\n-- #     if not index <= length:\n-- #         continue\n-- #     if last_char == '%20' and char == ' ':\n-- #         continue\n-- #     if char == ' ':\n-- #         char = '%20'\n-- #     last_char = char\n-- #     chars.append(char)\n-- # return ''.join(chars)\n-- \n-- Question 3: Write a method to replace all spaces in a string with '%20'. You may\n-- assume that the string has suffcient space at the end to hold the additional characters,\n-- and that you are given the \"true\" length of the string.\n-- In python I can use the replace method. string.replace(' ', '%20')\nlocal function urlify(string, length)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_277039_urlify.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = urlify\n    lu.assertEquals(candidate('abc', 10), 'abc')\n    lu.assertEquals(candidate('Hello, world', 4), 'Hello')\n    lu.assertEquals(candidate('a bc', 4), 'a%20bc')\n    lu.assertEquals(candidate('abc  def', 10), 'abc%20def')\n    lu.assertEquals(candidate('hello', 5), 'hello')\n    lu.assertEquals(candidate('abc def ghi', 1000), 'abc%20def%20ghi')\n    lu.assertEquals(candidate(None, 0), None)\n    lu.assertEquals(candidate('abc def ghi', 10), 'abc%20def%20ghi')\n    lu.assertEquals(candidate('a b', 2), 'a%20b')\n    lu.assertEquals(candidate('abcdefg', 6), 'abcdefg')\n    lu.assertEquals(candidate('a', 1), 'a')\n    lu.assertEquals(candidate('a bc', 5), 'a%20bc')\n    lu.assertEquals(candidate('abcdefg ', 8), 'abcdefg%20')\n    lu.assertEquals(candidate('a b', 4), 'a%20b')\n    lu.assertEquals(candidate('abcdefg', 0), None)\n    lu.assertEquals(candidate('abc defg ', 8), 'abc%20defg%20')\n    lu.assertEquals(candidate('a b', 3), 'a%20b')\n    lu.assertEquals(candidate(' ', 1), '%20')\n    lu.assertEquals(candidate('hello world', 11), 'hello%20world')\n    lu.assertEquals(candidate('hello world', 13), 'hello%20world')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_277349_pingpong", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # def contains_eight(number):\n-- #     if number <= 10:\n-- #         if number == 8:\n-- #             return True\n-- #         return False\n-- #     return contains_eight(number // 10) or contains_eight(number % 10)\n-- # def is_multiple_of_eight(number):\n-- #     return True if number % 8 == 0 else False\n-- # def counting(n, iteration, end_value, direction):\n-- #     if iteration == n:\n-- #         return end_value\n-- #     else:\n-- #         if is_multiple_of_eight(iteration) or contains_eight(iteration):\n-- #             return counting(n, iteration + 1, end_value - direction, -direction)\n-- #         else:\n-- #             return counting(n, iteration + 1, end_value + direction, direction)\n-- # return counting(n, 1, 1, 1)\n-- \n-- Return the nth element of the ping-pong sequence.\n-- >>> pingpong(8)\n-- 8\n-- >>> pingpong(10)\n-- 6\n-- >>> pingpong(15)\n-- 1\n-- >>> pingpong(21)\n-- -1\n-- >>> pingpong(22)\n-- -2\n-- >>> pingpong(30)\n-- -2\n-- >>> pingpong(68)\n-- 0\n-- >>> pingpong(69)\n-- -1\n-- >>> pingpong(80)\n-- 0\n-- >>> pingpong(81)\n-- 1\n-- >>> pingpong(82)\n-- 0\n-- >>> pingpong(100)\n-- -6\n-- >>> from construct_check import check\n-- >>> # ban assignment statements\n-- >>> check(HW_SOURCE_FILE, 'pingpong',\n-- ...       ['Assign', 'AnnAssign', 'AugAssign', 'NamedExpr'])\n-- True\nlocal function pingpong(n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_277349_pingpong.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = pingpong\n    lu.assertEquals(candidate(100), -6)\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(81), 1)\n    lu.assertEquals(candidate(14), 2)\n    lu.assertEquals(candidate(19), 1)\n    lu.assertEquals(candidate(69), -1)\n    lu.assertEquals(candidate(21), -1)\n    lu.assertEquals(candidate(22), -2)\n    lu.assertEquals(candidate(10), 6)\n    lu.assertEquals(candidate(17), 1)\n    lu.assertEquals(candidate(80), 0)\n    lu.assertEquals(candidate(15), 1)\n    lu.assertEquals(candidate(18), 2)\n    lu.assertEquals(candidate(82), 0)\n    lu.assertEquals(candidate(68), 0)\n    lu.assertEquals(candidate(8), 8)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(30), -2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_277573_covers_alphabet", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # chars = set(''.join(e for e in sentence.lower() if e.isalpha()))\n-- # return len(chars) == 26\n-- # # return set(s.lower()) >= set(\"abcdefghijk...\")\n-- \n-- This function takes a string and returns if the given string contains all the alphabets\nlocal function covers_alphabet(sentence)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_277573_covers_alphabet.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = covers_alphabet\n    lu.assertEquals(candidate('Hello World'), false)\n    lu.assertEquals(candidate('1234567890'), false)\n    lu.assertEquals(candidate('This is a sentence. This is another sentence.'), false)\n    lu.assertEquals(candidate('abc def'), false)\n    lu.assertEquals(candidate('abcdefghijklmnopqrstuvwxyz'), true)\n    lu.assertEquals(candidate('The quick brown fox jumps over the lazy dog.'), true)\n    lu.assertEquals(candidate('abc def ghi'), false)\n    lu.assertEquals(candidate('This is a sentence.'), false)\n    lu.assertEquals(candidate('abc def ghi jkl'), false)\n    lu.assertEquals(candidate('The narwhal bacons at midnight.'), false)\n    lu.assertEquals(candidate('abc de'), false)\n    lu.assertEquals(candidate('ABCDEFGHIJKLMNOPQRSTUVWXYZ'), true)\n    lu.assertEquals(candidate('The quick brown fox jumps over the lazy eog.'), false)\n    lu.assertEquals(candidate('a'), false)\n    lu.assertEquals(candidate('abc'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_278103_check_parens", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # counter = 0\n-- # for i in range(len(str)):\n-- #     # if loop encounters (, counter is incremented\n-- #     if str[i] == '(':\n-- #         counter += 1\n-- #     # else if loop encounters ), decrement counter\n-- #     elif str[i] == ')':\n-- #         if counter == 0:\n-- #             return -1\n-- #         else:\n-- #             counter -= 1\n-- # # after going through str looking for parentheses, if counter still\n-- # # positive, we have too many (, but if counter 0 we have balanced parens.\n-- # if counter > 0:\n-- #     return 1\n-- # elif counter == 0:\n-- #     return 0\n-- \n-- check_parens takes a string and:\n-- returns 0 if the number of parentheses is balanced and matched.\n-- returns 1 if more left parentheses than right.\n-- returns -1 if string has broken (unmatched) parentheses.\nlocal function check_parens(str)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_278103_check_parens.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = check_parens\n    lu.assertEquals(candidate('())(())'), -1)\n    lu.assertEquals(candidate('((()))'), 0)\n    lu.assertEquals(candidate('(()()))'), -1)\n    lu.assertEquals(candidate('()'), 0)\n    lu.assertEquals(candidate(')'), -1)\n    lu.assertEquals(candidate('(()()())'), 0)\n    lu.assertEquals(candidate('(()'), 1)\n    lu.assertEquals(candidate('((())'), 1)\n    lu.assertEquals(candidate('()()()()'), 0)\n    lu.assertEquals(candidate('((()())(())())'), 0)\n    lu.assertEquals(candidate(')('), -1)\n    lu.assertEquals(candidate(')()('), -1)\n    lu.assertEquals(candidate('())'), -1)\n    lu.assertEquals(candidate('()()())'), -1)\n    lu.assertEquals(candidate('()()()'), 0)\n    lu.assertEquals(candidate('())'), -1)\n    lu.assertEquals(candidate('('), 1)\n    lu.assertEquals(candidate('(())'), 0)\n    lu.assertEquals(candidate(''), 0)\n    lu.assertEquals(candidate('(()())'), 0)\n    lu.assertEquals(candidate(')()(())'), -1)\n    lu.assertEquals(candidate('(()))'), -1)\n    lu.assertEquals(candidate('()'), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_278605_convert_mwh_bbtu", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # bbtu = value * 0.003412\n-- # return bbtu\n-- \n-- converts energy in MWh to energy in billion btu.\n-- :param value:           value in megawatt-hours of energy\n-- :type value:            float\n-- :return:                value in bbtu\nlocal function convert_mwh_bbtu(value)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_278605_convert_mwh_bbtu.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = convert_mwh_bbtu\n    lu.assertEquals(candidate(0), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_279010_prepare_url", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Parameter - IP\n-- # url = \"http://\" + valip\n-- # # Parameter - URL - action\n-- # url += \"/api/v100/dali_devices.ssi?action=get\"\n-- # # Parameter - Channel\n-- # url += \"&ch=\" + valch\n-- # return url\n-- \n--     Prepare the URL\nlocal function prepare_url(valip, valch)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_279010_prepare_url.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = prepare_url\n    lu.assertEquals(candidate('1.1.1.1', 'ch1'), 'http://1.1.1.1/api/v100/dali_devices.ssi?action=get&ch=ch1')\n    lu.assertEquals(candidate('1', '1'), 'http://1/api/v100/dali_devices.ssi?action=get&ch=1')\n    lu.assertEquals(candidate('1.2.3.4', '1'), 'http://1.2.3.4/api/v100/dali_devices.ssi?action=get&ch=1')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_279176_check_port", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # \"\"\"port value == None, means -1 or any\"\"\"\n-- # if str(port) == 'None':\n-- #     return '-1'\n-- # else:\n-- #     return port\n-- \n-- return port value\nlocal function check_port(port)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_279176_check_port.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = check_port\n    lu.assertEquals(candidate('None'), '-1')\n    lu.assertEquals(candidate('12345'), '12345')\n    lu.assertEquals(candidate('1234'), '1234')\n    lu.assertEquals(candidate('None'), '-1')\n    lu.assertEquals(candidate('1'), '1')\n    lu.assertEquals(candidate(None), '-1')\n    lu.assertEquals(candidate('-1'), '-1')\n    lu.assertEquals(candidate('0'), '0')\n    lu.assertEquals(candidate('8080'), '8080')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_279297_get_best_indexes", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # indices = sorted(range(len(logits)), key=logits.__getitem__, reverse=True)\n-- # return indices[:n_best_size]\n-- \n-- Gets the indices of the n-best logits from a list.\nlocal function get_best_indexes(logits, n_best_size)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_279297_get_best_indexes.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_best_indexes\n    lu.assertEquals(candidate({0.1, 0.2}, 0), {})\n    lu.assertEquals(candidate({0.1, 0.3, 0.2}, 2), {1, 2})\n    lu.assertEquals(candidate({}, 1), {})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_279313_next_question_id", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # index = next_ids.get(id_base, 1)\n-- # next_ids[id_base] = index + 1\n-- # return \"{}-{}\".format(index, id_base)\n-- \n-- Incrementally fetches the next question ID based on the base passage ID.\n-- Some questions have the same ID in the RACE dataset (if they are\n-- in the same file). We try to make those unique by appending an\n-- index before the id. @q_ids is used to keep the counter for each\n-- question ID - it is essentially a map from the file name to the count.\n-- It will generate ids as follows:\n-- 1) 1-middle1548.txt\n-- 2) 2-middle1548.txt\n-- 3) 3-middle1548.txt\n-- 4) ...\n-- Use this function to get incremental question IDs.\nlocal function next_question_id(next_ids, id_base)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_279313_next_question_id.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = next_question_id\n    lu.assertEquals(candidate({['0-1-1548.txt'] = 1, ['0-2-1548.txt'] = 1}, '0-2-1548.txt'), '1-0-2-1548.txt')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_279421_hex_to_long", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return int(hex_string, 16)\n-- \n-- Convert hex to long.\nlocal function hex_to_long(hex_string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_279421_hex_to_long.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = hex_to_long\n    lu.assertEquals(candidate('0x4141'), 16705)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_279964_enable_option", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return 'ON' if value else 'OFF'\n-- \n-- Converts a boolean option to a CMake ON/OFF switch\nlocal function enable_option(value)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_279964_enable_option.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = enable_option\n    lu.assertEquals(candidate(true), 'ON')\n    lu.assertEquals(candidate(false), 'OFF')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_280016__get_by_path_kw", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # kw = {}\n-- # key = 'short_name'\n-- # for short_name in reversed(pathlist):\n-- #     kw[key] = short_name\n-- #     key = 'parentnode__' + key\n-- # return kw\n-- \n--  Used by :meth:`get_by_path` to create the required kwargs for\n-- Node.objects.get(). Might be a starting point for more sophisticated\n-- queries including paths. Example::\n--     ifi = Node.objects.get(**Node._get_by_path_kw(['uio', 'ifi']))\n-- :param pathlist: A list of node-names, like ``['uio', 'ifi']``.\nlocal function _get_by_path_kw(pathlist)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_280016__get_by_path_kw.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _get_by_path_kw\n    lu.assertEquals(candidate({'uio', 'ifi', 'studentservices', 'exams'}), {['short_name'] = 'exams', ['parentnode__short_name'] = 'studentservices', ['parentnode__parentnode__short_name'] = 'ifi', ['parentnode__parentnode__parentnode__short_name'] = 'uio'})\n    lu.assertEquals(candidate({'uio', 'ifi', 'studentservices', 'exams', 'test', 'blabla'}), {['short_name'] = 'blabla', ['parentnode__short_name'] = 'test', ['parentnode__parentnode__short_name'] = 'exams', ['parentnode__parentnode__parentnode__short_name'] = 'studentservices', ['parentnode__parentnode__parentnode__parentnode__short_name'] = 'ifi', ['parentnode__parentnode__parentnode__parentnode__parentnode__short_name'] = 'uio'})\n    lu.assertEquals(candidate({'uio', 'ifi', 'studentservices', 'exams', 'test'}), {['short_name'] = 'test', ['parentnode__short_name'] = 'exams', ['parentnode__parentnode__short_name'] = 'studentservices', ['parentnode__parentnode__parentnode__short_name'] = 'ifi', ['parentnode__parentnode__parentnode__parentnode__short_name'] = 'uio'})\n    lu.assertEquals(candidate({'uio', 'ifi', 'ifiok', 'test'}), {['short_name'] = 'test', ['parentnode__short_name'] = 'ifiok', ['parentnode__parentnode__short_name'] = 'ifi', ['parentnode__parentnode__parentnode__short_name'] = 'uio'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_280991_str_to_dict", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # output_dict = {}\n-- # pairs = string[1:-1].split(\", \")\n-- # for i in pairs:\n-- #     elements = i.split(\": \")\n-- #     output_dict[int(elements[0])] = int(elements[1])\n-- # return output_dict\n-- \n-- Input example: \"{0: 214, 1: 224}\".\nlocal function str_to_dict(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_280991_str_to_dict.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = str_to_dict\n    lu.assertEquals(candidate('{0: 1, 1: 2}'), {[0] = 1, [1] = 2})\n    lu.assertEquals(candidate('{0: 214, 1: 224, 2: 253, 3: 254, 4: 255}'), {[0] = 214, [1] = 224, [2] = 253, [3] = 254, [4] = 255})\n    lu.assertEquals(candidate('{0: 214, 1: 224}'), {[0] = 214, [1] = 224})\n    lu.assertEquals(candidate('{0: 1, 1: 2, 2: 3}'), {[0] = 1, [1] = 2, [2] = 3})\n    lu.assertEquals(candidate('{0: 1}'), {[0] = 1})\n    lu.assertEquals(candidate('{0: 1, 1: 2, 2: 3, 3: 4}'), {[0] = 1, [1] = 2, [2] = 3, [3] = 4})\n    lu.assertEquals(candidate('{1: 1, 2: 2}'), {[1] = 1, [2] = 2})\n    lu.assertEquals(candidate('{0: 214, 2: 253, 4: 255}'), {[0] = 214, [2] = 253, [4] = 255})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_282648_get_keywords_prefix", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if model == \"cyclerank\" or model == \"cyclerank_pageviews\":\n-- #     return \"keywords_cyclerank\"\n-- # elif model == \"pagerank\" or model == \"pagerank_pageviews\":\n-- #     return \"keywords_pagerank\"\n-- # else:\n-- #     return \"keywords\"\n-- \n-- Return the correct keyword's file prefix given the model\n-- :param model: name of the model\n-- :return: keyword's file prefix\nlocal function get_keywords_prefix(model)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_282648_get_keywords_prefix.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_keywords_prefix\n    lu.assertEquals(candidate('cyclerank'), 'keywords_cyclerank')\n    lu.assertEquals(candidate('pagerank_pageviews'), 'keywords_pagerank')\n    lu.assertEquals(candidate('pagerank'), 'keywords_pagerank')\n    lu.assertEquals(candidate('cyclerank_pageviews'), 'keywords_cyclerank')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_282664_make_anagram_1", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # count_dict = {}\n-- # for n in a:\n-- #     if n in count_dict.keys():\n-- #         count_dict[n] += 1\n-- #     else:\n-- #         count_dict[n] = 1\n-- # for n in b:\n-- #     if n in count_dict.keys():\n-- #         count_dict[n] -= 1\n-- #     else:\n-- #         count_dict[n] = -1\n-- # return sum(map(abs, count_dict.values()))\n-- \n-- Using a dictionary: O(n_a+n_b) time\nlocal function make_anagram_1(a, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_282664_make_anagram_1.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = make_anagram_1\n    lu.assertEquals(candidate(list('codewars'), list('code')), 4)\n    lu.assertEquals(candidate(list('abcd'), list('abcd')), 0)\n    lu.assertEquals(candidate(list('code'), list('code')), 0)\n    lu.assertEquals(candidate(list('codewars'), list('codewars')), 0)\n    lu.assertEquals(candidate(list('aab'), list('aab')), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_282812_extract_bits", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # assert all(isinstance(val, int) for val in bit_dict.values()), \\\n-- #     \"bit_dict: all elements are expected to be 'int'\"\n-- # valid_bit = [\n-- #     key for key, val in bit_dict.items() if not int(bit) & val == 0\n-- # ]\n-- # return valid_bit\n-- \n-- Extract bits which is turend on (1).\n-- Args:\n--     bit (int): Bit to check.\n--     bit_dict (dict): Correspondance dict of bit and status.\n-- Return:\n--     valid_bit (:obj:`list` of :obj:`str`): List of bit which is\n--         turned on (1).\n-- Example:\n--     >>> sample_dict = {\n--     ...     \"S1\": 0b001,\n--     ...     \"S2\": 0b010,\n--     ...     \"S3\": 0b100,\n--     ... }\n--     >>> extract_bits(0b101, sample_dict)\n--     [\"S1\", \"S3\"]\nlocal function extract_bits(bit, bit_dict)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_282812_extract_bits.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = extract_bits\n    lu.assertEquals(candidate(7, {['S1'] = 1, ['S2'] = 2, ['S3'] = 4}), {'S1', 'S2', 'S3'})\n    lu.assertEquals(candidate(6, {['S1'] = 1, ['S2'] = 2, ['S3'] = 4}), {'S2', 'S3'})\n    lu.assertEquals(candidate(7, {['S1'] = 1, ['S2'] = 2, ['S3'] = 4}), {'S1', 'S2', 'S3'})\n    lu.assertEquals(candidate(6, {['S1'] = 1, ['S2'] = 2, ['S3'] = 4}), {'S2', 'S3'})\n    lu.assertEquals(candidate(3, {['S1'] = 1, ['S2'] = 2, ['S3'] = 4}), {'S1', 'S2'})\n    lu.assertEquals(candidate(2, {['S1'] = 1, ['S2'] = 2, ['S3'] = 4}), {'S2'})\n    lu.assertEquals(candidate(0, {['S1'] = 1, ['S2'] = 2, ['S3'] = 4}), {})\n    lu.assertEquals(candidate(1, {['S1'] = 1, ['S2'] = 2, ['S3'] = 4}), {'S1'})\n    lu.assertEquals(candidate(4, {['S1'] = 1, ['S2'] = 2, ['S3'] = 4}), {'S3'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_283724__string_tolist", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return [v.strip() for v in s.split(',') if v]\n-- \n-- Convert the authorization comma separated string to list\nlocal function _string_tolist(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_283724__string_tolist.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _string_tolist\n    lu.assertEquals(candidate('abc\\ndef'), {'abc\\ndef'})\n    lu.assertEquals(candidate('foo,bar'), {'foo', 'bar'})\n    lu.assertEquals(candidate(' abc '), {'abc'})\n    lu.assertEquals(candidate(' a, b, c '), {'a', 'b', 'c'})\n    lu.assertEquals(candidate('foo'), {'foo'})\n    lu.assertEquals(candidate('a'), {'a'})\n    lu.assertEquals(candidate(' abc,def '), {'abc', 'def'})\n    lu.assertEquals(candidate('a,b,c'), {'a', 'b', 'c'})\n    lu.assertEquals(candidate(' a,b,c '), {'a', 'b', 'c'})\n    lu.assertEquals(candidate('a, b, c, d'), {'a', 'b', 'c', 'd'})\n    lu.assertEquals(candidate(',a,b,'), {'a', 'b'})\n    lu.assertEquals(candidate('a b c'), {'a b c'})\n    lu.assertEquals(candidate('a,,b'), {'a', 'b'})\n    lu.assertEquals(candidate('a,'), {'a'})\n    lu.assertEquals(candidate(',a'), {'a'})\n    lu.assertEquals(candidate('abc,def'), {'abc', 'def'})\n    lu.assertEquals(candidate(''), {})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_283945_to_bin", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # b = bin(n)\n-- # return b[2:]\n-- \n-- convert number to binary \nlocal function to_bin(n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_283945_to_bin.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = to_bin\n    lu.assertEquals(candidate(2), '10')\n    lu.assertEquals(candidate(2), '10')\n    lu.assertEquals(candidate(6), '110')\n    lu.assertEquals(candidate(0), '0')\n    lu.assertEquals(candidate(3), '11')\n    lu.assertEquals(candidate(23), '10111')\n    lu.assertEquals(candidate(1), '1')\n    lu.assertEquals(candidate(5), '101')\n    lu.assertEquals(candidate(0), '0')\n    lu.assertEquals(candidate(42), '101010')\n    lu.assertEquals(candidate(1), '1')\n    lu.assertEquals(candidate(4), '100')\n    lu.assertEquals(candidate(42), '101010')\n    lu.assertEquals(candidate(3), '11')\n    lu.assertEquals(candidate(11), '1011')\n    lu.assertEquals(candidate(7), '111')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_284769_make_stat", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # howmany = 1 more than fp (opt_fp is a % of max_guess)\n-- # opt_howmany = round(opt_fp*max_fp) + 1\n-- # # how many did it find out of all, e.g. 10/100\n-- # opt_tp_str = str(round(opt_tp*len_df)) + \"/\" + str(len_df)\n-- # # how many percent is that, e.g. 10/100 would be 10%\n-- # opt_tpr = str(round(opt_tp*100)) + \"%\"\n-- # return opt_howmany, opt_tp_str, opt_tpr\n-- \n-- Called by loanpy.sanity.postprocess2.\n-- Calculates  statistics from optimum, max nr of guesses and length     of input data frame.\n-- :param opt_fp: The optimal false positive rate as a fraction of the     maximal false positive rate, i.e. last (=highest) element of     list passed to param <guesslist> in loanpy.sanity.eval_all.\n-- :type opt_fp: float\n-- :param opt_tp: The optimal true positive rate as a fraction of the     total number of input words for predictions, i.e. length of data frame.\n-- :type opt_tp: float\n-- :param max_fp: The maximal false positive rate is the     highest number of possible guesses, i.e. the last element of the list     passed to param <guesslist> in loanpy.sanity.eval_all.\n-- :type max_fp: int | float\n-- :param len_df: The total number of input words for predictions.\n-- :type len_df: int\n-- :returns: The optimal setting for param <howmany> in     loanpy.adrc.Adrc.adapt or loanpy.adrc.Adrc.reconstruct.\n-- :rtype: tuple of int, str, str\n-- :Example:\n-- >>> from loanpy.sanity import make_stat\n-- >>> make_stat(opt_fp=0.099, opt_tp=0.6, max_fp=1000, len_df=10)\n-- (100, \"6/10\", \"60%\")\nlocal function make_stat(opt_fp, opt_tp, max_fp, len_df)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_284769_make_stat.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = make_stat\n    lu.assertEquals(candidate(0.099, 0.9, 1000, 10), {100, '9/10', '90%'})\n    lu.assertEquals(candidate(0.099, 0.4, 1000, 10), {100, '4/10', '40%'})\n    lu.assertEquals(candidate(0.099, 0.0, 1000, 10), {100, '0/10', '0%'})\n    lu.assertEquals(candidate(0.099, 0.3, 1000, 10), {100, '3/10', '30%'})\n    lu.assertEquals(candidate(0.099, 0.8, 1000, 10), {100, '8/10', '80%'})\n    lu.assertEquals(candidate(0.099, 0.2, 1000, 10), {100, '2/10', '20%'})\n    lu.assertEquals(candidate(0.099, 0.7, 1000, 10), {100, '7/10', '70%'})\n    lu.assertEquals(candidate(0.099, 0.1, 1000, 10), {100, '1/10', '10%'})\n    lu.assertEquals(candidate(0.099, 0.6, 1000, 10), {100, '6/10', '60%'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_285975_get_alphas", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # get the index of the first digit\n-- # for i, char in enumerate(revision_str):\n-- #     if char.isdigit():\n-- #         if i == 0:\n-- #             return '', revision_str\n-- #         return revision_str[0:i], revision_str[i:]\n-- # # string is entirely alphas\n-- # return revision_str, ''\n-- \n-- Return a tuple of the first non-digit characters of a revision (which\n-- may be empty) and the remaining characters.\nlocal function get_alphas(revision_str)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_285975_get_alphas.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_alphas\n    lu.assertEquals(candidate('b'), {'b', ''})\n    lu.assertEquals(candidate('a'), {'a', ''})\n    lu.assertEquals(candidate('None'), {'None', ''})\n    lu.assertEquals(candidate('abc'), {'abc', ''})\n    lu.assertEquals(candidate('abc-'), {'abc-', ''})\n    lu.assertEquals(candidate(''), {'', ''})\n    lu.assertEquals(candidate('34'), {'', '34'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_28615_split_instruction", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # newins = ins.replace(',', ' ')\n-- # splitins = newins.split()\n-- # return splitins\n-- \n-- Split an assembly instruction into seperate parts.\n-- :param ins: The assembly line.\n-- :return: A list with the parts of the instruction.\nlocal function split_instruction(ins)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_28615_split_instruction.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = split_instruction\n    lu.assertEquals(candidate('addi $t1, $t2, 3'), {'addi', '$t1', '$t2', '3'})\n    lu.assertEquals(candidate('addi $10, $1, 1000,'), {'addi', '$10', '$1', '1000'})\n    lu.assertEquals(candidate(' addi 3, 5, 7  '), {'addi', '3', '5', '7'})\n    lu.assertEquals(candidate('add    $t0,  $t1, $t2'), {'add', '$t0', '$t1', '$t2'})\n    lu.assertEquals(candidate('bltz $t1, 100000'), {'bltz', '$t1', '100000'})\n    lu.assertEquals(candidate(' slti $t7, $t8, -20'), {'slti', '$t7', '$t8', '-20'})\n    lu.assertEquals(candidate('add.d $f4, $f5, $f2'), {'add.d', '$f4', '$f5', '$f2'})\n    lu.assertEquals(candidate('lw $t1, 0($t2)'), {'lw', '$t1', '0($t2)'})\n    lu.assertEquals(candidate('addi $10, $1, 1000, 123'), {'addi', '$10', '$1', '1000', '123'})\n    lu.assertEquals(candidate('b 100000'), {'b', '100000'})\n    lu.assertEquals(candidate('bnez $t1, 100000'), {'bnez', '$t1', '100000'})\n    lu.assertEquals(candidate('add $t0, $t1, $t2'), {'add', '$t0', '$t1', '$t2'})\n    lu.assertEquals(candidate(' bltz  10, 12  '), {'bltz', '10', '12'})\n    lu.assertEquals(candidate(' jal  10  '), {'jal', '10'})\n    lu.assertEquals(candidate(' jal  10, 12  '), {'jal', '10', '12'})\n    lu.assertEquals(candidate('addi $10, $1, 1000, 123, 5'), {'addi', '$10', '$1', '1000', '123', '5'})\n    lu.assertEquals(candidate('add $t1, $t2, $t3'), {'add', '$t1', '$t2', '$t3'})\n    lu.assertEquals(candidate(' move $s1, $s2'), {'move', '$s1', '$s2'})\n    lu.assertEquals(candidate('   sub   $t3, $t4'), {'sub', '$t3', '$t4'})\n    lu.assertEquals(candidate(' move $t5, $t6'), {'move', '$t5', '$t6'})\n    lu.assertEquals(candidate('add $t1, $t2, 0x10'), {'add', '$t1', '$t2', '0x10'})\n    lu.assertEquals(candidate('addi $10, $1, 1000'), {'addi', '$10', '$1', '1000'})\n    lu.assertEquals(candidate('add $t1, $t2'), {'add', '$t1', '$t2'})\n    lu.assertEquals(candidate('add $t1, $t2, $zero'), {'add', '$t1', '$t2', '$zero'})\n    lu.assertEquals(candidate('j 0x10'), {'j', '0x10'})\n    lu.assertEquals(candidate('addi $10, $1, 1000, '), {'addi', '$10', '$1', '1000'})\n    lu.assertEquals(candidate('sw $t1, 0($t2)'), {'sw', '$t1', '0($t2)'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_287521_normalize_alef_maksura_hsb", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return s.replace(u'\\u00fd', u'y')\n-- \n-- Normalize all occurences of Alef Maksura characters to a Yeh character\n-- in a Habash-Soudi-Buckwalter encoded string.\n-- Args:\n--     s (:obj:`str`): The string to be normalized.\n-- Returns:\n--     :obj:`str`: The normalized string.\nlocal function normalize_alef_maksura_hsb(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_287521_normalize_alef_maksura_hsb.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = normalize_alef_maksura_hsb\n    lu.assertEquals(candidate('z\u00fd'), 'zy')\n    lu.assertEquals(candidate('z\u00fd\u00fd'), 'zyy')\n    lu.assertEquals(candidate('x\u00fd\u00fd\u00fd'), 'xyyy')\n    lu.assertEquals(candidate('\u00fd'), 'y')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_287855_binary_combinations", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # combinations = []\n-- # for i in range(2**n):\n-- #     bin_value = str(bin(i)).split('b')[1]\n-- #     while len(bin_value) < n:\n-- #         bin_value = \"0\" + bin_value\n-- #     combinations.append(bin_value)\n-- # return combinations\n-- \n--     Returns all possible combinations of length n binary numbers as strings\nlocal function binary_combinations(n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_287855_binary_combinations.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = binary_combinations\n    lu.assertEquals(candidate(3), {'000', '001', '010', '011', '100', '101', '110', '111'})\n    lu.assertEquals(candidate(2), {'00', '01', '10', '11'})\n    lu.assertEquals(candidate(1), {'0', '1'})\n    lu.assertEquals(candidate(1), {'0', '1'})\n    lu.assertEquals(candidate(2), {'00', '01', '10', '11'})\n    lu.assertEquals(candidate(3), {'000', '001', '010', '011', '100', '101', '110', '111'})\n    lu.assertEquals(candidate(0), {'0'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_291839_format_hostmaster", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # name, domain = hostmaster.split('@')\n-- # if '.' in name:\n-- #     name = name.replace('.', '\\.')\n-- # return \"%s.%s.\" % (name, domain)\n-- \n-- The DNS encodes the <local-part> as a single label, and encodes the\n-- <mail-domain> as a domain name.  The single label from the <local-part>\n-- is prefaced to the domain name from <mail-domain> to form the domain\n-- name corresponding to the mailbox.  Thus the mailbox HOSTMASTER@SRI-\n-- NIC.ARPA is mapped into the domain name HOSTMASTER.SRI-NIC.ARPA.  If the\n-- <local-part> contains dots or other special characters, its\n-- representation in a master file will require the use of backslash\n-- quoting to ensure that the domain name is properly encoded.  For\n-- example, the mailbox Action.domains@ISI.EDU would be represented as\n-- Action\\.domains.ISI.EDU.\n-- http://www.ietf.org/rfc/rfc1035.txt\nlocal function format_hostmaster(hostmaster)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_291839_format_hostmaster.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = format_hostmaster\n    lu.assertEquals(candidate('foo@bar'), 'foo.bar.')\n    lu.assertEquals(candidate('hostmaster@example.com'), 'hostmaster.example.com.')\n    lu.assertEquals(candidate('HOSTMASTER@SRI-NIC.ARPA'), 'HOSTMASTER.SRI-NIC.ARPA.')\n    lu.assertEquals(candidate('mail-hostmaster@example.com'), 'mail-hostmaster.example.com.')\n    lu.assertEquals(candidate('mailhostmaster@example.com'), 'mailhostmaster.example.com.')\n    lu.assertEquals(candidate('foo@bar.baz'), 'foo.bar.baz.')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_291887_is_templated_secret", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # try:\n-- #     if (\n-- #         (secret[0] == '{' and secret[-1] == '}')\n-- #         or (secret[0] == '<' and secret[-1] == '>')\n-- #         or (secret[0] == '$' and secret[1] == '{' and secret[-1] == '}')\n-- #     ):\n-- #         return True\n-- # except IndexError:\n-- #     # Any one character secret (that causes this to raise an IndexError) is highly\n-- #     # likely to be a false positive (or if a true positive, INCREDIBLY weak password).\n-- #     return True\n-- # return False\n-- \n--     Filters secrets that are shaped like: {secret}, <secret>, or ${secret}.\nlocal function is_templated_secret(secret)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_291887_is_templated_secret.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_templated_secret\n    lu.assertEquals(candidate('password}'), false)\n    lu.assertEquals(candidate('pass${'), false)\n    lu.assertEquals(candidate('${password}'), true)\n    lu.assertEquals(candidate('password'), false)\n    lu.assertEquals(candidate('<<PASSWORD>>'), true)\n    lu.assertEquals(candidate('{password'), false)\n    lu.assertEquals(candidate('}password'), false)\n    lu.assertEquals(candidate('pass<PASSWORD>'), false)\n    lu.assertEquals(candidate('${password'), false)\n    lu.assertEquals(candidate('$password}'), false)\n    lu.assertEquals(candidate('password{'), false)\n    lu.assertEquals(candidate('{password}'), true)\n    lu.assertEquals(candidate('pass$'), false)\n    lu.assertEquals(candidate('pass}'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_291988_virtual_temperature", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return temperature_k * (1 + 0.61 * mixing_ratio_g_kg / 1000.0)\n-- \n-- Convert temperature and mixing ratio to virtual temperature.\n-- Args:\n--     temperature_k: The temperature or potential temperature in units K.\n--     mixing_ratio_kg_kg: The mixing ratio in units kg kg-1.\n-- Returns:\n--     The virtual temperature in units K.\nlocal function virtual_temperature(temperature_k, mixing_ratio_g_kg)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_291988_virtual_temperature.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = virtual_temperature\n    lu.assertEquals(candidate(290.0, 0.0), 290.0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_292668_rearrange_unsorted", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # n = len(nums)\n-- # def recurse(start, stop):\n-- #     if start == stop:\n-- #         return nums  # Base case, finished sorting list\n-- #     if nums[start] <= k:\n-- #         return recurse(start+1, stop)\n-- #     nums[stop], nums[start] = nums[start], nums[stop]\n-- #     return recurse(start, stop-1)\n-- # return recurse(0, n-1)\n-- \n-- Solution to exercise C-4.20.\n-- Given an unsorted sequence, S, of integers and an integer k, describe a\n-- recursive algorithm for rearranging the elements in S so that all elements\n-- less than or equal to k come before any elements larger than k. What is\n-- the running time of your algorithm on a sequence of n values?\n-- --------------------------------------------------------------------------\n-- Solution:\n-- --------------------------------------------------------------------------\n-- The algorithm terminates when the start index equals the stop index.  That\n-- requires n recursive calls.  Each recursive call will worst case swap two\n-- values in the list.  Replacing a value in a list is O(1) according to the\n-- text (table 5.4), and so this algorithm is O(n).\nlocal function rearrange_unsorted(nums, k)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_292668_rearrange_unsorted.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = rearrange_unsorted\n    lu.assertEquals(candidate({3, 1, 4, 2, 5}, 6), {3, 1, 4, 2, 5})\n    lu.assertEquals(candidate({3, 1, 4, 2, 5}, 4), {3, 1, 4, 2, 5})\n    lu.assertEquals(candidate({9, 8, 7, 6, 5, 4, 3, 2, 1, 0}, 4), {0, 1, 2, 3, 4, 5, 6, 7, 8, 9})\n    lu.assertEquals(candidate(list(range(10)), 9), list(range(10)))\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_292769_get_board_columns", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # columns = []\n-- # for i in range(1, 5 + 1):\n-- #     column = ''\n-- #     for row in board:\n-- #         column += row[i]\n-- #     columns.append(column)\n-- # return columns\n-- \n-- Get board columns\n-- >>> get_board_columns(['***21**', '412453*', '423145*', '*543215',     '*35214*', '*41532*', '*2*1***'])\n-- ['*125342', '*23451*', '2413251', '154213*', '*35142*']\nlocal function get_board_columns(board)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_292769_get_board_columns.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_board_columns\n    lu.assertEquals(candidate({'***21**', '412453*', '423145*', '*543215', '*35214*', '*41532*', '*2*1***'}), {'*125342', '*23451*', '2413251', '154213*', '*35142*'})\n    lu.assertEquals(candidate({'***21**', '412453*', '423145*', '*543215', '*35214*', '*41532*', '*2*1***'}), {'*125342', '*23451*', '2413251', '154213*', '*35142*'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_292929_edit_distance_dp", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # rows = len(str1) + 1\n-- # cols = len(str2) + 1\n-- # dp_table = [[0 for j in range(cols)] for i in range(rows)]\n-- # for row in range(rows):\n-- #     for col in range(cols):\n-- #         if row == 0:\n-- #             dp_table[row][col] = col\n-- #         elif col == 0:\n-- #             dp_table[row][col] = row\n-- #         elif str1[row-1] == str2[col-1]:\n-- #             dp_table[row][col] = dp_table[row-1][col-1]\n-- #         else:\n-- #             insert = dp_table[row][col-1]\n-- #             remove = dp_table[row-1][col]\n-- #             replace = dp_table[row-1][col-1]\n-- #             dp_table[row][col] = min(insert, remove, replace) + 1\n-- # return dp_table[rows-1][cols-1]\n-- \n-- Compute the Edit Distance between 2 strings.\nlocal function edit_distance_dp(str1, str2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_292929_edit_distance_dp.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = edit_distance_dp\n    lu.assertEquals(candidate('A', 'B'), 1)\n    lu.assertEquals(candidate('gumbo', 'gambol'), 2)\n    lu.assertEquals(candidate('Saturday', 'Sunday'), 3)\n    lu.assertEquals(candidate('TCA', 'TAAA'), 2)\n    lu.assertEquals(candidate('kitten', 'kitten'), 0)\n    lu.assertEquals(candidate('GAGATTCTACGGA', 'GAGCATTCTACGAG'), 3)\n    lu.assertEquals(candidate('', ''), 0)\n    lu.assertEquals(candidate('A', 'A'), 0)\n    lu.assertEquals(candidate('GAGATTCTACGGA', 'GAGATTCTACGGC'), 1)\n    lu.assertEquals(candidate('test', 'text'), 1)\n    lu.assertEquals(candidate('GAGATTCTACGGA', 'GAGCATTCTACGGC'), 2)\n    lu.assertEquals(candidate('foo', 'bar'), 3)\n    lu.assertEquals(candidate('TCA', 'TCAA'), 1)\n    lu.assertEquals(candidate('book', 'back'), 2)\n    lu.assertEquals(candidate('GAGATTCTACGGA', 'GAGATTCTACGGC'), 1)\n    lu.assertEquals(candidate('kitten', 'sitting'), 3)\n    lu.assertEquals(candidate('A', 'AG'), 1)\n    lu.assertEquals(candidate('AG', 'A'), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_293032_greatest_common_divisor_with_coefficient", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if b == 0:\n-- #     return a, 1, 0\n-- # else:\n-- #     __greatest_common_divisor, __x, __y = greatest_common_divisor_with_coefficient(b, a % b)\n-- #     _x = __y\n-- #     _y = __x - (a // b) * __y\n-- #     return __greatest_common_divisor, _x, _y\n-- \n-- calc the greatest common divisor between a and b, and find two numbers x, y to fit formula:\n-- a * x + b * y = the greatest common divisor.\n-- :param a: (int)\n-- :param b: (int)\n-- :return: (tuple) the greatest common divisor, x, y\nlocal function greatest_common_divisor_with_coefficient(a, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_293032_greatest_common_divisor_with_coefficient.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = greatest_common_divisor_with_coefficient\n    lu.assertEquals(candidate(0, 1), {1, 0, 1})\n    lu.assertEquals(candidate(4, 2), {2, 0, 1})\n    lu.assertEquals(candidate(12, 14), {2, -1, 1})\n    lu.assertEquals(candidate(1, 2), {1, 1, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_294243_is_hidden", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if type(file_name) != str:\n-- #     raise TypeError(\"Type of file name must be <class 'str'>, but {}\".format(type(file_name)))\n-- # return True if file_name.startswith('.') else False\n-- \n-- Get boolean if a file (or a directory) is hidden or not, with linux based OS.\n-- Raises\n-- ------\n-- TypeError\n--     If the input data is not a string.\n-- Parameters\n-- ----------\n-- file_name : String\n--     Target file (required)\n-- Returns\n-- -------\n-- Boolean\n--     True if a file is hidden, False elsewhere\nlocal function is_hidden(file_name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_294243_is_hidden.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_hidden\n    lu.assertEquals(candidate('/.git/info/refs/remotes/origin/master/'), false)\n    lu.assertEquals(candidate('/.git/info/refs/remotes/origin'), false)\n    lu.assertEquals(candidate('a.'), false)\n    lu.assertEquals(candidate('/.git/info/refs/remotes/origin/'), false)\n    lu.assertEquals(candidate('/.git'), false)\n    lu.assertEquals(candidate('/.git/info/refs/heads/'), false)\n    lu.assertEquals(candidate('/.git/info/refs/remotes'), false)\n    lu.assertEquals(candidate('/..'), false)\n    lu.assertEquals(candidate('/.git/info/refs/remotes/'), false)\n    lu.assertEquals(candidate('.'), true)\n    lu.assertEquals(candidate('/.git/info/'), false)\n    lu.assertEquals(candidate('.test.py.swp'), true)\n    lu.assertEquals(candidate('test.py.swp'), false)\n    lu.assertEquals(candidate('/.'), false)\n    lu.assertEquals(candidate('/.git/HEAD'), false)\n    lu.assertEquals(candidate('/.git/info/refs'), false)\n    lu.assertEquals(candidate('/.git/'), false)\n    lu.assertEquals(candidate('/.git/info'), false)\n    lu.assertEquals(candidate('/.git/info/refs/heads/master/'), false)\n    lu.assertEquals(candidate(' '), false)\n    lu.assertEquals(candidate('a.a.a'), false)\n    lu.assertEquals(candidate('..test.py'), true)\n    lu.assertEquals(candidate('/.git/info/refs/remotes/origin/master'), false)\n    lu.assertEquals(candidate('/.git/HEAD/master'), false)\n    lu.assertEquals(candidate('.a'), true)\n    lu.assertEquals(candidate('/.git/info/refs/heads/master'), false)\n    lu.assertEquals(candidate('a.a.'), false)\n    lu.assertEquals(candidate('/.git/info/refs/'), false)\n    lu.assertEquals(candidate('a'), false)\n    lu.assertEquals(candidate('/.git/HEAD/'), false)\n    lu.assertEquals(candidate('Test.py'), false)\n    lu.assertEquals(candidate(''), false)\n    lu.assertEquals(candidate('a.a'), false)\n    lu.assertEquals(candidate('test.py'), false)\n    lu.assertEquals(candidate('.test.py'), true)\n    lu.assertEquals(candidate('/.git/info/refs/heads'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_294425_signExp", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # arr = list(expression)\n-- # if sign == \"-\":\n-- #     for i in range(len(expression)):\n-- #         # Invert the sign if the 'sign' is '-'\n-- #         if arr[i] == \"+\":\n-- #             arr[i] = \"-\"\n-- #         elif arr[i] == \"-\":\n-- #             arr[i] = \"+\"\n-- # # If the first characters is not a sign, it is a '+' and we need to\n-- # # add it to the subexpression\n-- # if arr[0] != \"+\" and arr[0] != \"-\":\n-- #     arr.insert(0, sign)\n-- # return \"\".join(x for x in arr)\n-- \n--     Opens the brackets, depending upon the Sign\nlocal function signExp(expression, sign)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_294425_signExp.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = signExp\n    lu.assertEquals(candidate('-2 - 2 + 1', '+'), '-2 - 2 + 1')\n    lu.assertEquals(candidate('a-b-c', '-'), '-a+b+c')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_295062_prime", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # x = 1  # This Will Be 0 If Not Prime\n-- # for i in range(2, n):  # Divide By Any Number\n-- #     if n % i == 0:  # If Divisible By Any Number\n-- #         x = 0  # Take x As 0\n-- #         break\n-- #     else:\n-- #         x = 1  # else Take x As 1\n-- # return x\n-- \n--  To Check If n Is Prime Or Not \nlocal function prime(n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_295062_prime.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = prime\n    lu.assertEquals(candidate(2), 1)\n    lu.assertEquals(candidate(25), 0)\n    lu.assertEquals(candidate(9), 0)\n    lu.assertEquals(candidate(30), 0)\n    lu.assertEquals(candidate(13), 1)\n    lu.assertEquals(candidate(32), 0)\n    lu.assertEquals(candidate(35), 0)\n    lu.assertEquals(candidate(22), 0)\n    lu.assertEquals(candidate(19), 1)\n    lu.assertEquals(candidate(3), 1)\n    lu.assertEquals(candidate(10000), 0)\n    lu.assertEquals(candidate(200), 0)\n    lu.assertEquals(candidate(17), 1)\n    lu.assertEquals(candidate(4), 0)\n    lu.assertEquals(candidate(12), 0)\n    lu.assertEquals(candidate(28), 0)\n    lu.assertEquals(candidate(24), 0)\n    lu.assertEquals(candidate(5), 1)\n    lu.assertEquals(candidate(18), 0)\n    lu.assertEquals(candidate(34), 0)\n    lu.assertEquals(candidate(36), 0)\n    lu.assertEquals(candidate(37), 1)\n    lu.assertEquals(candidate(10), 0)\n    lu.assertEquals(candidate(7), 1)\n    lu.assertEquals(candidate(40), 0)\n    lu.assertEquals(candidate(8), 0)\n    lu.assertEquals(candidate(11), 1)\n    lu.assertEquals(candidate(6), 0)\n    lu.assertEquals(candidate(31), 1)\n    lu.assertEquals(candidate(16), 0)\n    lu.assertEquals(candidate(38), 0)\n    lu.assertEquals(candidate(341), 0)\n    lu.assertEquals(candidate(15), 0)\n    lu.assertEquals(candidate(14), 0)\n    lu.assertEquals(candidate(26), 0)\n    lu.assertEquals(candidate(4000), 0)\n    lu.assertEquals(candidate(23), 1)\n    lu.assertEquals(candidate(1000), 0)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(29), 1)\n    lu.assertEquals(candidate(199), 1)\n    lu.assertEquals(candidate(2000), 0)\n    lu.assertEquals(candidate(21), 0)\n    lu.assertEquals(candidate(20), 0)\n    lu.assertEquals(candidate(400), 0)\n    lu.assertEquals(candidate(41), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_295233_char_count", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if ignore_spaces:\n-- #     text = text.replace(\" \", \"\")\n-- # return len(text)\n-- \n-- Function to return total character counts in a text,\n-- pass the following parameter `ignore_spaces = False`\n-- to ignore whitespaces\nlocal function char_count(text, ignore_spaces)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_295233_char_count.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = char_count\n    lu.assertEquals(candidate('hello'), 5)\n    lu.assertEquals(candidate('abc'), 3)\n    lu.assertEquals(candidate('a '), 1)\n    lu.assertEquals(candidate('abc   '), 3)\n    lu.assertEquals(candidate('123 45'), 5)\n    lu.assertEquals(candidate('Hello world ', false), 12)\n    lu.assertEquals(candidate('hi 123'), 5)\n    lu.assertEquals(candidate('hi'), 2)\n    lu.assertEquals(candidate('123 456 7890'), 10)\n    lu.assertEquals(candidate('abc d e '), 5)\n    lu.assertEquals(candidate('Hello'), 5)\n    lu.assertEquals(candidate('12345'), 5)\n    lu.assertEquals(candidate('1234567890'), 10)\n    lu.assertEquals(candidate('abc d e'), 5)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_295406_code_snippet", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return '```python\\n{}\\n```'.format(snippet)\n-- \n-- Change a string-typed code snippet into Markdown-style code fence.\n-- # Argument\n--     snippet: `str`. A code snippet.\n-- # Return\n--     `str`: Markdown-style code fence.\nlocal function code_snippet(snippet)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_295406_code_snippet.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = code_snippet\n    lu.assertEquals(candidate(''), '```python\\n\\n```')\n    lu.assertEquals(candidate('snippet'), '```python\\nsnippet\\n```')\n    lu.assertEquals(candidate('2_snippet'), '```python\\n2_snippet\\n```')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_295538_unchunk", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return string.replace(\" \", \"\")\n-- \n--     Remove spaces in string.\nlocal function unchunk(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_295538_unchunk.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = unchunk\n    lu.assertEquals(candidate('abc  def'), 'abcdef')\n    lu.assertEquals(candidate('Wonderful'), 'Wonderful')\n    lu.assertEquals(candidate('I   am   a    cat!'), 'Iamacat!')\n    lu.assertEquals(candidate('abc def'), 'abcdef')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_295921_day_to_iso", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return '%(year)s-%(month)s-%(day)s' % day\n-- \n--     day to iso format\nlocal function day_to_iso(day)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_295921_day_to_iso.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = day_to_iso\n    lu.assertEquals(candidate({['year'] = 2020, ['month'] = 1, ['day'] = 1}), '2020-1-1')\n    lu.assertEquals(candidate({['year'] = 2020, ['month'] = 1, ['day'] = 31}), '2020-1-31')\n    lu.assertEquals(candidate({['year'] = 2016, ['month'] = 12, ['day'] = 31}), '2016-12-31')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_296478__Indentation", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"    \" * indentation_level\n-- \n-- Returns the indentation string.\nlocal function _Indentation(indentation_level)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_296478__Indentation.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _Indentation\n    lu.assertEquals(candidate(-2), '')\n    lu.assertEquals(candidate(1), '    ')\n    lu.assertEquals(candidate(6), '                        ')\n    lu.assertEquals(candidate(8), '                                ')\n    lu.assertEquals(candidate(10), '                                        ')\n    lu.assertEquals(candidate(9), '                                    ')\n    lu.assertEquals(candidate(4), '                ')\n    lu.assertEquals(candidate(1), candidate(1))\n    lu.assertEquals(candidate(7), '                            ')\n    lu.assertEquals(candidate(-1), '')\n    lu.assertEquals(candidate(2), '        ')\n    lu.assertEquals(candidate(5), '                    ')\n    lu.assertEquals(candidate(0), '')\n    lu.assertEquals(candidate(-4), '')\n    lu.assertEquals(candidate(3), '            ')\n    lu.assertEquals(candidate(-3), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_296667_lower_case", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return text.lower()\n-- \n-- Simple function to convert text to lowercase\n-- Used in pipeline as workaround\nlocal function lower_case(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_296667_lower_case.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = lower_case\n    lu.assertEquals(candidate('lower case'), 'lower case')\n    lu.assertEquals(candidate('This is another sentence'), 'this is another sentence')\n    lu.assertEquals(candidate('THIS IS A MIXED CASE STRING'), 'this is a mixed case string')\n    lu.assertEquals(candidate('This is a sentence'), 'this is a sentence')\n    lu.assertEquals(candidate('A new sentence.'), 'a new sentence.')\n    lu.assertEquals(candidate('ANOTHER STRING'), 'another string')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_29671_get_node_flow", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # flow = 0\n-- # n = len(flow_net)\n-- # for i in range(n):\n-- #     flow += flow_net[i][node]\n-- #     flow -= flow_net[node][i]\n-- # return flow\n-- \n-- Returns the sum of the flow into minus the sum of the flow out from the\n-- node.\n-- In a maximum flow network, this function returns 0 for all nodes except\n-- for the source (wich returns -max_flow) and drain (wich returns max_flow).\nlocal function get_node_flow(flow_net, node)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_29671_get_node_flow.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_node_flow\n    lu.assertEquals(candidate({{0, 1, 1, 0}, {0, 0, 1, 0}, {0, 0, 0, 1}, {0, 0, 0, 0}}, 2), 1)\n    lu.assertEquals(candidate({{0, 0, 0, 0, 0, 0}, {0, 0, 0, 1, 0, 0}, {0, 0, 1, 1, 1, 0}, {0, 0, 1, 0, 1, 0}, {0, 0, 0, 0, 0, 0}, {0, 0, 0, 0, 0, 0}}, 0), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_296818_multistep_lr", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for milestone in milestones:\n-- #     lr *= gamma if iters >= milestone else 1.0\n-- # return lr\n-- \n-- MultiStep learning rate\nlocal function multistep_lr(lr, milestones, gamma, iters)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_296818_multistep_lr.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = multistep_lr\n    lu.assertEquals(candidate(0.5, {2, 3}, 0.1, 1), 0.5)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_298293_url_to_repo_org", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # check that the upstream_repo is a github repo\n-- # if 'github.com' not in url:\n-- #     raise RuntimeError(\n-- #         'Extraction of repository and owner info from non-GitHub'\n-- #         'repositories is not yet supported!'\n-- #     )\n-- # url = url.replace('https://github.com/', '').split('/')\n-- # return url[0], url[1]\n-- \n-- Extract owner and repository from GitHub url.\nlocal function url_to_repo_org(url)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_298293_url_to_repo_org.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = url_to_repo_org\n    lu.assertEquals(candidate('https://github.com/lsst-sqre/templatekit'), {'lsst-sqre', 'templatekit'})\n    lu.assertEquals(candidate('https://github.com/lsst-sqre/nbreport'), {'lsst-sqre', 'nbreport'})\n    lu.assertEquals(candidate('https://github.com/pytorch/pytorch'), {'pytorch', 'pytorch'})\n    lu.assertEquals(candidate('https://github.com/pytorch/vision'), {'pytorch', 'vision'})\n    lu.assertEquals(candidate('https://github.com/pytorch/vision/'), {'pytorch', 'vision'})\n    lu.assertEquals(candidate('https://github.com/fairlearn/fairlearn/blob/master/README.md'), {'fairlearn', 'fairlearn'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_298370_timestamp_seconds", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return float(ts / 1000000)\n-- \n-- Returns seconds float from value generated by `timestamp`.\nlocal function timestamp_seconds(ts)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_298370_timestamp_seconds.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = timestamp_seconds\n    lu.assertEquals(candidate(0), 0.0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_298424_is_sane_slack_webhook", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if url is None:\n-- #     return False\n-- # return \"https://hooks.slack.com/\" in url.strip()\n-- \n-- Really basic sanity checking.\nlocal function is_sane_slack_webhook(url)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_298424_is_sane_slack_webhook.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_sane_slack_webhook\n    lu.assertEquals(candidate(''), false)\n    lu.assertEquals(candidate('   '), false)\n    lu.assertEquals(candidate('http://hooks.slack.com'), false)\n    lu.assertEquals(candidate('https://hooks.slack/A/B/C'), false)\n    lu.assertEquals(candidate(None), false)\n    lu.assertEquals(candidate('https://hooks.com/A/B/C'), false)\n    lu.assertEquals(candidate('not a url'), false)\n    lu.assertEquals(candidate('https://hooks.slack.com/services/ABC123/ABC123/ABC123'), true)\n    lu.assertEquals(candidate('https://hooks.com/A/B/C/D'), false)\n    lu.assertEquals(candidate('https://hooks.slack.com/services/A/B/C'), true)\n    lu.assertEquals(candidate('https://hooks.slack/A/B/C/D'), false)\n    lu.assertEquals(candidate('https://hooks.slack.com/services/ABC123/DEF456/XYZ789'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_299447_any", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for item in iterable:\n-- #     if item:\n-- #         return True\n-- # return False\n-- \n-- Return True if at least one element is set to True.\n-- This function does not support predicates explicitely,\n-- but this behaviour can be simulated easily using\n-- list comprehension.\n-- >>> any( [False, False, False] )\n-- False\n-- >>> any( [False, True, False] )\n-- True\n-- >>> any( [ x % 2 == 1 for x in [2, 6, 8] ] )\n-- False\n-- >>> any( [ x % 2 == 1 for x in [2, 6, 7] ] )\n-- True\n-- NOTE: Starting from Python 2.5 this a built-in.\nlocal function any(iterable)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_299447_any.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = any\n    lu.assertEquals(candidate({false, false, false}), false)\n    lu.assertEquals(candidate({false, true, false}), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_299708_get_command_from_argument", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for arg in argv[1:]:\n-- #     if not arg.startswith('-'):\n-- #         return arg\n-- # return None\n-- \n--  extract command from the command line arguments \nlocal function get_command_from_argument(argv)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_299708_get_command_from_argument.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_command_from_argument\n    lu.assertEquals(candidate({'foo', 'bar'}), 'bar')\n    lu.assertEquals(candidate({'prog', 'subcommand'}), 'subcommand')\n    lu.assertEquals(candidate({'prog', 'subcommand', 'argument'}), 'subcommand')\n    lu.assertEquals(candidate({'/path/to/cli', 'list'}), 'list')\n    lu.assertEquals(candidate({'/path/to/cli', 'list', 'foo'}), 'list')\n    lu.assertEquals(candidate({'foo', 'bar', '-abc'}), 'bar')\n    lu.assertEquals(candidate({'foo', '-abc', 'bar', 'baz'}), 'bar')\n    lu.assertEquals(candidate({'prog', 'subcommand', '--flag'}), 'subcommand')\n    lu.assertEquals(candidate({'/path/to/cli', 'list', 'foo', '--bar'}), 'list')\n    lu.assertEquals(candidate({'/path/to/cli', 'list', '--foo', 'foo'}), 'list')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_300012_solution", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # output = ''                                                     # O(1)\n-- # length = len(strs)                                              # O(1)\n-- # if length == 0:                                                 # O(1)\n-- #     return output                                               # O(1)\n-- # if length == 1:                                                 # O(1)\n-- #     return strs[0]                                              # O(1)\n-- # options = strs[0]                                               # O(1)\n-- # for i in range(1, length):                                      # O(M)\n-- #     word = strs[i]                                              # O(1)\n-- #     for j, _ in enumerate(word):                                # O(N)\n-- #         if j >= len(options):                                   # O(1)\n-- #             break                                               # O(1)\n-- #         char = options[j]                                       # O(1)\n-- #         if word[j] != char:                                     # O(1)\n-- #             break                                               # O(1)\n-- #         output += char                                          # O(1)\n-- #     options = output                                            # O(1)\n-- #     output = ''                                                 # O(1)\n-- # return options                                                  # O(1)\n-- \n-- Write a function to find the longest common prefix string amongst an array of strings.\n-- If there is no common prefix, return an empty string \"\".\n-- >>> solution(['flower', 'flow', 'flight'])\n-- 'fl'\n-- >>> solution(['dog', 'racecar', 'car'])\n-- ''\n-- >>> solution(['amazing', 'amazingly', 'amazing'])\n-- 'amazing'\n-- >>> solution([])\n-- ''\nlocal function solution(strs)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_300012_solution.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = solution\n    lu.assertEquals(candidate({}), '')\n    lu.assertEquals(candidate({'dog', 'racecar', 'car'}), '')\n    lu.assertEquals(candidate({'amazing', 'amazingly', 'amazing'}), 'amazing')\n    lu.assertEquals(candidate({'amazing', 'amazingly', 'amazing'}), 'amazing')\n    lu.assertEquals(candidate({'flower', 'flow', 'flight'}), 'fl')\n    lu.assertEquals(candidate({'dog', 'racecar', 'car'}), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_300495_previous_month", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # month = month - 1\n-- # if month == 0:\n-- #     month = 12\n-- #     year = year - 1\n-- # else:\n-- #     year = year\n-- # return year, month\n-- \n--     Returns a tuple of the month prior to the year and month provided.\nlocal function previous_month(year, month)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_300495_previous_month.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = previous_month\n    lu.assertEquals(candidate(2017, 12), {2017, 11})\n    lu.assertEquals(candidate(2018, 1), {2017, 12})\n    lu.assertEquals(candidate(1999, 1), {1998, 12})\n    lu.assertEquals(candidate(2018, 12), {2018, 11})\n    lu.assertEquals(candidate(2017, 1), {2016, 12})\n    lu.assertEquals(candidate(2020, 7), {2020, 6})\n    lu.assertEquals(candidate(2019, 2), {2019, 1})\n    lu.assertEquals(candidate(2000, 2), {2000, 1})\n    lu.assertEquals(candidate(1999, 2), {1999, 1})\n    lu.assertEquals(candidate(2021, 11), {2021, 10})\n    lu.assertEquals(candidate(2016, 1), {2015, 12})\n    lu.assertEquals(candidate(2017, 2), {2017, 1})\n    lu.assertEquals(candidate(2017, 3), {2017, 2})\n    lu.assertEquals(candidate(2012, 12), {2012, 11})\n    lu.assertEquals(candidate(2018, 2), {2018, 1})\n    lu.assertEquals(candidate(2012, 2), {2012, 1})\n    lu.assertEquals(candidate(2016, 12), {2016, 11})\n    lu.assertEquals(candidate(2018, 10), {2018, 9})\n    lu.assertEquals(candidate(2000, 1), {1999, 12})\n    lu.assertEquals(candidate(2020, 1), {2019, 12})\n    lu.assertEquals(candidate(2012, 1), {2011, 12})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_300533_difference_p", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return round(abs(first * 100 / second - 100))\n-- \n-- Calculate difference between 2 values in percent\nlocal function difference_p(first, second)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_300533_difference_p.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = difference_p\n    lu.assertEquals(candidate(10, 10), 0)\n    lu.assertEquals(candidate(2, 1), 100)\n    lu.assertEquals(candidate(10, 10), 0)\n    lu.assertEquals(candidate(30, 30), 0)\n    lu.assertEquals(candidate(1, 1), 0)\n    lu.assertEquals(candidate(2, 10), 80)\n    lu.assertEquals(candidate(5, 10), 50)\n    lu.assertEquals(candidate(1000, 1000), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_300987_cu_mask_to_int", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # n = 0\n-- # for b in reversed(cu_mask):\n-- #     n = n << 1\n-- #     if b:\n-- #         n |= 1\n-- # return n\n-- \n--  A utility function that takes an array of booleans and returns an\n-- integer with 1s wherever there was a \"True\" in the array. The value at\n-- index 0 is the least significant bit. \nlocal function cu_mask_to_int(cu_mask)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_300987_cu_mask_to_int.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = cu_mask_to_int\n    lu.assertEquals(candidate({true, true, true, true}), 15)\n    lu.assertEquals(candidate({false, false, false, false}), 0)\n    lu.assertEquals(candidate({true, true, true, false, false, false, false, false}), 7)\n    lu.assertEquals(candidate({true, true, true, true}), 15)\n    lu.assertEquals(candidate({true, false, true, false, false, false, false, false}), 5)\n    lu.assertEquals(candidate({true, true, false, false, false, false, false, false}), 3)\n    lu.assertEquals(candidate({false, false, false, false, false, false, false, false}), 0)\n    lu.assertEquals(candidate({false, false, false, false}), 0)\n    lu.assertEquals(candidate({true, false, false, false, false, false, false, false}), 1)\n    lu.assertEquals(candidate({true, false, false, true}), 9)\n    lu.assertEquals(candidate({true, true, true, true, false, false, false, false}), 15)\n    lu.assertEquals(candidate({true, true, true, true, true, false, false, false}), 31)\n    lu.assertEquals(candidate({true, true, true, true, true, true, false, false}), 63)\n    lu.assertEquals(candidate({false, true, true, false}), 6)\n    lu.assertEquals(candidate({true, true, true, true, true, true, true, false}), 127)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_301359_shift_leftward", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if nbits < 0:\n-- #     return n >> -nbits\n-- # else:\n-- #     return n << nbits\n-- \n-- Shift positive left, or negative right.  Same as n * 2**nbits\nlocal function shift_leftward(n, nbits)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_301359_shift_leftward.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = shift_leftward\n    lu.assertEquals(candidate(1024, -2), 256)\n    lu.assertEquals(candidate(16, 5), 512)\n    lu.assertEquals(candidate(-1, 2), -4)\n    lu.assertEquals(candidate(128, 4), 2048)\n    lu.assertEquals(candidate(16, 0), 16)\n    lu.assertEquals(candidate(0, 0), 0)\n    lu.assertEquals(candidate(127, 6), 8128)\n    lu.assertEquals(candidate(0, 1), 0)\n    lu.assertEquals(candidate(1024, 0), 1024)\n    lu.assertEquals(candidate(16, 1), 32)\n    lu.assertEquals(candidate(2, 4), 32)\n    lu.assertEquals(candidate(2, -1), 1)\n    lu.assertEquals(candidate(1, 6), 64)\n    lu.assertEquals(candidate(16, 6), 1024)\n    lu.assertEquals(candidate(19088743, 32), 81985526906748928)\n    lu.assertEquals(candidate(-1, 31), -2147483648)\n    lu.assertEquals(candidate(127, 1), 254)\n    lu.assertEquals(candidate(-1, 7), -128)\n    lu.assertEquals(candidate(0, 31), 0)\n    lu.assertEquals(candidate(1, 1), 2)\n    lu.assertEquals(candidate(5, 0), 5)\n    lu.assertEquals(candidate(128, 3), 1024)\n    lu.assertEquals(candidate(16, 9), 8192)\n    lu.assertEquals(candidate(-1, 20), -1048576)\n    lu.assertEquals(candidate(-2, 1), -4)\n    lu.assertEquals(candidate(256, 0), 256)\n    lu.assertEquals(candidate(31, 1), 62)\n    lu.assertEquals(candidate(0, 5), 0)\n    lu.assertEquals(candidate(1, 32), 4294967296)\n    lu.assertEquals(candidate(1, 7), 128)\n    lu.assertEquals(candidate(16, 3), 128)\n    lu.assertEquals(candidate(128, 5), 4096)\n    lu.assertEquals(candidate(1, 5), 32)\n    lu.assertEquals(candidate(1, 63), 9223372036854775808)\n    lu.assertEquals(candidate(256, 2), 1024)\n    lu.assertEquals(candidate(256, 3), 2048)\n    lu.assertEquals(candidate(128, 0), 128)\n    lu.assertEquals(candidate(0, 64), 0)\n    lu.assertEquals(candidate(16, 2), 64)\n    lu.assertEquals(candidate(-2, 0), -2)\n    lu.assertEquals(candidate(2, 7), 256)\n    lu.assertEquals(candidate(128, 2), 512)\n    lu.assertEquals(candidate(2, 0), 2)\n    lu.assertEquals(candidate(-1, 32), -4294967296)\n    lu.assertEquals(candidate(16, -1), 8)\n    lu.assertEquals(candidate(1, 3), 8)\n    lu.assertEquals(candidate(5, 2), 20)\n    lu.assertEquals(candidate(31, 2), 124)\n    lu.assertEquals(candidate(2, 3), 16)\n    lu.assertEquals(candidate(-31, 2), -124)\n    lu.assertEquals(candidate(128, 1), 256)\n    lu.assertEquals(candidate(127, 2), 508)\n    lu.assertEquals(candidate(1, -20), 0)\n    lu.assertEquals(candidate(1024, 10), 1048576)\n    lu.assertEquals(candidate(16, 4), 256)\n    lu.assertEquals(candidate(1, 4), 16)\n    lu.assertEquals(candidate(255, -1), 127)\n    lu.assertEquals(candidate(0, 63), 0)\n    lu.assertEquals(candidate(0, -4), 0)\n    lu.assertEquals(candidate(31, 0), 31)\n    lu.assertEquals(candidate(127, 3), 1016)\n    lu.assertEquals(candidate(-1, -20), -1)\n    lu.assertEquals(candidate(1024, -1), 512)\n    lu.assertEquals(candidate(2, 5), 64)\n    lu.assertEquals(candidate(255, 1), 510)\n    lu.assertEquals(candidate(255, 3), 2040)\n    lu.assertEquals(candidate(-1, 5), -32)\n    lu.assertEquals(candidate(1, 1), 2)\n    lu.assertEquals(candidate(1, 20), 1048576)\n    lu.assertEquals(candidate(1, 0), 1)\n    lu.assertEquals(candidate(0, 32), 0)\n    lu.assertEquals(candidate(0, 3), 0)\n    lu.assertEquals(candidate(-128, 0), -128)\n    lu.assertEquals(candidate(-1, 1), -2)\n    lu.assertEquals(candidate(0, 4), 0)\n    lu.assertEquals(candidate(0, 4), 0)\n    lu.assertEquals(candidate(-1, 0), -1)\n    lu.assertEquals(candidate(-1, 63), -9223372036854775808)\n    lu.assertEquals(candidate(16, 10), 16384)\n    lu.assertEquals(candidate(2, 2), 8)\n    lu.assertEquals(candidate(1024, -3), 128)\n    lu.assertEquals(candidate(-1, 4), -16)\n    lu.assertEquals(candidate(255, 0), 255)\n    lu.assertEquals(candidate(125, 0), 125)\n    lu.assertEquals(candidate(2, 6), 128)\n    lu.assertEquals(candidate(0, 2), 0)\n    lu.assertEquals(candidate(2, 0), 2)\n    lu.assertEquals(candidate(127, 4), 2032)\n    lu.assertEquals(candidate(10, 4), 160)\n    lu.assertEquals(candidate(16, 7), 2048)\n    lu.assertEquals(candidate(0, -3), 0)\n    lu.assertEquals(candidate(15, 0), 15)\n    lu.assertEquals(candidate(1, 64), 18446744073709551616)\n    lu.assertEquals(candidate(16, 8), 4096)\n    lu.assertEquals(candidate(127, 5), 4064)\n    lu.assertEquals(candidate(1024, 1), 2048)\n    lu.assertEquals(candidate(-1, 6), -64)\n    lu.assertEquals(candidate(1, 0), 1)\n    lu.assertEquals(candidate(-31, 0), -31)\n    lu.assertEquals(candidate(1, 2), 4)\n    lu.assertEquals(candidate(0, 0), 0)\n    lu.assertEquals(candidate(1, 4), 16)\n    lu.assertEquals(candidate(0, 1), 0)\n    lu.assertEquals(candidate(1, 31), 2147483648)\n    lu.assertEquals(candidate(255, 2), 1020)\n    lu.assertEquals(candidate(-2, 2), -8)\n    lu.assertEquals(candidate(-125, 0), -125)\n    lu.assertEquals(candidate(2, 1), 4)\n    lu.assertEquals(candidate(-31, 1), -62)\n    lu.assertEquals(candidate(31, 3), 248)\n    lu.assertEquals(candidate(256, 1), 512)\n    lu.assertEquals(candidate(1, -1), 0)\n    lu.assertEquals(candidate(2, 1), 4)\n    lu.assertEquals(candidate(-31, 3), -248)\n    lu.assertEquals(candidate(2, 4), 32)\n    lu.assertEquals(candidate(128, 6), 8192)\n    lu.assertEquals(candidate(16, -2), 4)\n    lu.assertEquals(candidate(-1, 3), -8)\n    lu.assertEquals(candidate(127, 0), 127)\n    lu.assertEquals(candidate(128, -1), 64)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_301474__string_lower", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return string.lower()\n-- \n-- Convenience function to lowercase a string.\n-- :param string:\n--     The string which will be lower-cased.\n-- :returns:\n--     Lower-cased copy of string s.\nlocal function _string_lower(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_301474__string_lower.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _string_lower\n    lu.assertEquals(candidate('HELLO'), 'hello')\n    lu.assertEquals(candidate('Hello'), 'hello')\n    lu.assertEquals(candidate('HELLO WORLD'), 'hello world')\n    lu.assertEquals(candidate('hello world'), 'hello world')\n    lu.assertEquals(candidate('hello     world'), 'hello     world')\n    lu.assertEquals(candidate('HeLlO wOrLd'), 'hello world')\n    lu.assertEquals(candidate('hello'), 'hello')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_301724_suffixer", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if n == 1:\n-- #     return 'st'\n-- # elif n == 2:\n-- #     return 'nd'\n-- # elif n == 3:\n-- #     return 'rd'\n-- # else:\n-- #     return 'th'\n-- \n--     Provides the suffix for printing out a podium spot based on the spot number.\nlocal function suffixer(n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_301724_suffixer.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = suffixer\n    lu.assertEquals(candidate(1113), 'th')\n    lu.assertEquals(candidate(3), 'rd')\n    lu.assertEquals(candidate(14), 'th')\n    lu.assertEquals(candidate(30), 'th')\n    lu.assertEquals(candidate(6), 'th')\n    lu.assertEquals(candidate(9), 'th')\n    lu.assertEquals(candidate(10), 'th')\n    lu.assertEquals(candidate(111), 'th')\n    lu.assertEquals(candidate(28), 'th')\n    lu.assertEquals(candidate(1115), 'th')\n    lu.assertEquals(candidate(1114), 'th')\n    lu.assertEquals(candidate(11), 'th')\n    lu.assertEquals(candidate(4), 'th')\n    lu.assertEquals(candidate(12), 'th')\n    lu.assertEquals(candidate(16), 'th')\n    lu.assertEquals(candidate(18), 'th')\n    lu.assertEquals(candidate(1112), 'th')\n    lu.assertEquals(candidate(25), 'th')\n    lu.assertEquals(candidate(17), 'th')\n    lu.assertEquals(candidate(27), 'th')\n    lu.assertEquals(candidate(1), 'st')\n    lu.assertEquals(candidate(15), 'th')\n    lu.assertEquals(candidate(2), 'nd')\n    lu.assertEquals(candidate(7), 'th')\n    lu.assertEquals(candidate(29), 'th')\n    lu.assertEquals(candidate(1011), 'th')\n    lu.assertEquals(candidate(8), 'th')\n    lu.assertEquals(candidate(1111), 'th')\n    lu.assertEquals(candidate(19), 'th')\n    lu.assertEquals(candidate(24), 'th')\n    lu.assertEquals(candidate(34), 'th')\n    lu.assertEquals(candidate(20), 'th')\n    lu.assertEquals(candidate(5), 'th')\n    lu.assertEquals(candidate(13), 'th')\n    lu.assertEquals(candidate(26), 'th')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_303347_fake_headers", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return {'referrer': url, 'User-agent': 'Mozilla/5.0'}\n-- \n--     Bricklink does referrer and user-agent checks so we need to fake those.\nlocal function fake_headers(url)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_303347_fake_headers.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = fake_headers\n    lu.assertEquals(candidate('https://www.bricklink.com/catalogPG.asp?catID=25&colorID=5&page=4'), {['referrer'] = 'https://www.bricklink.com/catalogPG.asp?catID=25&colorID=5&page=4', ['User-agent'] = 'Mozilla/5.0'})\n    lu.assertEquals(candidate('https://www.bricklink.com/v2/api/register_consumer.page'), {['referrer'] = 'https://www.bricklink.com/v2/api/register_consumer.page', ['User-agent'] = 'Mozilla/5.0'})\n    lu.assertEquals(candidate(''), {['referrer'] = '', ['User-agent'] = 'Mozilla/5.0'})\n    lu.assertEquals(candidate('https://www.bricklink.com/catalogPG.asp?catID=25&colorID=5&page=1'), {['referrer'] = 'https://www.bricklink.com/catalogPG.asp?catID=25&colorID=5&page=1', ['User-agent'] = 'Mozilla/5.0'})\n    lu.assertEquals(candidate('https://www.bricklink.com/catalogPG.asp?catID=25&colorID=5&page=3'), {['referrer'] = 'https://www.bricklink.com/catalogPG.asp?catID=25&colorID=5&page=3', ['User-agent'] = 'Mozilla/5.0'})\n    lu.assertEquals(candidate('https://example.com'), {['referrer'] = 'https://example.com', ['User-agent'] = 'Mozilla/5.0'})\n    lu.assertEquals(candidate('https://www.bricklink.com/catalogPG.asp?catID=25&colorID=5&page=2'), {['referrer'] = 'https://www.bricklink.com/catalogPG.asp?catID=25&colorID=5&page=2', ['User-agent'] = 'Mozilla/5.0'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_303816__ps", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # s = \"({0[0]:.3f}, {0[1]:.3f})\".format(score)\n-- # s = \"{0:.3f}\".format(score)\n-- # return s\n-- \n--  Convenience function for score printing\nlocal function _ps(score)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_303816__ps.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _ps\n    lu.assertEquals(candidate(0.0), '0.000')\n    lu.assertEquals(candidate(0.001234), '0.001')\n    lu.assertEquals(candidate(0.0012349), '0.001')\n    lu.assertEquals(candidate(0.0012345), '0.001')\n    lu.assertEquals(candidate(0.1234), '0.123')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_304953_common_prefix_length", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Quick check for common null cases.\n-- # if not text1 or not text2 or text1[0] != text2[0]:\n-- #     return 0\n-- # # Binary search.\n-- # # Performance analysis: http://neil.fraser.name/news/2007/10/09/\n-- # pointermin = 0\n-- # pointermax = min(len(text1), len(text2))\n-- # pointermid = pointermax\n-- # pointerstart = 0\n-- # while pointermin < pointermid:\n-- #     if text1[pointerstart:pointermid] == text2[pointerstart:pointermid]:\n-- #         pointermin = pointermid\n-- #         pointerstart = pointermin\n-- #     else:\n-- #         pointermax = pointermid\n-- #     pointermid = (pointermax - pointermin) // 2 + pointermin\n-- # return pointermid\n-- \n-- Determine the common prefix of two strings.\n-- Args:\n--     text1: First string.\n--     text2: Second string.\n-- Returns:\n--     The number of characters common to the start of each string.\nlocal function common_prefix_length(text1, text2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_304953_common_prefix_length.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = common_prefix_length\n    lu.assertEquals(candidate('abc', 'abc'), 3)\n    lu.assertEquals(candidate('ab', 'abc'), 2)\n    lu.assertEquals(candidate('aa', 'a'), 1)\n    lu.assertEquals(candidate('abc', 'abcabc'), 3)\n    lu.assertEquals(candidate('abc', 'a'), 1)\n    lu.assertEquals(candidate('', ''), 0)\n    lu.assertEquals(candidate('', ''), 0)\n    lu.assertEquals(candidate('abc', 'ab'), 2)\n    lu.assertEquals(candidate('abc', 'ab'), 2)\n    lu.assertEquals(candidate('abc', 'xabc'), 0)\n    lu.assertEquals(candidate('abc', 'ab'), 2)\n    lu.assertEquals(candidate('abc', 'abc'), 3)\n    lu.assertEquals(candidate('a', 'b'), 0)\n    lu.assertEquals(candidate('abc', 'abd'), 2)\n    lu.assertEquals(candidate('a', 'aa'), 1)\n    lu.assertEquals(candidate('abc', ''), 0)\n    lu.assertEquals(candidate('aaa', 'aaa'), 3)\n    lu.assertEquals(candidate('aaaa', 'aaaa'), 4)\n    lu.assertEquals(candidate('', ''), 0)\n    lu.assertEquals(candidate('abcd', 'abc'), 3)\n    lu.assertEquals(candidate('a', 'a'), 1)\n    lu.assertEquals(candidate('a', 'abc'), 1)\n    lu.assertEquals(candidate('abc', 'abbc'), 2)\n    lu.assertEquals(candidate('xabc', 'abc'), 0)\n    lu.assertEquals(candidate('abc', 'abcx'), 3)\n    lu.assertEquals(candidate('abc', ''), 0)\n    lu.assertEquals(candidate('ab', 'abc'), 2)\n    lu.assertEquals(candidate('aaa', 'a'), 1)\n    lu.assertEquals(candidate('abc', 'abcd'), 3)\n    lu.assertEquals(candidate('a', 'a'), 1)\n    lu.assertEquals(candidate('abc', 'abcd'), 3)\n    lu.assertEquals(candidate('a', 'aaa'), 1)\n    lu.assertEquals(candidate('abc', 'abc'), 3)\n    lu.assertEquals(candidate('abc', 'abcx'), 3)\n    lu.assertEquals(candidate('ab', 'abc'), 2)\n    lu.assertEquals(candidate('abc', 'a'), 1)\n    lu.assertEquals(candidate('abcx', 'abc'), 3)\n    lu.assertEquals(candidate('', 'abc'), 0)\n    lu.assertEquals(candidate('abc', 'xabc'), 0)\n    lu.assertEquals(candidate('', 'abc'), 0)\n    lu.assertEquals(candidate('ab', 'ab'), 2)\n    lu.assertEquals(candidate('abc', 'abcd'), 3)\n    lu.assertEquals(candidate('a', 'abc'), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_305196_count_words", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # content = content.replace(\"{{Languages}}\", \"\")\n-- # content = content.replace(\"\\n\", \" \")\n-- # content = content.replace(\"'\", \" \")\n-- # content = content.replace(\"[\", \" \")\n-- # content = content.replace(\"]\", \" \")\n-- # content = content.replace(\"=\", \" \")\n-- # content = content.replace(\">\", \" \")\n-- # content = content.replace(\"<\", \" \")\n-- # content = content.replace(\"{\", \" \")\n-- # content = content.replace(\"}\", \" \")\n-- # content = content.replace(\"|\", \" \")\n-- # content = content.replace(\"(\", \" \")\n-- # content = content.replace(\")\", \" \")\n-- # content = content.replace(\"*\", \" \")\n-- # content = content.replace(\":\", \" \")\n-- # content = content.replace(\"-\", \" \")\n-- # content = content.replace(\"!\", \" \")\n-- # content = content.replace(\"\\\"\", \" \")\n-- # content = content.replace(\"#\", \" \")\n-- # content = content.replace(\"_\", \" \")\n-- # content = content.replace(\"+\", \" \")\n-- # content = content.replace(\"/\", \" \")\n-- # content = content.replace(\".\", \" \")\n-- # content = content.replace(\"0\", \"\")\n-- # content = content.replace(\"1\", \"\")\n-- # content = content.replace(\"2\", \"\")\n-- # content = content.replace(\"3\", \"\")\n-- # content = content.replace(\"4\", \"\")\n-- # content = content.replace(\"5\", \"\")\n-- # content = content.replace(\"6\", \"\")\n-- # content = content.replace(\"7\", \"\")\n-- # content = content.replace(\"8\", \"\")\n-- # content = content.replace(\"9\", \"\")\n-- # content = content.replace(\"`\", \"\")\n-- # content = content.replace(\"&\", \"\")\n-- # content = content.replace(\"%\", \"\")\n-- # content = content.replace(\"\", \"\")\n-- # content = content.replace(\" q \", \"\")\n-- # content = content.replace(\" w \", \"\")\n-- # content = content.replace(\" e \", \"\")\n-- # content = content.replace(\" r \", \"\")\n-- # content = content.replace(\" t \", \"\")\n-- # content = content.replace(\" y \", \"\")\n-- # content = content.replace(\" u \", \"\")\n-- # content = content.replace(\" i \", \"\")\n-- # content = content.replace(\" o \", \"\")\n-- # content = content.replace(\" p \", \"\")\n-- # content = content.replace(\" s \", \"\")\n-- # content = content.replace(\" d \", \"\")\n-- # content = content.replace(\" f \", \"\")\n-- # content = content.replace(\" g \", \"\")\n-- # content = content.replace(\" h \", \"\")\n-- # content = content.replace(\" j \", \"\")\n-- # content = content.replace(\" k \", \"\")\n-- # content = content.replace(\" l \", \"\")\n-- # content = content.replace(\" z \", \"\")\n-- # content = content.replace(\" x \", \"\")\n-- # content = content.replace(\" c \", \"\")\n-- # content = content.replace(\" v \", \"\")\n-- # content = content.replace(\" b \", \"\")\n-- # content = content.replace(\" n \", \"\")\n-- # content = content.replace(\" m \", \"\")\n-- # content = content.replace(\"        \", \" \")\n-- # content = content.replace(\"     \", \" \")\n-- # content = content.replace(\"    \", \" \")\n-- # content = content.replace(\"   \", \" \")\n-- # content = content.replace(\"  \", \" \")\n-- # content = content.strip()\n-- # while content.find(\"  \") > -1:\n-- #     content = content.replace(\"  \", \" \")\n-- # return len(content.split(\" \"))\n-- \n--  dirty code for count word \nlocal function count_words(content)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_305196_count_words.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = count_words\n    lu.assertEquals(candidate('Python is a programming language and is fun to learn'), 10)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_305323_string_to_ascii_html", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # html = []\n-- # for c in string:\n-- #     cord = ord(c)\n-- #     if 31 < cord < 127:\n-- #         html.append(c)\n-- #     else:\n-- #         html.append('&#{};'.format(cord))\n-- # return ''.join(html)\n-- \n-- Convert unicode chars of str to HTML entities if chars are not ASCII.\nlocal function string_to_ascii_html(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_305323_string_to_ascii_html.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = string_to_ascii_html\n    lu.assertEquals(candidate('abc'), 'abc')\n    lu.assertEquals(candidate('hello world'), 'hello world')\n    lu.assertEquals(candidate('123'), '123')\n    lu.assertEquals(candidate('Hello \u4f60\u597d World'), 'Hello &#20320;&#22909; World')\n    lu.assertEquals(candidate('\ud83d\udc18'), '&#128024;')\n    lu.assertEquals(candidate('123 abc ABC'), '123 abc ABC')\n    lu.assertEquals(candidate('Hello World'), 'Hello World')\n    lu.assertEquals(candidate('ABC'), 'ABC')\n    lu.assertEquals(candidate('This is a test.'), 'This is a test.')\n    lu.assertEquals(candidate('ABC 123'), 'ABC 123')\n    lu.assertEquals(candidate('123 abc ABC 123'), '123 abc ABC 123')\n    lu.assertEquals(candidate('a'), 'a')\n    lu.assertEquals(candidate('Hello World!'), 'Hello World!')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_305496__hex_to_char", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return bytearray.fromhex(chr_pair).decode()\n-- \n--  '61' => 'a' \nlocal function _hex_to_char(chr_pair)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_305496__hex_to_char.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _hex_to_char\n    lu.assertEquals(candidate('7e'), '~')\n    lu.assertEquals(candidate('01'), '\\x01')\n    lu.assertEquals(candidate('00'), '\\x00')\n    lu.assertEquals(candidate('61'), 'a')\n    lu.assertEquals(candidate('02'), '\\x02')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_305558_make_shard_files", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # idx = 0\n-- # shard_files = []\n-- # for dataset_file in dataset_files:\n-- #     if idx % num_shards == shard_id:\n-- #         shard_files.append((dataset_file, -1, -1, True))\n-- #     idx += 1\n-- # return shard_files\n-- \n--  Make sharding files when shard_equal_rows is False. \nlocal function make_shard_files(dataset_files, num_shards, shard_id)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_305558_make_shard_files.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = make_shard_files\n    lu.assertEquals(candidate({'dataset_file1.csv', 'dataset_file2.csv', 'dataset_file3.csv'}, 2, 1), {{'dataset_file2.csv', -1, -1, true}})\n    lu.assertEquals(candidate({'dataset_file1.csv', 'dataset_file2.csv', 'dataset_file3.csv'}, 3, 2), {{'dataset_file3.csv', -1, -1, true}})\n    lu.assertEquals(candidate({'dataset_file1.csv', 'dataset_file2.csv', 'dataset_file3.csv'}, 3, 0), {{'dataset_file1.csv', -1, -1, true}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_305605_idx2off", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return i * 32 - (8 * (i//3))\n-- \n--  Produces [0, 32, 64,   88, 120, 152,   176, 208, 240,   264, 296, 328]\n-- These are the byte offsets when dividing into 44-coeff chunks\nlocal function idx2off(i)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_305605_idx2off.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = idx2off\n    lu.assertEquals(candidate(13), 384)\n    lu.assertEquals(candidate(1), 32)\n    lu.assertEquals(candidate(14), 416)\n    lu.assertEquals(candidate(9), 264)\n    lu.assertEquals(candidate(10), 296)\n    lu.assertEquals(candidate(3), 88)\n    lu.assertEquals(candidate(12), 352)\n    lu.assertEquals(candidate(11), 328)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(5), 152)\n    lu.assertEquals(candidate(4), 120)\n    lu.assertEquals(candidate(7), 208)\n    lu.assertEquals(candidate(6), 176)\n    lu.assertEquals(candidate(2), 64)\n    lu.assertEquals(candidate(8), 240)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_305774_translate", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return x+dx, y+dy\n-- \n-- Translate vector(x,y) by (dx,dy).\nlocal function translate(x, y, dx, dy)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_305774_translate.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = translate\n    lu.assertEquals(candidate(100, 200, 10, -10), {110, 190})\n    lu.assertEquals(candidate(-100, 200, 10, -10), {-90, 190})\n    lu.assertEquals(candidate(2, 2, 1, 0), {3, 2})\n    lu.assertEquals(candidate(100, 200, -10, 10), {90, 210})\n    lu.assertEquals(candidate(2, 2, 1, 1), {3, 3})\n    lu.assertEquals(candidate(-1, -2, 2, -2), {1, -4})\n    lu.assertEquals(candidate(0, 0, 1, 1), {1, 1})\n    lu.assertEquals(candidate(1, 1, 1, 0), {2, 1})\n    lu.assertEquals(candidate(0, 0, 10, -10), {10, -10})\n    lu.assertEquals(candidate(0, 0, -10, 10), {-10, 10})\n    lu.assertEquals(candidate(1, 1, 0, 0), {1, 1})\n    lu.assertEquals(candidate(100, 200, 10, 10), {110, 210})\n    lu.assertEquals(candidate(0, 0, 0, 0), {0, 0})\n    lu.assertEquals(candidate(0, 0, 10, 10), {10, 10})\n    lu.assertEquals(candidate(0, 1, 0, 0), {0, 1})\n    lu.assertEquals(candidate(0, 1, 0, 2), {0, 3})\n    lu.assertEquals(candidate(0, 0, 0, 0), {0, 0})\n    lu.assertEquals(candidate(1, 0, 0, 2), {1, 2})\n    lu.assertEquals(candidate(100, 200, -10, -10), {90, 190})\n    lu.assertEquals(candidate(-100, 200, 10, 10), {-90, 210})\n    lu.assertEquals(candidate(1, 0, 2, 0), {3, 0})\n    lu.assertEquals(candidate(-1, -1, 1, 1), {0, 0})\n    lu.assertEquals(candidate(2, 2, 0, 1), {2, 3})\n    lu.assertEquals(candidate(1, 2, 0, 0), {1, 2})\n    lu.assertEquals(candidate(1, 2, 3, 4), {4, 6})\n    lu.assertEquals(candidate(0, 1, 2, 0), {2, 1})\n    lu.assertEquals(candidate(0, 0, -10, -10), {-10, -10})\n    lu.assertEquals(candidate(1, 2, 2, -2), {3, 0})\n    lu.assertEquals(candidate(-100, 200, -10, 10), {-110, 210})\n    lu.assertEquals(candidate(2, 2, 0, 0), {2, 2})\n    lu.assertEquals(candidate(1, 1, 1, 1), {2, 2})\n    lu.assertEquals(candidate(-100, 200, -10, -10), {-110, 190})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_306162_get_target_delta", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # den = 1\n-- # while data_size // den >= 1:\n-- #     den *= 10\n-- # return 1 / den\n-- \n-- Generate target delta given the size of a dataset. Delta should be\n-- less than the inverse of the datasize.\n-- Parameters\n-- ----------\n-- data_size : int\n--     The size of the dataset.\n-- Returns\n-- -------\n-- float\n--     The target delta value.\nlocal function get_target_delta(data_size)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_306162_get_target_delta.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_target_delta\n    lu.assertEquals(candidate(10000000), 1e-08)\n    lu.assertEquals(candidate(1), 0.1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_306499_getKmers", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # import itertools\n-- # kmers = [''.join(p) for p in itertools.product(bases, repeat=k)]\n-- # return kmers\n-- \n-- Generate k-mers of size k\nlocal function getKmers(k, bases)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_306499_getKmers.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = getKmers\n    lu.assertEquals(candidate(1, 'ACGT'), {'A', 'C', 'G', 'T'})\n    lu.assertEquals(candidate(2, 'ACGT'), {'AA', 'AC', 'AG', 'AT', 'CA', 'CC', 'CG', 'CT', 'GA', 'GC', 'GG', 'GT', 'TA', 'TC', 'TG', 'TT'})\n    lu.assertEquals(candidate(3, 'ACGT'), {'AAA', 'AAC', 'AAG', 'AAT', 'ACA', 'ACC', 'ACG', 'ACT', 'AGA', 'AGC', 'AGG', 'AGT', 'ATA', 'ATC', 'ATG', 'ATT', 'CAA', 'CAC', 'CAG', 'CAT', 'CCA', 'CCC', 'CCG', 'CCT', 'CGA', 'CGC', 'CGG', 'CGT', 'CTA', 'CTC', 'CTG', 'CTT', 'GAA', 'GAC', 'GAG', 'GAT', 'GCA', 'GCC', 'GCG', 'GCT', 'GGA', 'GGC', 'GGG', 'GGT', 'GTA', 'GTC', 'GTG', 'GTT', 'TAA', 'TAC', 'TAG', 'TAT', 'TCA', 'TCC', 'TCG', 'TCT', 'TGA', 'TGC', 'TGG', 'TGT', 'TTA', 'TTC', 'TTG', 'TTT'})\n    lu.assertEquals(candidate(2, 'ACGT'), {'AA', 'AC', 'AG', 'AT', 'CA', 'CC', 'CG', 'CT', 'GA', 'GC', 'GG', 'GT', 'TA', 'TC', 'TG', 'TT'})\n    lu.assertEquals(candidate(1, 'A'), {'A'})\n    lu.assertEquals(candidate(1, 'ATCGT'), {'A', 'T', 'C', 'G', 'T'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_306534_alphabetical", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return list(reversed(sorted(lst, key=lambda x: x[0])))\n-- \n--  Sorts a list of tuples in reverse alphabetical order by the first key\n-- in the tuple.\n-- Arguments:\n-- lst -- the list to sort\n-- Returns:\n-- the sorted list\nlocal function alphabetical(lst)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_306534_alphabetical.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = alphabetical\n    lu.assertEquals(candidate({{'a', 1}, {'a', 2}, {'a', 3}, {'a', 4}, {'a', 5}, {'a', 6}, {'a', 7}, {'a', 8}, {'a', 9}, {'a', 10}}), {{'a', 10}, {'a', 9}, {'a', 8}, {'a', 7}, {'a', 6}, {'a', 5}, {'a', 4}, {'a', 3}, {'a', 2}, {'a', 1}})\n    lu.assertEquals(candidate(list()), list())\n    lu.assertEquals(candidate({{'a', 1}, {'b', 2}, {'c', 3}, {'d', 4}, {'e', 5}, {'f', 6}, {'g', 7}, {'h', 8}, {'i', 9}, {'j', 10}}), {{'j', 10}, {'i', 9}, {'h', 8}, {'g', 7}, {'f', 6}, {'e', 5}, {'d', 4}, {'c', 3}, {'b', 2}, {'a', 1}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_307233_toWidthHeightInverse", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # anchor = [0., 0., 0., 0.]\n-- # # We need to subtract 1 from the x2,y2 values because the  heights\n-- # # and widths of the areas is of the area taken up by the pixels, they\n-- # # go across the top-right pixel\n-- # anchor[0] = wh[2] - .5 * wh[0]\n-- # anchor[1] = wh[3] - .5 * wh[1]\n-- # anchor[2] = wh[2] + .5 * wh[0] - 1\n-- # anchor[3] = wh[3] + .5 * wh[1] - 1\n-- # return anchor\n-- \n--  Transforms from [w, h, x, y] to [x0,y0,x1,y1] format\nlocal function toWidthHeightInverse(wh)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_307233_toWidthHeightInverse.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = toWidthHeightInverse\n    lu.assertEquals(candidate({2, 2, 1, 1}), {0, 0, 1, 1})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_307309_binV2_to_A2", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return S2*(mv_per_bin*1.0e-3)**2/(R_acq**2)\n-- \n--     Used to convert SII(t)[bin_V**2] to SII(t)[A**2]\nlocal function binV2_to_A2(S2, R_acq, mv_per_bin)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_307309_binV2_to_A2.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = binV2_to_A2\n    lu.assertEquals(candidate(0, 1, 1), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_307930_find_all", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # location = 0\n-- # locations = []\n-- # while location != -1:\n-- #     location = searchin.find(substr, location)\n-- #     if location != -1:\n-- #         locations.append(location)\n-- #         location += len(substr)\n-- # return locations\n-- \n-- returns a list of locations where substr occurs in searchin\n-- locations are not allowed to overlap\nlocal function find_all(searchin, substr)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_307930_find_all.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = find_all\n    lu.assertEquals(candidate('aa', 'aaaa'), {})\n    lu.assertEquals(candidate('abracadabra', 'bra'), {1, 8})\n    lu.assertEquals(candidate('a a a a a a a a a', 'b'), {})\n    lu.assertEquals(candidate('aa', 'a'), {0, 1})\n    lu.assertEquals(candidate('a', 'a'), {0})\n    lu.assertEquals(candidate('', 'a'), {})\n    lu.assertEquals(candidate('aaa', 'aaaaaaa'), {})\n    lu.assertEquals(candidate('abcdefghijklmnopqrstuvwxyz', 'xyz'), {23})\n    lu.assertEquals(candidate('aa', 'aabbaa'), {})\n    lu.assertEquals(candidate('aaaa', 'aa'), {0, 2})\n    lu.assertEquals(candidate('', 'x'), {})\n    lu.assertEquals(candidate('a', 'aa'), {})\n    lu.assertEquals(candidate('abcde', 'cde'), {2})\n    lu.assertEquals(candidate('123456', '0'), {})\n    lu.assertEquals(candidate('aaa', 'a'), {0, 1, 2})\n    lu.assertEquals(candidate('a', 'aaa'), {})\n    lu.assertEquals(candidate('x    y', 'x'), {0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_308818_event_independence_check", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # print('Probablity {0} and {1} = {2} and joint probability = {3}'.format(prob_event1, prob_event2, prob_event1*prob_event2, prob_event1_event2))\n-- # return (prob_event1_event2 == prob_event1 * prob_event2)\n-- \n-- Checks if two events are independent.\n-- This function accepts the probability of 2 events and their joint probability.\n-- And prints if the events are independent or not.\n-- Keyword arguments:\n-- prob_event1 -- probability of event1\n-- prob_event2 -- probability of event2\n-- prob_event1_event2 -- probability of event1 and event2\nlocal function event_independence_check(prob_event1, prob_event2, prob_event1_event2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_308818_event_independence_check.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = event_independence_check\n    lu.assertEquals(candidate(0.33, 0.67, 0.19), false)\n    lu.assertEquals(candidate(0.3, 0.6, 0.132), false)\n    lu.assertEquals(candidate(0.2, 0.8, 0.08), false)\n    lu.assertEquals(candidate(0.25, 0.75, 0.2), false)\n    lu.assertEquals(candidate(0.5, 0.5, 0.25), true)\n    lu.assertEquals(candidate(0.3, 0.6, 0.054), false)\n    lu.assertEquals(candidate(0.9, 0.7, 0.635), false)\n    lu.assertEquals(candidate(0.6, 0.6, 0.36), true)\n    lu.assertEquals(candidate(0.2, 0.6, 0.096), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_309108_deserialize_sanitizer_options", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # pairs = options.split(':')\n-- # return_dict = {}\n-- # for pair in pairs:\n-- #     k, v = pair.split('=')\n-- #     return_dict[k] = v\n-- # return return_dict\n-- \n-- Read options from a variable like ASAN_OPTIONS into a dict.\nlocal function deserialize_sanitizer_options(options)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_309108_deserialize_sanitizer_options.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = deserialize_sanitizer_options\n    lu.assertEquals(candidate('handle_segv=1'), {['handle_segv'] = '1'})\n    lu.assertEquals(candidate('handle_segv=1:detect_stack_use_after_return=1'), {['handle_segv'] = '1', ['detect_stack_use_after_return'] = '1'})\n    lu.assertEquals(candidate('foo=bar:baz=123'), {['foo'] = 'bar', ['baz'] = '123'})\n    lu.assertEquals(candidate('allocator_may_return_null=1'), {['allocator_may_return_null'] = '1'})\n    lu.assertEquals(candidate('detect_leaks=0'), {['detect_leaks'] = '0'})\n    lu.assertEquals(candidate('handle_segv=1'), {['handle_segv'] = '1'})\n    lu.assertEquals(candidate('detect_odr_violation=0'), {['detect_odr_violation'] = '0'})\n    lu.assertEquals(candidate('strict_memcmp=1:detect_leaks=0:allocator_may_return_null=1'), {['strict_memcmp'] = '1', ['detect_leaks'] = '0', ['allocator_may_return_null'] = '1'})\n    lu.assertEquals(candidate('allow_user_segv_handler=0'), {['allow_user_segv_handler'] = '0'})\n    lu.assertEquals(candidate('detect_leaks=0:allocator_may_return_null=1'), {['detect_leaks'] = '0', ['allocator_may_return_null'] = '1'})\n    lu.assertEquals(candidate('allocator_may_return_null=1'), {['allocator_may_return_null'] = '1'})\n    lu.assertEquals(candidate('detect_stack_use_after_return=1'), {['detect_stack_use_after_return'] = '1'})\n    lu.assertEquals(candidate('detect_leaks=0:handle_segv=1'), {['detect_leaks'] = '0', ['handle_segv'] = '1'})\n    lu.assertEquals(candidate('print_suppressions=1'), {['print_suppressions'] = '1'})\n    lu.assertEquals(candidate('detect_stack_use_after_return=1:handle_segv=1'), {['detect_stack_use_after_return'] = '1', ['handle_segv'] = '1'})\n    lu.assertEquals(candidate('detect_leaks=0'), {['detect_leaks'] = '0'})\n    lu.assertEquals(candidate('detect_leaks=0'), {['detect_leaks'] = '0'})\n    lu.assertEquals(candidate('detect_leaks=1'), {['detect_leaks'] = '1'})\n    lu.assertEquals(candidate('detect_leaks=1:allocator_may_return_null=0'), {['detect_leaks'] = '1', ['allocator_may_return_null'] = '0'})\n    lu.assertEquals(candidate('coverage=1:coverage_dir=/somewhere'), {['coverage'] = '1', ['coverage_dir'] = '/somewhere'})\n    lu.assertEquals(candidate('print_suppressions=0'), {['print_suppressions'] = '0'})\n    lu.assertEquals(candidate('detect_leaks=0:handle_segv=1:detect_stack_use_after_return=1'), {['detect_leaks'] = '0', ['handle_segv'] = '1', ['detect_stack_use_after_return'] = '1'})\n    lu.assertEquals(candidate('handle_sigbus=1'), {['handle_sigbus'] = '1'})\n    lu.assertEquals(candidate('detect_leaks=0:detect_leaks=1'), {['detect_leaks'] = '1'})\n    lu.assertEquals(candidate('allocator_may_return_null=0'), {['allocator_may_return_null'] = '0'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_310088_indentation_value", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # value = 0\n-- # for char in spaces:\n-- #     value += (8 - (value % 8)) if char == '\\t' else 1\n-- # return value\n-- \n-- Given an indentation string of spaces and tabs,\n-- returns the equivalent number of spaces per Python\n-- indentation rule.\nlocal function indentation_value(spaces)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_310088_indentation_value.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = indentation_value\n    lu.assertEquals(candidate('      '), 6)\n    lu.assertEquals(candidate('        '), 8)\n    lu.assertEquals(candidate('    '), 4)\n    lu.assertEquals(candidate('\\t'), 8)\n    lu.assertEquals(candidate(''), 0)\n    lu.assertEquals(candidate(''), 0)\n    lu.assertEquals(candidate('        '), 8)\n    lu.assertEquals(candidate('  '), 2)\n    lu.assertEquals(candidate('  '), 2)\n    lu.assertEquals(candidate('    '), 4)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_3108_lines_in_file", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # with open(filename, \"r\") as f:\n-- #     return len(f.readlines())\n-- \n-- Count the number of lines in a file\n-- :param filename: A string containing the relative or absolute path to a file\n-- :returns: The number of lines in the file\nlocal function lines_in_file(filename)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_3108_lines_in_file.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = lines_in_file\n    lu.assertEquals(candidate('no_such_file'), 0)\n    lu.assertEquals(candidate('no_such_file'), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_312017_deserialize_datetime", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # try:\n-- #     from dateutil.parser import parse\n-- #     return parse(string)\n-- # except ImportError:\n-- #     return string\n-- \n-- Deserializes string to datetime.\n-- The string should be in iso8601 datetime format.\n-- :param string: str.\n-- :type string: str\n-- :return: datetime.\n-- :rtype: datetime\nlocal function deserialize_datetime(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_312017_deserialize_datetime.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = deserialize_datetime\n    lu.assertEquals(candidate('2018-01-01T12:34:56'), '2018-01-01T12:34:56')\n    lu.assertEquals(candidate('2018-01-01T12:34:56.123456-01:00'), '2018-01-01T12:34:56.123456-01:00')\n    lu.assertEquals(candidate('2018-01-01T12:34:56.123456Z'), '2018-01-01T12:34:56.123456Z')\n    lu.assertEquals(candidate('2018-01-01T12:34:56Z'), '2018-01-01T12:34:56Z')\n    lu.assertEquals(candidate('2018-01-01T12:34:56.123456'), '2018-01-01T12:34:56.123456')\n    lu.assertEquals(candidate('2018-01-01T12:34:56.123456-01:00Z'), '2018-01-01T12:34:56.123456-01:00Z')\n    lu.assertEquals(candidate(None), None)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_312491_unsignedIntegerToBytes", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # bytes = list(range(numbytes))\n-- # for i in bytes:\n-- #     bytes[i] = int(integer % 256)\n-- #     integer -= integer % 256\n-- #     integer = int(integer//256)\n-- # if integer > 0:\n-- #     raise IndexError('Overflow in conversion between uint and byte list.')\n-- # else:\n-- #     return bytes\n-- \n-- Converts an unsigned integer into a sequence of bytes, LSB first.\n-- integer -- the number to be converted\n-- numbytes -- the number of bytes to be used in representing the integer\nlocal function unsignedIntegerToBytes(integer, numbytes)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_312491_unsignedIntegerToBytes.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = unsignedIntegerToBytes\n    lu.assertEquals(candidate(0, 2), {0, 0})\n    lu.assertEquals(candidate(65535, 3), {255, 255, 0})\n    lu.assertEquals(candidate(0, 3), {0, 0, 0})\n    lu.assertEquals(candidate(255, 1), {255})\n    lu.assertEquals(candidate(0, 4), {0, 0, 0, 0})\n    lu.assertEquals(candidate(0, 1), {0})\n    lu.assertEquals(candidate(1, 1), {1})\n    lu.assertEquals(candidate(257, 2), {1, 1})\n    lu.assertEquals(candidate(256, 4), {0, 1, 0, 0})\n    lu.assertEquals(candidate(1, 2), {1, 0})\n    lu.assertEquals(candidate(257, 4), {1, 1, 0, 0})\n    lu.assertEquals(candidate(42, 2), {42, 0})\n    lu.assertEquals(candidate(4294967295, 4), {255, 255, 255, 255})\n    lu.assertEquals(candidate(256, 3), {0, 1, 0})\n    lu.assertEquals(candidate(256, 2), {0, 1})\n    lu.assertEquals(candidate(65536, 4), {0, 0, 1, 0})\n    lu.assertEquals(candidate(257, 3), {1, 1, 0})\n    lu.assertEquals(candidate(1, 3), {1, 0, 0})\n    lu.assertEquals(candidate(65537, 4), {1, 0, 1, 0})\n    lu.assertEquals(candidate(65535, 2), {255, 255})\n    lu.assertEquals(candidate(1, 4), {1, 0, 0, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_314698_is_non_negative", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return all(value >= 0 for value in input_dict.values())\n-- \n-- Check if the input dictionary values are non negative.\n-- Args:\n--     input_dict (dict): dictionary.\n-- Returns:\n--     bool: boolean variable indicating whether dict values are non negative or not.\nlocal function is_non_negative(input_dict)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_314698_is_non_negative.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_non_negative\n    lu.assertEquals(candidate({[1] = 1, [2] = 2}), true)\n    lu.assertEquals(candidate({[1] = 1, [2] = 2, [3] = -3, [4] = 4}), false)\n    lu.assertEquals(candidate({[1] = 1, [2] = 2, [3] = -3}), false)\n    lu.assertEquals(candidate({}), true)\n    lu.assertEquals(candidate({[1] = 1}), true)\n    lu.assertEquals(candidate({[1] = 1, [2] = 2, [3] = 3}), true)\n    lu.assertEquals(candidate({[1] = 1, [2] = 2, [3] = -3, [4] = -4}), false)\n    lu.assertEquals(candidate({[1] = 1, [2] = 2, [3] = 3, [4] = 4}), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_315387_num_desc_seq_given_total_and_head", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if total < 1 or head < 1:\n-- #     return 0\n-- # # base case: sequence has only one term\n-- # if total == head:\n-- #     return 1\n-- # # recursive case: sequence has more than one term\n-- # # the second term cannot exceed the head; take advantage of transitivity\n-- # num_seq = 0\n-- # for _second in range(1, head + 1):\n-- #     num_seq += num_desc_seq_given_total_and_head(total - head, _second)\n-- # return num_seq\n-- \n-- Subproblem in dynamic programming.\n-- Count the number of descending sequences given a total and the head.\n-- Note that a one-term sequence is also considered a sequence.\nlocal function num_desc_seq_given_total_and_head(total, head)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_315387_num_desc_seq_given_total_and_head.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = num_desc_seq_given_total_and_head\n    lu.assertEquals(candidate(5, 25), 0)\n    lu.assertEquals(candidate(10, 10), 1)\n    lu.assertEquals(candidate(2, 3), 0)\n    lu.assertEquals(candidate(4, 3), 1)\n    lu.assertEquals(candidate(2, 1), 1)\n    lu.assertEquals(candidate(5, 3), 2)\n    lu.assertEquals(candidate(4, 30), 0)\n    lu.assertEquals(candidate(1, 1), 1)\n    lu.assertEquals(candidate(4, 7), 0)\n    lu.assertEquals(candidate(1, 2), 0)\n    lu.assertEquals(candidate(4, 2), 2)\n    lu.assertEquals(candidate(3, 2), 1)\n    lu.assertEquals(candidate(10, 100), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_316104_fontname", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # global _fontname\n-- # if name is not None:\n-- #     _fontname = name\n-- # return _fontname\n-- \n--  Sets the current font used when drawing text.\nlocal function fontname(name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_316104_fontname.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = fontname\n    lu.assertEquals(candidate('Arial'), 'Arial')\n    lu.assertEquals(candidate('Helvetica'), 'Helvetica')\n    lu.assertEquals(candidate('Arial'), 'Arial')\n    lu.assertEquals(candidate('Courier'), 'Courier')\n    lu.assertEquals(candidate('Times-Roman'), 'Times-Roman')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_316766_get_flowcell_name_from_desc", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if \"flow_cell_id\" in description_dict:\n-- #     flowcell_name = description_dict[\"flow_cell_id\"]\n-- # elif \"sample_id\" in description_dict:\n-- #     flowcell_name = description_dict[\"sample_id\"]\n-- # elif \"sampleid\" in description_dict:\n-- #     flowcell_name = description_dict[\"sampleid\"]\n-- # else:\n-- #     flowcell_name = user_run_name\n-- # return flowcell_name\n-- \n-- Get the flowcell name from the description\n-- Parameters\n-- ----------\n-- description_dict: dict\n--     A parsed dictionary created from the description from the fastq record\n-- user_run_name: str\n--     The user run name that we have been given on the command line\n-- Returns\n-- -------\n-- flowcell_name: str\n--     The flowcell name that we are gonna be using\nlocal function get_flowcell_name_from_desc(description_dict, user_run_name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_316766_get_flowcell_name_from_desc.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_flowcell_name_from_desc\n    lu.assertEquals(candidate({}, 'my_run'), 'my_run')\n    lu.assertEquals(candidate({}, 'foo'), 'foo')\n    lu.assertEquals(candidate({['sampleid'] = 'ABCD1234'}, 'ignored'), 'ABCD1234')\n    lu.assertEquals(candidate({['flow_cell_id'] = '1234'}, 'ignored'), '1234')\n    lu.assertEquals(candidate({['sampleid'] = 'foo'}, 'bar'), 'foo')\n    lu.assertEquals(candidate({['flow_cell_id'] = 'AB12345'}, None), 'AB12345')\n    lu.assertEquals(candidate({['sample_id'] = 'ABCD1234'}, 'ignored'), 'ABCD1234')\n    lu.assertEquals(candidate({['flow_cell_id'] = '12345', ['sample_id'] = '54321', ['sampleid'] = '55555'}, 'user_run_name'), '12345')\n    lu.assertEquals(candidate({}, 'user_run_name'), 'user_run_name')\n    lu.assertEquals(candidate({['flow_cell_id'] = 'foo'}, 'bar'), 'foo')\n    lu.assertEquals(candidate({['sample_id'] = 'AB12345'}, None), 'AB12345')\n    lu.assertEquals(candidate({['sampleid'] = 'AB12345'}, 'user_run_name'), 'AB12345')\n    lu.assertEquals(candidate({['flow_cell_id'] = '1234'}, '1111'), '1234')\n    lu.assertEquals(candidate({}, '1234'), '1234')\n    lu.assertEquals(candidate({['flow_cell_id'] = 'AB12345'}, 'user_run_name'), 'AB12345')\n    lu.assertEquals(candidate({['sampleid'] = '1234'}, '1111'), '1234')\n    lu.assertEquals(candidate({['sample_id'] = '1234'}, '1111'), '1234')\n    lu.assertEquals(candidate({['sample_id'] = '54321', ['sampleid'] = '55555'}, 'user_run_name'), '54321')\n    lu.assertEquals(candidate({['sample_id'] = 'AB12345'}, 'user_run_name'), 'AB12345')\n    lu.assertEquals(candidate({['sample_id'] = 'foo'}, 'bar'), 'foo')\n    lu.assertEquals(candidate({['flow_cell_id'] = '12345', ['sample_id'] = '54321'}, 'user_run_name'), '12345')\n    lu.assertEquals(candidate({['sample_id'] = '54321', ['sampleid'] = '555555'}, 'user_run_name'), '54321')\n    lu.assertEquals(candidate({}, 'bar'), 'bar')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_316829_pretty_print_ec2_res_id", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return res['ReservedInstancesId'].split('-')[0]+'...'\n-- \n--  Pretty-print the EC2 reservation ID \nlocal function pretty_print_ec2_res_id(res)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_316829_pretty_print_ec2_res_id.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = pretty_print_ec2_res_id\n    lu.assertEquals(candidate({['ReservedInstancesId'] = '73599745-9067-4a6c-8629-99a4154e1606'}), '73599745...')\n    lu.assertEquals(candidate({['ReservedInstancesId'] = '4a928483-9067-4a6c-8629-99a4154e1606'}), '4a928483...')\n    lu.assertEquals(candidate({['ReservedInstancesId'] = '89599745-9067-4a6c-8629-99a4154e1606'}), '89599745...')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_316883_code_to_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # geo_name = ''\n-- # if code in code_dict:\n-- #     geo_name = code_dict[code]\n-- # return geo_name\n-- \n--  returns a country name if the given dictionary contains a code for it\n-- if the code is not in the dictionary it will return an empty string\n-- Args:\n--     code (str):         country code\n--     code_dict (dict):   dictionary with code as key and name as value\n-- Returns:\n--     geo_name (str):     name of country\nlocal function code_to_name(code, code_dict)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_316883_code_to_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = code_to_name\n    lu.assertEquals(candidate('XY', {['AL'] = 'Albania'}), '')\n    lu.assertEquals(candidate('AL', {['AL'] = 'Albania'}), 'Albania')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_317662_delete_keys_from_dict", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for a in keys_to_remove:\n-- #     del dictionary[a]\n-- # return dictionary\n-- \n-- Utility to remove specific keys from a dictionary.\n-- Parameters\n-- ----------\n-- dictionary : dict\n--     Input dictionary.\n-- keys_to_remove : list\n--     list of keys to remove from the dictionary. If this list is\n--     non-unique (i.e. has repeats), then there will be an error. Also, if\n--     any element in this list is not a key in the dictionary, there will\n--     be an error as well.\n-- Returns\n-- -------\n-- dictionary : dict\n--     Dictionary with keys removed.\nlocal function delete_keys_from_dict(dictionary, keys_to_remove)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_317662_delete_keys_from_dict.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = delete_keys_from_dict\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3}, {}), {['a'] = 1, ['b'] = 2, ['c'] = 3})\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3}, {'a', 'c', 'b'}), {})\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3}, {'a', 'c'}), {['b'] = 2})\n    lu.assertEquals(candidate({['a'] = 0, ['b'] = 1, ['c'] = 2}, {'a'}), {['b'] = 1, ['c'] = 2})\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3}, {'a', 'b'}), {['c'] = 3})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_317743_wordCount", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # words = cleantext.split()\n-- # # Counting words and saving count in dictionary\n-- # count = {}\n-- # for word in words:\n-- #     if word in count:\n-- #         count[word] += 1\n-- #     else:\n-- #         count[word] = 1\n-- # return count\n-- \n-- This function counts words from a text and returns a dictionary. \n-- Does not fix for punctuation and special characters, so for example 'why' and 'why?' will be counted as two different words. This doesn't matter in the case of youtube subtitles.\n-- INPUT: string\n-- OUTPUT: dictionary where keys are words, and the values are counts.\nlocal function wordCount(cleantext)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_317743_wordCount.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = wordCount\n    lu.assertEquals(candidate(''), {})\n    lu.assertEquals(candidate(' '), {})\n    lu.assertEquals(candidate('It was a bright cold day in April, and the clocks were striking thirteen.'), {['It'] = 1, ['was'] = 1, ['a'] = 1, ['bright'] = 1, ['cold'] = 1, ['day'] = 1, ['in'] = 1, ['April,'] = 1, ['and'] = 1, ['the'] = 1, ['clocks'] = 1, ['were'] = 1, ['striking'] = 1, ['thirteen.'] = 1})\n    lu.assertEquals(candidate('Hello world'), {['Hello'] = 1, ['world'] = 1})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_317929_get_split_parts", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # same_part = num // num_part\n-- # remain_num = num % num_part\n-- # if remain_num == 0:\n-- #     return [same_part] * num_part\n-- # return [same_part] * num_part + [remain_num]\n-- \n-- get split parts\nlocal function get_split_parts(num, num_part)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_317929_get_split_parts.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_split_parts\n    lu.assertEquals(candidate(26, 2), {13, 13})\n    lu.assertEquals(candidate(1000, 10), {100, 100, 100, 100, 100, 100, 100, 100, 100, 100})\n    lu.assertEquals(candidate(10, 2), {5, 5})\n    lu.assertEquals(candidate(10, 5), {2, 2, 2, 2, 2})\n    lu.assertEquals(candidate(100, 1), {100})\n    lu.assertEquals(candidate(12, 1), {12})\n    lu.assertEquals(candidate(11, 1), {11})\n    lu.assertEquals(candidate(12, 3), {4, 4, 4})\n    lu.assertEquals(candidate(26, 1), {26})\n    lu.assertEquals(candidate(10, 1), {10})\n    lu.assertEquals(candidate(13, 1), {13})\n    lu.assertEquals(candidate(100, 5), {20, 20, 20, 20, 20})\n    lu.assertEquals(candidate(100, 4), {25, 25, 25, 25})\n    lu.assertEquals(candidate(15, 3), {5, 5, 5})\n    lu.assertEquals(candidate(25, 1), {25})\n    lu.assertEquals(candidate(100, 2), {50, 50})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_318311__safe_parse", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if \"PRIVMSG\" in msg:\n-- #     try:\n-- #         user = msg.split(\":\")[1].split(\"!\")[0]\n-- #         message = msg.split(\":\")[2].strip()\n-- #         return {\"user\": user, \"msg\": message}\n-- #     except IndexError:\n-- #         return None\n-- \n-- Parse an irc message.\n-- Args:\n--     msg (str): raw message\n-- Return:\n--     dict: {'user': user, 'msg': message}\nlocal function _safe_parse(msg)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_318311__safe_parse.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _safe_parse\n    lu.assertEquals(candidate('PRIVMSG 12345!@#$%^&*()'), None)\n    lu.assertEquals(candidate('PRIVMSG!@#$%^&*()!@#$%^&*()'), None)\n    lu.assertEquals(candidate('PRIVMSG 12345 12345'), None)\n    lu.assertEquals(candidate(':nick!<EMAIL> PRIVMSG'), None)\n    lu.assertEquals(candidate('PRIVMSG'), None)\n    lu.assertEquals(candidate('PRIVMSG'), None)\n    lu.assertEquals(candidate(':nick!<EMAIL> PRIVMSG #chan :I have a message'), {['user'] = 'nick', ['msg'] = 'I have a message'})\n    lu.assertEquals(candidate('PRIVMSG!@#$%^&*() 12345'), None)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_319216_sum_digits", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # sum_of_digits = 0\n-- # count = 0\n-- # for item in s:\n-- #     if item in list('0123456789'):\n-- #         sum_of_digits += int(item)\n-- #         count += 1\n-- # if not count:\n-- #     raise ValueError\n-- # return sum_of_digits\n-- \n-- assumes s a string\n-- Returns an int that is the sum of all of the digits in s.\n-- If there are no digits in s it raises a ValueError exception.\nlocal function sum_digits(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_319216_sum_digits.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sum_digits\n    lu.assertEquals(candidate('-123'), 6)\n    lu.assertEquals(candidate(str(-123)), 6)\n    lu.assertEquals(candidate(str(123)), 6)\n    lu.assertEquals(candidate('a0123456789b'), 45)\n    lu.assertEquals(candidate('123'), 6)\n    lu.assertEquals(candidate('987'), 24)\n    lu.assertEquals(candidate('abc123'), 6)\n    lu.assertEquals(candidate('123abc'), 6)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_319388_bit_len", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # length = 0\n-- # while int_type:\n-- #     int_type >>= 1\n-- #     length += 1\n-- # return length\n-- \n-- Helper function returning the number of bits required to binary encode an integer.\nlocal function bit_len(int_type)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_319388_bit_len.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = bit_len\n    lu.assertEquals(candidate(5), 3)\n    lu.assertEquals(candidate(7), 3)\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(127), 7)\n    lu.assertEquals(candidate(6), 3)\n    lu.assertEquals(candidate(23), 5)\n    lu.assertEquals(candidate(30), 5)\n    lu.assertEquals(candidate(26), 5)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(29), 5)\n    lu.assertEquals(candidate(19), 5)\n    lu.assertEquals(candidate(13), 4)\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(3), 2)\n    lu.assertEquals(candidate(20), 5)\n    lu.assertEquals(candidate(128), 8)\n    lu.assertEquals(candidate(4), 3)\n    lu.assertEquals(candidate(24), 5)\n    lu.assertEquals(candidate(63), 6)\n    lu.assertEquals(candidate(25), 5)\n    lu.assertEquals(candidate(16383), 14)\n    lu.assertEquals(candidate(17), 5)\n    lu.assertEquals(candidate(511), 9)\n    lu.assertEquals(candidate(512), 10)\n    lu.assertEquals(candidate(4), 3)\n    lu.assertEquals(candidate(2048), 12)\n    lu.assertEquals(candidate(31), 5)\n    lu.assertEquals(candidate(21), 5)\n    lu.assertEquals(candidate(25), 5)\n    lu.assertEquals(candidate(64), 7)\n    lu.assertEquals(candidate(33), 6)\n    lu.assertEquals(candidate(7), 3)\n    lu.assertEquals(candidate(16), 5)\n    lu.assertEquals(candidate(21), 5)\n    lu.assertEquals(candidate(8), 4)\n    lu.assertEquals(candidate(11), 4)\n    lu.assertEquals(candidate(15), 4)\n    lu.assertEquals(candidate(5), 3)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(100), 7)\n    lu.assertEquals(candidate(28), 5)\n    lu.assertEquals(candidate(16), 5)\n    lu.assertEquals(candidate(18), 5)\n    lu.assertEquals(candidate(10), 4)\n    lu.assertEquals(candidate(8191), 13)\n    lu.assertEquals(candidate(19), 5)\n    lu.assertEquals(candidate(14), 4)\n    lu.assertEquals(candidate(255), 8)\n    lu.assertEquals(candidate(1023), 10)\n    lu.assertEquals(candidate(14), 4)\n    lu.assertEquals(candidate(12), 4)\n    lu.assertEquals(candidate(27), 5)\n    lu.assertEquals(candidate(256), 9)\n    lu.assertEquals(candidate(22), 5)\n    lu.assertEquals(candidate(22), 5)\n    lu.assertEquals(candidate(4096), 13)\n    lu.assertEquals(candidate(12), 4)\n    lu.assertEquals(candidate(9), 4)\n    lu.assertEquals(candidate(2047), 11)\n    lu.assertEquals(candidate(34), 6)\n    lu.assertEquals(candidate(3), 2)\n    lu.assertEquals(candidate(17), 5)\n    lu.assertEquals(candidate(16384), 15)\n    lu.assertEquals(candidate(32), 6)\n    lu.assertEquals(candidate(28), 5)\n    lu.assertEquals(candidate(4095), 12)\n    lu.assertEquals(candidate(20), 5)\n    lu.assertEquals(candidate(10), 4)\n    lu.assertEquals(candidate(13), 4)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(6), 3)\n    lu.assertEquals(candidate(26), 5)\n    lu.assertEquals(candidate(8), 4)\n    lu.assertEquals(candidate(24), 5)\n    lu.assertEquals(candidate(27), 5)\n    lu.assertEquals(candidate(23), 5)\n    lu.assertEquals(candidate(18), 5)\n    lu.assertEquals(candidate(8192), 14)\n    lu.assertEquals(candidate(15), 4)\n    lu.assertEquals(candidate(1024), 11)\n    lu.assertEquals(candidate(11), 4)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(9), 4)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_320259_bitInBitmap", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # flag = False\n-- # for i in range(10, -1, - 1):\n-- #     if bitmap - 2**i >= 0:\n-- #         bitmap = bitmap - 2**i\n-- #         if 2**i == bit:\n-- #             flag = True\n-- #     else:\n-- #         continue\n-- # return flag\n-- \n-- bit map decoding\nlocal function bitInBitmap(bitmap, bit)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_320259_bitInBitmap.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = bitInBitmap\n    lu.assertEquals(candidate(23, 10), false)\n    lu.assertEquals(candidate(10, 100), false)\n    lu.assertEquals(candidate(255, 128), true)\n    lu.assertEquals(candidate(13, 100), false)\n    lu.assertEquals(candidate(42, 7), false)\n    lu.assertEquals(candidate(0, 1), false)\n    lu.assertEquals(candidate(2, 1), false)\n    lu.assertEquals(candidate(2047, 128), true)\n    lu.assertEquals(candidate(0, 0), false)\n    lu.assertEquals(candidate(128, 128), true)\n    lu.assertEquals(candidate(1, 1), true)\n    lu.assertEquals(candidate(8, 1), false)\n    lu.assertEquals(candidate(256, 128), false)\n    lu.assertEquals(candidate(42, 4), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_320360_get_rounds", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return [number, number + 1, number + 2]\n-- \n-- Get list of current and next rounds\n-- :param number: int - current round number.\n-- :return: list - current round and the two that follow.\nlocal function get_rounds(number)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_320360_get_rounds.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_rounds\n    lu.assertEquals(candidate(3), {3, 4, 5})\n    lu.assertEquals(candidate(12), {12, 13, 14})\n    lu.assertEquals(candidate(0), {0, 1, 2})\n    lu.assertEquals(candidate(2), {2, 3, 4})\n    lu.assertEquals(candidate(25), {25, 26, 27})\n    lu.assertEquals(candidate(1), {1, 2, 3})\n    lu.assertEquals(candidate(7), {7, 8, 9})\n    lu.assertEquals(candidate(5), {5, 6, 7})\n    lu.assertEquals(candidate(20), {20, 21, 22})\n    lu.assertEquals(candidate(100), {100, 101, 102})\n    lu.assertEquals(candidate(17), {17, 18, 19})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_32048_color_str_yellow", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"\\033[93m{}\\033[00m\".format(s)\n-- \n-- Color string YELLOW for writing to STDIN.\nlocal function color_str_yellow(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_32048_color_str_yellow.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = color_str_yellow\n    lu.assertEquals(candidate('hello'), '\\x1b[93mhello\\x1b[00m')\n    lu.assertEquals(candidate('foo'), '\\x1b[93mfoo\\x1b[00m')\n    lu.assertEquals(candidate('hello'), '\\x1b[93mhello\\x1b[00m')\n    lu.assertEquals(candidate('\u2603'), '\\x1b[93m\u2603\\x1b[00m')\n    lu.assertEquals(candidate(str(1)), '\\x1b[93m1\\x1b[00m')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_321019_decompose_dateint", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # year = int(dateint / 10000)\n-- # leftover = dateint - year * 10000\n-- # month = int(leftover / 100)\n-- # day = leftover - month * 100\n-- # return year, month, day\n-- \n-- Decomposes the given dateint into its year, month and day components.\n-- Arguments\n-- ---------\n-- dateint : int\n--     An integer object decipting a specific calendaric day; e.g. 20161225.\n-- Returns\n-- -------\n-- year : int\n--     The year component of the given dateint.\n-- month : int\n--     The month component of the given dateint.\n-- day : int\n--     The day component of the given dateint.\nlocal function decompose_dateint(dateint)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_321019_decompose_dateint.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = decompose_dateint\n    lu.assertEquals(candidate(20161225), {2016, 12, 25})\n    lu.assertEquals(candidate(19860915), {1986, 9, 15})\n    lu.assertEquals(candidate(19000101), {1900, 1, 1})\n    lu.assertEquals(candidate(20120229), {2012, 2, 29})\n    lu.assertEquals(candidate(20160229), {2016, 2, 29})\n    lu.assertEquals(candidate(19130814), {1913, 8, 14})\n    lu.assertEquals(candidate(10102030), {1010, 20, 30})\n    lu.assertEquals(candidate(20101231), {2010, 12, 31})\n    lu.assertEquals(candidate(20210615), {2021, 6, 15})\n    lu.assertEquals(candidate(20110101), {2011, 1, 1})\n    lu.assertEquals(candidate(19000229), {1900, 2, 29})\n    lu.assertEquals(candidate(20000229), {2000, 2, 29})\n    lu.assertEquals(candidate(21000229), {2100, 2, 29})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_321523_get_voxel_coord", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # s_squared = s ** 2\n-- # z = index // s_squared\n-- # remainder = index - (z * s_squared)\n-- # y = remainder // s\n-- # x = remainder - (y * s)\n-- # return x, y, z\n-- \n-- Based on provided integer voxel index and lateral size of the 3D neighborhood, determines the coordinates of the\n-- voxel in the neighborhood\n-- :type index: int\n-- :param index:\n-- :type s: int\n-- :param s:\n-- :return:\nlocal function get_voxel_coord(index, s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_321523_get_voxel_coord.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_voxel_coord\n    lu.assertEquals(candidate(14, 4), {2, 3, 0})\n    lu.assertEquals(candidate(16, 3), {1, 2, 1})\n    lu.assertEquals(candidate(2, 4), {2, 0, 0})\n    lu.assertEquals(candidate(4, 3), {1, 1, 0})\n    lu.assertEquals(candidate(14, 3), {2, 1, 1})\n    lu.assertEquals(candidate(7, 3), {1, 2, 0})\n    lu.assertEquals(candidate(15, 3), {0, 2, 1})\n    lu.assertEquals(candidate(11, 3), {2, 0, 1})\n    lu.assertEquals(candidate(8, 3), {2, 2, 0})\n    lu.assertEquals(candidate(5, 3), {2, 1, 0})\n    lu.assertEquals(candidate(10, 3), {1, 0, 1})\n    lu.assertEquals(candidate(6, 4), {2, 1, 0})\n    lu.assertEquals(candidate(1, 3), {1, 0, 0})\n    lu.assertEquals(candidate(1, 4), {1, 0, 0})\n    lu.assertEquals(candidate(0, 3), {0, 0, 0})\n    lu.assertEquals(candidate(3, 4), {3, 0, 0})\n    lu.assertEquals(candidate(0, 4), {0, 0, 0})\n    lu.assertEquals(candidate(10, 4), {2, 2, 0})\n    lu.assertEquals(candidate(5, 4), {1, 1, 0})\n    lu.assertEquals(candidate(3, 3), {0, 1, 0})\n    lu.assertEquals(candidate(12, 3), {0, 1, 1})\n    lu.assertEquals(candidate(9, 4), {1, 2, 0})\n    lu.assertEquals(candidate(3, 2), {1, 1, 0})\n    lu.assertEquals(candidate(2, 3), {2, 0, 0})\n    lu.assertEquals(candidate(13, 3), {1, 1, 1})\n    lu.assertEquals(candidate(16, 4), {0, 0, 1})\n    lu.assertEquals(candidate(4, 4), {0, 1, 0})\n    lu.assertEquals(candidate(12, 4), {0, 3, 0})\n    lu.assertEquals(candidate(7, 4), {3, 1, 0})\n    lu.assertEquals(candidate(9, 3), {0, 0, 1})\n    lu.assertEquals(candidate(11, 4), {3, 2, 0})\n    lu.assertEquals(candidate(8, 4), {0, 2, 0})\n    lu.assertEquals(candidate(13, 4), {1, 3, 0})\n    lu.assertEquals(candidate(6, 3), {0, 2, 0})\n    lu.assertEquals(candidate(15, 4), {3, 3, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_321626_truncate_string", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return (input_string[:length] +\n-- #         '..') if len(input_string) > 1024 else input_string\n-- \n-- Truncate a string.\n-- Params:\n-- - in_string: (type: string) string to truncate.\n-- - length: (type: int) length of output string.\n-- Returns:\n-- - result: (type: string) truncated string.\nlocal function truncate_string(input_string, length)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_321626_truncate_string.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = truncate_string\n    lu.assertEquals(candidate('1234567890', 10), '1234567890')\n    lu.assertEquals(candidate('12345678901', 11), '12345678901')\n    lu.assertEquals(candidate('12345678901', 12), '12345678901')\n    lu.assertEquals(candidate('a', 1), 'a')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_321996_join_env", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # line = '='.join(list(env.items())[0])\n-- # return line\n-- \n-- Convert a single intercepted environment variable from dictionary to envs.txt line.\nlocal function join_env(env)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_321996_join_env.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = join_env\n    lu.assertEquals(candidate({['a'] = 'b'}), 'a=b')\n    lu.assertEquals(candidate({['USER'] = 'root'}), 'USER=root')\n    lu.assertEquals(candidate({['a'] = '1'}), 'a=1')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_322935_isNarcissistic", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # your code here\n-- # narciss = False\n-- # sum = 0\n-- # length = len(str(x))\n-- # # Traversing through the string\n-- # for i in str(x):\n-- #     # Converting character to int\n-- #     sum = sum + int(i) ** length\n-- # # Converting string to integer\n-- # number = int(x)\n-- # # Comparing number and sum\n-- # if (number == sum):\n-- #     narciss = True\n-- # else:\n-- #     narciss = False\n-- # return narciss\n-- \n-- Returns whether or not a given number is Narcissistic.\n-- A positive integer is called a narcissistic number if it\n-- is equal to the sum of its own digits each raised to the\n-- power of the number of digits.\n-- Example: 153 is narcissistic because 1^3 + 5^3 + 3^3 = 1 + 125 + 27 = 153.\n-- Note that by this definition all single digit numbers are narcissistic.\nlocal function isNarcissistic(x)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_322935_isNarcissistic.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = isNarcissistic\n    lu.assertEquals(candidate(1), true)\n    lu.assertEquals(candidate(10000000), false)\n    lu.assertEquals(candidate(9926315), true)\n    lu.assertEquals(candidate(0), true)\n    lu.assertEquals(candidate(370), true)\n    lu.assertEquals(candidate(153), true)\n    lu.assertEquals(candidate(122), false)\n    lu.assertEquals(candidate(409), false)\n    lu.assertEquals(candidate(4888), false)\n    lu.assertEquals(candidate(88), false)\n    lu.assertEquals(candidate(371), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_323207_validate_color", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for val in color:\n-- #     if val < 0 or val > 255:\n-- #         return False\n-- # return True\n-- \n-- Check whether or not an RGB tuple is acceptable.\nlocal function validate_color(color)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_323207_validate_color.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = validate_color\n    lu.assertEquals(candidate({0, 0, 255}), true)\n    lu.assertEquals(candidate({255, 255, 0}), true)\n    lu.assertEquals(candidate({-10, 255, 255}), false)\n    lu.assertEquals(candidate({1, 2, 3}), true)\n    lu.assertEquals(candidate({10, 10, 255}), true)\n    lu.assertEquals(candidate({0, -20, 0}), false)\n    lu.assertEquals(candidate({255, 256, 255}), false)\n    lu.assertEquals(candidate({255, 255, 255}), true)\n    lu.assertEquals(candidate({255, 10, 10}), true)\n    lu.assertEquals(candidate({254, 253, 252}), true)\n    lu.assertEquals(candidate({0, 255, 0}), true)\n    lu.assertEquals(candidate({10, 0, 255}), true)\n    lu.assertEquals(candidate({0, 10, 0}), true)\n    lu.assertEquals(candidate({255, 255, -10}), false)\n    lu.assertEquals(candidate({255, 0, 0}), true)\n    lu.assertEquals(candidate({10, 255, 10}), true)\n    lu.assertEquals(candidate({255, 10, 255}), true)\n    lu.assertEquals(candidate({10, 255, 255}), true)\n    lu.assertEquals(candidate({10, 255, 0}), true)\n    lu.assertEquals(candidate({255, 255, 254}), true)\n    lu.assertEquals(candidate({0, 255, 255}), true)\n    lu.assertEquals(candidate({10, 10, 10}), true)\n    lu.assertEquals(candidate({255, 0, 255}), true)\n    lu.assertEquals(candidate({0, 255, 10}), true)\n    lu.assertEquals(candidate({0, 10, 255}), true)\n    lu.assertEquals(candidate({0, 0, 0}), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_323236_int2signed", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if num < 0:\n-- #     return 2 ** nbits + num\n-- # else:\n-- #     return num\n-- \n--  Given a Python integer, return its 2s complement \n-- word representation.\nlocal function int2signed(num, nbits)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_323236_int2signed.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = int2signed\n    lu.assertEquals(candidate(-20), 4294967276)\n    lu.assertEquals(candidate(-1, 2), 3)\n    lu.assertEquals(candidate(1, 2), 1)\n    lu.assertEquals(candidate(2, 2), 2)\n    lu.assertEquals(candidate(32767), 32767)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(65535), 65535)\n    lu.assertEquals(candidate(2147483647), 2147483647)\n    lu.assertEquals(candidate(-2147483648), 2147483648)\n    lu.assertEquals(candidate(42), 42)\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(0, 3), 0)\n    lu.assertEquals(candidate(1, 3), 1)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(-1, 3), 7)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_323414_urlstring", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if f[0] == \".\":\n-- #     u = f[1:]\n-- # else:\n-- #     u = f\n-- # if len(u) >= 11 and u[-11:] == \"/index.html\":\n-- #     u = u[:-10]\n-- # elif u == \"index.html\":\n-- #     u = \"\"\n-- # if len(u) >= 1 and u[0] == \"/\" and len(baseUrl) >= 1 and baseUrl[-1] == \"/\":\n-- #     u = u[1:]\n-- # elif (len(u) == 0 or u[0] != \"/\") and (len(baseUrl) == 0 or baseUrl[-1] != \"/\"):\n-- #     u = \"/\" + u\n-- # return baseUrl + u\n-- \n-- Forms a string with the full url from a filename and base url.\n-- Keyword arguments:\n-- f - filename\n-- baseUrl - address of the root of the website\nlocal function urlstring(f, baseUrl)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_323414_urlstring.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = urlstring\n    lu.assertEquals(candidate('/images/google.png', 'http://code.google.com'), 'http://code.google.com/images/google.png')\n    lu.assertEquals(candidate('./images/google.png', 'http://code.google.com/'), 'http://code.google.com/images/google.png')\n    lu.assertEquals(candidate('images/google.png', 'http://code.google.com/'), 'http://code.google.com/images/google.png')\n    lu.assertEquals(candidate('about.html', 'http://code.google.com'), 'http://code.google.com/about.html')\n    lu.assertEquals(candidate('./my_page.html', 'http://codeskulptor.org/'), 'http://codeskulptor.org/my_page.html')\n    lu.assertEquals(candidate('/my_page.html', 'http://codeskulptor.org'), 'http://codeskulptor.org/my_page.html')\n    lu.assertEquals(candidate('./a/b/c/', 'http://example.com'), 'http://example.com/a/b/c/')\n    lu.assertEquals(candidate('my_page.html', 'http://codeskulptor.org/'), 'http://codeskulptor.org/my_page.html')\n    lu.assertEquals(candidate('a/b/c/index.html', 'http://example.com/'), 'http://example.com/a/b/c/')\n    lu.assertEquals(candidate('images/google.png', 'http://code.google.com'), 'http://code.google.com/images/google.png')\n    lu.assertEquals(candidate('./about.html', 'http://code.google.com'), 'http://code.google.com/about.html')\n    lu.assertEquals(candidate('./a/b/c/', 'http://example.com/'), 'http://example.com/a/b/c/')\n    lu.assertEquals(candidate('./images/google.png', 'http://code.google.com'), 'http://code.google.com/images/google.png')\n    lu.assertEquals(candidate('index.html', 'http://example.com/'), 'http://example.com/')\n    lu.assertEquals(candidate('./my_page.html', 'http://codeskulptor.org'), 'http://codeskulptor.org/my_page.html')\n    lu.assertEquals(candidate('my_page.html', 'http://codeskulptor.org'), 'http://codeskulptor.org/my_page.html')\n    lu.assertEquals(candidate('/images/google.png', 'http://code.google.com/'), 'http://code.google.com/images/google.png')\n    lu.assertEquals(candidate('./a/b/c/index.html', 'http://example.com/'), 'http://example.com/a/b/c/')\n    lu.assertEquals(candidate('/my_page.html', 'http://codeskulptor.org/'), 'http://codeskulptor.org/my_page.html')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_323665_ini_value", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # equals_idx = key_value.index('=') + 1\n-- # return key_value[equals_idx:]\n-- \n-- Strips key= from key=value from ini configuration data\nlocal function ini_value(key_value)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_323665_ini_value.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = ini_value\n    lu.assertEquals(candidate('key=value with spaces and\\nnewlines'), 'value with spaces and\\nnewlines')\n    lu.assertEquals(candidate('key=value with spaces and\\ttabs'), 'value with spaces and\\ttabs')\n    lu.assertEquals(candidate('key=value\\twith\\ttabs'), 'value\\twith\\ttabs')\n    lu.assertEquals(candidate('key=value'), 'value')\n    lu.assertEquals(candidate('key=value\\n\\nwith\\n\\nnewlines\\n\\n'), 'value\\n\\nwith\\n\\nnewlines\\n\\n')\n    lu.assertEquals(candidate('key=value with spaces'), 'value with spaces')\n    lu.assertEquals(candidate('key=value\\nwith\\nnewlines'), 'value\\nwith\\nnewlines')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_325056_toggle_collapse", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if n:\n-- #     return not is_open\n-- # return is_open\n-- \n--     collapse the side bar\nlocal function toggle_collapse(n, is_open)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_325056_toggle_collapse.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = toggle_collapse\n    lu.assertEquals(candidate(3, false), true)\n    lu.assertEquals(candidate(5, false), true)\n    lu.assertEquals(candidate(2, true), false)\n    lu.assertEquals(candidate(1, true), false)\n    lu.assertEquals(candidate(0, false), false)\n    lu.assertEquals(candidate(4, false), true)\n    lu.assertEquals(candidate(1, false), true)\n    lu.assertEquals(candidate(3, true), false)\n    lu.assertEquals(candidate(2, false), true)\n    lu.assertEquals(candidate(4, true), false)\n    lu.assertEquals(candidate(5, true), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_325573_format_data", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # account_name = account[\"name\"]\n-- # account_desc = account[\"description\"]\n-- # account_country = account[\"country\"]\n-- # return f\"{account_name}, a {account_desc}, from {account_country}\"\n-- \n-- Takes the account data and returns in a printable format.\nlocal function format_data(account)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_325573_format_data.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = format_data\n    lu.assertEquals(candidate({['name'] = 'John', ['description'] = 'Doctor', ['country'] = 'USA'}), 'John, a Doctor, from USA')\n    lu.assertEquals(candidate({['name'] = 'Alice', ['description'] = 'A normal account', ['country'] = 'Canada'}), 'Alice, a A normal account, from Canada')\n    lu.assertEquals(candidate({['name'] = 'Bob', ['description'] = 'The bad account', ['country'] = 'France'}), 'Bob, a The bad account, from France')\n    lu.assertEquals(candidate({['name'] = 'Charlie', ['description'] = 'An alright account', ['country'] = 'Mexico'}), 'Charlie, a An alright account, from Mexico')\n    lu.assertEquals(candidate({['name'] = '', ['description'] = 'Doctor', ['country'] = 'USA'}), ', a Doctor, from USA')\n    lu.assertEquals(candidate({['name'] = 'John', ['description'] = 'Doctor', ['country'] = ''}), 'John, a Doctor, from ')\n    lu.assertEquals(candidate({['name'] = '<NAME>', ['description'] = 'Super Mario Maker 2', ['country'] = 'Japan'}), '<NAME>, a Super Mario Maker 2, from Japan')\n    lu.assertEquals(candidate({['name'] = 'Bob', ['description'] = 'The bad account', ['country'] = 'France'}), 'Bob, a The bad account, from France')\n    lu.assertEquals(candidate({['name'] = 'Alice', ['description'] = 'A normal account', ['country'] = 'Canada'}), 'Alice, a A normal account, from Canada')\n    lu.assertEquals(candidate({['name'] = 'Charlie', ['description'] = 'An alright account', ['country'] = 'Mexico'}), 'Charlie, a An alright account, from Mexico')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_325906_strQ2B", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # rstring = \"\"\n-- # for uchar in ustring:\n-- #     inside_code = ord(uchar)\n-- #     if inside_code == 12288:\n-- #         inside_code = 32\n-- #     elif (inside_code >= 65281 and inside_code <= 65374):\n-- #         inside_code -= 65248\n-- #     rstring += chr(inside_code)\n-- # return rstring\n-- \n--     Converting full-width characters to half-width characters\nlocal function strQ2B(ustring)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_325906_strQ2B.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = strQ2B\n    lu.assertEquals(candidate('\uff54\uff45\uff53\uff54'), 'test')\n    lu.assertEquals(candidate('\uff21\uff22\uff23'), 'ABC')\n    lu.assertEquals(candidate('\uff41\uff42\uff43\uff21\uff22\uff23'), 'abcABC')\n    lu.assertEquals(candidate('\uff11\uff12\uff13456'), '123456')\n    lu.assertEquals(candidate('\uff10\uff11\uff12\uff13'), '0123')\n    lu.assertEquals(candidate(candidate('abc')), 'abc')\n    lu.assertEquals(candidate('\uff05\uff41\uff42\uff43\uff44\uff45\uff46\uff47\uff48\uff49\uff4a\uff4b\uff4c\uff4d\uff4e\uff4f\uff50\uff51\uff52\uff53\uff54\uff55\uff56\uff57\uff58\uff59\uff5a'), '%abcdefghijklmnopqrstuvwxyz')\n    lu.assertEquals(candidate('\uff11\uff12\uff13'), '123')\n    lu.assertEquals(candidate('\uff21\uff22\uff23\uff10\uff11\uff12\uff13'), 'ABC0123')\n    lu.assertEquals(candidate('123'), '123')\n    lu.assertEquals(candidate('\uff21\uff22\uff23'), 'ABC')\n    lu.assertEquals(candidate('\uff41\uff42\uff43\uff41\uff42\uff43\uff21\uff22\uff23\uff21\uff22\uff23'), 'abcabcABCABC')\n    lu.assertEquals(candidate('abc'), 'abc')\n    lu.assertEquals(candidate('\u554a\u9f44\u4e02\u72db\u72dc'), '\u554a\u9f44\u4e02\u72db\u72dc')\n    lu.assertEquals(candidate('\uff41\uff42\uff43'), 'abc')\n    lu.assertEquals(candidate('\uff3a\uff41\uff42\uff43\uff11\uff12\uff13'), 'Zabc123')\n    lu.assertEquals(candidate('abc'), 'abc')\n    lu.assertEquals(candidate('\u4f60\u597d'), '\u4f60\u597d')\n    lu.assertEquals(candidate('\uff21\uff22\uff23'), 'ABC')\n    lu.assertEquals(candidate('\uff41\uff42\uff43\uff41\uff42\uff43'), 'abcabc')\n    lu.assertEquals(candidate('\uff41\uff42\uff43'), 'abc')\n    lu.assertEquals(candidate('\uff41\uff42\uff43\uff10\uff11\uff12\uff13'), 'abc0123')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_326010_job_id_from_reponse", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # #\n-- # # The output from SGE is of the form\n-- # # \"Your job 3681 (\"TEST\") has been submitted\"\n-- # # Your job-array 4321.1-3:1 (\"wrfpost\") has been submitted\n-- # #\n-- # job_id = text.split(' ')[2]\n-- # if \".\" in job_id:\n-- #     # job was an array job\n-- #     job_id = job_id.split('.')[0]\n-- # return job_id\n-- \n-- Return a string representation of integer job id from the qsub response to stdout\nlocal function job_id_from_reponse(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_326010_job_id_from_reponse.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = job_id_from_reponse\n    lu.assertEquals(candidate('Your job-array 4321.1-3:1 (\"wrfpost\") has been submitted'), '4321')\n    lu.assertEquals(candidate(\"Your job-array 4321.1-3:1 ('wrfpost') has been submitted\"), '4321')\n    lu.assertEquals(candidate('Your job 4321.1 (\"wrfpost\") has been submitted'), '4321')\n    lu.assertEquals(candidate('Your job 3681 (\"TEST\") has been submitted'), '3681')\n    lu.assertEquals(candidate('Your job-array 4321.1-3:1 (\"wrfpost\") has been submitted'), '4321')\n    lu.assertEquals(candidate('Your job 3681 () has been submitted'), '3681')\n    lu.assertEquals(candidate('Your job 3681 (\"TEST\") has been submitted'), '3681')\n    lu.assertEquals(candidate(\"Your job 3681 ('TEST') has been submitted\"), '3681')\n    lu.assertEquals(candidate('Your job 4321 (\"wrfpost\") has been submitted'), '4321')\n    lu.assertEquals(candidate('Your job-array 4321.1-3:1 (\"wrfpost\") has been submitted'), '4321')\n    lu.assertEquals(candidate('Your job 3681 (\"TEST\") has been submitted'), '3681')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_326484_vectorize", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # data = list()\n-- # for word in words:\n-- #     if word in dictionary:\n-- #         index = dictionary[word]\n-- #     else:\n-- #         index = 0\n-- #     data.append(index)\n-- # return data\n-- \n-- Converts a list of words into a list of frequency position numbers.\n-- Args:\n--     dictionary(dict): Dictionary containing the words in the vocabulary together\n--         with their frequency position.\n--     words(list): List of words that are to be converted.\n-- Returns:\n--     A list of frequency position numbers in place of the actual words in the list.\nlocal function vectorize(dictionary, words)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_326484_vectorize.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = vectorize\n    lu.assertEquals(candidate(dict({['first'] = 1, ['second'] = 2}), {'first', 'second', 'third'}), {1, 2, 0})\n    lu.assertEquals(candidate(dict({['first'] = 0, ['second'] = 0}), {'first', 'second'}), {0, 0})\n    lu.assertEquals(candidate(dict({['first'] = 0, ['second'] = 0}), {'first', 'second', 'third'}), {0, 0, 0})\n    lu.assertEquals(candidate(dict({['first'] = 1, ['second'] = 2}), {}), {})\n    lu.assertEquals(candidate(dict(), {}), {})\n    lu.assertEquals(candidate(dict({['first'] = 1, ['second'] = 2}), {'first', 'first'}), {1, 1})\n    lu.assertEquals(candidate(dict(), {'first', 'second'}), {0, 0})\n    lu.assertEquals(candidate(dict({['first'] = 0}), {}), {})\n    lu.assertEquals(candidate(dict({['first'] = 1, ['second'] = 2}), {'first', 'first', 'third'}), {1, 1, 0})\n    lu.assertEquals(candidate(dict({['first'] = 1, ['second'] = 2}), {'first', 'second'}), {1, 2})\n    lu.assertEquals(candidate(dict({['first'] = 0}), {'first', 'second'}), {0, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_327793_encode_pdf", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # count = len(pdf)\n-- # pdf = map(lambda x: '(' + str(x[0]) + ', ' + str(x[1]) + ')', pdf)\n-- # pdf = '[' + ', '.join(pdf) + ']'\n-- # return pdf\n-- \n-- Encode the probability density function.\nlocal function encode_pdf(pdf)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_327793_encode_pdf.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = encode_pdf\n    lu.assertEquals(candidate({{1, 1}}), '[(1, 1)]')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_327885_is_between_strict", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return (lo < val < hi) or (lo > val > hi)\n-- \n-- Shorthand for `(lo < val < hi) or (lo > val > hi)`.\nlocal function is_between_strict(lo, val, hi)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_327885_is_between_strict.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_between_strict\n    lu.assertEquals(candidate(2, 3, 2), false)\n    lu.assertEquals(candidate(3, 2, 3), false)\n    lu.assertEquals(candidate(5, 10, 10), false)\n    lu.assertEquals(candidate(2, 1, 2), false)\n    lu.assertEquals(candidate(1, 3, 2), false)\n    lu.assertEquals(candidate(3, 3, 1), false)\n    lu.assertEquals(candidate(3, 1, 3), false)\n    lu.assertEquals(candidate(0, 1, 2), true)\n    lu.assertEquals(candidate(3, 2, 5), false)\n    lu.assertEquals(candidate(2, 2, 3), false)\n    lu.assertEquals(candidate(3, 3, 5), false)\n    lu.assertEquals(candidate(3, 4, 5), true)\n    lu.assertEquals(candidate(1, 3, 3), false)\n    lu.assertEquals(candidate(0, 1, -1), false)\n    lu.assertEquals(candidate(3, 1, 1), false)\n    lu.assertEquals(candidate(1, 2, 3), true)\n    lu.assertEquals(candidate(10, 10, 15), false)\n    lu.assertEquals(candidate(3, 6, 5), false)\n    lu.assertEquals(candidate(1, 3, 1), false)\n    lu.assertEquals(candidate(1, 2, 2), false)\n    lu.assertEquals(candidate(5, 6, 3), false)\n    lu.assertEquals(candidate(1, 4, 3), false)\n    lu.assertEquals(candidate(5, 10, 5), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_328378_parse_line", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # line_list = line.strip().split(' ')\n-- # return line_list\n-- \n-- Takes a line of space seperated values, returns the values in a list.\nlocal function parse_line(line)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_328378_parse_line.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_line\n    lu.assertEquals(candidate('1111 22'), {'1111', '22'})\n    lu.assertEquals(candidate('1111 22 3'), {'1111', '22', '3'})\n    lu.assertEquals(candidate('   This is a line with 10 spaces in front of it.   '), {'This', 'is', 'a', 'line', 'with', '10', 'spaces', 'in', 'front', 'of', 'it.'})\n    lu.assertEquals(candidate('This is a line without spaces.'), {'This', 'is', 'a', 'line', 'without', 'spaces.'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_328945_loop_add", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # while y != 0:\n-- #     x += 1\n-- #     y -= 1\n-- # return x\n-- \n-- An imperative implementation using a loop and in-place mutation.\nlocal function loop_add(x, y)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_328945_loop_add.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = loop_add\n    lu.assertEquals(candidate(123, 456), 579)\n    lu.assertEquals(candidate(2, 2), 4)\n    lu.assertEquals(candidate(-700, 300), -400)\n    lu.assertEquals(candidate(-123, 456), 333)\n    lu.assertEquals(candidate(0, 100), 100)\n    lu.assertEquals(candidate(3, 2), 5)\n    lu.assertEquals(candidate(1, 1), 2)\n    lu.assertEquals(candidate(0, 1), 1)\n    lu.assertEquals(candidate(10, 0), 10)\n    lu.assertEquals(candidate(1, 2), 3)\n    lu.assertEquals(candidate(2, 3), 5)\n    lu.assertEquals(candidate(1000, 0), 1000)\n    lu.assertEquals(candidate(-10, 0), -10)\n    lu.assertEquals(candidate(-10, 1), -9)\n    lu.assertEquals(candidate(0, 1000), 1000)\n    lu.assertEquals(candidate(1, 0), 1)\n    lu.assertEquals(candidate(0, 0), 0)\n    lu.assertEquals(candidate(100, 20), 120)\n    lu.assertEquals(candidate(-1, 1000), 999)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_329540_invertEvent", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if e == 1:\n-- #     return 0\n-- # else:\n-- #     return 1\n-- \n-- Return the inverted event.\nlocal function invertEvent(e)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_329540_invertEvent.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = invertEvent\n    lu.assertEquals(candidate(0), 1)\n    lu.assertEquals(candidate(1), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_33050_check_event_attrs", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if 'app' in attrs and attrs['app'] == 'customApp':\n-- #     # custom app\n-- #     expected_attrs = ['activity_type', 'categories', 'dest_country', 'dest_ip', 'dest_port', 'device_type', 'domain',\n-- #                       'forwarder', 'id', 'is_phishing_domain', 'is_ransomware_dest_ip', 'is_ransomware_src_ip', 'is_threat_dest_ip',\n-- #                       'is_threat_src_ip', 'outcome', 'source_component', 'src_country', 'src_ip', 'src_port',\n-- #                       'subcategory', 'username', 'version']\n-- # elif 'dataset' in attrs and attrs['dataset'] == 'accesslog':\n-- #     # filebeats flog apache log\n-- #     expected_attrs = ['agent', 'authUser', 'bytes', 'ip', 'protocol', 'referrer', 'status', 'uriPath', 'user']\n-- # elif 'tag' in attrs and attrs['tag'] == 'fluentd-apache':\n-- #     # fluentd flog apache log\n-- #     expected_attrs = ['agent', 'code', 'host', 'method', 'path', 'size', 'container_id', 'container_name']\n-- # elif 'tag' in attrs and attrs['tag'] == 'fluentbit-cpu':\n-- #     # fluentbit cpu\n-- #     expected_attrs = ['cpu_p', 'system_p', 'user_p', 'cpu0.p_cpu', 'cpu0.p_system', 'cpu0.p_user']\n-- # else:\n-- #     print(\"Unexpected event.  Event did not match expected event types.  {0}\".format(attrs))\n-- #     return False\n-- # has_expected_attrs = all(k in attrs for k in expected_attrs)\n-- # if not has_expected_attrs:\n-- #     print(\"Did not get expected attributes {0}.  Query returned attributes {1}\".format(expected_attrs, attrs))\n-- # return has_expected_attrs\n-- \n--     Verify the event has the expected attributes for Flog Apache logs and custom app logs\nlocal function check_event_attrs(attrs)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_33050_check_event_attrs.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = check_event_attrs\n    lu.assertEquals(candidate({['id'] = '1', ['source_component'] = '1.2.3.4'}), false)\n    lu.assertEquals(candidate({['app'] = 'customApp', ['id'] = '99999', ['activity_type'] = 'ssh', ['categories'] = 'auth, ssh', ['dest_country'] = 'US', ['dest_ip'] = '192.168.1.1', ['dest_port'] = '3389', ['device_type'] = 'desktop', ['domain'] = 'myDomain.com', ['forwarder'] = 'my.forwarder.net', ['is_phishing_domain'] = 'False', ['is_ransomware_dest_ip'] = 'False', ['is_ransomware_src_ip'] = 'False', ['is_threat_dest_ip'] = 'False', ['is_threat_src_ip'] = 'False', ['outcome'] = 'failure', ['source_component'] = 'ssh-desktop', ['src_country'] = 'US', ['src_ip'] = '192.168.1.2', ['src_port'] = '53269', ['subcategory'] = 'auth, ssh', ['username'] = 'joeuser', ['version'] = '0.1.0'}), true)\n    lu.assertEquals(candidate({['app'] = 'customApp', ['activity_type'] = 'activity', ['categories'] = 'cat1', ['dest_country'] = 'US', ['dest_ip'] = '172.16.17.32', ['dest_port'] = '35084', ['device_type'] = 'Laptop', ['domain'] = 'example.com', ['forwarder'] = 'forwarder1', ['id'] = '123', ['is_phishing_domain'] = 'False', ['is_ransomware_dest_ip'] = 'False', ['is_ransomware_src_ip'] = 'False', ['is_threat_dest_ip'] = 'False', ['is_threat_src_ip'] = 'False', ['outcome'] = 'success', ['source_component'] = 'source_component1', ['src_country'] = 'US', ['src_ip'] = '172.16.17.32', ['src_port'] = '443', ['subcategory'] = 'subcategory1', ['username'] = 'username1', ['version'] = 'version1'}), true)\n    lu.assertEquals(candidate({['dataset'] = 'accesslog', ['agent'] = 'client', ['authUser'] = 'joeuser', ['bytes'] = '1234', ['ip'] = '192.168.1.1', ['protocol'] = 'http', ['referrer'] = 'http://myDomain.com/login', ['status'] = '404', ['uriPath'] = '/login', ['user'] = 'joeuser'}), true)\n    lu.assertEquals(candidate({['app'] = 'customApp', ['activity_type'] = 'fileDownload', ['categories'] = 'fileDownload', ['dest_country'] = 'US', ['dest_ip'] = '1.2.3.4', ['dest_port'] = '80', ['device_type'] = 'server', ['domain'] = 'example.com', ['forwarder'] = 'filebeat-8.0.0-darwin-x86_64', ['id'] = '80071', ['is_phishing_domain'] = 'no', ['is_ransomware_dest_ip'] = 'no', ['is_ransomware_src_ip'] = 'no', ['is_threat_dest_ip'] = 'no', ['is_threat_src_ip'] = 'no', ['outcome'] = 'success', ['source_component'] = 'Falcon', ['src_country'] = 'US', ['src_ip'] = '1.2.3.4', ['src_port'] = '5672', ['subcategory'] = 'File Downloads', ['username'] = 'user', ['version'] = '20.2.0.19344'}), true)\n    lu.assertEquals(candidate({['id'] = '1', ['app'] = 'customApp', ['source_component'] = '1.2.3.4', ['outcome'] = 'succeded'}), false)\n    lu.assertEquals(candidate({['id'] = '1', ['source_component'] = '1.2.3.4', ['outcome'] = 'succeded'}), false)\n    lu.assertEquals(candidate({['id'] = '1', ['app'] = 'customApp', ['outcome'] = 'succeded'}), false)\n    lu.assertEquals(candidate({['id'] = '1', ['app'] = 'customApp'}), false)\n    lu.assertEquals(candidate({['id'] = '1', ['outcome'] = 'succeded'}), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_330601_makeHtmlText", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # str_in = '<textarea rows=\"2\" cols=\"100\" style=\"border:double 2px blue;\">' + str_in + '</textarea>'\n-- # return str_in\n-- \n-- add formatting for an html textarea to a string\nlocal function makeHtmlText(str_in)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_330601_makeHtmlText.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = makeHtmlText\n    lu.assertEquals(candidate('I eat\\ncats.'), '<textarea rows=\"2\" cols=\"100\" style=\"border:double 2px blue;\">I eat\\ncats.</textarea>')\n    lu.assertEquals(candidate('I eat cats!'), '<textarea rows=\"2\" cols=\"100\" style=\"border:double 2px blue;\">I eat cats!</textarea>')\n    lu.assertEquals(candidate('hello world'), '<textarea rows=\"2\" cols=\"100\" style=\"border:double 2px blue;\">hello world</textarea>')\n    lu.assertEquals(candidate('It was the best of times, it was the worst of times. It was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way \u2013 in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only...'), '<textarea rows=\"2\" cols=\"100\" style=\"border:double 2px blue;\">It was the best of times, it was the worst of times. It was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way \u2013 in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only...</textarea>')\n    lu.assertEquals(candidate('hello world'), '<textarea rows=\"2\" cols=\"100\" style=\"border:double 2px blue;\">hello world</textarea>')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_331914_is_tracked_zone", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for zone in zones:\n-- #     if cname.endswith(\".\" + zone) or cname == zone:\n-- #         return True\n-- # return False\n-- \n--     Is the root domain for the provided cname one of the known domains?\nlocal function is_tracked_zone(cname, zones)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_331914_is_tracked_zone.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_tracked_zone\n    lu.assertEquals(candidate('example.gov', {'example.org', 'example.net', 'example.biz', 'example.gov'}), true)\n    lu.assertEquals(candidate('example.net', {'example.org', 'example.net', 'example.biz', 'example.gov'}), true)\n    lu.assertEquals(candidate('example.com', {'example.org', 'example.net', 'example.biz', 'example.gov'}), false)\n    lu.assertEquals(candidate('example.com', {'example.org', 'example.net', 'example.biz'}), false)\n    lu.assertEquals(candidate('example.com', {'example.com', 'example.org', 'example.net', 'example.biz'}), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_332162_dict_bool", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return len(x) != 0\n-- \n-- Implementation of `dict_bool`.\nlocal function dict_bool(x)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_332162_dict_bool.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = dict_bool\n    lu.assertEquals(candidate(dict()), false)\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3}), true)\n    lu.assertEquals(candidate({['a'] = 1}), true)\n    lu.assertEquals(candidate({['a'] = 1}), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_332746_month_str", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug',\n-- #           'sep', 'oct', 'nov', 'dec']\n-- # mstr = months[month - 1]\n-- # if upper:\n-- #     mstr = mstr.upper()\n-- # return mstr\n-- \n-- Returns the string e.g. 'JAN' corresponding to month\nlocal function month_str(month, upper)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_332746_month_str.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = month_str\n    lu.assertEquals(candidate(5, false), 'may')\n    lu.assertEquals(candidate(8), 'AUG')\n    lu.assertEquals(candidate(1), 'JAN')\n    lu.assertEquals(candidate(6), 'JUN')\n    lu.assertEquals(candidate(4, false), 'apr')\n    lu.assertEquals(candidate(10, false), 'oct')\n    lu.assertEquals(candidate(12), 'DEC')\n    lu.assertEquals(candidate(9, true), 'SEP')\n    lu.assertEquals(candidate(3, false), 'mar')\n    lu.assertEquals(candidate(6, false), 'jun')\n    lu.assertEquals(candidate(7), 'JUL')\n    lu.assertEquals(candidate(3, true), 'MAR')\n    lu.assertEquals(candidate(1, true), 'JAN')\n    lu.assertEquals(candidate(6, true), 'JUN')\n    lu.assertEquals(candidate(3), 'MAR')\n    lu.assertEquals(candidate(2, false), 'feb')\n    lu.assertEquals(candidate(6), 'JUN')\n    lu.assertEquals(candidate(9, false), 'sep')\n    lu.assertEquals(candidate(11), 'NOV')\n    lu.assertEquals(candidate(1, false), 'jan')\n    lu.assertEquals(candidate(9), 'SEP')\n    lu.assertEquals(candidate(10), 'OCT')\n    lu.assertEquals(candidate(11, false), 'nov')\n    lu.assertEquals(candidate(7, true), 'JUL')\n    lu.assertEquals(candidate(2), 'FEB')\n    lu.assertEquals(candidate(12, false), 'dec')\n    lu.assertEquals(candidate(8, false), 'aug')\n    lu.assertEquals(candidate(4), 'APR')\n    lu.assertEquals(candidate(7, false), 'jul')\n    lu.assertEquals(candidate(5), 'MAY')\n    lu.assertEquals(candidate(12), 'DEC')\n    lu.assertEquals(candidate(8, true), 'AUG')\n    lu.assertEquals(candidate(5, true), 'MAY')\n    lu.assertEquals(candidate(1), 'JAN')\n    lu.assertEquals(candidate(4, true), 'APR')\n    lu.assertEquals(candidate(7), 'JUL')\n    lu.assertEquals(candidate(2, true), 'FEB')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_333098_to_bool", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # valid = {\n-- #     'true':  True,  't': True,  '1': True,  'y': True,\n-- #     'false': False, 'f': False, '0': False, 'n': False\n-- # }\n-- # if not isinstance(value, str):\n-- #     raise ValueError('Cannot check boolean value. Not a string.')\n-- # lower_value = value.lower()\n-- # if lower_value in valid:\n-- #     return valid[lower_value]\n-- # else:\n-- #     raise ValueError('Not a boolean string: \"%s\"' % value)\n-- \n-- Helper function for translating strings into booleans\n-- @see test/TestReadConfig.py\nlocal function to_bool(value)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_333098_to_bool.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = to_bool\n    lu.assertEquals(candidate('f'), false)\n    lu.assertEquals(candidate('y'), true)\n    lu.assertEquals(candidate('TRUE'), true)\n    lu.assertEquals(candidate('true'), true)\n    lu.assertEquals(candidate('false'), false)\n    lu.assertEquals(candidate('n'), false)\n    lu.assertEquals(candidate('N'), false)\n    lu.assertEquals(candidate('0'), false)\n    lu.assertEquals(candidate('true'), true)\n    lu.assertEquals(candidate('f'), false)\n    lu.assertEquals(candidate('N'), false)\n    lu.assertEquals(candidate('y'), true)\n    lu.assertEquals(candidate('T'), true)\n    lu.assertEquals(candidate('t'), true)\n    lu.assertEquals(candidate('1'), true)\n    lu.assertEquals(candidate('n'), false)\n    lu.assertEquals(candidate('F'), false)\n    lu.assertEquals(candidate('1'), true)\n    lu.assertEquals(candidate('Y'), true)\n    lu.assertEquals(candidate('f'), false)\n    lu.assertEquals(candidate('1'), true)\n    lu.assertEquals(candidate('0'), false)\n    lu.assertEquals(candidate('y'), true)\n    lu.assertEquals(candidate('1'), true)\n    lu.assertEquals(candidate('FALSE'), false)\n    lu.assertEquals(candidate('T'), true)\n    lu.assertEquals(candidate('y'), true)\n    lu.assertEquals(candidate('t'), true)\n    lu.assertEquals(candidate('t'), true)\n    lu.assertEquals(candidate('n'), false)\n    lu.assertEquals(candidate('false'), false)\n    lu.assertEquals(candidate('Y'), true)\n    lu.assertEquals(candidate('F'), false)\n    lu.assertEquals(candidate('t'), true)\n    lu.assertEquals(candidate('true'), true)\n    lu.assertEquals(candidate('0'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_333149_manhattan_distance", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # x_a, y_a = xy_a\n-- # x_b, y_b = xy_b\n-- # return abs(x_a-x_b) + abs(y_a-y_b)\n-- \n-- Number of steps between two squares allowing only\n-- up, down, left and right steps.\nlocal function manhattan_distance(xy_a, xy_b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_333149_manhattan_distance.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = manhattan_distance\n    lu.assertEquals(candidate({0, 0}, {1, 0}), 1)\n    lu.assertEquals(candidate({0, 0}, {1, 1}), 2)\n    lu.assertEquals(candidate({0, 0}, {0, -1}), 1)\n    lu.assertEquals(candidate({3, 2}, {2, 3}), 2)\n    lu.assertEquals(candidate({0, 0}, {1, 0}), 1)\n    lu.assertEquals(candidate({0, 0}, {0, 0}), 0)\n    lu.assertEquals(candidate({0, 0}, {0, 1}), 1)\n    lu.assertEquals(candidate({3, 4}, {2, 3}), 2)\n    lu.assertEquals(candidate({0, 0}, {1, -1}), 2)\n    lu.assertEquals(candidate({0, 0}, {1, 1}), 2)\n    lu.assertEquals(candidate({1, 1}, {2, 2}), 2)\n    lu.assertEquals(candidate({2, 2}, {2, 2}), 0)\n    lu.assertEquals(candidate({-2, 2}, {-4, -2}), 6)\n    lu.assertEquals(candidate({0, 0}, {2, 2}), 4)\n    lu.assertEquals(candidate({0, 0}, {0, 0}), 0)\n    lu.assertEquals(candidate({2, 3}, {3, 4}), 2)\n    lu.assertEquals(candidate({0, 0}, {2, 2}), 4)\n    lu.assertEquals(candidate({0, 0}, {0, 1}), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_333447_urlsafe_address", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # addr, port, *rest = address\n-- # if rest:\n-- #     # An IPv6 address needs to be surrounded by square brackets\n-- #     addr = f'[{addr}]'\n-- # return addr, port\n-- \n-- Make an address safe to use in a URL.\n-- Args:\n--     address: A tuple of address information.\n-- Returns:\n--     A 2-tuple of url-safe (address, port)\nlocal function urlsafe_address(address)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_333447_urlsafe_address.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = urlsafe_address\n    lu.assertEquals(candidate({'localhost', 80}), {'localhost', 80})\n    lu.assertEquals(candidate({'localhost', 8080}), {'localhost', 8080})\n    lu.assertEquals(candidate({'example.com', 8080}), {'example.com', 8080})\n    lu.assertEquals(candidate({'::1', 80}), {'::1', 80})\n    lu.assertEquals(candidate({'127.0.0.1', 80}), {'127.0.0.1', 80})\n    lu.assertEquals(candidate({'example.com', 80}), {'example.com', 80})\n    lu.assertEquals(candidate({'127.0.0.1', 8080}), {'127.0.0.1', 8080})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_333685_get_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # idx = info.strip().split('_')\n-- # if len(idx) == 1:\n-- #     if idx[0] in idx2name:\n-- #         return idx2name[idx[0]]\n-- #     else:\n-- #         return idx[0]\n-- # elif len(idx) > 1:\n-- #     ret = []\n-- #     for i in idx:\n-- #         if i in idx2name:\n-- #             ret.append(idx2name[i])\n-- #         else:\n-- #             ret.append(i)\n-- #     return '#'.join(ret)\n-- \n--         get name from idx2name\nlocal function get_name(idx2name, info)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_333685_get_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_name\n    lu.assertEquals(candidate(), 'a#b#c')\n    lu.assertEquals(candidate(), 'a')\n    lu.assertEquals(candidate({['3'] = '3', ['4'] = '4'}, '3'), '3')\n    lu.assertEquals(candidate({['1'] = 'tom', ['2'] = 'dick', ['3'] = 'harry'}, '3'), 'harry')\n    lu.assertEquals(candidate({['1'] = '1', ['2'] = '2', ['3'] = '3', ['4'] = '4'}, '1#2#3#4#5'), '1#2#3#4#5')\n    lu.assertEquals(candidate({['1'] = 'Alice', ['2'] = 'Bob', ['3'] = 'Charlie', ['4'] = 'David', ['5'] = 'Eve', ['6'] = 'Fred', ['7'] = 'Ginny', ['8'] = 'Harriet', ['9'] = 'Ileana', ['10'] = 'Joseph', ['11'] = 'Kincaid', ['12'] = 'Larry'}, '12'), 'Larry')\n    lu.assertEquals(candidate({['1'] = 'tom', ['2'] = 'dick', ['3'] = 'harry'}, '1'), 'tom')\n    lu.assertEquals(candidate({['1'] = '1', ['2'] = '2', ['3'] = '3', ['4'] = '4'}, '3#4'), '3#4')\n    lu.assertEquals(candidate({['1'] = '1', ['2'] = '2', ['3'] = '3', ['4'] = '4'}, '1#2#3'), '1#2#3')\n    lu.assertEquals(candidate({['1'] = '1', ['2'] = '2', ['3'] = '3', ['4'] = '4'}, '2#3#4'), '2#3#4')\n    lu.assertEquals(candidate({['1'] = '1', ['2'] = '2', ['3'] = '3', ['4'] = '4'}, '1#2#3#4'), '1#2#3#4')\n    lu.assertEquals(candidate({['1'] = 'tom', ['2'] = 'dick', ['3'] = 'harry'}, '5'), '5')\n    lu.assertEquals(candidate(), 'a#a#a#b#c')\n    lu.assertEquals(candidate({['3'] = '3', ['4'] = '4'}, '4'), '4')\n    lu.assertEquals(candidate({['1'] = '1', ['2'] = '2', ['3'] = '3', ['4'] = '4', ['5'] = '5', ['6'] = '6', ['7'] = '7', ['8'] = '8', ['9'] = '9', ['10'] = '10'}, '1'), '1')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_334071_prepend_scheme", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if scheme == \"\":\n-- #     scheme = \"file\"\n-- # if path.startswith(scheme + \"://\"):\n-- #     return path\n-- # else:\n-- #     path = path[1:] if path.startswith(\"/\") else path\n-- #     return f\"{scheme}://{path}\"\n-- \n-- Prepend scheme to a remote path.\n-- Scheme is only prepended if not already present\n-- Parameters\n-- ----------\n-- scheme: str\n--     a scheme like 'file', 's3' or 'gs'\n-- path: str\n--     path which will possibly get a scheme prepended\n-- Returns\n-- -------\n-- full_path: str\nlocal function prepend_scheme(scheme, path)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_334071_prepend_scheme.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = prepend_scheme\n    lu.assertEquals(candidate('gs', '/tmp/test_folder/file.txt'), 'gs://tmp/test_folder/file.txt')\n    lu.assertEquals(candidate('', '/test/test.txt'), 'file://test/test.txt')\n    lu.assertEquals(candidate('file', 'tmp/test_folder/file.txt'), 'file://tmp/test_folder/file.txt')\n    lu.assertEquals(candidate('gs', 'tmp/test_folder/file.txt'), 'gs://tmp/test_folder/file.txt')\n    lu.assertEquals(candidate('s3', '/tmp/test_folder/file.txt'), 's3://tmp/test_folder/file.txt')\n    lu.assertEquals(candidate('file', '/tmp/test_folder/'), 'file://tmp/test_folder/')\n    lu.assertEquals(candidate('gs', '/tmp/test_folder/'), 'gs://tmp/test_folder/')\n    lu.assertEquals(candidate('s3', 'tmp/test_folder/file.txt'), 's3://tmp/test_folder/file.txt')\n    lu.assertEquals(candidate('', 'test.txt'), 'file://test.txt')\n    lu.assertEquals(candidate('file', '/tmp/test_folder/file.txt'), 'file://tmp/test_folder/file.txt')\n    lu.assertEquals(candidate('file', 'tmp/test_folder/'), 'file://tmp/test_folder/')\n    lu.assertEquals(candidate('s3', '/tmp/test_folder/'), 's3://tmp/test_folder/')\n    lu.assertEquals(candidate('s3', 'tmp/test_folder/'), 's3://tmp/test_folder/')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_3342_parse_cigar", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # tlength = 0\n-- # coordinate = []\n-- # # count matches, indels and mismatches\n-- # oplist = (0, 1, 2, 7, 8)\n-- # for operation, length in cigarlist:\n-- #     if operation == ope:\n-- #         coordinate.append([length, tlength])\n-- #     if operation in oplist:\n-- #         tlength += length\n-- # return coordinate\n-- \n--  for a specific operation (mismach, match, insertion, deletion... see above)\n-- return occurences and index in the alignment \nlocal function parse_cigar(cigarlist, ope)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_3342_parse_cigar.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_cigar\n    lu.assertEquals(candidate({{2, 100}}, 1), {})\n    lu.assertEquals(candidate(candidate({{'M', 5}, {'M', 3}, {'I', 5}}, 'D'), 'D'), {})\n    lu.assertEquals(candidate({{0, 100}}, 2), {})\n    lu.assertEquals(candidate({{1, 100}}, 1), {{100, 0}})\n    lu.assertEquals(candidate({{1, 100}}, 0), {})\n    lu.assertEquals(candidate({{0, 100}}, 8), {})\n    lu.assertEquals(candidate({{1, 100}}, 2), {})\n    lu.assertEquals(candidate({{2, 100}}, 0), {})\n    lu.assertEquals(candidate({{1, 100}}, 7), {})\n    lu.assertEquals(candidate({{2, 100}}, 8), {})\n    lu.assertEquals(candidate({{0, 100}}, 1), {})\n    lu.assertEquals(candidate(candidate({{'M', 5}, {'M', 3}, {'I', 5}}, 'N'), 'N'), {})\n    lu.assertEquals(candidate({{2, 100}}, 7), {})\n    lu.assertEquals(candidate({{0, 100}}, 0), {{100, 0}})\n    lu.assertEquals(candidate({{7, 100}}, 0), {})\n    lu.assertEquals(candidate({{0, 100}}, 7), {})\n    lu.assertEquals(candidate({{7, 100}}, 2), {})\n    lu.assertEquals(candidate({{2, 100}}, 2), {{100, 0}})\n    lu.assertEquals(candidate({{7, 100}}, 1), {})\n    lu.assertEquals(candidate({{1, 100}}, 8), {})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_33486_is_int_type_malicious_score", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return params['override_confidence_score_malicious_threshold'] and isinstance(confidence_score, int) and int(\n-- #     params['override_confidence_score_malicious_threshold']) <= confidence_score\n-- \n--         determine if integer type confidence score is malicious in reputation_params\nlocal function is_int_type_malicious_score(confidence_score, params)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_33486_is_int_type_malicious_score.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_int_type_malicious_score\n    lu.assertEquals(candidate(100, {['override_confidence_score_malicious_threshold'] = 100000}), false)\n    lu.assertEquals(candidate(0, {['override_confidence_score_malicious_threshold'] = 100000}), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_335015_base10_to_7", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # s = \"\"\n-- # while num:\n-- #     s += chr(num & 0x7F)\n-- #     num = num >> 7\n-- # return s[::-1]\n-- \n-- Take a base 10 number and convert it to an ASCII string.\n-- :param num: the base 10 number\n-- :return: the ASCII string\nlocal function base10_to_7(num)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_335015_base10_to_7.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = base10_to_7\n    lu.assertEquals(candidate(0), '')\n    lu.assertEquals(candidate(0), '')\n    lu.assertEquals(candidate(127), '\\x7f')\n    lu.assertEquals(candidate(1), '\\x01')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_335336_gcd_by_subtracting", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # while m != n:\n-- #     if m > n:\n-- #         m -= n\n-- #     else:\n-- #         n -= m\n-- # return m\n-- \n-- Computes the greatest common divisor of two numbers by continuously subtracting the smaller\n-- number from the bigger one till they became equal.\n-- :param int m: First number.\n-- :param int n: Second number.\n-- :returns: GCD as a number.\nlocal function gcd_by_subtracting(m, n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_335336_gcd_by_subtracting.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = gcd_by_subtracting\n    lu.assertEquals(candidate(1, 7), 1)\n    lu.assertEquals(candidate(18, 12), 6)\n    lu.assertEquals(candidate(200, 100), 100)\n    lu.assertEquals(candidate(6, 90), 6)\n    lu.assertEquals(candidate(24, 36), 12)\n    lu.assertEquals(candidate(42, 42), 42)\n    lu.assertEquals(candidate(6, 12), 6)\n    lu.assertEquals(candidate(252, 105), 21)\n    lu.assertEquals(candidate(100, 10), 10)\n    lu.assertEquals(candidate(12, 16), 4)\n    lu.assertEquals(candidate(1, 1), 1)\n    lu.assertEquals(candidate(12, 28), 4)\n    lu.assertEquals(candidate(10, 100), 10)\n    lu.assertEquals(candidate(6, 24), 6)\n    lu.assertEquals(candidate(13, 27), 1)\n    lu.assertEquals(candidate(7, 1), 1)\n    lu.assertEquals(candidate(10, 20), 10)\n    lu.assertEquals(candidate(6, 8), 2)\n    lu.assertEquals(candidate(12, 18), 6)\n    lu.assertEquals(candidate(5, 125), 5)\n    lu.assertEquals(candidate(6, 14), 2)\n    lu.assertEquals(candidate(22, 13), 1)\n    lu.assertEquals(candidate(6, 18), 6)\n    lu.assertEquals(candidate(10000000000, 123456789), 1)\n    lu.assertEquals(candidate(6, 30), 6)\n    lu.assertEquals(candidate(3, 5), 1)\n    lu.assertEquals(candidate(24, 8), 8)\n    lu.assertEquals(candidate(9, 6), 3)\n    lu.assertEquals(candidate(12, 6), 6)\n    lu.assertEquals(candidate(2000, 1000), 1000)\n    lu.assertEquals(candidate(999, 1), 1)\n    lu.assertEquals(candidate(2, 10), 2)\n    lu.assertEquals(candidate(20, 10), 10)\n    lu.assertEquals(candidate(1000, 2000), 1000)\n    lu.assertEquals(candidate(125, 5), 5)\n    lu.assertEquals(candidate(5, 3), 1)\n    lu.assertEquals(candidate(6, 15), 3)\n    lu.assertEquals(candidate(12, 20), 4)\n    lu.assertEquals(candidate(100, 200), 100)\n    lu.assertEquals(candidate(20, 25), 5)\n    lu.assertEquals(candidate(25, 20), 5)\n    lu.assertEquals(candidate(6, 4), 2)\n    lu.assertEquals(candidate(23456789, 123456789), 1)\n    lu.assertEquals(candidate(6, 10), 2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_336336_format_role_order", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for role in roles:\n-- #     chunks = role.split(',')\n-- #     if 'saml-provider' in chunks[0]:\n-- #         newrole = chunks[1] + ',' + chunks[0]\n-- #         index = roles.index(role)\n-- #         roles.insert(index, newrole)\n-- #         roles.remove(role)\n-- # return roles\n-- \n-- Given roles, returns them in the format: role_arn,principal_arn.\n-- The format of the attribute value should be role_arn,principal_arn\n-- but lots of blogs list it as principal_arn,role_arn so let's reverse\n-- them if needed.\n-- Args:\n--     roles: List of roles.\n-- Returns:\n--     List of roles in the format: role_arn,principal_arn\nlocal function format_role_order(roles)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_336336_format_role_order.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = format_role_order\n    lu.assertEquals(candidate({'role1,principal1', 'role2,principal2'}), {'role1,principal1', 'role2,principal2'})\n    lu.assertEquals(candidate({'role1,principal1', 'role2,principal2', 'role3,principal3', 'role4,principal4'}), {'role1,principal1', 'role2,principal2', 'role3,principal3', 'role4,principal4'})\n    lu.assertEquals(candidate({'role1', 'role2'}), {'role1', 'role2'})\n    lu.assertEquals(candidate({'arn:aws:iam::123456789012:saml-provider/ADFS,arn:aws:iam::123456789012:role/Admin'}), {'arn:aws:iam::123456789012:role/Admin,arn:aws:iam::123456789012:saml-provider/ADFS'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_337003_power", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # 1- base case\n-- # if n == 0:\n-- #     return 1\n-- # # 2- solve sub-problems\n-- # previous_power = power(x, n-1)\n-- # # 3- combine sub-solutions\n-- # res = x * previous_power\n-- # return res\n-- \n-- Compute x to the power of n (with n>=0)\nlocal function power(x, n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_337003_power.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = power\n    lu.assertEquals(candidate(2, 4), 16)\n    lu.assertEquals(candidate(2, 6), 64)\n    lu.assertEquals(candidate(5, 2), 25)\n    lu.assertEquals(candidate(2, 5), 32)\n    lu.assertEquals(candidate(5, 3), 125)\n    lu.assertEquals(candidate(5, 5), 3125)\n    lu.assertEquals(candidate(10, 1), 10)\n    lu.assertEquals(candidate(2, 0), 1)\n    lu.assertEquals(candidate(3, 0), 1)\n    lu.assertEquals(candidate(3, 4), 81)\n    lu.assertEquals(candidate(2, 1), 2)\n    lu.assertEquals(candidate(2, 2), 4)\n    lu.assertEquals(candidate(5, 4), 625)\n    lu.assertEquals(candidate(10, 0), 1)\n    lu.assertEquals(candidate(5, 0), 1)\n    lu.assertEquals(candidate(2, 3), 8)\n    lu.assertEquals(candidate(10, 2), 100)\n    lu.assertEquals(candidate(3, 5), 243)\n    lu.assertEquals(candidate(3, 3), 27)\n    lu.assertEquals(candidate(3, 1), 3)\n    lu.assertEquals(candidate(10, 4), 10000)\n    lu.assertEquals(candidate(1, 0), 1)\n    lu.assertEquals(candidate(3, 6), 729)\n    lu.assertEquals(candidate(10, 3), 1000)\n    lu.assertEquals(candidate(5, 1), 5)\n    lu.assertEquals(candidate(3, 2), 9)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_338620_get_distance", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # def distance(a, b, size):\n-- #     # size -= 1\n-- #     if a > b:\n-- #         a, b = b, a\n-- #     if abs(a - b) > size / 2:\n-- #         a += size\n-- #     return abs(a - b)\n-- # return distance(coord1[0], coord2[0], size[0]) + distance(coord1[1], coord2[1], size[1])\n-- \n-- Get distance between two point in a tore space.\n-- Parameters\n-- ----------\n-- coord1: coordinate of the first point (tupe(int, int)).\n-- coord2: coordinate of the second point (tupe(int, int)).\n-- size: size of the tore (tupe(int, int))\n-- Return\n-- ------\n-- Distance: distance between the two points (int).\n-- Version\n-- -------\n-- Specification: Alisson Leist, Bayron Mahy, Nicolas Van Bossuyt (v1. 10/02/17)\n--                Bayron Mahy (v2. 19/03/17)\n-- Implementation: Nicolas Van Bossuyt, Alisson Leist (v1. 14/02/17)\n--                 Nicolas Van Bossuyt (v2. 09/03/17)\n--                 Nicolas Van Bossuyt (v3. 03/05/17)\nlocal function get_distance(coord1, coord2, size)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_338620_get_distance.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_distance\n    lu.assertEquals(candidate({0, 1}, {1, 0}, {2, 3}), 2)\n    lu.assertEquals(candidate({0, 0}, {3, 7}, {10, 10}), 6)\n    lu.assertEquals(candidate({1, 0}, {0, 0}, {2, 1}), 1)\n    lu.assertEquals(candidate({0, 1}, {1, 0}, {2, 2}), 2)\n    lu.assertEquals(candidate({0, 0}, {0, 0}, {10, 10}), 0)\n    lu.assertEquals(candidate({0, 0}, {7, 3}, {10, 10}), 6)\n    lu.assertEquals(candidate({0, 0}, {1, 0}, {2, 1}), 1)\n    lu.assertEquals(candidate({0, 0}, {1, 0}, {2, 2}), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_338738_power", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not isinstance(exponent, int):\n-- #     raise ValueError(\"Exponent should be an integer. \"\n-- #                      \"You provided {}.\".format(\n-- #                          type(exponent)\n-- #                      ))\n-- # result = term**exponent\n-- # return result\n-- \n-- Raise term to exponent.\n-- This function raises ``term`` to ``exponent``.\n-- Parameters\n-- ----------\n-- term : Number\n--     Term to be raised.\n-- exponent : int\n--     Exponent.\n-- Returns\n-- -------\n-- result : Number\n--     Result of the operation.\n-- Raises\n-- ------\n-- ValueError\n--     If exponent is not an integer.\n-- See Also\n-- --------\n-- add : Addition\n-- subtract : Subtraction\n-- multiply : Multiplication\n-- divide : Division\n-- Examples\n-- --------\n-- >>> power(1, 1)\n-- 1\n-- >>> power(2, 2)\n-- 4\n-- >>> power(4, 2)\n-- 16\n-- >>> power(10, 2)\n-- 100\n-- >>> power(100, 1)\n-- 100\n-- >>> power(10, 3)\n-- 1000\nlocal function power(term, exponent)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_338738_power.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = power\n    lu.assertEquals(candidate(10, 2), 100)\n    lu.assertEquals(candidate(2, 0), 1)\n    lu.assertEquals(candidate(-2, 3), -8)\n    lu.assertEquals(candidate(5, 4), 625)\n    lu.assertEquals(candidate(2, 2), 4)\n    lu.assertEquals(candidate(100, 1), 100)\n    lu.assertEquals(candidate(2, 3), 8)\n    lu.assertEquals(candidate(10, 3), 1000)\n    lu.assertEquals(candidate(0, 0), 1)\n    lu.assertEquals(candidate(4, 2), 16)\n    lu.assertEquals(candidate(1, 1), 1)\n    lu.assertEquals(candidate(5, 2), 25)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_339249_urlsplit", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # proto, rest = url.split(':', 1)\n-- # host = ''\n-- # if rest[:2] == '//':\n-- #     host, rest = rest[2:].split('/', 1)\n-- #     rest = '/' + rest\n-- # return proto, host, rest\n-- \n-- Split an arbitrary url into protocol, host, rest\n-- The standard urlsplit does not want to provide 'netloc' for arbitrary\n-- protocols, this works around that.\n-- :param url: The url to split into component parts\nlocal function urlsplit(url)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_339249_urlsplit.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = urlsplit\n    lu.assertEquals(candidate('http://www.python.org/doc/2.5.2/lib/module-time.html'), {'http', 'www.python.org', '/doc/2.5.2/lib/module-time.html'})\n    lu.assertEquals(candidate('http://www.example.com/a/b/c'), {'http', 'www.example.com', '/a/b/c'})\n    lu.assertEquals(candidate('https://127.0.0.1/foo/bar'), {'https', '127.0.0.1', '/foo/bar'})\n    lu.assertEquals(candidate('http://www.example.com/'), {'http', 'www.example.com', '/'})\n    lu.assertEquals(candidate('http://www.example.com/foo'), {'http', 'www.example.com', '/foo'})\n    lu.assertEquals(candidate('http://www.example.com/a/b/c?a=1&b=2'), {'http', 'www.example.com', '/a/b/c?a=1&b=2'})\n    lu.assertEquals(candidate('http://www.example.com/a/b/c?a=1&b=2#frag'), {'http', 'www.example.com', '/a/b/c?a=1&b=2#frag'})\n    lu.assertEquals(candidate('http://www.python.org/'), {'http', 'www.python.org', '/'})\n    lu.assertEquals(candidate('http://www.example.com/?a=1&b=2'), {'http', 'www.example.com', '/?a=1&b=2'})\n    lu.assertEquals(candidate('ftp://ftp.debian.org/debian/README'), {'ftp', 'ftp.debian.org', '/debian/README'})\n    lu.assertEquals(candidate('http://www.python.org:80/'), {'http', 'www.python.org:80', '/'})\n    lu.assertEquals(candidate('https://localhost/foo/bar'), {'https', 'localhost', '/foo/bar'})\n    lu.assertEquals(candidate('file:///foo/bar/baz.html'), {'file', '', '/foo/bar/baz.html'})\n    lu.assertEquals(candidate('http://localhost/foo/bar'), {'http', 'localhost', '/foo/bar'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_339342__extract_prop_option", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # line = line[7:]\n-- # pos = line.find(' ')\n-- # return line[:pos], line[pos + 1:]\n-- \n-- Extract the (key,value)-tuple from a string like:\n-- >>> \"option foobar 123\"\n-- :param line:\n-- :return: tuple (key, value)\nlocal function _extract_prop_option(line)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_339342__extract_prop_option.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _extract_prop_option\n    lu.assertEquals(candidate('option foobar:baz 123'), {'foobar:baz', '123'})\n    lu.assertEquals(candidate('option foobar 123'), {'foobar', '123'})\n    lu.assertEquals(candidate('option foobar -123'), {'foobar', '-123'})\n    lu.assertEquals(candidate('option foobar 123'), {'foobar', '123'})\n    lu.assertEquals(candidate('option foobar 123.4e5_0'), {'foobar', '123.4e5_0'})\n    lu.assertEquals(candidate('option foobar 123'), {'foobar', '123'})\n    lu.assertEquals(candidate('option foobar 123.4e5'), {'foobar', '123.4e5'})\n    lu.assertEquals(candidate('option foobar:baz 123 123 123'), {'foobar:baz', '123 123 123'})\n    lu.assertEquals(candidate('option foobar 123'), {'foobar', '123'})\n    lu.assertEquals(candidate('option foobar 123 123 123'), {'foobar', '123 123 123'})\n    lu.assertEquals(candidate('option foobar +123.4'), {'foobar', '+123.4'})\n    lu.assertEquals(candidate('option foobar 123'), {'foobar', '123'})\n    lu.assertEquals(candidate('option foobar -123.4'), {'foobar', '-123.4'})\n    lu.assertEquals(candidate('option foobar 123 456'), {'foobar', '123 456'})\n    lu.assertEquals(candidate('option foobar 123.4e-5'), {'foobar', '123.4e-5'})\n    lu.assertEquals(candidate('option foobar 123.4'), {'foobar', '123.4'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_340491_int_with_radix", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return int(string, 0)\n-- \n-- Parse integer with or without a base prefix\n-- :param string: String representation of integer\n-- :type string: str\n-- :return: Parsed integer\n-- :rtype: int\n-- :raise ValueError: If string is no valid integer representation\nlocal function int_with_radix(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_340491_int_with_radix.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = int_with_radix\n    lu.assertEquals(candidate('0xFF'), 255)\n    lu.assertEquals(candidate('0b10'), 2)\n    lu.assertEquals(candidate('-0b00011011'), -27)\n    lu.assertEquals(candidate('0'), 0)\n    lu.assertEquals(candidate('1010101'), 1010101)\n    lu.assertEquals(candidate('0x10'), 16)\n    lu.assertEquals(candidate('0x0000'), 0)\n    lu.assertEquals(candidate('1'), 1)\n    lu.assertEquals(candidate('0x100'), 256)\n    lu.assertEquals(candidate('0x100'), 256)\n    lu.assertEquals(candidate('0x00'), 0)\n    lu.assertEquals(candidate('0x0'), 0)\n    lu.assertEquals(candidate('10'), 10)\n    lu.assertEquals(candidate('0x000'), 0)\n    lu.assertEquals(candidate('-0x1234'), -4660)\n    lu.assertEquals(candidate('0x1234'), 4660)\n    lu.assertEquals(candidate('0b00011011'), 27)\n    lu.assertEquals(candidate('-0o1234'), -668)\n    lu.assertEquals(candidate('0o1234'), 668)\n    lu.assertEquals(candidate('0b10101010'), 170)\n    lu.assertEquals(candidate('0x1234'), 4660)\n    lu.assertEquals(candidate('-1'), -1)\n    lu.assertEquals(candidate('0x10'), 16)\n    lu.assertEquals(candidate('0o77'), 63)\n    lu.assertEquals(candidate('1'), 1)\n    lu.assertEquals(candidate('0o10'), 8)\n    lu.assertEquals(candidate('0x1234567890abcdefabcdef'), 22007822917795467892608495)\n    lu.assertEquals(candidate('1234'), 1234)\n    lu.assertEquals(candidate('0x1234567890abcdef'), 1311768467294899695)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_34192_parse_envs", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # envs = {}\n-- # if not arg:\n-- #     return envs\n-- # i = 0\n-- # fields = arg.split(\"=\")\n-- # if len(fields) < 2:\n-- #     return envs\n-- # pre_key = \"\"\n-- # while i < len(fields):\n-- #     if i == 0:\n-- #         pre_key = fields[i]\n-- #     elif i == len(fields) - 1:\n-- #         envs[pre_key] = fields[i]\n-- #     else:\n-- #         r = fields[i].rfind(\",\")\n-- #         envs[pre_key] = fields[i][:r]\n-- #         pre_key = fields[i][r + 1:]  # noqa: E203\n-- #     i += 1\n-- # return envs\n-- \n-- Parse environment configs as a dict.\n-- Support format 'k1=v1,k2=v2,k3=v3..'. Note that comma is supported\n-- in value field.\nlocal function parse_envs(arg)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_34192_parse_envs.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_envs\n    lu.assertEquals(candidate('key'), {})\n    lu.assertEquals(candidate('k1=v1,,'), {['k1'] = 'v1,,'})\n    lu.assertEquals(candidate('a=1'), {['a'] = '1'})\n    lu.assertEquals(candidate('x=y,z=z'), {['x'] = 'y', ['z'] = 'z'})\n    lu.assertEquals(candidate('a=a,b=b,c=c,d=d,e=e'), {['a'] = 'a', ['b'] = 'b', ['c'] = 'c', ['d'] = 'd', ['e'] = 'e'})\n    lu.assertEquals(candidate('A=B,C=D,E=F,G=H'), {['A'] = 'B', ['C'] = 'D', ['E'] = 'F', ['G'] = 'H'})\n    lu.assertEquals(candidate('foo=bar'), {['foo'] = 'bar'})\n    lu.assertEquals(candidate('k1=v1'), {['k1'] = 'v1'})\n    lu.assertEquals(candidate('key=value,key1=value1,key2=value2'), {['key'] = 'value', ['key1'] = 'value1', ['key2'] = 'value2'})\n    lu.assertEquals(candidate('key=value'), {['key'] = 'value'})\n    lu.assertEquals(candidate('a='), {['a'] = ''})\n    lu.assertEquals(candidate('a=1,b=2,c=3,d=a,b,c'), {['a'] = '1', ['b'] = '2', ['c'] = '3', ['d'] = 'a,b,c'})\n    lu.assertEquals(candidate('key=value1,value2'), {['key'] = 'value1,value2'})\n    lu.assertEquals(candidate('k1=v1,k2=v2,k3='), {['k1'] = 'v1', ['k2'] = 'v2', ['k3'] = ''})\n    lu.assertEquals(candidate('key1='), {['key1'] = ''})\n    lu.assertEquals(candidate('a=1,b=2'), {['a'] = '1', ['b'] = '2'})\n    lu.assertEquals(candidate('a=b,c=d,e=f,g=h'), {['a'] = 'b', ['c'] = 'd', ['e'] = 'f', ['g'] = 'h'})\n    lu.assertEquals(candidate('foo=bar,baz=qux,quux=corge'), {['foo'] = 'bar', ['baz'] = 'qux', ['quux'] = 'corge'})\n    lu.assertEquals(candidate(' a '), {})\n    lu.assertEquals(candidate('abc'), {})\n    lu.assertEquals(candidate(''), {})\n    lu.assertEquals(candidate('key1=value1,key2=value2'), {['key1'] = 'value1', ['key2'] = 'value2'})\n    lu.assertEquals(candidate('A=B,C=D,E=F,G=H,I=J,K=L,M=N'), {['A'] = 'B', ['C'] = 'D', ['E'] = 'F', ['G'] = 'H', ['I'] = 'J', ['K'] = 'L', ['M'] = 'N'})\n    lu.assertEquals(candidate(' '), {})\n    lu.assertEquals(candidate('A=B'), {['A'] = 'B'})\n    lu.assertEquals(candidate('a=a,b=b,c=c,d=d'), {['a'] = 'a', ['b'] = 'b', ['c'] = 'c', ['d'] = 'd'})\n    lu.assertEquals(candidate('A=B,C=D,E=F,G=H,I=J,K=L,M=N,O=P'), {['A'] = 'B', ['C'] = 'D', ['E'] = 'F', ['G'] = 'H', ['I'] = 'J', ['K'] = 'L', ['M'] = 'N', ['O'] = 'P'})\n    lu.assertEquals(candidate('a=b'), {['a'] = 'b'})\n    lu.assertEquals(candidate(' a b '), {})\n    lu.assertEquals(candidate('A=B,C=D'), {['A'] = 'B', ['C'] = 'D'})\n    lu.assertEquals(candidate('key=value,key2=value2'), {['key'] = 'value', ['key2'] = 'value2'})\n    lu.assertEquals(candidate('a=b,c=d,e=f'), {['a'] = 'b', ['c'] = 'd', ['e'] = 'f'})\n    lu.assertEquals(candidate('a=a'), {['a'] = 'a'})\n    lu.assertEquals(candidate('a=b,c=d,e=f,g=h,i=j,k=l,m=n'), {['a'] = 'b', ['c'] = 'd', ['e'] = 'f', ['g'] = 'h', ['i'] = 'j', ['k'] = 'l', ['m'] = 'n'})\n    lu.assertEquals(candidate('A=B,C=D,E=F'), {['A'] = 'B', ['C'] = 'D', ['E'] = 'F'})\n    lu.assertEquals(candidate('hello=world'), {['hello'] = 'world'})\n    lu.assertEquals(candidate('a=a,b=b'), {['a'] = 'a', ['b'] = 'b'})\n    lu.assertEquals(candidate('A=B,C=D,E=F,G=H,I=J'), {['A'] = 'B', ['C'] = 'D', ['E'] = 'F', ['G'] = 'H', ['I'] = 'J'})\n    lu.assertEquals(candidate('k1=v1,k2=v2,k3=v3'), {['k1'] = 'v1', ['k2'] = 'v2', ['k3'] = 'v3'})\n    lu.assertEquals(candidate('k1=v1,k2=v2,k3=v3'), {['k1'] = 'v1', ['k2'] = 'v2', ['k3'] = 'v3'})\n    lu.assertEquals(candidate('k1=,k2=v3'), {['k1'] = '', ['k2'] = 'v3'})\n    lu.assertEquals(candidate('key1=value1'), {['key1'] = 'value1'})\n    lu.assertEquals(candidate('a=a,b=b,c=c'), {['a'] = 'a', ['b'] = 'b', ['c'] = 'c'})\n    lu.assertEquals(candidate('key=value,key2=value2,key3=value3'), {['key'] = 'value', ['key2'] = 'value2', ['key3'] = 'value3'})\n    lu.assertEquals(candidate('k1=v1,k2=v2,k3=v3,k4=v4'), {['k1'] = 'v1', ['k2'] = 'v2', ['k3'] = 'v3', ['k4'] = 'v4'})\n    lu.assertEquals(candidate('k1=v1,k2=v2'), {['k1'] = 'v1', ['k2'] = 'v2'})\n    lu.assertEquals(candidate(' a b c '), {})\n    lu.assertEquals(candidate('a=b,c=d,e=f,g=h,i=j,k=l,m=n,o=p,q=r,s=t,u=v,w=x,y=z'), {['a'] = 'b', ['c'] = 'd', ['e'] = 'f', ['g'] = 'h', ['i'] = 'j', ['k'] = 'l', ['m'] = 'n', ['o'] = 'p', ['q'] = 'r', ['s'] = 't', ['u'] = 'v', ['w'] = 'x', ['y'] = 'z'})\n    lu.assertEquals(candidate('a=b'), {['a'] = 'b'})\n    lu.assertEquals(candidate('x=y'), {['x'] = 'y'})\n    lu.assertEquals(candidate(None), {})\n    lu.assertEquals(candidate('k1=v1,k2=v2,k3=v3,k4=v4,k5=v5'), {['k1'] = 'v1', ['k2'] = 'v2', ['k3'] = 'v3', ['k4'] = 'v4', ['k5'] = 'v5'})\n    lu.assertEquals(candidate('a=b,c=d,e=f,g=h,i=j'), {['a'] = 'b', ['c'] = 'd', ['e'] = 'f', ['g'] = 'h', ['i'] = 'j'})\n    lu.assertEquals(candidate('a=1,b=2,c=3'), {['a'] = '1', ['b'] = '2', ['c'] = '3'})\n    lu.assertEquals(candidate('a=b,c=d'), {['a'] = 'b', ['c'] = 'd'})\n    lu.assertEquals(candidate('k1=,k2='), {['k1'] = '', ['k2'] = ''})\n    lu.assertEquals(candidate('hello'), {})\n    lu.assertEquals(candidate('hello=world,foo=bar,baz=qux'), {['hello'] = 'world', ['foo'] = 'bar', ['baz'] = 'qux'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_342149_checkpid", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # import os\n-- # return os.getpid() == pid\n-- \n-- return the pid of the engine\nlocal function checkpid(pid)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_342149_checkpid.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = checkpid\n    lu.assertEquals(candidate(12345), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_342437_problem48", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Lazy\n-- # result = 0\n-- # for x in range(1, limit + 1):\n-- #     result += x ** x\n-- # return int(str(result)[-10:])\n-- \n-- Problem 48 - Self powers\nlocal function problem48(limit)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_342437_problem48.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = problem48\n    lu.assertEquals(candidate(1000), 9110846700)\n    lu.assertEquals(candidate(10), 405071317)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_342875_naive_add", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if y == 0:\n-- #     return x\n-- # return naive_add(x, y - 1) + 1\n-- \n-- A naive implementation which can blow the call stack.\nlocal function naive_add(x, y)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_342875_naive_add.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = naive_add\n    lu.assertEquals(candidate(1000, 0), 1000)\n    lu.assertEquals(candidate(3, 10), 13)\n    lu.assertEquals(candidate(100, 100), 200)\n    lu.assertEquals(candidate(1, 10), 11)\n    lu.assertEquals(candidate(1, 1), 2)\n    lu.assertEquals(candidate(100, 500), 600)\n    lu.assertEquals(candidate(2, 0), 2)\n    lu.assertEquals(candidate(2, 3), 5)\n    lu.assertEquals(candidate(10, 100), 110)\n    lu.assertEquals(candidate(1, 0), 1)\n    lu.assertEquals(candidate(0, 0), 0)\n    lu.assertEquals(candidate(1, 2), 3)\n    lu.assertEquals(candidate(5, 100), 105)\n    lu.assertEquals(candidate(0, 100), 100)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_344339__estim_determ_p", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # p = 0.0\n-- # if num_zeros == 0:\n-- #     p += 0.5\n-- # if num_ones == 0:\n-- #     p += 0.5\n-- # return p\n-- \n-- An estimator that beliefs just in\n-- deterministic memory-less models.\nlocal function _estim_determ_p(num_zeros, num_ones)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_344339__estim_determ_p.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _estim_determ_p\n    lu.assertEquals(candidate(1, 0), 0.5)\n    lu.assertEquals(candidate(0, 1), 0.5)\n    lu.assertEquals(candidate(1, 1), 0.0)\n    lu.assertEquals(candidate(2, 2), 0.0)\n    lu.assertEquals(candidate(2, 1), 0.0)\n    lu.assertEquals(candidate(0, 2), 0.5)\n    lu.assertEquals(candidate(0, 3), 0.5)\n    lu.assertEquals(candidate(3, 0), 0.5)\n    lu.assertEquals(candidate(1, 2), 0.0)\n    lu.assertEquals(candidate(2, 0), 0.5)\n    lu.assertEquals(candidate(0, 0), 1.0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_345591_categorizeClass", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # str_class = \"\"\n-- # if h1 == 0 and h2 == 0:\n-- #     str_class = 0\n-- # elif h1 == 0 and h2 == 1:\n-- #     str_class = 1\n-- # elif h1 == 1 and h2 == 0:\n-- #     str_class = 2\n-- # elif h1 == 1 and h2 == 1:\n-- #     str_class = 3\n-- # return str_class\n-- \n-- Greeble classification with respect to horns [0 down, 1 up]\n-- h1  |h2     |class\n-- 0   |0      |1\n-- 0   |1      |2\n-- 1   |0      |3\n-- 1   |1      |4\nlocal function categorizeClass(h1, h2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_345591_categorizeClass.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = categorizeClass\n    lu.assertEquals(candidate(0, 0), 0)\n    lu.assertEquals(candidate(0, 1), 1)\n    lu.assertEquals(candidate(1, 1), 3)\n    lu.assertEquals(candidate(1, 0), 2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_346321_reverse_int", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # r = 0\n-- # while n > 0:\n-- #     r = r * 10 + n % 10\n-- #     n //= 10\n-- # return r\n-- \n-- Late to the party, but here's a good one.\n-- Integer Reverse:\n-- Given: Any random integer in decimal.\n-- Challenge: Reverse it, without using any of the obvious toString tricks, or transient conversions to a data type other than an int.\nlocal function reverse_int(n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_346321_reverse_int.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = reverse_int\n    lu.assertEquals(candidate(88888888), 88888888)\n    lu.assertEquals(candidate(1000), 1)\n    lu.assertEquals(candidate(987654321), 123456789)\n    lu.assertEquals(candidate(10), 1)\n    lu.assertEquals(candidate(123456789), 987654321)\n    lu.assertEquals(candidate(987), 789)\n    lu.assertEquals(candidate(4), 4)\n    lu.assertEquals(candidate(4321), 1234)\n    lu.assertEquals(candidate(4560), 654)\n    lu.assertEquals(candidate(12345), 54321)\n    lu.assertEquals(candidate(123), 321)\n    lu.assertEquals(candidate(321), 123)\n    lu.assertEquals(candidate(123), 321)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(0), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_34678_url", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return '[' + text + '](' + url_ + ')'\n-- \n-- Url\n-- Args:\n--     text (str): text for url.\n--     url (str): url for text.\n-- Returns:\n--     str: url.\nlocal function url(text, url_)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_34678_url.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = url\n    lu.assertEquals(candidate('title', 'candidate'), '[title](candidate)')\n    lu.assertEquals(candidate('title with underscore', 'candidate'), '[title with underscore](candidate)')\n    lu.assertEquals(candidate('title with space', 'candidate'), '[title with space](candidate)')\n    lu.assertEquals(candidate('title with underscore and space', 'candidate'), '[title with underscore and space](candidate)')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_348306_is_allowed_conference", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if c1 not in conf_names:\n-- #     c1 = 'other'\n-- # if c2 not in conf_names:\n-- #     c2 = 'other'\n-- # if c1 not in allowed_confs and c2 not in allowed_confs:\n-- #     return False\n-- # return True\n-- \n-- Return True if at least one of c1/c2 is an allowed_conference\n-- conf_names is a list of all defined conferences (used to group things into an \"other\" category)\nlocal function is_allowed_conference(c1, c2, conf_names, allowed_confs)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_348306_is_allowed_conference.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_allowed_conference\n    lu.assertEquals(candidate(1, 2, {'a', 'b'}, {}), false)\n    lu.assertEquals(candidate(1, 2, {'a', 'b'}, {'other', 'a'}), true)\n    lu.assertEquals(candidate(1, 2, {'a', 'b'}, {'other', 'a', 'b'}), true)\n    lu.assertEquals(candidate(1, 2, {'a', 'b'}, {'a', 'other'}), true)\n    lu.assertEquals(candidate(1, 2, {'a', 'b'}, {'other', 'b', 'c'}), true)\n    lu.assertEquals(candidate(1, 2, {'a', 'b'}, {'a'}), false)\n    lu.assertEquals(candidate(1, 2, {'a', 'b'}, {'c'}), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_349011_mml_namelist", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # N = max([len(s) for s in namelist])\n-- # return [\"'%s'\" % s.ljust(N, ' ') for s in namelist]\n-- \n-- padding to be same length\nlocal function mml_namelist(namelist)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_349011_mml_namelist.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = mml_namelist\n    lu.assertEquals(candidate({'a', 'b'}), {\"'a'\", \"'b'\"})\n    lu.assertEquals(candidate({'foo', 'bar', 'baz'}), {\"'foo'\", \"'bar'\", \"'baz'\"})\n    lu.assertEquals(candidate({'a', 'bbbb', 'ccccc'}), {\"'a    '\", \"'bbbb '\", \"'ccccc'\"})\n    lu.assertEquals(candidate({'foo', 'bar'}), {\"'foo'\", \"'bar'\"})\n    lu.assertEquals(candidate({'a', 'b', 'c'}), {\"'a'\", \"'b'\", \"'c'\"})\n    lu.assertEquals(candidate({'a', 'b', 'c', 'd'}), {\"'a'\", \"'b'\", \"'c'\", \"'d'\"})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_350170_welcome", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if name is None:\n-- #     return {'message': 'Welcome to API Star!'}\n-- # return {'message': 'Welcome to API Star, %s!' % name}\n-- \n--  Welcome to API Star. Personalized for name \nlocal function welcome(name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_350170_welcome.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = welcome\n    lu.assertEquals(candidate('Michael'), {['message'] = 'Welcome to API Star, Michael!'})\n    lu.assertEquals(candidate(None), {['message'] = 'Welcome to API Star!'})\n    lu.assertEquals(candidate('Python'), {['message'] = 'Welcome to API Star, Python!'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_350340__get_acl_username", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # i = 0\n-- # output = ''\n-- # while i < len(acl) and acl[i] != '=':\n-- #     # If user name isn't quoted, then just add it to the output buffer\n-- #     if acl[i] != '\"':\n-- #         output += acl[i]\n-- #         i += 1\n-- #     else:\n-- #         # Otherwise, it's a quoted username\n-- #         i += 1\n-- #         if i == len(acl):\n-- #             raise ValueError('ACL syntax error: unterminated quote.')\n-- #         # Loop until we come across an unescaped quote\n-- #         while not (acl[i] == '\"' and acl[i + 1:i + 2] != '\"'):\n-- #             # Quoting convention is to escape \" as \"\".\n-- #             if acl[i] == '\"' and acl[i + 1:i + 2] == '\"':\n-- #                 i += 1\n-- #             output += acl[i]\n-- #             i += 1\n-- #             if i == len(acl):\n-- #                 raise ValueError('ACL syntax error: unterminated quote.')\n-- #         i += 1\n-- # return i, output\n-- \n-- Port of ``copyAclUserName`` from ``dumputils.c``\nlocal function _get_acl_username(acl)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_350340__get_acl_username.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _get_acl_username\n    lu.assertEquals(candidate('foo'), {3, 'foo'})\n    lu.assertEquals(candidate('='), {0, ''})\n    lu.assertEquals(candidate('\"foo\"'), {5, 'foo'})\n    lu.assertEquals(candidate('\"a\"'), {3, 'a'})\n    lu.assertEquals(candidate('\"foo\"\"=bar\"'), {11, 'foo\"=bar'})\n    lu.assertEquals(candidate('a'), {1, 'a'})\n    lu.assertEquals(candidate(''), {0, ''})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_350862_count_values", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # value_list = []\n-- # for value in input_dict.values():\n-- #     value_list.append(value)\n-- # return len(set(value_list))\n-- \n-- :param input_dict:\n-- :return:\n-- Takes a dict and counts the unique values in that dict.\nlocal function count_values(input_dict)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_350862_count_values.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = count_values\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 1, ['c'] = 1, ['d'] = 2, ['e'] = 2, ['f'] = 3}), 3)\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 1, ['c'] = 1}), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_351488_hexToBytes", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # bInt = [int(hexStr[i*3:i*3+2], 16) for i in range(int((len(hexStr)+1)/3))]\n-- # return bInt\n-- \n-- Provide hex sting in format 'ab ab ab...'\n-- Returns the byte values\nlocal function hexToBytes(hexStr)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_351488_hexToBytes.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = hexToBytes\n    lu.assertEquals(candidate('123'), {18})\n    lu.assertEquals(candidate(''), {})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_351618_substring_edit_distance", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not s:\n-- #     return len(t)\n-- # if not t:\n-- #     return 0\n-- # M = [[0 for _ in range(len(t) + 1)] for _ in range(len(s) + 1)]\n-- # # M[i][j] is the minimum number of t-edits required to make t[:j] a suffix of s[:i].\n-- # for i in range(len(s) + 1):\n-- #     M[i][0] = 0\n-- # for j in range(len(t) + 1):\n-- #     M[0][j] = j\n-- # for i in range(1, len(s) + 1):\n-- #     for j in range(1, len(t) + 1):\n-- #         cost = 0 if s[i - 1] == t[j - 1] else 1\n-- #         M[i][j] = min(\n-- #             [\n-- #                 1 + M[i - 1][j],\n-- #                 1 + M[i][j - 1],\n-- #                 cost + M[i - 1][j - 1],\n-- #             ])\n-- # return min(M[i][len(t)] for i in range(len(s) + 1))\n-- \n-- The minimum number of edits required to make t a substring of s.\n-- An edit is the addition, deletion, or replacement of a character.\nlocal function substring_edit_distance(s, t)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_351618_substring_edit_distance.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = substring_edit_distance\n    lu.assertEquals(candidate('abcd', 'abcdef'), 2)\n    lu.assertEquals(candidate('abcd', 'abcd'), 0)\n    lu.assertEquals(candidate('abcdefghij', 'abcdefghijk'), 1)\n    lu.assertEquals(candidate('abcde', 'abcde'), 0)\n    lu.assertEquals(candidate('abcdefghijklmnopqrstuvwxyz', 'abcdefghijklmnopqrstuvwxyza'), 1)\n    lu.assertEquals(candidate('abc', 'def'), 3)\n    lu.assertEquals(candidate('abc', 'abc'), 0)\n    lu.assertEquals(candidate('abcdefghij', 'abcdefghij'), 0)\n    lu.assertEquals(candidate('', 'abcd'), 4)\n    lu.assertEquals(candidate('abcd', 'abc'), 0)\n    lu.assertEquals(candidate('a', 'abcdef'), 5)\n    lu.assertEquals(candidate('abcdef', 'abccef'), 1)\n    lu.assertEquals(candidate('abc', 'abcd'), 1)\n    lu.assertEquals(candidate('abc', 'abc'), 0)\n    lu.assertEquals(candidate('abc', 'abcde'), 2)\n    lu.assertEquals(candidate('abcdefghijklmnop', 'abcdefghijklmnopq'), 1)\n    lu.assertEquals(candidate('abc', 'abcdef'), 3)\n    lu.assertEquals(candidate('abcdefghijklmnopqrstuvwxyz', 'abcdefghijklmnopqrstuvwxyzab'), 2)\n    lu.assertEquals(candidate('abcdefghijklmnopqrstuvwxyz', 'abcdefghijklmnopqrstuvwxyz'), 0)\n    lu.assertEquals(candidate('', ''), 0)\n    lu.assertEquals(candidate('', 'abc'), 3)\n    lu.assertEquals(candidate('abc', 'abcxyz'), 3)\n    lu.assertEquals(candidate('abcdef', 'abcdef'), 0)\n    lu.assertEquals(candidate('abcd', 'abcde'), 1)\n    lu.assertEquals(candidate('abc', 'xyz'), 3)\n    lu.assertEquals(candidate('', 'abcdef'), 6)\n    lu.assertEquals(candidate('abcd', 'ac'), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_351752_xyminmax_to_xywh", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return [xmin, ymin, xmax - xmin, ymax - ymin]\n-- \n-- convert box coordinates from (xmin, ymin, xmax, ymax) form to (x, y, w , h) form\nlocal function xyminmax_to_xywh(xmin, ymin, xmax, ymax)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_351752_xyminmax_to_xywh.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = xyminmax_to_xywh\n    lu.assertEquals(candidate(1, 2, 1, 2), {1, 2, 0, 0})\n    lu.assertEquals(candidate(1, 2, 3, 3), {1, 2, 2, 1})\n    lu.assertEquals(candidate(-1, -1, 1, 1), {-1, -1, 2, 2})\n    lu.assertEquals(candidate(1, 1, 3, 5), {1, 1, 2, 4})\n    lu.assertEquals(candidate(1, 1, 3, 4), {1, 1, 2, 3})\n    lu.assertEquals(candidate(1, 2, 3, 4), {1, 2, 2, 2})\n    lu.assertEquals(candidate(1, 2, 3, 2), {1, 2, 2, 0})\n    lu.assertEquals(candidate(1, 2, 1, 4), {1, 2, 0, 2})\n    lu.assertEquals(candidate(0, 0, 1, 1), {0, 0, 1, 1})\n    lu.assertEquals(candidate(3, 7, 5, 9), {3, 7, 2, 2})\n    lu.assertEquals(candidate(1, 1, 3, 3), {1, 1, 2, 2})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_351898_lat_to_km", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # km_north = 110.574 * latitude\n-- # return km_north\n-- \n-- Expresses given latitude in kilometers to the north\n-- Args:\n--     latitude (float): Latitude in degrees.\n-- Returns:\n--     float: Latitude expressed in kilometers to the north\nlocal function lat_to_km(latitude)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_351898_lat_to_km.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = lat_to_km\n    lu.assertEquals(candidate(0.0), 0.0)\n    lu.assertEquals(candidate(0.0), 0.0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_35240_int_to_str", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # UNUSED\n-- # n = int(number)\n-- # s = str(n)\n-- # return s if len(s) > 1 else ('0' + s)\n-- \n-- Formats integer to string.\n-- This function is used for operating with image names.\n-- Example: int_to_str(2) -> '02'\nlocal function int_to_str(number)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_35240_int_to_str.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = int_to_str\n    lu.assertEquals(candidate(8), '08')\n    lu.assertEquals(candidate(2), '02')\n    lu.assertEquals(candidate(99), '99')\n    lu.assertEquals(candidate(13), '13')\n    lu.assertEquals(candidate(18), '18')\n    lu.assertEquals(candidate(7), '07')\n    lu.assertEquals(candidate(40), '40')\n    lu.assertEquals(candidate(1), '01')\n    lu.assertEquals(candidate(9), '09')\n    lu.assertEquals(candidate(16), '16')\n    lu.assertEquals(candidate(20), '20')\n    lu.assertEquals(candidate(1000), '1000')\n    lu.assertEquals(candidate(11), '11')\n    lu.assertEquals(candidate(14), '14')\n    lu.assertEquals(candidate(30), '30')\n    lu.assertEquals(candidate(50), '50')\n    lu.assertEquals(candidate(6), '06')\n    lu.assertEquals(candidate(19), '19')\n    lu.assertEquals(candidate(4), '04')\n    lu.assertEquals(candidate(38), '38')\n    lu.assertEquals(candidate(100), '100')\n    lu.assertEquals(candidate(0), '00')\n    lu.assertEquals(candidate(5), '05')\n    lu.assertEquals(candidate(15), '15')\n    lu.assertEquals(candidate(3), '03')\n    lu.assertEquals(candidate(12), '12')\n    lu.assertEquals(candidate(10), '10')\n    lu.assertEquals(candidate(17), '17')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_352460_mat_mul", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # result = []\n-- # if len(mat1[0]) == len(mat2):\n-- #     for i in range(len(mat1)):\n-- #         aux_row = []\n-- #         for j in range(len(mat2[0])):\n-- #             aux = 0\n-- #             for k in range(len(mat2)):\n-- #                 aux += mat1[i][k] * mat2[k][j]\n-- #             aux_row.append(aux)\n-- #         result.append(aux_row)\n-- #     return result\n-- # return None\n-- \n-- Function to performs matrix multiplication\n-- Returns the a new matrix\nlocal function mat_mul(mat1, mat2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_352460_mat_mul.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = mat_mul\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}}, {{1, 2}, {3, 4}, {5, 6}}), {{22, 28}, {49, 64}, {76, 100}})\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5, 6}}, {{7, 8}, {9, 10}}), None)\n    lu.assertEquals(candidate({{1, 2}, {3, 4}}, {{5, 6}, {7, 8}}), {{19, 22}, {43, 50}})\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5, 6}}, {{1, 2}, {3, 4}, {5, 6}}), {{22, 28}, {49, 64}})\n    lu.assertEquals(candidate({{1, 2, 3, 4}, {5, 6, 7, 8}}, {{1, 2, 3}, {4, 5, 6}}), None)\n    lu.assertEquals(candidate({{1, 2}, {3, 4}}, {{1, 2}, {3, 4}}), {{7, 10}, {15, 22}})\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5, 6}}, {{7, 8}, {9, 10}, {11, 12}, {13, 14}}), None)\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5, 6}}, {{1}, {2}, {3}}), {{14}, {32}})\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5, 6}}, {{7, 8}, {9, 10}, {11, 12}}), {{58, 64}, {139, 154}})\n    lu.assertEquals(candidate({{1, 2}, {3, 4}}, {{1, 2}, {3, 4}, {5, 6}}), None)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_353572_name_cleanup", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if s is None:\n-- #     return None\n-- # s = s.replace('[%s]', '%s')\n-- # return s\n-- \n-- cleanup a register name\nlocal function name_cleanup(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_353572_name_cleanup.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = name_cleanup\n    lu.assertEquals(candidate('a'), 'a')\n    lu.assertEquals(candidate('a'), 'a')\n    lu.assertEquals(candidate(None), None)\n    lu.assertEquals(candidate('Foo'), 'Foo')\n    lu.assertEquals(candidate('r1'), 'r1')\n    lu.assertEquals(candidate('reg'), 'reg')\n    lu.assertEquals(candidate('[%s]'), '%s')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('foo'), 'foo')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_354029_get_full_path_file_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # full_path_file_name = ''\n-- # if folder_name > '':\n-- #     full_path_file_name = folder_name + '/'\n-- # full_path_file_name += file_name\n-- # return full_path_file_name\n-- \n--  Build full path to file given folder and file name \nlocal function get_full_path_file_name(folder_name, file_name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_354029_get_full_path_file_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_full_path_file_name\n    lu.assertEquals(candidate('my_folder', 'my_file.txt'), 'my_folder/my_file.txt')\n    lu.assertEquals(candidate('test_folder', 'test_file'), 'test_folder/test_file')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_354107__version2int", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if '-' in v:\n-- #     v = v.split(\"-\")[0]\n-- # if v.endswith(\".*\"):\n-- #     v = v.replace(\".*\", \".0\")  # X.X.* => X.X.0\n-- # v_list = v.split(\".\")\n-- # if len(v_list) < 3:\n-- #     v_list.append('0')\n-- # v_new = \"\"\n-- # for i, value in enumerate(v_list):\n-- #     if i != 0:\n-- #         if len(value) < 2:\n-- #             v_new += \"0\"+value\n-- #         else:\n-- #             v_new += value\n-- #     else:\n-- #         v_new += value\n-- # return int(v_new)\n-- \n--     X.X.X => X0X0X\nlocal function _version2int(v)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_354107__version2int.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _version2int\n    lu.assertEquals(candidate('1.0.2'), 10002)\n    lu.assertEquals(candidate('1.1.3'), 10103)\n    lu.assertEquals(candidate('0.0.*'), 0)\n    lu.assertEquals(candidate('1.2.3-dev'), 10203)\n    lu.assertEquals(candidate('1.0.12'), 10012)\n    lu.assertEquals(candidate('1.2.3'), 10203)\n    lu.assertEquals(candidate('0.0.0'), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_354564_enforce_key_consistency", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return str(key.replace(' ', '_').lower())\n-- \n--  Forces all keys to lowercase and replaces spaces with underscores \nlocal function enforce_key_consistency(key)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_354564_enforce_key_consistency.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = enforce_key_consistency\n    lu.assertEquals(candidate('snake case'), 'snake_case')\n    lu.assertEquals(candidate('Test'), 'test')\n    lu.assertEquals(candidate('UPPERCASE'), 'uppercase')\n    lu.assertEquals(candidate('FOO'), 'foo')\n    lu.assertEquals(candidate('snake_Case'), 'snake_case')\n    lu.assertEquals(candidate(' 100'), '_100')\n    lu.assertEquals(candidate('Test Test'), 'test_test')\n    lu.assertEquals(candidate('snake_case'), 'snake_case')\n    lu.assertEquals(candidate('lowercase'), 'lowercase')\n    lu.assertEquals(candidate('Foo'), 'foo')\n    lu.assertEquals(candidate('foo'), 'foo')\n    lu.assertEquals(candidate('UPPER SNAKE CASE'), 'upper_snake_case')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_35470_rgb_to_tk", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"#%02x%02x%02x\" % rgb\n-- \n-- Converts rgb values to tkinter color codes.\n-- :param rgb: Tuple of 3 ints.\n-- :return: tk color code string\nlocal function rgb_to_tk(rgb)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_35470_rgb_to_tk.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = rgb_to_tk\n    lu.assertEquals(candidate({0, 128, 255}), '#0080ff')\n    lu.assertEquals(candidate({255, 0, 0}), '#ff0000')\n    lu.assertEquals(candidate({255, 255, 255}), '#ffffff')\n    lu.assertEquals(candidate({0, 0, 0}), '#000000')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_355601_g_speed", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # default values\n-- # speed = actual_speed\n-- # # parse text\n-- # params = parameters.split(' ')\n-- # for param in params:\n-- #     coordinate = param[0]\n-- #     value = float(param[1:])\n-- #     if coordinate == 'f':\n-- #         speed = value\n-- # return (speed)\n-- \n-- returns a speed from a g-code\nlocal function g_speed(parameters, actual_speed)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_355601_g_speed.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = g_speed\n    lu.assertEquals(candidate('f1000 x0 y0 z0', 1000), 1000)\n    lu.assertEquals(candidate('x0 y0 z0', 1000), 1000)\n    lu.assertEquals(candidate('f500', 200), 500)\n    lu.assertEquals(candidate('f200', 500), 200)\n    lu.assertEquals(candidate('f1000', 1000), 1000)\n    lu.assertEquals(candidate('f2000 x0 y0 z0', 1000), 2000)\n    lu.assertEquals(candidate('f2000', 1000), 2000)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_356550_substitute_file_extension", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if filename[-5:] not in ['.vert', '.frag', '.tesc', '.tese',\n-- #                          '.geom', '.comp', '.spvasm']:\n-- #     return filename.rsplit('.', 1)[0] + '.' + extension\n-- # else:\n-- #     return filename + '.' + extension\n-- \n-- Substitutes file extension, respecting known shader extensions.\n-- foo.vert -> foo.vert.[extension] [similarly for .frag, .comp, etc.]\n-- foo.glsl -> foo.[extension]\n-- foo.unknown -> foo.[extension]\n-- foo -> foo.[extension]\nlocal function substitute_file_extension(filename, extension)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_356550_substitute_file_extension.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = substitute_file_extension\n    lu.assertEquals(candidate('foo', 'frag'), 'foo.frag')\n    lu.assertEquals(candidate('foo.vert.frag', 'frag'), 'foo.vert.frag.frag')\n    lu.assertEquals(candidate('foo.vert.frag.frag', 'frag'), 'foo.vert.frag.frag.frag')\n    lu.assertEquals(candidate('foo.vert', 'spvasm'), 'foo.vert.spvasm')\n    lu.assertEquals(candidate('foo.vert', 'frag'), 'foo.vert.frag')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_356733_strip_article_title_word", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return word.strip('\":;?!<>\\'').lower()\n-- \n-- Used when tokenizing the titles of articles\n-- in order to index them for search\nlocal function strip_article_title_word(word)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_356733_strip_article_title_word.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = strip_article_title_word\n    lu.assertEquals(candidate('\"\"'), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_356963_FunList", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # rtnList = \"\\n\"\n-- # for i in range(len(List)):\n-- #     rtnList += f\"{i+1} - {List[i]}\\n\"\n-- # return rtnList\n-- \n-- [Takes list and prints it vertically assigning each item its index number on CLI]\n-- Args:\n--     List ([string]): [The items to be displayed]\n-- Returns:\n--     [string]: [a string displaying entries vertically with its indexes]\nlocal function FunList(List)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_356963_FunList.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = FunList\n    lu.assertEquals(candidate({'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'}), '\\n1 - a\\n2 - b\\n3 - c\\n4 - d\\n5 - e\\n6 - f\\n7 - g\\n8 - h\\n')\n    lu.assertEquals(candidate({'a', 'b', 'c'}), '\\n1 - a\\n2 - b\\n3 - c\\n')\n    lu.assertEquals(candidate({'a', 'b', 'c', 'd', 'e', 'f', 'g'}), '\\n1 - a\\n2 - b\\n3 - c\\n4 - d\\n5 - e\\n6 - f\\n7 - g\\n')\n    lu.assertEquals(candidate({'a'}), '\\n1 - a\\n')\n    lu.assertEquals(candidate({}), '\\n')\n    lu.assertEquals(candidate({}), '\\n')\n    lu.assertEquals(candidate({'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i'}), '\\n1 - a\\n2 - b\\n3 - c\\n4 - d\\n5 - e\\n6 - f\\n7 - g\\n8 - h\\n9 - i\\n')\n    lu.assertEquals(candidate({}), '\\n')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_357580_ld_to_m", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return ld * 384402 * 10**3\n-- \n--     Converts the input distance (or velocity) of the input from Lunar distances to meters.\nlocal function ld_to_m(ld)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_357580_ld_to_m.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = ld_to_m\n    lu.assertEquals(candidate(-1), -384402000)\n    lu.assertEquals(candidate(0), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_357625_phase", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # 96 = 0b1100000, bits 6 to 7\n-- # return (flags & 96) >> 5\n-- \n-- Returns the layer thermodynamical phase, as identified from the\n-- feature classification flag\n-- 0 = unknown / not determined 1 = randomly oriented ice\n-- 2 = water\n-- 3 = horizontally oriented ice\nlocal function phase(flags)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_357625_phase.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = phase\n    lu.assertEquals(candidate(63), 1)\n    lu.assertEquals(candidate(18), 0)\n    lu.assertEquals(candidate(29), 0)\n    lu.assertEquals(candidate(4), 0)\n    lu.assertEquals(candidate(255), 3)\n    lu.assertEquals(candidate(11), 0)\n    lu.assertEquals(candidate(5), 0)\n    lu.assertEquals(candidate(17), 0)\n    lu.assertEquals(candidate(9), 0)\n    lu.assertEquals(candidate(22), 0)\n    lu.assertEquals(candidate(10), 0)\n    lu.assertEquals(candidate(21), 0)\n    lu.assertEquals(candidate(31), 0)\n    lu.assertEquals(candidate(6), 0)\n    lu.assertEquals(candidate(27), 0)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(23), 0)\n    lu.assertEquals(candidate(112), 3)\n    lu.assertEquals(candidate(48), 1)\n    lu.assertEquals(candidate(19), 0)\n    lu.assertEquals(candidate(26), 0)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(12), 0)\n    lu.assertEquals(candidate(208), 2)\n    lu.assertEquals(candidate(208), 2)\n    lu.assertEquals(candidate(8), 0)\n    lu.assertEquals(candidate(7), 0)\n    lu.assertEquals(candidate(28), 0)\n    lu.assertEquals(candidate(24), 0)\n    lu.assertEquals(candidate(20), 0)\n    lu.assertEquals(candidate(256), 0)\n    lu.assertEquals(candidate(64), 2)\n    lu.assertEquals(candidate(14), 0)\n    lu.assertEquals(candidate(1), 0)\n    lu.assertEquals(candidate(25), 0)\n    lu.assertEquals(candidate(2124414975), 3)\n    lu.assertEquals(candidate(16), 0)\n    lu.assertEquals(candidate(13), 0)\n    lu.assertEquals(candidate(30), 0)\n    lu.assertEquals(candidate(15), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_357773_new_mean_temperature", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # volume_air = area * height_external\n-- # specific_heat_capacity = 1005  # J/kgK\n-- # density_air = 1.29 * 273 / temperature_external  # kg/m^3\n-- # energy_air = volume_air * specific_heat_capacity * density_air * temperature_external  # J\n-- # return (energy_air + heat)/(volume_air * density_air * specific_heat_capacity)\n-- \n-- Calculates a new mean temperature for the volume.\n-- :param area: in m^2\n-- :param height_external: in m\n-- :param temperature_external: in K\n-- :param heat: in J\n-- :return: temperature in K\nlocal function new_mean_temperature(area, height_external, temperature_external, heat)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_357773_new_mean_temperature.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = new_mean_temperature\n    lu.assertEquals(candidate(1, 1, 273.15, 0), 273.15)\n    lu.assertEquals(candidate(1, 1, 274.15, 0), 274.15)\n    lu.assertEquals(candidate(1, 1, 373.15, 0), 373.15)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_358759_pep8_to_camel_case", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # chunks = name.split('_')\n-- # converted = [s[0].upper() + s[1:].lower() for s in chunks]\n-- # if initial:\n-- #     return ''.join(converted)\n-- # else:\n-- #     return chunks[0].lower() + ''.join(converted[1:])\n-- \n-- Convert a PEP8 style name to camel case.\nlocal function pep8_to_camel_case(name, initial)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_358759_pep8_to_camel_case.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = pep8_to_camel_case\n    lu.assertEquals(candidate('the_quick_brown_fox_jumps_over_the_lazy_dog', false), 'theQuickBrownFoxJumpsOverTheLazyDog')\n    lu.assertEquals(candidate('the_quick_brown_fox_jumps_over_the_lazy_dog', true), 'TheQuickBrownFoxJumpsOverTheLazyDog')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_35877__if_unmodified_since_passes", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return last_modified and last_modified <= if_unmodified_since\n-- \n-- Test the If-Unmodified-Since comparison as defined in section 3.4 of\n-- RFC 7232.\nlocal function _if_unmodified_since_passes(last_modified, if_unmodified_since)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_35877__if_unmodified_since_passes.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _if_unmodified_since_passes\n    lu.assertEquals(candidate(0, 1), false)\n    lu.assertEquals(candidate(1, 1), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_359341_rfact", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return rfact(n - 1) * n if n > 1 else 1\n-- \n-- Recursive\nlocal function rfact(n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_359341_rfact.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = rfact\n    lu.assertEquals(candidate(17), 355687428096000)\n    lu.assertEquals(candidate(10), 3628800)\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(19), 121645100408832000)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(16), 20922789888000)\n    lu.assertEquals(candidate(11), 39916800)\n    lu.assertEquals(candidate(14), 87178291200)\n    lu.assertEquals(candidate(12), 479001600)\n    lu.assertEquals(candidate(6), 720)\n    lu.assertEquals(candidate(7), 5040)\n    lu.assertEquals(candidate(5), 120)\n    lu.assertEquals(candidate(4), 24)\n    lu.assertEquals(candidate(18), 6402373705728000)\n    lu.assertEquals(candidate(3), 6)\n    lu.assertEquals(candidate(0), 1)\n    lu.assertEquals(candidate(9), 362880)\n    lu.assertEquals(candidate(15), 1307674368000)\n    lu.assertEquals(candidate(8), 40320)\n    lu.assertEquals(candidate(13), 6227020800)\n    lu.assertEquals(candidate(20), 2432902008176640000)\n    lu.assertEquals(candidate(21), 51090942171709440000)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_35954_trunc32", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # w = int((w & 0x7fffFFFF) | -(w & 0x80000000))\n-- # assert type(w) == int\n-- # return w\n-- \n--  Return the bottom 32 bits of w as a Python int.\n-- This creates longs temporarily, but returns an int. \nlocal function trunc32(w)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_35954_trunc32.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = trunc32\n    lu.assertEquals(candidate(candidate(19088743)), 19088743)\n    lu.assertEquals(candidate(4294967295), -1)\n    lu.assertEquals(candidate(18446744073709551615), -1)\n    lu.assertEquals(candidate(4294967297), 1)\n    lu.assertEquals(candidate(-1), -1)\n    lu.assertEquals(candidate(123456789), 123456789)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(2147483647), 2147483647)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(2147483648), -2147483648)\n    lu.assertEquals(candidate(4294967295), -1)\n    lu.assertEquals(candidate(4294967296), 0)\n    lu.assertEquals(candidate(-17), -17)\n    lu.assertEquals(candidate(candidate(0)), 0)\n    lu.assertEquals(candidate(19088743), 19088743)\n    lu.assertEquals(candidate(87112285931760246646623899502532662132736), 0)\n    lu.assertEquals(candidate(2147483647), 2147483647)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(candidate(2147483647)), 2147483647)\n    lu.assertEquals(candidate(147573952589676412928), 0)\n    lu.assertEquals(candidate(18446744073709551616), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_359972_add_to_whitelist", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # (fp, analyzed, valid_after) = fingerprint\n-- # fp.update(additions)\n-- # return (fp, analyzed, valid_after)\n-- \n--  Given a fingerprint, add to the whitelist \nlocal function add_to_whitelist(fingerprint, additions)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_359972_add_to_whitelist.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = add_to_whitelist\n    lu.assertEquals(candidate({{['1'] = '1', ['2'] = '2', ['3'] = '3'}, '2012-12-13T12:12:12Z', '2012-12-13T12:12:12Z'}, {['4'] = '4', ['5'] = '5', ['6'] = '6'}), {{['1'] = '1', ['2'] = '2', ['3'] = '3', ['4'] = '4', ['5'] = '5', ['6'] = '6'}, '2012-12-13T12:12:12Z', '2012-12-13T12:12:12Z'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_360740_subtractFrom", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"{} - {}\".format(acc, curr)\n-- \n-- Subtraction formatter\nlocal function subtractFrom(acc, curr)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_360740_subtractFrom.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = subtractFrom\n    lu.assertEquals(candidate(5, 6), '5 - 6')\n    lu.assertEquals(candidate(12, -5), '12 - -5')\n    lu.assertEquals(candidate(5, 3), '5 - 3')\n    lu.assertEquals(candidate(0, 0), '0 - 0')\n    lu.assertEquals(candidate(0, 5), '0 - 5')\n    lu.assertEquals(candidate(3, 2), '3 - 2')\n    lu.assertEquals(candidate(12, 5), '12 - 5')\n    lu.assertEquals(candidate(5, 0), '5 - 0')\n    lu.assertEquals(candidate(-5, -5), '-5 - -5')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_361436_get_actual_objname", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return full_object_name.split('&')[2]\n-- \n-- Given a object string full name (e.g 0&0&DEFINING_ORIGIN), returns\n-- its name (e.g DEFINING_ORIGIN).\nlocal function get_actual_objname(full_object_name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_361436_get_actual_objname.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_actual_objname\n    lu.assertEquals(candidate('0&0&DEFINING_ORIGIN&0'), 'DEFINING_ORIGIN')\n    lu.assertEquals(candidate('0&0&DEFINING_ORIGIN'), 'DEFINING_ORIGIN')\n    lu.assertEquals(candidate('0&0&DEFINING_ORIGIN'), 'DEFINING_ORIGIN')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_362047_make_divisible", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if min_value is None:\n-- #     min_value = divisor\n-- # new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n-- # # Make sure that round down does not go down by more than 10%.\n-- # if new_v < 0.9 * v:\n-- #     new_v += divisor\n-- # return new_v\n-- \n-- This function is taken from the original tf repo.\n-- It ensures that all layers have a channel number that is divisible by 8\n-- It can be seen here:\n-- https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n-- :param v:\n-- :param divisor:\n-- :param min_value:\n-- :return:\nlocal function make_divisible(v, divisor, min_value)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_362047_make_divisible.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = make_divisible\n    lu.assertEquals(candidate(12, 2), 12)\n    lu.assertEquals(candidate(3, 4), 4)\n    lu.assertEquals(candidate(64, 4), 64)\n    lu.assertEquals(candidate(13, 4), 12)\n    lu.assertEquals(candidate(17, 8), 16)\n    lu.assertEquals(candidate(5, 2), 6)\n    lu.assertEquals(candidate(18, 2), 18)\n    lu.assertEquals(candidate(12, 8), 16)\n    lu.assertEquals(candidate(13, 8), 16)\n    lu.assertEquals(candidate(24, 8), 24)\n    lu.assertEquals(candidate(3, 3), 3)\n    lu.assertEquals(candidate(256, 8), 256)\n    lu.assertEquals(candidate(8, 2), 8)\n    lu.assertEquals(candidate(33, 8), 32)\n    lu.assertEquals(candidate(256, 32), 256)\n    lu.assertEquals(candidate(7, 4), 8)\n    lu.assertEquals(candidate(23, 8), 24)\n    lu.assertEquals(candidate(20, 2), 20)\n    lu.assertEquals(candidate(22, 8), 24)\n    lu.assertEquals(candidate(100, 128), 128)\n    lu.assertEquals(candidate(21, 4), 20)\n    lu.assertEquals(candidate(15, 16), 16)\n    lu.assertEquals(candidate(32, 4), 32)\n    lu.assertEquals(candidate(17, 32), 32)\n    lu.assertEquals(candidate(512, 32), 512)\n    lu.assertEquals(candidate(25, 16), 32)\n    lu.assertEquals(candidate(17, 3), 18)\n    lu.assertEquals(candidate(10, 2), 10)\n    lu.assertEquals(candidate(32, 8), 32)\n    lu.assertEquals(candidate(16, 16), 16)\n    lu.assertEquals(candidate(100, 64), 128)\n    lu.assertEquals(candidate(6, 2), 6)\n    lu.assertEquals(candidate(14, 2), 14)\n    lu.assertEquals(candidate(14, 8), 16)\n    lu.assertEquals(candidate(14, 4), 16)\n    lu.assertEquals(candidate(5, 4), 8)\n    lu.assertEquals(candidate(128, 8), 128)\n    lu.assertEquals(candidate(4, 8), 8)\n    lu.assertEquals(candidate(7, 2), 8)\n    lu.assertEquals(candidate(256, 16), 256)\n    lu.assertEquals(candidate(25, 4), 24)\n    lu.assertEquals(candidate(4, 2), 4)\n    lu.assertEquals(candidate(25, 8), 24)\n    lu.assertEquals(candidate(21, 2), 22)\n    lu.assertEquals(candidate(16, 4), 16)\n    lu.assertEquals(candidate(16, 8), 16)\n    lu.assertEquals(candidate(5, 3), 6)\n    lu.assertEquals(candidate(40, 16), 48)\n    lu.assertEquals(candidate(12, 4), 12)\n    lu.assertEquals(candidate(4, 4), 4)\n    lu.assertEquals(candidate(8, 8), 8)\n    lu.assertEquals(candidate(21, 8), 24)\n    lu.assertEquals(candidate(8, 3), 9)\n    lu.assertEquals(candidate(15, 8), 16)\n    lu.assertEquals(candidate(15, 4), 16)\n    lu.assertEquals(candidate(6, 4), 8)\n    lu.assertEquals(candidate(15, 2), 16)\n    lu.assertEquals(candidate(3, 2), 4)\n    lu.assertEquals(candidate(24, 4), 24)\n    lu.assertEquals(candidate(128, 4), 128)\n    lu.assertEquals(candidate(11, 2), 12)\n    lu.assertEquals(candidate(23, 4), 24)\n    lu.assertEquals(candidate(8, 4), 8)\n    lu.assertEquals(candidate(512, 4), 512)\n    lu.assertEquals(candidate(5, 8), 8)\n    lu.assertEquals(candidate(100, 128, 256), 256)\n    lu.assertEquals(candidate(512, 16), 512)\n    lu.assertEquals(candidate(512, 8), 512)\n    lu.assertEquals(candidate(3, 8), 8)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_362264_compute_padding", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # N_pad = 2**J_pad\n-- # if N_pad < N:\n-- #     raise ValueError('Padding support should be larger than the original '\n-- #                      'signal size!')\n-- # to_add = 2**J_pad - N\n-- # pad_right = to_add // 2\n-- # pad_left = to_add - pad_right\n-- # return pad_left, pad_right\n-- \n-- Computes the padding to be added on the left and on the right\n-- of the signal.\n-- It should hold that 2**J_pad >= N\n-- Parameters\n-- ----------\n-- J_pad : int\n--     2**J_pad is the support of the padded signal\n-- N : int\n--     original signal support size\n-- Returns\n-- -------\n-- pad_left: amount to pad on the left (\"beginning\" of the support)\n-- pad_right: amount to pad on the right (\"end\" of the support)\n-- References\n-- ----------\n-- This is a modification of\n-- https://github.com/kymatio/kymatio/blob/master/kymatio/scattering1d/utils.py\n-- Kymatio, (C) 2018-present. The Kymatio developers.\nlocal function compute_padding(J_pad, N)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_362264_compute_padding.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = compute_padding\n    lu.assertEquals(candidate(1, 2), {0, 0})\n    lu.assertEquals(candidate(12, 2048), {1024, 1024})\n    lu.assertEquals(candidate(5, 16), {8, 8})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_362650_scale_100_to_10", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # value = int(value or 0)\n-- # if value == 0:\n-- #     return 0\n-- # else:\n-- #     return max(1, min(10, int(value) / 10))\n-- \n-- Convert a value from 0-100 range to 0-10 range.\n-- Args:\n--   value: an integer from 0 to 100.\n-- Returns:\n--   an integer from 0 to 10\nlocal function scale_100_to_10(value)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_362650_scale_100_to_10.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = scale_100_to_10\n    lu.assertEquals(candidate(), 10)\n    lu.assertEquals(candidate(None), 0)\n    lu.assertEquals(candidate(100), 10)\n    lu.assertEquals(candidate(10), 1)\n    lu.assertEquals(candidate(1000), 10)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(), 0)\n    lu.assertEquals(candidate(), 0)\n    lu.assertEquals(candidate(50), 5)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_36313_I_form", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # I = []\n-- # item = []\n-- # for i in range(n):\n-- #     for j in range(n):\n-- #         if i == j:\n-- #             item.append(1)\n-- #         else:\n-- #             item.append(0)\n-- #     I.append(item)\n-- #     item = []\n-- # return I\n-- \n--     returns Identity matrix of order n\nlocal function I_form(n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_36313_I_form.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = I_form\n    lu.assertEquals(candidate(0), {})\n    lu.assertEquals(candidate(4), {{1, 0, 0, 0}, {0, 1, 0, 0}, {0, 0, 1, 0}, {0, 0, 0, 1}})\n    lu.assertEquals(candidate(2), {{1, 0}, {0, 1}})\n    lu.assertEquals(candidate(3), {{1, 0, 0}, {0, 1, 0}, {0, 0, 1}})\n    lu.assertEquals(candidate(3), {{1, 0, 0}, {0, 1, 0}, {0, 0, 1}})\n    lu.assertEquals(candidate(1), {{1}})\n    lu.assertEquals(candidate(5), {{1, 0, 0, 0, 0}, {0, 1, 0, 0, 0}, {0, 0, 1, 0, 0}, {0, 0, 0, 1, 0}, {0, 0, 0, 0, 1}})\n    lu.assertEquals(candidate(2), {{1, 0}, {0, 1}})\n    lu.assertEquals(candidate(2), {{1, 0}, {0, 1}})\n    lu.assertEquals(candidate(2), {{1, 0}, {0, 1}})\n    lu.assertEquals(candidate(3), {{1, 0, 0}, {0, 1, 0}, {0, 0, 1}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_363184_process", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # container_ = sorted(container_)\n-- # left = 0\n-- # right = len(container_) - 1\n-- # iterations = 0\n-- # while left <= right:\n-- #     mid = (left + right) // 2\n-- #     if search_element_ == container_[mid]:\n-- #         return mid\n-- #     elif container_[mid] < search_element_:\n-- #         left = mid + 1\n-- #     else:\n-- #         right = mid - 1\n-- #     iterations += 1\n-- # return -1\n-- \n-- Search the container for the element.\n-- :param container_: The list of elements\n-- :param search_element_: The element to search\n-- :return: return position of the element.\nlocal function process(container_, search_element_)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_363184_process.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = process\n    lu.assertEquals(candidate({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, 8), 8)\n    lu.assertEquals(candidate({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, 20), -1)\n    lu.assertEquals(candidate({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, 3), 3)\n    lu.assertEquals(candidate({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, 6), 6)\n    lu.assertEquals(candidate({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, 9), 9)\n    lu.assertEquals(candidate({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, 0), 0)\n    lu.assertEquals(candidate({0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, 4), 4)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_363888_escape_string", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return value.replace('\"', '\\\\\"')\n-- \n--     Escape a string for command line use.\nlocal function escape_string(value)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_363888_escape_string.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = escape_string\n    lu.assertEquals(candidate('foo\\\\nbar'), 'foo\\\\nbar')\n    lu.assertEquals(candidate('\"'), '\\\\\"')\n    lu.assertEquals(candidate('abc'), 'abc')\n    lu.assertEquals(candidate('foo'), 'foo')\n    lu.assertEquals(candidate('foo'), 'foo')\n    lu.assertEquals(candidate('foo'), 'foo')\n    lu.assertEquals(candidate('a'), 'a')\n    lu.assertEquals(candidate(\"{'a': 'b'}\"), \"{'a': 'b'}\")\n    lu.assertEquals(candidate('abc'), 'abc')\n    lu.assertEquals(candidate('abc'), 'abc')\n    lu.assertEquals(candidate('x'), 'x')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_364333__ConvertBoxToCOCOFormat", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return [\n-- #     float(box[1]),\n-- #     float(box[0]),\n-- #     float(box[3] - box[1]),\n-- #     float(box[2] - box[0])\n-- # ]\n-- \n-- Converts a box in [ymin, xmin, ymax, xmax] format to COCO format.\n-- This is a utility function for converting from our internal\n-- [ymin, xmin, ymax, xmax] convention to the convention used by the COCO API\n-- i.e., [xmin, ymin, width, height].\n-- Args:\n--   box: a [ymin, xmin, ymax, xmax] numpy array\n-- Returns:\n--   a list of floats representing [xmin, ymin, width, height]\nlocal function _ConvertBoxToCOCOFormat(box)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_364333__ConvertBoxToCOCOFormat.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _ConvertBoxToCOCOFormat\n    lu.assertEquals(candidate({0, 0, 100, 100}), {0.0, 0.0, 100.0, 100.0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_364399_allowed_file_models", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return '.' in filename and \\\n-- #        filename.rsplit('.', 1)[1].lower() in ['bpmn']\n-- \n-- Check whether the type is allowed for a bpmn model file.\n-- :param filename: name of the file\n-- :return: True or False\nlocal function allowed_file_models(filename)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_364399_allowed_file_models.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = allowed_file_models\n    lu.assertEquals(candidate('my_filename.xml'), false)\n    lu.assertEquals(candidate('my_filename.jpeg'), false)\n    lu.assertEquals(candidate('my_filename.yaml'), false)\n    lu.assertEquals(candidate('my_filename.txt'), false)\n    lu.assertEquals(candidate('my_filename.png'), false)\n    lu.assertEquals(candidate('my_filename.bpmn'), true)\n    lu.assertEquals(candidate('my_filename.bpn'), false)\n    lu.assertEquals(candidate('my_filename.jpg'), false)\n    lu.assertEquals(candidate('my_filename.bpnm'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_364462_maybe_scream", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if do_scream:\n-- #     text = text.upper()\n-- # return text\n-- \n-- Returns given text input as caps lock text, if do_scream is true.\n-- Args:\n--     text (str): Some input text\n--     do_scream (bool): Decide, whether to scream or not\n-- Returns:\n--     str: May be in caps lock\nlocal function maybe_scream(text, do_scream)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_364462_maybe_scream.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = maybe_scream\n    lu.assertEquals(candidate('hello', true), 'HELLO')\n    lu.assertEquals(candidate('Some text', true), 'SOME TEXT')\n    lu.assertEquals(candidate('hello', false), 'hello')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_3645_wizard_active", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if current == step:\n-- #     return 'selected'\n-- # elif (current + 1) == step:\n-- #     return 'next-selected'\n-- \n-- Return the proper classname for the step div in the badge wizard.\n-- The current step needs a 'selected' class while the following step needs a\n-- 'next-selected' class to color the tip of the arrow properly.\nlocal function wizard_active(step, current)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_3645_wizard_active.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = wizard_active\n    lu.assertEquals(candidate(4, 4), 'selected')\n    lu.assertEquals(candidate(3, 2), 'next-selected')\n    lu.assertEquals(candidate(3, 3), 'selected')\n    lu.assertEquals(candidate(2, 1), 'next-selected')\n    lu.assertEquals(candidate(5, 4), 'next-selected')\n    lu.assertEquals(candidate(2, 2), 'selected')\n    lu.assertEquals(candidate(1, 1), 'selected')\n    lu.assertEquals(candidate(4, 3), 'next-selected')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_364777_intervalsIntersect", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Intervals do not interset if [a1, b1) is wholly to the left\n-- # # or right of [a2, b2).\n-- # return not (b1 <= a2 or a1 >= b2)\n-- \n-- Returns True if the specified half-closed intervals [a1, b1)\n-- and [a2, b2) intersect.\nlocal function intervalsIntersect(a1, b1, a2, b2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_364777_intervalsIntersect.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = intervalsIntersect\n    lu.assertEquals(candidate(1, 2, 2, 3), false)\n    lu.assertEquals(candidate(0, 10, 11, 20), false)\n    lu.assertEquals(candidate(1, 3, 1, 3), true)\n    lu.assertEquals(candidate(0, 10, -1, 0), false)\n    lu.assertEquals(candidate(1, 3, 1, 2), true)\n    lu.assertEquals(candidate(2, 3, 2, 2), false)\n    lu.assertEquals(candidate(1, 3, 2, 4), true)\n    lu.assertEquals(candidate(0, 10, 5, 20), true)\n    lu.assertEquals(candidate(1, 2, 1, 3), true)\n    lu.assertEquals(candidate(1, 2, 1, 2), true)\n    lu.assertEquals(candidate(0, 10, 10, 100), false)\n    lu.assertEquals(candidate(0, 10, 10, 20), false)\n    lu.assertEquals(candidate(1, 3, 2, 3), true)\n    lu.assertEquals(candidate(2, 3, 1, 2), false)\n    lu.assertEquals(candidate(3, 4, 2, 3), false)\n    lu.assertEquals(candidate(1, 2, 2, 2), false)\n    lu.assertEquals(candidate(1, 1, 1, 2), false)\n    lu.assertEquals(candidate(1, 2, 3, 4), false)\n    lu.assertEquals(candidate(0, 10, 0, 10), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_364905_pick_username", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # max_index = 0\n-- # max_upper = 0\n-- # current_upper = 0\n-- # for i in range(0, len(names)):\n-- #     current_upper = 0\n-- #     for j in range(0, len(names[i])):\n-- #         if names[i][j].isupper():\n-- #             current_upper += 1\n-- #     if current_upper >= max_upper:\n-- #         max_index = i\n-- #         max_upper = current_upper\n-- # return names[max_index]\n-- \n-- >>> pick_username([\"JonaThan TanoTo\", \"WeiYue Li\"])\n-- 'JonaThan TanoTo'\n-- >>> pick_username([\"JonaThan TanoTo\", \"WeiYue Li\", \"ShuBham KauShal\"])\n-- 'ShuBham KauShal'\n-- >>> pick_username([\"JonaThan TanoTo\", \"WeiYue Li\", \"ShuBham KauShal\", \"MARINA\"])\n-- 'MARINA'\nlocal function pick_username(names)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_364905_pick_username.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = pick_username\n    lu.assertEquals(candidate({'<NAME>', '<NAME>', '<NAME>', 'MARINA'}), 'MARINA')\n    lu.assertEquals(candidate({'JonaThan TanoTo', 'WeiYue Li'}), 'JonaThan TanoTo')\n    lu.assertEquals(candidate({'JonaThan TanoTo', 'WeiYue Li', 'ShuBham KauShal', 'MARINA'}), 'MARINA')\n    lu.assertEquals(candidate({'<NAME>', '<NAME>', '<NAME>'}), '<NAME>')\n    lu.assertEquals(candidate({'JonaThan TanoTo', 'WeiYue Li', 'ShuBham KauShal'}), 'ShuBham KauShal')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_365886_sanitize_phone_field", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # chars = [\"(\", \")\", \"-\", \".\"]\n-- # return \"\".join(ch for ch in s if ch not in chars)\n-- \n-- Phone Number Format\n-- AAAEEENNNNXXXX, where\n-- AAA = Area Code\n-- EEE = Exchange\n-- NNNN = Number\n-- XXXX = Extension\nlocal function sanitize_phone_field(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_365886_sanitize_phone_field.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sanitize_phone_field\n    lu.assertEquals(candidate('5551212'), '5551212')\n    lu.assertEquals(candidate('800-555-1212'), '8005551212')\n    lu.assertEquals(candidate('(800)555-3211'), '8005553211')\n    lu.assertEquals(candidate('800-555-3535'), '8005553535')\n    lu.assertEquals(candidate('912-456-7890'), '9124567890')\n    lu.assertEquals(candidate('912.456.7890'), '9124567890')\n    lu.assertEquals(candidate('800-555-3211'), '8005553211')\n    lu.assertEquals(candidate('(123)456-7890'), '1234567890')\n    lu.assertEquals(candidate('123.456.7890'), '1234567890')\n    lu.assertEquals(candidate('8005551212'), '8005551212')\n    lu.assertEquals(candidate('800(555)3535'), '8005553535')\n    lu.assertEquals(candidate('555-1212'), '5551212')\n    lu.assertEquals(candidate('(800)555-3535'), '8005553535')\n    lu.assertEquals(candidate('123-456-7890'), '1234567890')\n    lu.assertEquals(candidate('546-234-8900'), '5462348900')\n    lu.assertEquals(candidate('800.555.3535'), '8005553535')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_366538_find_stem", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Determine size of the array\n-- # n = len(arr)\n-- # # Take first word from array\n-- # # as reference\n-- # s = arr[0]\n-- # ll = len(s)\n-- # res = \"\"\n-- # for i in range(ll):\n-- #     for j in range(i + 1, ll + 1):\n-- #         # generating all possible substrings of our ref string arr[0] i.e s\n-- #         stem = s[i:j]\n-- #         k = 1\n-- #         for k in range(1, n):\n-- #             # Check if the generated stem is common to to all words\n-- #             if stem not in arr[k]:\n-- #                 break\n-- #         # If current substring is present in all strings and its length is\n-- #         # greater than current result\n-- #         if k + 1 == n and len(res) < len(stem):\n-- #             res = stem\n-- # return res\n-- \n-- Find longest common substring in array of strings.\n-- From https://www.geeksforgeeks.org/longest-common-substring-array-strings/\nlocal function find_stem(arr)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_366538_find_stem.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = find_stem\n    lu.assertEquals(candidate({'Python', 'is', 'the', 'best', 'language'}), '')\n    lu.assertEquals(candidate({'Geeks', 'Geeks'}), 'Geeks')\n    lu.assertEquals(candidate({'Python', 'Python', 'Python'}), 'Python')\n    lu.assertEquals(candidate({'geek', 'gee', 'gee', 'geeksforgeeks'}), 'gee')\n    lu.assertEquals(candidate({'geeks', 'geeksforgeeks', 'geek'}), 'geeks')\n    lu.assertEquals(candidate({'geeksforgeeks', 'geeks', 'geek'}), 'geeks')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_366835_convertosides", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # sides = []\n-- # for i in range(len(polypointsx)-1):\n-- #     sides.append([polypointsx[i], polypointsx[i+1], polypointsy[i], polypointsy[i+1]])\n-- # sides.append([polypointsx[len(polypointsx)-1], polypointsx[0], polypointsy[len(polypointsx)-1], polypointsy[0]])\n-- # return sides\n-- \n-- Takes polypoints describing the corners of a polygon and returns list of quadruples describing sides of polygon \n-- Input:\n-- polypointsx (list of float): corners of the polygon in a consecutive order, any format, x\n-- polypointsy (list of float): corners of the polygon in a consecutive order, any format, y\n-- Output will be [[x1,x2,y1,y2], ...]\nlocal function convertosides(polypointsx, polypointsy)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_366835_convertosides.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = convertosides\n    lu.assertEquals(candidate({1, 2, 3, 4}, {5, 6, 7, 8, 9}), {{1, 2, 5, 6}, {2, 3, 6, 7}, {3, 4, 7, 8}, {4, 1, 8, 5}})\n    lu.assertEquals(candidate({1, 2, 3, 4, 5, 6}, {7, 8, 9, 10, 11, 12}), {{1, 2, 7, 8}, {2, 3, 8, 9}, {3, 4, 9, 10}, {4, 5, 10, 11}, {5, 6, 11, 12}, {6, 1, 12, 7}})\n    lu.assertEquals(candidate({1, 2, 3, 4, 5}, {6, 7, 8, 9, 10}), {{1, 2, 6, 7}, {2, 3, 7, 8}, {3, 4, 8, 9}, {4, 5, 9, 10}, {5, 1, 10, 6}})\n    lu.assertEquals(candidate({1, 2, 3, 4}, {5, 6, 7, 8}), {{1, 2, 5, 6}, {2, 3, 6, 7}, {3, 4, 7, 8}, {4, 1, 8, 5}})\n    lu.assertEquals(candidate({1, 2, 3, 4, 5, 6}, {7, 8, 9, 10, 11, 12}), {{1, 2, 7, 8}, {2, 3, 8, 9}, {3, 4, 9, 10}, {4, 5, 10, 11}, {5, 6, 11, 12}, {6, 1, 12, 7}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_366856_normalize_format", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if fmt == u\"md\":\n-- #     fmt = \"markdown\"\n-- # elif fmt == u\"mm\":\n-- #     fmt = \"moinmoin\"\n-- # return fmt\n-- \n-- Turns common shortenings into full format names.\nlocal function normalize_format(fmt)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_366856_normalize_format.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = normalize_format\n    lu.assertEquals(candidate('html'), 'html')\n    lu.assertEquals(candidate('html'), 'html')\n    lu.assertEquals(candidate('md'), 'markdown')\n    lu.assertEquals(candidate('markdown'), 'markdown')\n    lu.assertEquals(candidate('md'), 'markdown')\n    lu.assertEquals(candidate('mm'), 'moinmoin')\n    lu.assertEquals(candidate('rst'), 'rst')\n    lu.assertEquals(candidate('markdown'), 'markdown')\n    lu.assertEquals(candidate('mm'), 'moinmoin')\n    lu.assertEquals(candidate('txt'), 'txt')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_368353_GetComplimentaryHex", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # strip the # from the beginning\n-- # color = color[1:]\n-- # # convert the string into hex\n-- # color = int(color, 16)\n-- # # invert the three bytes\n-- # # as good as substracting each of RGB component by 255(FF)\n-- # comp_color = 0xFFFFFF ^ color\n-- # # convert the color back to hex by prefixing a #\n-- # comp_color = \"#%06X\" % comp_color\n-- # return comp_color\n-- \n--     :param color:\nlocal function GetComplimentaryHex(color)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_368353_GetComplimentaryHex.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = GetComplimentaryHex\n    lu.assertEquals(candidate('#0000000'), '#FFFFFF')\n    lu.assertEquals(candidate('#00000000000000'), '#FFFFFF')\n    lu.assertEquals(candidate('#000000000'), '#FFFFFF')\n    lu.assertEquals(candidate('#0000000000000'), '#FFFFFF')\n    lu.assertEquals(candidate('#FFFFFF'), '#000000')\n    lu.assertEquals(candidate('#00000000'), '#FFFFFF')\n    lu.assertEquals(candidate('#000000000000000'), '#FFFFFF')\n    lu.assertEquals(candidate('#00000000000'), '#FFFFFF')\n    lu.assertEquals(candidate('#0000000000'), '#FFFFFF')\n    lu.assertEquals(candidate('#000000000000'), '#FFFFFF')\n    lu.assertEquals(candidate('#000000'), '#FFFFFF')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_369037_levenshtein_distance", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if len(s1) > len(s2):\n-- #     s1, s2 = s2, s1\n-- # distances = range(len(s1) + 1)\n-- # for index2, char2 in enumerate(s2):\n-- #     new_distances = [index2+1]\n-- #     for index1, char1 in enumerate(s1):\n-- #         if char1 == char2:\n-- #             new_distances.append(distances[index1])\n-- #         else:\n-- #             new_distances.append(\n-- #                 1 + min(\n-- #                     (\n-- #                         distances[index1],\n-- #                         distances[index1+1],\n-- #                         new_distances[-1]\n-- #                     )\n-- #                 )\n-- #             )\n-- #     distances = new_distances\n-- # distance = distances[-1]\n-- # return distance\n-- \n-- The minimum amount of edits needed to make s2 into s1.\n-- Args:\n--     s1: string\n--     s2: string\n-- Returns:\n--     int\nlocal function levenshtein_distance(s1, s2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_369037_levenshtein_distance.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = levenshtein_distance\n    lu.assertEquals(candidate('kitten', 'sittingtt'), 5)\n    lu.assertEquals(candidate('abc', 'def'), 3)\n    lu.assertEquals(candidate('123456789', '123456789'), 0)\n    lu.assertEquals(candidate('abcd', 'abcd'), 0)\n    lu.assertEquals(candidate('kitten', 'sitting'), 3)\n    lu.assertEquals(candidate('abc', ''), 3)\n    lu.assertEquals(candidate('kitten', 'sitting'), 3)\n    lu.assertEquals(candidate('abcd', 'abc'), 1)\n    lu.assertEquals(candidate('abc', 'abcd'), 1)\n    lu.assertEquals(candidate('', 'abc'), 3)\n    lu.assertEquals(candidate('abc', 'abc'), 0)\n    lu.assertEquals(candidate('kitten', 'sitttnn'), 3)\n    lu.assertEquals(candidate('kitten', 'sitttn'), 2)\n    lu.assertEquals(candidate('', 'kitten'), 6)\n    lu.assertEquals(candidate('kitten', 'mittens'), 2)\n    lu.assertEquals(candidate('kitten', 'kitten'), 0)\n    lu.assertEquals(candidate('kitten', 'sittttttttnnn'), 9)\n    lu.assertEquals(candidate('one two three four', 'one two three five'), 3)\n    lu.assertEquals(candidate('hello', 'hello'), 0)\n    lu.assertEquals(candidate('kitten', 'kitten'), 0)\n    lu.assertEquals(candidate('kitten', 'sitting'), 3)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_37035_remove_cations", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Assertions\n-- # assert isinstance(SMILES, str), 'the SMILES must be a string'\n-- # # Functionality\n-- # split_SMILES = SMILES.split(\".\")\n-- # ion_list = ['[Li+]', '[Na+]', '[K+]', '[Rb+]', '[Cs+]', '[Fr+]', '[F-]',\n-- #             '[Cl-]', '[Br-]', '[I-]', '[At-]']\n-- # SMILES = [i for i in split_SMILES if i not in ion_list]\n-- # SMILES = '.'.join(SMILES)\n-- # return SMILES\n-- \n--     Removes periodic table group 1 and 7 counterions from the SMILES\n-- strings.\n--     Args:\n--     -----\n-- SMILES (str) -- the SMILES string representation of the\n--     molecule.\n--     Returns:\n--     --------\n-- SMILES (str) -- the string representation of the molecule with\n--     the counterions omitted.\nlocal function remove_cations(SMILES)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_37035_remove_cations.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = remove_cations\n    lu.assertEquals(candidate('N1CC(N=c2c(C(C)C)n(C(C)(C)C)c(=O)c2C)CC1'), 'N1CC(N=c2c(C(C)C)n(C(C)(C)C)c(=O)c2C)CC1')\n    lu.assertEquals(candidate('NC1=CC(=O)C(C(C)(C)C)=C1C(N)=O'), 'NC1=CC(=O)C(C(C)(C)C)=C1C(N)=O')\n    lu.assertEquals(candidate('CC(=O)N1CCC(C(=O)c2c(C)n(C(C)(C)C)c(=O)c2C(N)=O)CC1'), 'CC(=O)N1CCC(C(=O)c2c(C)n(C(C)(C)C)c(=O)c2C(N)=O)CC1')\n    lu.assertEquals(candidate('N[N+]1(CCCC1)C'), 'N[N+]1(CCCC1)C')\n    lu.assertEquals(candidate('CN1CCC(C(=O)c2c(C)n(C(C)(C)C)c(=O)c2C(N)=O)CC1'), 'CN1CCC(C(=O)c2c(C)n(C(C)(C)C)c(=O)c2C(N)=O)CC1')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_370597_inline_code", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return f\"`{code}`\"\n-- \n-- Covert code to inline code\n-- Args:\n--     code (str) : code to be converted to inline code\n-- Returns:\n--     str: inline code\nlocal function inline_code(code)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_370597_inline_code.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = inline_code\n    lu.assertEquals(candidate(\"print('Hello, World!')\"), \"`print('Hello, World!')`\")\n    lu.assertEquals(candidate(\"print('Hello, World!')\"), \"`print('Hello, World!')`\")\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_370651_set_paths_chemkin_files", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # thermo_path = my_path + '/data/thermo.dat'\n-- # smile_path = my_path + '/data/species_smiles.dat'\n-- # reactionlist_path = my_path + '/data/reaction.dat'\n-- # return thermo_path, smile_path, reactionlist_path\n-- \n-- Set the absolute path to required files on the current machine.\n-- *** only required if using chemkin files****\n-- Parameters\n-- -------\n-- my_path                 : str\n--                         path where all the imput files are located\n-- Returns\n-- -------\n-- thermo_path            : str\n--                         path to the chemkin thermo file\n-- smile_path             : str\n--                         path to the file `species_smiles.dat`\n-- reactionlist_path      : str\n--                         path to the chemkin reaction file\nlocal function set_paths_chemkin_files(my_path)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_370651_set_paths_chemkin_files.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = set_paths_chemkin_files\n    lu.assertEquals(candidate('path/to/data/directory'), {'path/to/data/directory/data/thermo.dat', 'path/to/data/directory/data/species_smiles.dat', 'path/to/data/directory/data/reaction.dat'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_37129_compare_log_to_resp", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # response_line_no = 0\n-- # response_line = resp[response_line_no].rstrip()\n-- # for log_line in log:\n-- #     log_line = log_line.rstrip()\n-- #     if response_line in log_line:\n-- #         # Found a match, step to the next non-blank line in the response\n-- #         # list\n-- #         while True:\n-- #             response_line_no += 1\n-- #             if response_line_no >= len(resp):\n-- #                 # We ran through all of our respones lines, success!\n-- #                 return None\n-- #             else:\n-- #                 response_line = resp[response_line_no].rstrip()\n-- #                 if len(response_line) > 0:\n-- #                     break\n-- # # print(\"Log missing '{0:s}'\".format(response_line))\n-- # return response_line\n-- \n--  Search the log list for the responses in the response list\n-- Search through the log list for the lines in the response list. The\n-- response list may contain substrings found in the log list lines. The\n-- response list lines must be found in the log list in the order they\n-- are specified in the response list (the log list may have extra lines\n-- which are ignored).\n-- Returns None if all the strings in the response list were found in the\n-- log list. Otherwise, returns the first missing response line.\nlocal function compare_log_to_resp(log, resp)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_37129_compare_log_to_resp.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = compare_log_to_resp\n    lu.assertEquals(candidate({'The quick brown fox jumps over the lazy dog'}, {'The quick brown fox jumps over the lazy dog', ' ', ' '}), None)\n    lu.assertEquals(candidate({'The quick brown fox jumps over the lazy dog'}, {'The quick brown fox jumps over the lazy dog'}), None)\n    lu.assertEquals(candidate({'The quick brown fox jumps over the lazy dog'}, {'The quick brown fox jumps over the lazy dog', ' ', ' ', ' ', ' '}), None)\n    lu.assertEquals(candidate({'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'}, {'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'}), None)\n    lu.assertEquals(candidate({'The quick brown fox jumps over the lazy dog'}, {'The quick brown fox jumps over the lazy dog', ' ', ' ', ' '}), None)\n    lu.assertEquals(candidate({'This is a test', 'This is another test'}, {'This is a test', 'This is another test'}), None)\n    lu.assertEquals(candidate({'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog'}, {'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog'}), None)\n    lu.assertEquals(candidate({'This is a test', 'This is another test'}, {'This is a test', 'This is yet another test'}), 'This is yet another test')\n    lu.assertEquals(candidate({'The quick brown fox jumps over the lazy dog'}, {'The quick brown fox jumps over the lazy dog', ''}), None)\n    lu.assertEquals(candidate({'The quick brown fox jumps over the lazy dog', 'The quick brown fox jumps over the lazy dog'}, {'The quick brown fox jumps over the lazy dog'}), None)\n    lu.assertEquals(candidate({'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'}, {'A', 'B', 'C', 'D', 'E', 'G', 'F'}), 'F')\n    lu.assertEquals(candidate({'This is a test', 'This is another test'}, {'This is a test', 'This is yet another test', ''}), 'This is yet another test')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_371762_query_delete_table", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return 'DROP TABLE IF EXISTS ' + table\n-- \n-- Generate table delete query for table with name 'table'\nlocal function query_delete_table(table)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_371762_query_delete_table.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = query_delete_table\n    lu.assertEquals(candidate('books'), 'DROP TABLE IF EXISTS books')\n    lu.assertEquals(candidate(), 'DROP TABLE IF EXISTS students')\n    lu.assertEquals(candidate('students'), 'DROP TABLE IF EXISTS students')\n    lu.assertEquals(candidate('my_table'), 'DROP TABLE IF EXISTS my_table')\n    lu.assertEquals(candidate('Users'), 'DROP TABLE IF EXISTS Users')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_372605_boolean_to_xml", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if obj:\n-- #     return \"true\"\n-- # else:\n-- #     return \"false\"\n-- \n-- serialize a boolean to XML\n-- :param obj: boolean\n-- :return: string in the XML accepted form\nlocal function boolean_to_xml(obj)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_372605_boolean_to_xml.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = boolean_to_xml\n    lu.assertEquals(candidate(true), 'true')\n    lu.assertEquals(candidate(false), 'false')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_372626_find_integer", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not array:\n-- #     return False\n-- # for i in array:\n-- #     if i[0] <= target <= i[len(i)-1]:\n-- #         if target in i:\n-- #             return True\n-- #     else:\n-- #         if i[0] > target:\n-- #             break\n-- # return False\n-- \n-- :params array: [[]]\n-- :params target: int\n-- :return bool\nlocal function find_integer(array, target)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_372626_find_integer.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = find_integer\n    lu.assertEquals(candidate({{1, 4}, {2, 3}, {3, 4}}, 5), false)\n    lu.assertEquals(candidate({{1, 4}, {2, 3}, {3, 4}}, 7), false)\n    lu.assertEquals(candidate({{1, 3, 5, 7, 9}, {10, 11, 16, 20}, {23, 30, 34, 50}}, 24), false)\n    lu.assertEquals(candidate({{1, 3, 5}, {7, 9, 11}, {20, 21}}, 21), true)\n    lu.assertEquals(candidate({{1, 2, 8, 9}, {2, 4, 9, 12}, {4, 7, 10, 13}, {6, 8, 11, 15}}, 7), true)\n    lu.assertEquals(candidate({{1, 4, 5, 8, 9}, {10, 11, 16, 20}, {23, 30, 34, 50}}, 15), false)\n    lu.assertEquals(candidate({{1, 3, 8, 9}, {2, 4, 9, 12}, {4, 7, 10, 13}, {6, 8, 11, 15}}, 12), true)\n    lu.assertEquals(candidate({}, 1), false)\n    lu.assertEquals(candidate({{1, 3, 5, 7, 9}, {10, 11, 16, 20}, {23, 30, 34, 50}}, 20), true)\n    lu.assertEquals(candidate({{1, 3, 8, 9}, {2, 4, 9, 12}, {4, 7, 10, 13}, {6, 8, 11, 15}}, 13), true)\n    lu.assertEquals(candidate({}, 100), false)\n    lu.assertEquals(candidate({{1, 3, 5}, {7, 9, 11}}, 15), false)\n    lu.assertEquals(candidate({{1, 3, 5, 7, 9}, {10, 11, 16, 20}, {23, 30, 34, 50}}, 10), true)\n    lu.assertEquals(candidate({{1, 3, 5, 7, 9}, {10, 11, 16, 20}, {23, 30, 34, 50}}, 3), true)\n    lu.assertEquals(candidate({{1, 4}, {2, 3}, {3, 4}}, 0), false)\n    lu.assertEquals(candidate({{1, 3, 8, 9}, {2, 4, 9, 12}, {4, 7, 10, 13}, {6, 8, 11, 15}}, 200), false)\n    lu.assertEquals(candidate({{1, 3, 5, 7, 9}, {10, 11, 16, 20}, {23, 30, 34, 50}}, 3), true)\n    lu.assertEquals(candidate({{1, 4}, {2, 3}, {3, 4}}, 4), true)\n    lu.assertEquals(candidate({{1, 4, 5, 8, 9}, {10, 11, 16, 20}, {23, 30, 34, 50}}, 45), false)\n    lu.assertEquals(candidate({{1, 3, 5, 7, 9}, {10, 11, 16, 20}, {23, 30, 34, 50}}, 20), true)\n    lu.assertEquals(candidate({{1, 4}, {2, 3}, {3, 4}}, 3), true)\n    lu.assertEquals(candidate({{1, 3, 5}, {7, 9, 11}}, 3), true)\n    lu.assertEquals(candidate({}, 10), false)\n    lu.assertEquals(candidate({{1, 3, 5, 7, 9}, {10, 11, 16, 20}, {23, 30, 34, 50}}, 4), false)\n    lu.assertEquals(candidate({{1, 3, 5, 7, 9}, {10, 11, 16, 20}, {23, 30, 34, 50}}, 23), true)\n    lu.assertEquals(candidate({{1, 3, 5}, {7, 9, 11}, {20, 21}}, 22), false)\n    lu.assertEquals(candidate({{1, 3, 8, 9}, {2, 4, 9, 12}, {4, 7, 10, 13}, {6, 8, 11, 15}}, 100), false)\n    lu.assertEquals(candidate({{1, 3, 8, 9}, {2, 4, 9, 12}, {4, 7, 10, 13}, {6, 8, 11, 15}}, 2), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_373273_merge_usage_periods", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # outlist = []\n-- # for period in periods:\n-- #     if new_period[0] > period[1]:\n-- #         # No overlap - past the end\n-- #         outlist.append(period)\n-- #         continue\n-- #     if new_period[1] < period[0]:\n-- #         # No overlap - before the beginning\n-- #         outlist.append(period)\n-- #         continue\n-- #     # There must now be some overlap\n-- #     merged = True\n-- #     if new_period[0] < period[0]:\n-- #         period[0] = new_period[0]\n-- #     if new_period[1] > period[1]:\n-- #         period[1] = new_period[1]\n-- #     new_period = period\n-- # outlist.append(new_period)\n-- # return outlist\n-- \n-- Merge a time period into an existing set of usage periods\nlocal function merge_usage_periods(periods, new_period)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_373273_merge_usage_periods.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = merge_usage_periods\n    lu.assertEquals(candidate({{1, 2}, {3, 4}}, {2, 5}), {{1, 5}})\n    lu.assertEquals(candidate({{10, 20}, {30, 40}, {50, 60}}, {1, 100}), {{1, 100}})\n    lu.assertEquals(candidate({{1, 2}, {3, 4}}, {1, 3}), {{1, 4}})\n    lu.assertEquals(candidate({{1, 2}, {3, 4}}, {1, 5}), {{1, 5}})\n    lu.assertEquals(candidate({{1, 2}}, {3, 4}), {{1, 2}, {3, 4}})\n    lu.assertEquals(candidate({{1, 2}, {3, 4}}, {2, 4}), {{1, 4}})\n    lu.assertEquals(candidate({{1, 2}, {3, 4}}, {1, 4}), {{1, 4}})\n    lu.assertEquals(candidate({{1, 2}, {3, 4}}, {2, 3}), {{1, 4}})\n    lu.assertEquals(candidate({{1, 2}, {3, 4}}, {5, 6}), {{1, 2}, {3, 4}, {5, 6}})\n    lu.assertEquals(candidate({{10, 20}, {30, 40}}, {10, 30}), {{10, 40}})\n    lu.assertEquals(candidate({{10, 20}, {30, 40}, {50, 60}}, {30, 50}), {{10, 20}, {30, 60}})\n    lu.assertEquals(candidate({}, {1, 2}), {{1, 2}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_373677_S_VAR_ASSIGN", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return vardec + \"=\" + data\n-- \n-- Evaluates an S_STATEMENT node\nlocal function S_VAR_ASSIGN(vardec, assign, data)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_373677_S_VAR_ASSIGN.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = S_VAR_ASSIGN\n    lu.assertEquals(candidate('v3', '=', '4'), 'v3=4')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_373751_sort_array_for_min_number", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # from functools import cmp_to_key\n-- # # def cmp(a, b):\n-- # #     return -1 if a + b < b + a else 1\n-- # nums = [str(i) for i in nums]\n-- # # nums.sort(key = cmp_to_key(cmp))\n-- # nums.sort(key=cmp_to_key(lambda a, b: -1 if a + b < b + a else 1))\n-- # return ''.join(nums).lstrip('0') or '0'\n-- \n-- :param nums: int list\n-- :return: min number string\nlocal function sort_array_for_min_number(nums)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_373751_sort_array_for_min_number.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sort_array_for_min_number\n    lu.assertEquals(candidate({1, 2, 4, 5, 3}), '12345')\n    lu.assertEquals(candidate({5, 4, 3, 2, 1}), '12345')\n    lu.assertEquals(candidate({1, 2, 3, 5, 4}), '12345')\n    lu.assertEquals(candidate({5, 1, 2, 3, 4}), '12345')\n    lu.assertEquals(candidate({1, 2, 3, 4, 5}), '12345')\n    lu.assertEquals(candidate({1, 2, 5, 4, 3}), '12345')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_373808_parse_time_cmd", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # s = s.strip()\n-- # mins, _, secs = s.partition('m')\n-- # mins = float(mins)\n-- # secs = float(secs.rstrip('s'))\n-- # return mins * 60.0 + secs\n-- \n--  Convert timing info from `time` into float seconds.\t\n-- E.g. parse_time('0m0.000s') -> 0.0\t\nlocal function parse_time_cmd(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_373808_parse_time_cmd.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_time_cmd\n    lu.assertEquals(candidate('10m0.000s'), 600.0)\n    lu.assertEquals(candidate('0m0.010s'), 0.01)\n    lu.assertEquals(candidate('0m0.000s'), 0.0)\n    lu.assertEquals(candidate('1m0.000s'), 60.0)\n    lu.assertEquals(candidate('0m0.001s'), 0.001)\n    lu.assertEquals(candidate('0m10.000s'), 10.0)\n    lu.assertEquals(candidate('0m0.010s'), 0.01)\n    lu.assertEquals(candidate('0m2.000s'), 2.0)\n    lu.assertEquals(candidate('0m0.100s'), 0.1)\n    lu.assertEquals(candidate('1m0.000s'), 60.0)\n    lu.assertEquals(candidate('100m0.000s'), 6000.0)\n    lu.assertEquals(candidate('0m0.000s'), 0.0)\n    lu.assertEquals(candidate('0m0.001s'), 0.001)\n    lu.assertEquals(candidate('0m0.100s'), 0.1)\n    lu.assertEquals(candidate('0m0.000s'), 0.0)\n    lu.assertEquals(candidate('0m1.000s'), 1.0)\n    lu.assertEquals(candidate('0m0.001s'), 0.001)\n    lu.assertEquals(candidate('0m1.000s'), 1.0)\n    lu.assertEquals(candidate('1000m0.000s'), 60000.0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_374512__convert_ratio_to_int", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return int((round(ratio, 2) * 100))\n-- \n--     Round the ratio to 2 decimal places, multiply by 100, and take the integer part.\nlocal function _convert_ratio_to_int(ratio)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_374512__convert_ratio_to_int.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _convert_ratio_to_int\n    lu.assertEquals(candidate(0.03), 3)\n    lu.assertEquals(candidate(0.41), 41)\n    lu.assertEquals(candidate(0.19), 19)\n    lu.assertEquals(candidate(0.39), 39)\n    lu.assertEquals(candidate(0.49), 49)\n    lu.assertEquals(candidate(0.5), 50)\n    lu.assertEquals(candidate(1.01), 101)\n    lu.assertEquals(candidate(0.11), 11)\n    lu.assertEquals(candidate(0.5), 50)\n    lu.assertEquals(candidate(1.234567891234568), 123)\n    lu.assertEquals(candidate(0.51), 51)\n    lu.assertEquals(candidate(0.2), 20)\n    lu.assertEquals(candidate(0.125), 12)\n    lu.assertEquals(candidate(0.1), 10)\n    lu.assertEquals(candidate(0.3), 30)\n    lu.assertEquals(candidate(0.75), 75)\n    lu.assertEquals(candidate(1.0), 100)\n    lu.assertEquals(candidate(0.4), 40)\n    lu.assertEquals(candidate(0.6), 60)\n    lu.assertEquals(candidate(0.01), 1)\n    lu.assertEquals(candidate(0.31), 31)\n    lu.assertEquals(candidate(0.99), 99)\n    lu.assertEquals(candidate(0.1), 10)\n    lu.assertEquals(candidate(0.21), 21)\n    lu.assertEquals(candidate(0.61), 61)\n    lu.assertEquals(candidate(0.59), 59)\n    lu.assertEquals(candidate(1.23456789), 123)\n    lu.assertEquals(candidate(0.1234), 12)\n    lu.assertEquals(candidate(0.0), 0)\n    lu.assertEquals(candidate(0.0), 0)\n    lu.assertEquals(candidate(0.09), 9)\n    lu.assertEquals(candidate(0.25), 25)\n    lu.assertEquals(candidate(0.02), 2)\n    lu.assertEquals(candidate(1.2345678912345), 123)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_374778_str2float", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # try:\n-- #     if \"/\" not in string:\n-- #         return float(string)\n-- #     else:\n-- #         return float(string.split(\"/\")[0]) / float(string.split(\"/\")[1])\n-- # except:\n-- #     return 1.0\n-- \n-- Converts a string to a float.\nlocal function str2float(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_374778_str2float.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = str2float\n    lu.assertEquals(candidate(1.0), 1.0)\n    lu.assertEquals(candidate('1.0/1.0'), 1.0)\n    lu.assertEquals(candidate('10'), 10.0)\n    lu.assertEquals(candidate('0'), 0.0)\n    lu.assertEquals(candidate('1/2'), 0.5)\n    lu.assertEquals(candidate('1'), 1.0)\n    lu.assertEquals(candidate('1/4'), 0.25)\n    lu.assertEquals(candidate('10.0'), 10.0)\n    lu.assertEquals(candidate('0.0/1.0'), 0.0)\n    lu.assertEquals(candidate(1), 1.0)\n    lu.assertEquals(candidate('1/1'), 1.0)\n    lu.assertEquals(candidate('1/2.0'), 0.5)\n    lu.assertEquals(candidate('0.0'), 0.0)\n    lu.assertEquals(candidate('0/1'), 0.0)\n    lu.assertEquals(candidate('1.0/2.0'), 0.5)\n    lu.assertEquals(candidate('1.0'), 1.0)\n    lu.assertEquals(candidate('1/1.0'), 1.0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_375173_format_sequence", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return ''.join([c if b else repl_char for b, c in zip(template, sequence)])\n-- \n--  Places the replacement character everywhere there is a False in the\n-- template list\n-- input: list list string\n-- returns: string\nlocal function format_sequence(template, sequence, repl_char)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_375173_format_sequence.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = format_sequence\n    lu.assertEquals(candidate(list('00000'), list('10011'), '?'), '10011')\n    lu.assertEquals(candidate(list('000'), list('011'), '?'), '011')\n    lu.assertEquals(candidate(list('0000'), list('1011'), '?'), '1011')\n    lu.assertEquals(candidate(list('000000'), list('100110'), '?'), '100110')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_375564_sort_list", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # sorted_data_list = sorted(data_list, key=lambda x: x[index], reverse=reverse)\n-- # return sorted_data_list\n-- \n--  index: int number, according to it to sort data_list\nlocal function sort_list(data_list, index, reverse)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_375564_sort_list.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sort_list\n    lu.assertEquals(candidate({{'oranges', 5}, {'apples', 2}, {'bananas', 1}}, 1), {{'oranges', 5}, {'apples', 2}, {'bananas', 1}})\n    lu.assertEquals(candidate({{'oranges', 5}, {'apples', 2}, {'bananas', 1}}, 0, false), {{'apples', 2}, {'bananas', 1}, {'oranges', 5}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_375823_hreflang_formatter", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if '_' in lang:\n-- #     return lang.replace(\"_\", \"-\")\n-- # return lang\n-- \n--     sitemap hreflang should follow correct format.\n-- Use hyphen instead of underscore in language and country value.\n--     ref: https://en.wikipedia.org/wiki/Hreflang#Common_Mistakes\n--     source: https://github.com/readthedocs/readthedocs.org/pull/5638\nlocal function hreflang_formatter(lang)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_375823_hreflang_formatter.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = hreflang_formatter\n    lu.assertEquals(candidate('en_GB'), 'en-GB')\n    lu.assertEquals(candidate('en_us'), 'en-us')\n    lu.assertEquals(candidate('zh-CN'), 'zh-CN')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('zh_CN'), 'zh-CN')\n    lu.assertEquals(candidate('zh-TW'), 'zh-TW')\n    lu.assertEquals(candidate('zh_TW'), 'zh-TW')\n    lu.assertEquals(candidate('zh_tw'), 'zh-tw')\n    lu.assertEquals(candidate('zh_cn'), 'zh-cn')\n    lu.assertEquals(candidate('en'), 'en')\n    lu.assertEquals(candidate('zh-tw'), 'zh-tw')\n    lu.assertEquals(candidate('zh-cn'), 'zh-cn')\n    lu.assertEquals(candidate('en'), 'en')\n    lu.assertEquals(candidate('en_US'), 'en-US')\n    lu.assertEquals(candidate('en-US'), 'en-US')\n    lu.assertEquals(candidate('en_gb'), 'en-gb')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_376349__to_swarming_dimensions", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return [\n-- #     {'key': key, 'value': value}\n-- #     for key, value in\n-- #     (s.split(':', 1) for s in dims)\n-- # ]\n-- \n-- Converts dimensions from buildbucket format to swarming format.\nlocal function _to_swarming_dimensions(dims)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_376349__to_swarming_dimensions.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _to_swarming_dimensions\n    lu.assertEquals(candidate({'os:mac', 'cpu:x86'}), {{['key'] = 'os', ['value'] = 'mac'}, {['key'] = 'cpu', ['value'] = 'x86'}})\n    lu.assertEquals(candidate({'pool:default', 'os:Ubuntu', 'cpu:x86-64', 'gce-vm:1', 'gpu:none'}), {{['key'] = 'pool', ['value'] = 'default'}, {['key'] = 'os', ['value'] = 'Ubuntu'}, {['key'] = 'cpu', ['value'] = 'x86-64'}, {['key'] = 'gce-vm', ['value'] = '1'}, {['key'] = 'gpu', ['value'] = 'none'}})\n    lu.assertEquals(candidate({'os:OS', 'cpu:arm', 'pool:Chrome', 'gpu:GTX660', 'device_type:tablet'}), {{['key'] = 'os', ['value'] = 'OS'}, {['key'] = 'cpu', ['value'] = 'arm'}, {['key'] = 'pool', ['value'] = 'Chrome'}, {['key'] = 'gpu', ['value'] = 'GTX660'}, {['key'] = 'device_type', ['value'] = 'tablet'}})\n    lu.assertEquals(candidate({'os:mac', 'cpu:x86', 'pool:'}), {{['key'] = 'os', ['value'] = 'mac'}, {['key'] = 'cpu', ['value'] = 'x86'}, {['key'] = 'pool', ['value'] = ''}})\n    lu.assertEquals(candidate({'os:Mac-10.9', 'pool:Chrome-perf', 'cpu:x86-64'}), {{['key'] = 'os', ['value'] = 'Mac-10.9'}, {['key'] = 'pool', ['value'] = 'Chrome-perf'}, {['key'] = 'cpu', ['value'] = 'x86-64'}})\n    lu.assertEquals(candidate({'os:Mac-10.9', 'pool:Chrome', 'cpu:x86-64'}), {{['key'] = 'os', ['value'] = 'Mac-10.9'}, {['key'] = 'pool', ['value'] = 'Chrome'}, {['key'] = 'cpu', ['value'] = 'x86-64'}})\n    lu.assertEquals(candidate({'os:mac', 'cpu:x86', 'pool:default'}), {{['key'] = 'os', ['value'] = 'mac'}, {['key'] = 'cpu', ['value'] = 'x86'}, {['key'] = 'pool', ['value'] = 'default'}})\n    lu.assertEquals(candidate({'os:Mac', 'pool:Chrome-perf'}), {{['key'] = 'os', ['value'] = 'Mac'}, {['key'] = 'pool', ['value'] = 'Chrome-perf'}})\n    lu.assertEquals(candidate({'os:Windows', 'pool:Chrome-perf'}), {{['key'] = 'os', ['value'] = 'Windows'}, {['key'] = 'pool', ['value'] = 'Chrome-perf'}})\n    lu.assertEquals(candidate({'os:mac', 'cpu:x86', 'pool:default:'}), {{['key'] = 'os', ['value'] = 'mac'}, {['key'] = 'cpu', ['value'] = 'x86'}, {['key'] = 'pool', ['value'] = 'default:'}})\n    lu.assertEquals(candidate({'os:Debian', 'cpu:x86-64', 'gce-vm:1', 'gpu:none'}), {{['key'] = 'os', ['value'] = 'Debian'}, {['key'] = 'cpu', ['value'] = 'x86-64'}, {['key'] = 'gce-vm', ['value'] = '1'}, {['key'] = 'gpu', ['value'] = 'none'}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_376708_build_matrix_row", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if score is None:\n-- #     score = 0.0\n-- # row = []\n-- # for factor in all_vfs:\n-- #     if factor in accepted_hits:\n-- #         row.append(score)\n-- #     else:\n-- #         row.append(0.5)\n-- # return row\n-- \n-- Populate row given all possible hits, accepted hits and an optional score\n-- :param all_vfs: a list of all virulence factor ids\n-- :param accepted_hits: a list of a hits that passed the cutoof\n-- :param score: the value to fill the matrix with (default = None which\n--               implies 0.5)\n-- :type all_vfs: list\n-- :type accepted_hits: list\n-- :type score: float\n-- :rtype: a list of floats\nlocal function build_matrix_row(all_vfs, accepted_hits, score)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_376708_build_matrix_row.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = build_matrix_row\n    lu.assertEquals(candidate({'10001', '10002', '10003', '10004', '10005'}, {'10001', '10002'}, 1.0), {1.0, 1.0, 0.5, 0.5, 0.5})\n    lu.assertEquals(candidate({'10001', '10002', '10003', '10004', '10005'}, {'10001', '10002'}, 0.0), {0.0, 0.0, 0.5, 0.5, 0.5})\n    lu.assertEquals(candidate({}, {}), {})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_376729_hosoya", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if (width == 0) and (height in (0, 1)):\n-- #     return 1\n-- # if (width == 1) and (height in (1, 2)):\n-- #     return 1\n-- # if height > width:\n-- #     return hosoya(height - 1, width) + hosoya(height - 2, width)\n-- # if width == height:\n-- #     return hosoya(height - 1, width - 1) + hosoya(height - 2, width - 2)\n-- # return 0\n-- \n--  Calculates the hosoya triangle\n-- height -- height of the triangle\nlocal function hosoya(height, width)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_376729_hosoya.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = hosoya\n    lu.assertEquals(candidate(3, 1), 2)\n    lu.assertEquals(candidate(1, 1), 1)\n    lu.assertEquals(candidate(1, 3), 0)\n    lu.assertEquals(candidate(5, 5), 8)\n    lu.assertEquals(candidate(5, 4), 5)\n    lu.assertEquals(candidate(0, 2), 0)\n    lu.assertEquals(candidate(0, 2), 0)\n    lu.assertEquals(candidate(3, 2), 2)\n    lu.assertEquals(candidate(4, 4), 5)\n    lu.assertEquals(candidate(1, 1), 1)\n    lu.assertEquals(candidate(2, 1), 1)\n    lu.assertEquals(candidate(1, 0), 1)\n    lu.assertEquals(candidate(1, 3), 0)\n    lu.assertEquals(candidate(3, 3), 3)\n    lu.assertEquals(candidate(1, 0), 1)\n    lu.assertEquals(candidate(0, 0), 1)\n    lu.assertEquals(candidate(2, 1), 1)\n    lu.assertEquals(candidate(0, 0), 1)\n    lu.assertEquals(candidate(0, 3), 0)\n    lu.assertEquals(candidate(0, 3), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_37718_how_many_days", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # days_in_month = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n-- # result = days_in_month[month_number - 1]\n-- # return result\n-- \n-- Returns the number of days in a month.\n-- WARNING: This function doesn't account for leap years!\nlocal function how_many_days(month_number)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_37718_how_many_days.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = how_many_days\n    lu.assertEquals(candidate(5), 31)\n    lu.assertEquals(candidate(2), 28)\n    lu.assertEquals(candidate(9), 30)\n    lu.assertEquals(candidate(12), 31)\n    lu.assertEquals(candidate(11), 30)\n    lu.assertEquals(candidate(4), 30)\n    lu.assertEquals(candidate(8), 31)\n    lu.assertEquals(candidate(10), 31)\n    lu.assertEquals(candidate(6), 30)\n    lu.assertEquals(candidate(7), 31)\n    lu.assertEquals(candidate(1), 31)\n    lu.assertEquals(candidate(3), 31)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_379101_line_parameters_xy", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # a = (pt_1[1]-pt_2[1])\n-- # b = (pt_2[0]-pt_1[0])\n-- # c = (pt_1[0]*pt_2[1]-pt_2[0]*pt_1[1])\n-- # return a, b, -c\n-- \n-- Used in lines_intersection\n-- from:\n-- https://stackoverflow.com/questions/20677795/how-do-i-compute-the-intersection-point-of-two-lines-in-python\nlocal function line_parameters_xy(pt_1, pt_2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_379101_line_parameters_xy.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = line_parameters_xy\n    lu.assertEquals(candidate({0, 0}, {1, 1}), {-1, 1, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_379304_countBits", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # assert 0 <= value\n-- # count = 1\n-- # bits = 1\n-- # while (1 << bits) <= value:\n-- #     count += bits\n-- #     value >>= bits\n-- #     bits <<= 1\n-- # while 2 <= value:\n-- #     if bits != 1:\n-- #         bits >>= 1\n-- #     else:\n-- #         bits -= 1\n-- #     while (1 << bits) <= value:\n-- #         count += bits\n-- #         value >>= bits\n-- # return count\n-- \n-- Count number of bits needed to store a (positive) integer number.\n-- >>> countBits(0)\n-- 1\n-- >>> countBits(1000)\n-- 10\n-- >>> countBits(44100)\n-- 16\n-- >>> countBits(18446744073709551615)\n-- 64\nlocal function countBits(value)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_379304_countBits.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = countBits\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(3), 2)\n    lu.assertEquals(candidate(10), 4)\n    lu.assertEquals(candidate(44100), 16)\n    lu.assertEquals(candidate(100), 7)\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(6), 3)\n    lu.assertEquals(candidate(8), 4)\n    lu.assertEquals(candidate(1000), 10)\n    lu.assertEquals(candidate(0), 1)\n    lu.assertEquals(candidate(4), 3)\n    lu.assertEquals(candidate(9), 4)\n    lu.assertEquals(candidate(7), 3)\n    lu.assertEquals(candidate(5), 3)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_379324_int_max_value", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if signed:\n-- #     max_value = pow(2, bits - 1) - 1\n-- # else:\n-- #     max_value = pow(2, bits) - 1\n-- # return max_value\n-- \n-- Returns the maximum int value of a signed or unsigned integer\n-- based on used bits.\n-- Arguments:\n-- bits -- How many bits, e.g., 16\n-- signed -- True if a signed int\n-- Returns:\n-- max_value -- The maximum int value based on given parameters\nlocal function int_max_value(bits, signed)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_379324_int_max_value.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = int_max_value\n    lu.assertEquals(candidate(8, true), 127)\n    lu.assertEquals(candidate(16, true), 32767)\n    lu.assertEquals(candidate(8, false), 255)\n    lu.assertEquals(candidate(8), 127)\n    lu.assertEquals(candidate(16, false), 65535)\n    lu.assertEquals(candidate(32, false), 4294967295)\n    lu.assertEquals(candidate(64, false), 18446744073709551615)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_379765_sum_numbers", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # n = upper - 1\n-- # return n * (n + 1) // 2\n-- \n-- Calculate the sum of integers less than upper.\nlocal function sum_numbers(upper)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_379765_sum_numbers.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sum_numbers\n    lu.assertEquals(candidate(3), 3)\n    lu.assertEquals(candidate(5), 10)\n    lu.assertEquals(candidate(2), 1)\n    lu.assertEquals(candidate(1001), 500500)\n    lu.assertEquals(candidate(1), 0)\n    lu.assertEquals(candidate(101), 5050)\n    lu.assertEquals(candidate(10), 45)\n    lu.assertEquals(candidate(100), 4950)\n    lu.assertEquals(candidate(0), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_380059_comment_lines", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not prefix:\n-- #     return lines\n-- # return [prefix + ' ' + line if line else prefix for line in lines]\n-- \n-- Return commented lines\nlocal function comment_lines(lines, prefix)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_380059_comment_lines.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = comment_lines\n    lu.assertEquals(candidate({'This is a short comment.'}, '#'), {'# This is a short comment.'})\n    lu.assertEquals(candidate({'This is the first line.', 'This is the second line.'}, ''), {'This is the first line.', 'This is the second line.'})\n    lu.assertEquals(candidate({''}, ''), {''})\n    lu.assertEquals(candidate({}, '#'), {})\n    lu.assertEquals(candidate({'This is the first line.', 'This is the second line.'}, '#'), {'# This is the first line.', '# This is the second line.'})\n    lu.assertEquals(candidate({'This is a long comment', 'that contains multiple lines.'}, '#'), {'# This is a long comment', '# that contains multiple lines.'})\n    lu.assertEquals(candidate({''}, '#'), {'#'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_380282_dict_item", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return dictionary.get(key, None)\n-- \n-- 'Template filter to allow accessing dictionary value by variable key.\n-- Example use::\n--     {{ mydict|dict_item:keyvar }}\nlocal function dict_item(dictionary, key)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_380282_dict_item.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = dict_item\n    lu.assertEquals(candidate({['a'] = 'A', ['b'] = 'B', ['c'] = 'C'}, 'c'), 'C')\n    lu.assertEquals(candidate({['a'] = '1', ['b'] = '2'}, 'a'), '1')\n    lu.assertEquals(candidate({['a'] = 'A', ['b'] = 'B', ['c'] = 'C'}, 'a'), 'A')\n    lu.assertEquals(candidate({['a'] = '1', ['b'] = '2'}, 'b'), '2')\n    lu.assertEquals(candidate({['a'] = 'A', ['b'] = 'B', ['c'] = 'C'}, 'b'), 'B')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_38089_respuesta", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"iguales\" if res else \"diferentes\"\n-- \n--  Funcion para formatear la respuesta. \nlocal function respuesta(res)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_38089_respuesta.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = respuesta\n    lu.assertEquals(candidate(true), 'iguales')\n    lu.assertEquals(candidate(false), 'diferentes')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_381395_camelize", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return ''.join((s[0].upper() +\n-- #                 ''.join([c.lower() for c in s[1:]])\n-- #                 for s in strings))\n-- \n-- Make a camelcase string\n-- Args:\n--   strings (list[string]): list of strings\n-- DocTests:\n--   >>> camelize(['one', 'two', 'three'])\n--   'OneTwoThree'\nlocal function camelize(strings)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_381395_camelize.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = camelize\n    lu.assertEquals(candidate({'123', 'four'}), '123Four')\n    lu.assertEquals(candidate({'this', 'is', 'a', 'sentence'}), 'ThisIsASentence')\n    lu.assertEquals(candidate({'one', 'two', 'three', 'four', 'five'}), 'OneTwoThreeFourFive')\n    lu.assertEquals(candidate({'one', 'two', 'three'}), 'OneTwoThree')\n    lu.assertEquals(candidate({'candidate', 'me'}), 'CamelizeMe')\n    lu.assertEquals(candidate({'one', 'two', 'three'}), 'OneTwoThree')\n    lu.assertEquals(candidate(list()), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_382168__remove_by_index_list", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # len_removed = 0\n-- # for i in index_list:\n-- #     i_start = i[0] - len_removed\n-- #     i_end = i[1]-len_removed\n-- #     text = text[:i_start] + text[i_end:]\n-- #     len_removed += i[1]-i[0]\n-- # return text\n-- \n-- Remove all substrings inside a string, thanks to the given list of indexes \nlocal function _remove_by_index_list(text, index_list)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_382168__remove_by_index_list.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _remove_by_index_list\n    lu.assertEquals(candidate('Hello, World!', {{100, 200}}), 'Hello, World!')\n    lu.assertEquals(candidate('Hello, World!', {{100, 100}}), 'Hello, World!')\n    lu.assertEquals(candidate('Hello, World!', {{1, 1}, {1, 1}}), 'Hello, World!')\n    lu.assertEquals(candidate('Hello, World!', {{100, 101}, {100, 101}}), 'Hello, World!')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_382424_weight_path", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # assert model_path.endswith('.xml'), \"Wrong topology path was provided\"\n-- # return model_path[:-3] + 'bin'\n-- \n--  Get path of weights based on path to IR\n-- Params:\n-- model_path: the string contains path to IR file\n-- Return:\n-- Path to weights file\nlocal function weight_path(model_path)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_382424_weight_path.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = weight_path\n    lu.assertEquals(candidate('test.xml'), 'test.bin')\n    lu.assertEquals(candidate('/path/to/file/mobilenet-ssd.xml'), '/path/to/file/mobilenet-ssd.bin')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_382473_string_rotation", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if len(x) != len(y):\n-- #     return False\n-- # new_str = x + x\n-- # if y in new_str:\n-- #     return True\n-- # return False\n-- \n-- :param x: string\n-- :param y: string\n-- :return:\nlocal function string_rotation(x, y)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_382473_string_rotation.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = string_rotation\n    lu.assertEquals(candidate('waterbottle', 'bottlewater'), true)\n    lu.assertEquals(candidate('waterbottle', 'waterbottle'), true)\n    lu.assertEquals(candidate('123456', '1234567'), false)\n    lu.assertEquals(candidate('12345', '12345'), true)\n    lu.assertEquals(candidate('waterbottle', 'waterbottle'), true)\n    lu.assertEquals(candidate('bottlewater', 'bottlewater'), true)\n    lu.assertEquals(candidate('12345', '12345'), true)\n    lu.assertEquals(candidate('waterbottle', 'bottlewater'), true)\n    lu.assertEquals(candidate('abcdefg', 'efgabcd'), true)\n    lu.assertEquals(candidate('waterbottle', 'erbottlewat'), true)\n    lu.assertEquals(candidate('waterbottle', 'waterbottle'), true)\n    lu.assertEquals(candidate('waterbottle', 'erbottlewa'), false)\n    lu.assertEquals(candidate('waterbottle', 'erbottlewat'), true)\n    lu.assertEquals(candidate('waterbottle', 'waterbottle'), true)\n    lu.assertEquals(candidate('waterbottle', 'erbottlewat'), true)\n    lu.assertEquals(candidate('a', 'b'), false)\n    lu.assertEquals(candidate('erbottlewax', 'erbottlewat'), false)\n    lu.assertEquals(candidate('', 'a'), false)\n    lu.assertEquals(candidate('waterbottle', 'erbottleaw'), false)\n    lu.assertEquals(candidate('123456', '123789'), false)\n    lu.assertEquals(candidate('123456', '123465'), false)\n    lu.assertEquals(candidate('123456', '123456'), true)\n    lu.assertEquals(candidate('a', 'a'), true)\n    lu.assertEquals(candidate('erbottlewat', 'waterbottle'), true)\n    lu.assertEquals(candidate('123456', '12345'), false)\n    lu.assertEquals(candidate('waterbottle', 'erbottlewat'), true)\n    lu.assertEquals(candidate('bottlewater', 'waterbottle'), true)\n    lu.assertEquals(candidate('waterbottle', 'waterrbottle'), false)\n    lu.assertEquals(candidate('waterbottle', 'erbottlewat'), true)\n    lu.assertEquals(candidate('erbottlewat', 'erbottlewax'), false)\n    lu.assertEquals(candidate('waterrbottle', 'waterbottle'), false)\n    lu.assertEquals(candidate('123456', '234567'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_383084_join_namespace", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return ':'.join([namespace, ident])\n-- \n-- Joins a namespace and a bare identifier into a full identifier.\n-- >>> join_namespace('a', 'b')\n-- 'a:b'\n-- >>> join_namespace('', 'b')\n-- ':b'\nlocal function join_namespace(namespace, ident)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_383084_join_namespace.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = join_namespace\n    lu.assertEquals(candidate('a', 'b'), 'a:b')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_383373_splitrip", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # vals = [x.strip() for x in obj.split(split) if x.strip()]\n-- # return vals\n-- \n-- Pass.\nlocal function splitrip(obj, split)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_383373_splitrip.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = splitrip\n    lu.assertEquals(candidate(' a b c d ', ' '), {'a', 'b', 'c', 'd'})\n    lu.assertEquals(candidate('Hello\\tWorld\\tHow\\tAre\\tYou', '\\t'), {'Hello', 'World', 'How', 'Are', 'You'})\n    lu.assertEquals(candidate('a\\tb\\t\\tc\\t\\td', '\\t'), {'a', 'b', 'c', 'd'})\n    lu.assertEquals(candidate('\\tHello\\tWorld\\tHow\\tAre\\tYou\\t', '\\t'), {'Hello', 'World', 'How', 'Are', 'You'})\n    lu.assertEquals(candidate('Hello\\tWorld\\tHow\\tAre\\tYou', ' '), {'Hello\\tWorld\\tHow\\tAre\\tYou'})\n    lu.assertEquals(candidate('a b c d', 'x'), {'a b c d'})\n    lu.assertEquals(candidate('\\t\\t', '\\t'), {})\n    lu.assertEquals(candidate('Hello\\tWorld\\tHow\\tAre\\tYou\\t\\t', '\\t'), {'Hello', 'World', 'How', 'Are', 'You'})\n    lu.assertEquals(candidate('', '\\t'), {})\n    lu.assertEquals(candidate('a b c d', ' '), {'a', 'b', 'c', 'd'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_383429_bool_", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return bool(input_) if input_.upper() not in [\"FALSE\", \"F\"] else False\n-- \n--  Convert boolean or string to boolean, also 'False' and 'F' to False \nlocal function bool_(input_)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_383429_bool_.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = bool_\n    lu.assertEquals(candidate('False'), false)\n    lu.assertEquals(candidate('Yes'), true)\n    lu.assertEquals(candidate('True'), true)\n    lu.assertEquals(candidate('Y'), true)\n    lu.assertEquals(candidate('T'), true)\n    lu.assertEquals(candidate('F'), false)\n    lu.assertEquals(candidate('f'), false)\n    lu.assertEquals(candidate('1'), true)\n    lu.assertEquals(candidate(''), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_383665_digitsaverage", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not d:\n-- #     return d\n-- # res = ''\n-- # if d < 10:\n-- #     return d\n-- # else:\n-- #     st_d = str(d)\n-- #     for x in range(len(st_d) - 1):\n-- #         z = st_d[x]\n-- #         y = st_d[x + 1]\n-- #         float = (int(z) + int(y)) % len(z + y)\n-- #         avg = ((int(z) + int(y)) // len(z + y)) + float\n-- #         res += str(avg)\n-- #     recurs = int(res)\n-- #     return digitsaverage(recurs)\n-- \n-- Return the average of the digits until number is one digit.\n-- input = integer\n-- output = integer, single digit\n-- ex. 246 = 4 i.e. avg of 2 and 4 is 3, average of 4 and 6 is 5\n--     so after first iteration 246 => 35\n--     avg of 3 and 5 is 4 so digitsAverage(246) returns 4\nlocal function digitsaverage(d)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_383665_digitsaverage.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = digitsaverage\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(7), 7)\n    lu.assertEquals(candidate(246), 4)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(987), 9)\n    lu.assertEquals(candidate(9), 9)\n    lu.assertEquals(candidate(6), 6)\n    lu.assertEquals(candidate(3), 3)\n    lu.assertEquals(candidate(10), 1)\n    lu.assertEquals(candidate(2222), 2)\n    lu.assertEquals(candidate(5), 5)\n    lu.assertEquals(candidate(8), 8)\n    lu.assertEquals(candidate(4), 4)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_384049__formatArraySplitWiden", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # width = max(len(line) for line in arFmtSplit)\n-- # for i in range(len(arFmtSplit)):\n-- #     arFmtSplit[i] = arFmtSplit[i].ljust(width)\n-- # if linecount > len(arFmtSplit):\n-- #     arFmtSplit.extend([blank*width]*(linecount - len(arFmtSplit)))\n-- # return arFmtSplit\n-- \n-- Private func that gets iterated over as part of enjambing multiple formatted arrays side-by-side.\n-- Adds spaces such that the mutable sequence arFmtSplit (if joined by `\n-- `) will print as a rectangle.\n-- Optionally also extends the \"height\" of the rectangle to match lineCount. If lineCount <= len(arFmtSplit), nothing happens.\nlocal function _formatArraySplitWiden(arFmtSplit, blank, linecount)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_384049__formatArraySplitWiden.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _formatArraySplitWiden\n    lu.assertEquals(candidate({'a1', 'b1', 'c1', 'd1'}, ' ', 4), {'a1', 'b1', 'c1', 'd1'})\n    lu.assertEquals(candidate({'a1', 'b1', 'c1', 'd1'}, '', 6), {'a1', 'b1', 'c1', 'd1', '', ''})\n    lu.assertEquals(candidate({'a1', 'b1', 'c1', 'd1'}, '', 5), {'a1', 'b1', 'c1', 'd1', ''})\n    lu.assertEquals(candidate({'a1', 'b1', 'c1', 'd1'}, '', 4), {'a1', 'b1', 'c1', 'd1'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_384332_pascal_case", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"\".join([s[0:1].upper() + s[1:] for s in what.split('.')])\n-- \n-- Convert the first letter of a string to uppercase, to make the identifier\n-- conform to PascalCase.\n-- If there are dots, remove the dots, and capitalize the letter following\n-- where the dot was. Letters that weren't following dots are left unchanged,\n-- except for the first letter of the string (which is made upper-case).\n-- Args:\n--   what: a string representing some identifier\n-- Returns:\n--   String with first letter capitalized\n-- Example:\n--   pascal_case(\"helloWorld\") == \"HelloWorld\"\n--   pascal_case(\"foo\") == \"Foo\"\n--   pascal_case(\"hello.world\") = \"HelloWorld\"\n--   pascal_case(\"fooBar.fooBar\") = \"FooBarFooBar\"\nlocal function pascal_case(what)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_384332_pascal_case.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = pascal_case\n    lu.assertEquals(candidate('HELLO WORLD'), 'HELLO WORLD')\n    lu.assertEquals(candidate('hello.world'), 'HelloWorld')\n    lu.assertEquals(candidate('foo'), 'Foo')\n    lu.assertEquals(candidate('abc'), 'Abc')\n    lu.assertEquals(candidate('fooBar.fooBar'), 'FooBarFooBar')\n    lu.assertEquals(candidate('helloWorld'), 'HelloWorld')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_384769_ip_to_binary", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"\".join([bin(int(x)+256)[3:] for x in ip.split('.')])\n-- \n-- Convert an IPv4 address to a string containing the binary conversion of the\n-- IP address.\n-- Args:\n--     ip - The ip address to convert\nlocal function ip_to_binary(ip)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_384769_ip_to_binary.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = ip_to_binary\n    lu.assertEquals(candidate('10.10.10.10'), '00001010000010100000101000001010')\n    lu.assertEquals(candidate('192.168.1.1'), '11000000101010000000000100000001')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_384893_remove_quotes", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return string.replace(\"'\", \"\").replace('\"', '')\n-- \n--  remove all (double) quotes\nlocal function remove_quotes(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_384893_remove_quotes.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = remove_quotes\n    lu.assertEquals(candidate('abc\"'), 'abc')\n    lu.assertEquals(candidate('a\"bc'), 'abc')\n    lu.assertEquals(candidate('\"a\"bc\"'), 'abc')\n    lu.assertEquals(candidate('\"abc'), 'abc')\n    lu.assertEquals(candidate('abc'), 'abc')\n    lu.assertEquals(candidate('a\"bc\"'), 'abc')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_385138_get_next_step_size", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if current_offset + block_size > total:\n-- #     step_size = total - current_offset\n-- # else:\n-- #     step_size = current_offset\n-- # return step_size\n-- \n-- Calculate next size of step for a TQDM progress-bar.\n-- :param total:\n-- :param block_size:\n-- :param current_offset:\n-- :return:\nlocal function get_next_step_size(total, block_size, current_offset)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_385138_get_next_step_size.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_next_step_size\n    lu.assertEquals(candidate(2, 2, 0), 0)\n    lu.assertEquals(candidate(10, 2, 10), 0)\n    lu.assertEquals(candidate(10, 5, 10), 0)\n    lu.assertEquals(candidate(1, 0, 0), 0)\n    lu.assertEquals(candidate(2, 1, 1), 1)\n    lu.assertEquals(candidate(2, 2, 1), 1)\n    lu.assertEquals(candidate(10, 2, 1), 1)\n    lu.assertEquals(candidate(100, 10, 100), 0)\n    lu.assertEquals(candidate(3, 0, 1), 1)\n    lu.assertEquals(candidate(3, 0, 0), 0)\n    lu.assertEquals(candidate(100, 10, 5), 5)\n    lu.assertEquals(candidate(2, 0, 0), 0)\n    lu.assertEquals(candidate(0, 0, 0), 0)\n    lu.assertEquals(candidate(2, 0, 2), 2)\n    lu.assertEquals(candidate(0, 1, 0), 0)\n    lu.assertEquals(candidate(1, 0, 1), 1)\n    lu.assertEquals(candidate(10, 5, 5), 5)\n    lu.assertEquals(candidate(2, 0, 1), 1)\n    lu.assertEquals(candidate(100, 10, 10), 10)\n    lu.assertEquals(candidate(2, 1, 0), 0)\n    lu.assertEquals(candidate(1, 1, 0), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_385358_get_package_version_key", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return pkg_name + '@' + pkg_version\n-- \n-- Return unique key combining package name and version.\nlocal function get_package_version_key(pkg_name, pkg_version)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_385358_get_package_version_key.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_package_version_key\n    lu.assertEquals(candidate('package_name', 'package_version'), 'package_name@package_version')\n    lu.assertEquals(candidate('package_name', 'package_version'), 'package_name@package_version')\n    lu.assertEquals(candidate('foo', '1.0'), 'foo@1.0')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_385738_read_file", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # with open(filepath, \"r\", encoding=\"utf-8\") as file:\n-- #     data = file.read()\n-- # return data\n-- \n-- Read file content\n-- :param filepath:\n-- :return:\nlocal function read_file(filepath)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_385738_read_file.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = read_file\n    lu.assertEquals(candidate(0), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_386243_sort_by_points", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # sorted_list = sorted(hn_list, key=lambda k: k[\"score\"], reverse=True)\n-- # return sorted_list\n-- \n-- Returns a sorted list of dictionaries from alternative_hacker_news to\n-- be ordered by score (highest first).\nlocal function sort_by_points(hn_list)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_386243_sort_by_points.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sort_by_points\n    lu.assertEquals(candidate({{['id'] = 1, ['score'] = 10}, {['id'] = 2, ['score'] = 10}, {['id'] = 3, ['score'] = 10}}), {{['id'] = 1, ['score'] = 10}, {['id'] = 2, ['score'] = 10}, {['id'] = 3, ['score'] = 10}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_38627__split_chunks", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # ret = [[l[0]]]\n-- # for c in l[1:]:\n-- #     if ret[-1][-1] == c - 1:\n-- #         ret[-1].append(c)\n-- #     else:\n-- #         ret.append([c])\n-- # return ret\n-- \n-- Generates a list of lists of neighbouring ints. `l` must not be empty.\n-- >>> _split_chunks([1,2,3,5,6,7,9])\n-- [[1,2,3],[5,6,7],[9]]\n-- :type l: list[int]\n-- :rtype list[list[int]]\nlocal function _split_chunks(l)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_38627__split_chunks.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _split_chunks\n    lu.assertEquals(candidate(list(range(10))), {{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}})\n    lu.assertEquals(candidate(list(range(1, 6))), {{1, 2, 3, 4, 5}})\n    lu.assertEquals(candidate(list(range(3))), {{0, 1, 2}})\n    lu.assertEquals(candidate({1, 2, 3, 5, 6}), {{1, 2, 3}, {5, 6}})\n    lu.assertEquals(candidate({1, 2, 3, 5, 6, 7, 9}), {{1, 2, 3}, {5, 6, 7}, {9}})\n    lu.assertEquals(candidate({1, 2, 3}), {{1, 2, 3}})\n    lu.assertEquals(candidate(list(range(6))), {{0, 1, 2, 3, 4, 5}})\n    lu.assertEquals(candidate(list(range(9))), {{0, 1, 2, 3, 4, 5, 6, 7, 8}})\n    lu.assertEquals(candidate({1, 2, 3, 5}), {{1, 2, 3}, {5}})\n    lu.assertEquals(candidate(list(range(5))), {{0, 1, 2, 3, 4}})\n    lu.assertEquals(candidate({1, 2, 3, 4, 6, 7, 8}), {{1, 2, 3, 4}, {6, 7, 8}})\n    lu.assertEquals(candidate(list(range(1))), {{0}})\n    lu.assertEquals(candidate(list(range(2))), {{0, 1}})\n    lu.assertEquals(candidate({1}), {{1}})\n    lu.assertEquals(candidate({1, 2, 3, 5, 6, 7}), {{1, 2, 3}, {5, 6, 7}})\n    lu.assertEquals(candidate({1, 2}), {{1, 2}})\n    lu.assertEquals(candidate(list(range(7))), {{0, 1, 2, 3, 4, 5, 6}})\n    lu.assertEquals(candidate(list(range(3, 8))), {{3, 4, 5, 6, 7}})\n    lu.assertEquals(candidate(list(range(4))), {{0, 1, 2, 3}})\n    lu.assertEquals(candidate(list(range(8))), {{0, 1, 2, 3, 4, 5, 6, 7}})\n    lu.assertEquals(candidate(list(range(2, 7))), {{2, 3, 4, 5, 6}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_386910_format_seconds", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # output_time = seconds\n-- # output_unit = \"seconds\"\n-- # if output_time >= 60.:\n-- #     output_time /= 60.\n-- #     output_unit = \"minutes\"\n-- #     if output_time >= 60.:\n-- #         output_time /= 60.\n-- #         output_unit = \"hours\"\n-- # return \"{:.2f} {}\".format(output_time, output_unit)\n-- \n-- Format a floating point representing seconds into\n-- a nice readable string.\n-- Arguments:\n--     seconds    The seconds to format.\n-- Returns:\n--     A formatted string as either seconds, minutes, or hours.\nlocal function format_seconds(seconds)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_386910_format_seconds.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = format_seconds\n    lu.assertEquals(candidate(1.0), '1.00 seconds')\n    lu.assertEquals(candidate(5.9), '5.90 seconds')\n    lu.assertEquals(candidate(60.0), '1.00 minutes')\n    lu.assertEquals(candidate(3600.0), '1.00 hours')\n    lu.assertEquals(candidate(1.0), '1.00 seconds')\n    lu.assertEquals(candidate(5.1), '5.10 seconds')\n    lu.assertEquals(candidate(2.0), '2.00 seconds')\n    lu.assertEquals(candidate(7200.0), '2.00 hours')\n    lu.assertEquals(candidate(90.0), '1.50 minutes')\n    lu.assertEquals(candidate(2.5), '2.50 seconds')\n    lu.assertEquals(candidate(120.0), '2.00 minutes')\n    lu.assertEquals(candidate(1.001), '1.00 seconds')\n    lu.assertEquals(candidate(5.0), '5.00 seconds')\n    lu.assertEquals(candidate(123.0), '2.05 minutes')\n    lu.assertEquals(candidate(60.0), '1.00 minutes')\n    lu.assertEquals(candidate(2.3), '2.30 seconds')\n    lu.assertEquals(candidate(3600.0), '1.00 hours')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_386946_coverageIsFailing", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # shouldFail = False\n-- # if coverage < minCoverage:\n-- #     shouldFail = True\n-- #     print(\"Coverage of\", coverage, \"is below passing threshold of\", minCoverage)\n-- # if branches < minBranches:\n-- #     shouldFail = True\n-- #     print(\"Branches of\", branches, \"is below passing threshold of\", minBranches)\n-- # return shouldFail\n-- \n-- Checks if coverage or branchs coverage or both are\n-- below minimum to pass workflow run. Logs messages if it is.\n-- Actual failing behavior should be handled by caller.\n-- Keyword arguments:\n-- coverage - instructions coverage in interval 0.0 to 1.0.\n-- branches - branches coverage in interval 0.0 to 1.0.\n-- minCoverage - minimum instructions coverage to pass in interval 0.0 to 1.0.\n-- minBranches - minimum branches coverage to pass in interval 0.0 to 1.0.\nlocal function coverageIsFailing(coverage, branches, minCoverage, minBranches)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_386946_coverageIsFailing.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = coverageIsFailing\n    lu.assertEquals(candidate(0.0, 0.1, 0.0, 0.0), false)\n    lu.assertEquals(candidate(0.1, 0.1, 0.0, 0.0), false)\n    lu.assertEquals(candidate(0.1, 0.1, 0.0, 0.1), false)\n    lu.assertEquals(candidate(0.1, 0.0, 0.1, 0.0), false)\n    lu.assertEquals(candidate(0.0, 1.0, 1.0, 0.0), true)\n    lu.assertEquals(candidate(0.75, 0.8, 0.9, 0.95), true)\n    lu.assertEquals(candidate(1.0, 1.0, 1.0, 1.0), false)\n    lu.assertEquals(candidate(0.0, 1.0, 1.0, 1.0), true)\n    lu.assertEquals(candidate(0.0, 0.1, 0.0, 0.1), false)\n    lu.assertEquals(candidate(0.0, 0.0, 1.0, 1.0), true)\n    lu.assertEquals(candidate(0.0, 0.0, 0.0, 1.0), true)\n    lu.assertEquals(candidate(1.0, 0.0, 1.0, 1.0), true)\n    lu.assertEquals(candidate(0.5, 0.75, 0.5, 0.5), false)\n    lu.assertEquals(candidate(0.4, 0.75, 0.5, 0.5), true)\n    lu.assertEquals(candidate(0.0, 0.0, 1.0, 0.0), true)\n    lu.assertEquals(candidate(1.0, 0.0, 0.0, 1.0), true)\n    lu.assertEquals(candidate(0.75, 0.7, 0.8, 0.9), true)\n    lu.assertEquals(candidate(0.8, 0.8, 0.9, 0.8), true)\n    lu.assertEquals(candidate(0.0, 0.0, 0.0, 0.0), false)\n    lu.assertEquals(candidate(0.1, 0.0, 0.0, 0.0), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_387299_remove_length10_word", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # final_list = []\n-- # for word in review_str.split():\n-- #     if len(word) < 10:\n-- #         final_list.append(word)\n-- # return \" \".join(final_list)\n-- \n-- remove any words have length more than 10 on str\nlocal function remove_length10_word(review_str)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_387299_remove_length10_word.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = remove_length10_word\n    lu.assertEquals(candidate('The quick brown fox jumped over the lazy dog'), 'The quick brown fox jumped over the lazy dog')\n    lu.assertEquals(candidate('this film was amazingly bad'), 'this film was amazingly bad')\n    lu.assertEquals(candidate('The quick brown fox jumped over the lazy dog.'), 'The quick brown fox jumped over the lazy dog.')\n    lu.assertEquals(candidate('The quick brown fox jumped over the lazy dog the quick brown fox jumped over the lazy dog.'), 'The quick brown fox jumped over the lazy dog the quick brown fox jumped over the lazy dog.')\n    lu.assertEquals(candidate('The quick brown fox jumped over the lazy dog. The quick brown fox jumped over the lazy dog.'), 'The quick brown fox jumped over the lazy dog. The quick brown fox jumped over the lazy dog.')\n    lu.assertEquals(candidate('A'), 'A')\n    lu.assertEquals(candidate(''), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_388125_f", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return r * x - x ** 3\n-- \n--  function to find bifurcations for \nlocal function f(x, r)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_388125_f.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = f\n    lu.assertEquals(candidate(1, 1), 0)\n    lu.assertEquals(candidate(0, 1), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_388515_getLine", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n-- # points = []\n-- # issteep = abs(y2-y1) > abs(x2-x1)\n-- # if issteep:\n-- #     x1, y1 = y1, x1\n-- #     x2, y2 = y2, x2\n-- # rev = False\n-- # if x1 > x2:\n-- #     x1, x2 = x2, x1\n-- #     y1, y2 = y2, y1\n-- #     rev = True\n-- # deltax = x2 - x1\n-- # deltay = abs(y2-y1)\n-- # error = int(deltax / 2)\n-- # y = y1\n-- # ystep = None\n-- # if y1 < y2:\n-- #     ystep = 1\n-- # else:\n-- #     ystep = -1\n-- # for x in range(x1, x2 + 1):\n-- #     if issteep:\n-- #         points.append((y, x))\n-- #     else:\n-- #         points.append((x, y))\n-- #     error -= deltay\n-- #     if error < 0:\n-- #         y += ystep\n-- #         error += deltax\n-- # # Reverse the list if the coordinates were reversed\n-- # if rev:\n-- #     points.reverse()\n-- # return points\n-- \n-- Returns a list of (x, y) tuples of every point on a line between\n-- (x1, y1) and (x2, y2). The x and y values inside the tuple are integers.\n-- Line generated with the Bresenham algorithm.\n-- Args:\n--   x1 (int, float): The x coordinate of the line's start point.\n--   y1 (int, float): The y coordinate of the line's start point.\n--   x2 (int, float): The x coordinate of the line's end point.\n--   y2 (int, float): The y coordiante of the line's end point.\n-- Returns:\n--   [(x1, y1), (x2, y2), (x3, y3), ...]\n-- Example:\n-- >>> getLine(0, 0, 6, 6)\n-- [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6)]\n-- >>> getLine(0, 0, 3, 6)\n-- [(0, 0), (0, 1), (1, 2), (1, 3), (2, 4), (2, 5), (3, 6)]\n-- >>> getLine(3, 3, -3, -3)\n-- [(3, 3), (2, 2), (1, 1), (0, 0), (-1, -1), (-2, -2), (-3, -3)]\nlocal function getLine(x1, y1, x2, y2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_388515_getLine.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = getLine\n    lu.assertEquals(candidate(0, 1, 0, 0), {{0, 1}, {0, 0}})\n    lu.assertEquals(candidate(3, 3, -3, -3), {{3, 3}, {2, 2}, {1, 1}, {0, 0}, {-1, -1}, {-2, -2}, {-3, -3}})\n    lu.assertEquals(candidate(0, 0, 3, 6), {{0, 0}, {0, 1}, {1, 2}, {1, 3}, {2, 4}, {2, 5}, {3, 6}})\n    lu.assertEquals(candidate(0, 0, 6, 6), {{0, 0}, {1, 1}, {2, 2}, {3, 3}, {4, 4}, {5, 5}, {6, 6}})\n    lu.assertEquals(candidate(2, 2, 1, 1), {{2, 2}, {1, 1}})\n    lu.assertEquals(candidate(0, 0, 0, 1), {{0, 0}, {0, 1}})\n    lu.assertEquals(candidate(3, 3, 0, 0), {{3, 3}, {2, 2}, {1, 1}, {0, 0}})\n    lu.assertEquals(candidate(0, 0, 0, 0), {{0, 0}})\n    lu.assertEquals(candidate(1, 0, 0, 0), {{1, 0}, {0, 0}})\n    lu.assertEquals(candidate(0, 0, 1, 1), {{0, 0}, {1, 1}})\n    lu.assertEquals(candidate(0, 0, 1, 0), {{0, 0}, {1, 0}})\n    lu.assertEquals(candidate(0, 0, 2, 2), {{0, 0}, {1, 1}, {2, 2}})\n    lu.assertEquals(candidate(0, 0, 2, -2), {{0, 0}, {1, -1}, {2, -2}})\n    lu.assertEquals(candidate(1, 1, 1, 1), {{1, 1}})\n    lu.assertEquals(candidate(0, 0, 2, 0), {{0, 0}, {1, 0}, {2, 0}})\n    lu.assertEquals(candidate(1, 1, 1, 0), {{1, 1}, {1, 0}})\n    lu.assertEquals(candidate(1, 1, 2, 2), {{1, 1}, {2, 2}})\n    lu.assertEquals(candidate(0, 0, 0, 2), {{0, 0}, {0, 1}, {0, 2}})\n    lu.assertEquals(candidate(0, 0, 2, 1), {{0, 0}, {1, 0}, {2, 1}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_388654_inverse_interleave", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if a % 2 == 0:\n-- #     return a + b % 2\n-- # else:\n-- #     return a - (b+1) % 2\n-- \n-- Given a coordinate where `a` has been interleaved and `b` hasn't, return \n-- the value that `a` would have at `b=0`.\nlocal function inverse_interleave(a, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_388654_inverse_interleave.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = inverse_interleave\n    lu.assertEquals(candidate(4, 2), 4)\n    lu.assertEquals(candidate(2, 2), 2)\n    lu.assertEquals(candidate(13, 7), 13)\n    lu.assertEquals(candidate(candidate(0, 0), 0), 0)\n    lu.assertEquals(candidate(0, 0), 0)\n    lu.assertEquals(candidate(2, 1), 3)\n    lu.assertEquals(candidate(5, 5), 5)\n    lu.assertEquals(candidate(candidate(2, 1), 0), 2)\n    lu.assertEquals(candidate(3, 3), 3)\n    lu.assertEquals(candidate(4, 0), 4)\n    lu.assertEquals(candidate(5, 1), 5)\n    lu.assertEquals(candidate(6, 0), 6)\n    lu.assertEquals(candidate(7, 7), 7)\n    lu.assertEquals(candidate(6, 1), 7)\n    lu.assertEquals(candidate(2, 0), 2)\n    lu.assertEquals(candidate(4, 1), 5)\n    lu.assertEquals(candidate(4, 4), 4)\n    lu.assertEquals(candidate(7, 1), 7)\n    lu.assertEquals(candidate(8, 8), 8)\n    lu.assertEquals(candidate(6, 6), 6)\n    lu.assertEquals(candidate(candidate(1, 0), 1), 1)\n    lu.assertEquals(candidate(0, 1), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_388924_escape_accent", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # mystr = mystr.replace('\\xc3\\xa9', 'e')  # e acute\n-- # mystr = mystr.replace('\\xc3\\x89', 'e')  # capital e acute\n-- # mystr = mystr.replace('\\xc3\\xae', 'i')  # i circumflex\n-- # return mystr\n-- \n--     escape the following accented character(s) (e/i) into non-accented equivalent\nlocal function escape_accent(mystr)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_388924_escape_accent.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = escape_accent\n    lu.assertEquals(candidate('\u00c3\u00a9'), 'e')\n    lu.assertEquals(candidate(\"L'a\u00c3\u00a9roport\"), \"L'aeroport\")\n    lu.assertEquals(candidate(\"D'a\u00c3\u00a9roport\"), \"D'aeroport\")\n    lu.assertEquals(candidate(\"L'a\u00c3\\x89roport\"), \"L'aeroport\")\n    lu.assertEquals(candidate('\u00c3\u00ae'), 'i')\n    lu.assertEquals(candidate(\"Je ne suis pas \u00e0 l'\u00e9cole. Je suis \u00e0 l'\u00e9tablissement. Je suis \u00e0 l'\u00e9cole. Je ne suis pas \u00e0 l'\u00e9tablissement.\"), candidate(\"Je ne suis pas \u00e0 l'\u00e9cole. Je suis \u00e0 l'\u00e9tablissement. Je suis \u00e0 l'\u00e9cole. Je ne suis pas \u00e0 l'\u00e9tablissement.\"))\n    lu.assertEquals(candidate(\"D'a\u00c3\\x89roport\"), \"D'aeroport\")\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_389434_findAnEven", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # first_even_n = 0\n-- # for num in l:\n-- #     if num % 2 == 0:\n-- #         first_even_n = num\n-- #         break\n-- # if first_even_n == 0:\n-- #     raise ValueError(\"l doesn't contain an even number\")\n-- # else:\n-- #     return first_even_n\n-- \n-- Assumes l is a list of ints\n-- Returns the first even num in l\n-- Raises ValueError if l doesn't contain an even num\nlocal function findAnEven(l)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_389434_findAnEven.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = findAnEven\n    lu.assertEquals(candidate({2, 3, 5}), 2)\n    lu.assertEquals(candidate({2, 4, 6, 8, 10}), 2)\n    lu.assertEquals(candidate({2, 3, 6}), 2)\n    lu.assertEquals(candidate({2, 3, 4}), 2)\n    lu.assertEquals(candidate({1, 2, 3}), 2)\n    lu.assertEquals(candidate({2, 5, 6}), 2)\n    lu.assertEquals(candidate({3, 5, 6}), 6)\n    lu.assertEquals(candidate({2, 4, 6, 8, 10, 12, 14, 16, 18}), 2)\n    lu.assertEquals(candidate({2, 4, 6}), 2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_390316_moffat_r", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return 2. * (beta - 1) / alpha ** 2 * (1 + (r/alpha) ** 2) ** (-beta)\n-- \n-- Moffat profile\n-- :param r: radial coordinate\n-- :param alpha:\n-- :param beta:\n-- :return:\nlocal function moffat_r(r, alpha, beta)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_390316_moffat_r.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = moffat_r\n    lu.assertEquals(candidate(2.0, 1.0, 1.0), 0.0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_391663_f_to_k", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if tf is not None:\n-- #     tk = 273.5 + ((tf - 32.0) * (5.0 / 9.0))\n-- #     return tk\n-- # else:\n-- #     return None\n-- \n--     Convierte temperaturas de Fahrenheit a Kelvin\n-- Parameters:\n--         tf : Temperatura en grados Fahrenheit\n-- Returns:\n--         tk : Temperatura en grados Kelvin\nlocal function f_to_k(tf)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_391663_f_to_k.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = f_to_k\n    lu.assertEquals(candidate(32), 273.5)\n    lu.assertEquals(candidate(None), None)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_391695_get_first_key", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for key, val in dictionary.items():\n-- #     if val == value:\n-- #         return key\n-- # return None\n-- \n-- Get first key in a dict for a given value.\n-- :param dict dictionary:\n-- :param string value:\nlocal function get_first_key(dictionary, value)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_391695_get_first_key.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_first_key\n    lu.assertEquals(candidate({['foo'] = 'bar', ['barfoo'] = 'foobar', ['foobar'] = 'barfoo'}, 'foobar'), 'barfoo')\n    lu.assertEquals(candidate({['foo'] = 'bar', ['foobar'] = 'barfoo'}, 'bar'), 'foo')\n    lu.assertEquals(candidate({['foo'] = 'bar', ['foobar'] = 'barfoo', ['bar'] = 'foo'}, 'bar'), 'foo')\n    lu.assertEquals(candidate({['foo'] = 'bar', ['barfoo'] = 'foobar', ['foobar'] = 'barfoo'}, 'bar'), 'foo')\n    lu.assertEquals(candidate({['foo'] = 'bar', ['foobar'] = 'barfoo', ['bar'] = 'foo'}, 'barfoo'), 'foobar')\n    lu.assertEquals(candidate({['foo'] = 'bar', ['foobar'] = 'barfoo'}, 'barfoo'), 'foobar')\n    lu.assertEquals(candidate({['foo'] = 'bar', ['foobar'] = 'barfoo', ['bar'] = 'foo'}, 'foo'), 'bar')\n    lu.assertEquals(candidate({['foo'] = 'bar', ['barfoo'] = 'foobar', ['foobar'] = 'barfoo'}, 'barfoo'), 'foobar')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_392592_startswith_word", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return len(phrase.split()) > 0 and phrase.split()[0] == startswith\n-- \n-- Check if a string starts with a word\n-- Basic startswith method doesn't separate into words, so checking if a string\n-- starts with '$foo' will return true for both '$foo' and '$fooooooo'.\n-- Parameters\n-- ==========\n-- phrase : str\n--     Phrase to check startswith against.\n-- startswith : str\n--     Word to check whether or not phrase starts with it.\n-- Returns\n-- =======\n-- bool\n--     True if the first word of phrase is startswith, False otherwise.\nlocal function startswith_word(phrase, startswith)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_392592_startswith_word.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = startswith_word\n    lu.assertEquals(candidate('test', 'test'), true)\n    lu.assertEquals(candidate('$bar $bar $foo', '$foo'), false)\n    lu.assertEquals(candidate('$foo $bar $foo', '$foo'), true)\n    lu.assertEquals(candidate('This is a $bar', '$foo'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_392962_is_prime", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if num == 0 or num == 1:\n-- #     return False\n-- # for x in range(2, num//2):\n-- #     if num % x == 0:\n-- #         return False\n-- # else:\n-- #     return True\n-- \n-- Returns True if the number is prime\n-- else False.\nlocal function is_prime(num)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_392962_is_prime.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_prime\n    lu.assertEquals(candidate(6), false)\n    lu.assertEquals(candidate(24), false)\n    lu.assertEquals(candidate(33), false)\n    lu.assertEquals(candidate(36), false)\n    lu.assertEquals(candidate(14), false)\n    lu.assertEquals(candidate(27), false)\n    lu.assertEquals(candidate(29), true)\n    lu.assertEquals(candidate(35), false)\n    lu.assertEquals(candidate(17), true)\n    lu.assertEquals(candidate(2), true)\n    lu.assertEquals(candidate(0), false)\n    lu.assertEquals(candidate(30), false)\n    lu.assertEquals(candidate(12), false)\n    lu.assertEquals(candidate(22), false)\n    lu.assertEquals(candidate(16), false)\n    lu.assertEquals(candidate(7), true)\n    lu.assertEquals(candidate(28), false)\n    lu.assertEquals(candidate(9), false)\n    lu.assertEquals(candidate(19), true)\n    lu.assertEquals(candidate(34), false)\n    lu.assertEquals(candidate(10), false)\n    lu.assertEquals(candidate(11), true)\n    lu.assertEquals(candidate(5), true)\n    lu.assertEquals(candidate(23), true)\n    lu.assertEquals(candidate(1), false)\n    lu.assertEquals(candidate(26), false)\n    lu.assertEquals(candidate(32), false)\n    lu.assertEquals(candidate(3), true)\n    lu.assertEquals(candidate(13), true)\n    lu.assertEquals(candidate(38), false)\n    lu.assertEquals(candidate(20), false)\n    lu.assertEquals(candidate(21), false)\n    lu.assertEquals(candidate(15), false)\n    lu.assertEquals(candidate(8), false)\n    lu.assertEquals(candidate(-10000000001), true)\n    lu.assertEquals(candidate(100), false)\n    lu.assertEquals(candidate(25), false)\n    lu.assertEquals(candidate(18), false)\n    lu.assertEquals(candidate(37), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_394041_escape", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # s = s.replace(\"&\", \"&amp;\")  # Must be done first!\n-- # s = s.replace(\"'\", \"&apos;\")\n-- # s = s.replace(\"<\", \"&lt;\")\n-- # s = s.replace(\">\", \"&gt;\")\n-- # s = s.replace('\"', \"&quot;\")\n-- # return s\n-- \n-- Replace special characters '&', \"'\", '<', '>' and '\"' by XML entities.\nlocal function escape(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_394041_escape.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = escape\n    lu.assertEquals(candidate('a>b'), 'a&gt;b')\n    lu.assertEquals(candidate('a<b'), 'a&lt;b')\n    lu.assertEquals(candidate('1 < 2'), '1 &lt; 2')\n    lu.assertEquals(candidate('one < two'), 'one &lt; two')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('<'), '&lt;')\n    lu.assertEquals(candidate('a\"b'), 'a&quot;b')\n    lu.assertEquals(candidate('\"1 < 2\"'), '&quot;1 &lt; 2&quot;')\n    lu.assertEquals(candidate(' a b c '), ' a b c ')\n    lu.assertEquals(candidate('\"'), '&quot;')\n    lu.assertEquals(candidate(\"'1 < 2'\"), '&apos;1 &lt; 2&apos;')\n    lu.assertEquals(candidate('<a>'), '&lt;a&gt;')\n    lu.assertEquals(candidate('>'), '&gt;')\n    lu.assertEquals(candidate(' '), ' ')\n    lu.assertEquals(candidate('abc '), 'abc ')\n    lu.assertEquals(candidate('foo'), 'foo')\n    lu.assertEquals(candidate('&'), '&amp;')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate(\"a'b\"), 'a&apos;b')\n    lu.assertEquals(candidate('a'), 'a')\n    lu.assertEquals(candidate(' abc'), ' abc')\n    lu.assertEquals(candidate(\"'\"), '&apos;')\n    lu.assertEquals(candidate(\"&<>'\"), '&amp;&lt;&gt;&apos;')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_395176_py_type_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return {\n-- #     'blob': 'bytes',\n-- #     'character': 'string',\n-- #     'double': 'float',\n-- #     'long': 'integer',\n-- #     'map': 'dict',\n-- #     'structure': 'dict',\n-- #     'timestamp': 'datetime',\n-- # }.get(type_name, type_name)\n-- \n-- Get the Python type name for a given model type.\n-- >>> py_type_name('list')\n-- 'list'\n-- >>> py_type_name('structure')\n-- 'dict'\n--     :rtype: string\nlocal function py_type_name(type_name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_395176_py_type_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = py_type_name\n    lu.assertEquals(candidate('list'), 'list')\n    lu.assertEquals(candidate('structure'), 'dict')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_396186_calculate_stations_adjoint", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # script = f\"ibrun -n 1 {py} -m seisflow.scripts.shared.get_stations_adjoint --stations_path {stations_path} --misfit_windows_directory {misfit_windows_directory} --output_directory {output_directory}; \\n\"\n-- # return script\n-- \n--     get the files STATIONS_ADJOINT.\nlocal function calculate_stations_adjoint(py, stations_path, misfit_windows_directory, output_directory)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_396186_calculate_stations_adjoint.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = calculate_stations_adjoint\n    lu.assertEquals(candidate('seisflow/scripts/shared/get_stations_adjoint.py', 'asdf', 'asdf', 'asdf'), 'ibrun -n 1 seisflow/scripts/shared/get_stations_adjoint.py -m seisflow.scripts.shared.get_stations_adjoint --stations_path asdf --misfit_windows_directory asdf --output_directory asdf; \\n')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_396471_comment_string_with_block", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not string:\n-- #     return string\n-- # beg, end = block_comment\n-- # return beg + ' ' + string.replace(beg, '').replace(end, '') + ' ' + end\n-- \n-- Return string commented using block comments\nlocal function comment_string_with_block(string, block_comment)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_396471_comment_string_with_block.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = comment_string_with_block\n    lu.assertEquals(candidate('', {'#', ''}), '')\n    lu.assertEquals(candidate('', {'/*', '*/'}), '')\n    lu.assertEquals(candidate('a', {'#begin', '#end'}), '#begin a #end')\n    lu.assertEquals(candidate('', None), '')\n    lu.assertEquals(candidate(None, {'#', ''}), None)\n    lu.assertEquals(candidate('abc', {'<!--', '-->'}), '<!-- abc -->')\n    lu.assertEquals(candidate('a = 1', {'/*', '*/'}), '/* a = 1 */')\n    lu.assertEquals(candidate('This is a string.', {'\"\"\"', '\"\"\"'}), '\"\"\" This is a string. \"\"\"')\n    lu.assertEquals(candidate('Hello World', {'/*', '*/'}), '/* Hello World */')\n    lu.assertEquals(candidate('', {'#begin', '#end'}), '')\n    lu.assertEquals(candidate('a = 1\\nb = 2\\nc = 3', {'/*', '*/'}), '/* a = 1\\nb = 2\\nc = 3 */')\n    lu.assertEquals(candidate('a', {'<!--', '-->'}), '<!-- a -->')\n    lu.assertEquals(candidate('This is a string.', {'/*', '*/'}), '/* This is a string. */')\n    lu.assertEquals(candidate('Hello World\\nHow are you?', {'/*', '*/'}), '/* Hello World\\nHow are you? */')\n    lu.assertEquals(candidate(None, {'/*', '*/'}), None)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_396730__remapLanguageCode", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if code == \"zh-Hans\":  # Simplified Chinese\n-- #     code = \"zh_CN\"\n-- # elif code == \"zh-Hant\":  # Traditional Chinese\n-- #     code = \"zh_TW\"\n-- # return code\n-- \n--     Remap certain language codes to others, per the localization team\nlocal function _remapLanguageCode(code)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_396730__remapLanguageCode.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _remapLanguageCode\n    lu.assertEquals(candidate('zh-Hans'), 'zh_CN')\n    lu.assertEquals(candidate(None), None)\n    lu.assertEquals(candidate('zh-Hant'), 'zh_TW')\n    lu.assertEquals(candidate('en_US'), 'en_US')\n    lu.assertEquals(candidate('en_GB'), 'en_GB')\n    lu.assertEquals(candidate('ja_JP'), 'ja_JP')\n    lu.assertEquals(candidate('en'), 'en')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_397294_prism_item_in_list", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for item_to_check in list_of_items_to_check:\n-- #     matches = [item_to_check[key] == key_contents for key, key_contents in qury_item.items()]\n-- #     if all(matches):\n-- #         return True\n-- # return False\n-- \n-- Compare a single prism specifiation to a list of PRISM files\n-- returned by pyPRISMClimate.prism_iterator\nlocal function prism_item_in_list(qury_item, list_of_items_to_check)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_397294_prism_item_in_list.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = prism_item_in_list\n    lu.assertEquals(candidate({['model'] = 'ACCESS1-3', ['experiment'] = 'rcp45', ['frequency'] = 'monthly', ['variable'] = 'ppt'}, {{['model'] = 'ACCESS1-3', ['experiment'] = 'rcp45', ['frequency'] = 'monthly', ['variable'] = 'tmax'}, {['model'] = 'ACCESS1-3', ['experiment'] = 'rcp45', ['frequency'] = 'monthly', ['variable'] = 'tmax'}, {['model'] = 'ACCESS1-3', ['experiment'] = 'historical', ['frequency'] = 'monthly', ['variable'] = 'ppt'}}), false)\n    lu.assertEquals(candidate({['model'] = 'ACCESS1-3', ['experiment'] = 'rcp45', ['frequency'] = 'monthly', ['variable'] = 'ppt'}, {{['model'] = 'ACCESS1-3', ['experiment'] = 'rcp45', ['frequency'] = 'monthly', ['variable'] = 'ppt'}, {['model'] = 'ACCESS1-3', ['experiment'] = 'rcp45', ['frequency'] = 'monthly', ['variable'] = 'tmax'}, {['model'] = 'ACCESS1-3', ['experiment'] = 'historical', ['frequency'] = 'monthly', ['variable'] = 'ppt'}}), true)\n    lu.assertEquals(candidate({['model'] = 'ACCESS1-3', ['experiment'] = 'rcp45', ['frequency'] = 'monthly', ['variable'] = 'ppt'}, {{['model'] = 'ACCESS1-3', ['experiment'] = 'rcp45', ['frequency'] = 'monthly', ['variable'] = 'ppt'}, {['model'] = 'ACCESS1-3', ['experiment'] = 'rcp45', ['frequency'] = 'monthly', ['variable'] = 'tmax'}, {['model'] = 'ACCESS1-3', ['experiment'] = 'historical', ['frequency'] = 'monthly', ['variable'] = 'ppt'}, {['model'] = 'ACCESS1-3', ['experiment'] = 'historical', ['frequency'] = 'monthly', ['variable'] = 'ppt'}}), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_3974_str_igrep", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return [i for (i, s) in enumerate(strs) if s.find(S) >= 0]\n-- # # return [i for (s,i) in zip(strs,xrange(len(strs))) if s.find(S) >= 0]\n-- \n-- Returns a list of the indices of the strings wherein the substring S\n-- is found.\nlocal function str_igrep(S, strs)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_3974_str_igrep.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = str_igrep\n    lu.assertEquals(candidate('', {''}), {0})\n    lu.assertEquals(candidate('abc', {'aa', 'ab', 'abc'}), {2})\n    lu.assertEquals(candidate('a', {}), {})\n    lu.assertEquals(candidate('b', {'a', 'b'}), {1})\n    lu.assertEquals(candidate('b', {'b'}), {0})\n    lu.assertEquals(candidate('', {}), {})\n    lu.assertEquals(candidate('a', {'a', 'b', 'c', 'a'}), {0, 3})\n    lu.assertEquals(candidate('a', {'a', 'b'}), {0})\n    lu.assertEquals(candidate('ab', {}), {})\n    lu.assertEquals(candidate('z', {}), {})\n    lu.assertEquals(candidate('a', {'a', 'a', 'a'}), {0, 1, 2})\n    lu.assertEquals(candidate('a', {'a'}), {0})\n    lu.assertEquals(candidate('z', {'aa', 'ab', 'abc'}), {})\n    lu.assertEquals(candidate('a', {'b', 'a'}), {1})\n    lu.assertEquals(candidate('b', {'a', 'b', 'c'}), {1})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_397801_make_msg", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"{}: {}:{}, {}\".format(err_or_warn, file, line, msg)\n-- \n-- Formats the error message with a file and line number that can be used by\n-- IDEs to quickly go to the exact line\nlocal function make_msg(err_or_warn, file, line, msg)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_397801_make_msg.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = make_msg\n    lu.assertEquals(candidate('warning', 'main.c', 10, 'variable used before assignment'), 'warning: main.c:10, variable used before assignment')\n    lu.assertEquals(candidate('error', 'test_messages.py', 27, 'this should be an error'), 'error: test_messages.py:27, this should be an error')\n    lu.assertEquals(candidate('error', 'main.c', 10, 'variable used before assignment'), 'error: main.c:10, variable used before assignment')\n    lu.assertEquals(candidate('warning', 'test_messages.py', 27, 'this should be a warning'), 'warning: test_messages.py:27, this should be a warning')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_397849_move_up", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # x, y = t\n-- # if x == 0:\n-- #     return None\n-- # else:\n-- #     return (x - 1, y)\n-- \n--  A method that takes coordinates of bomb's\n-- position and returns coordinates of neighbour\n-- located above the bomb. It returns None if\n-- there isn't such a neighbour \nlocal function move_up(t)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_397849_move_up.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = move_up\n    lu.assertEquals(candidate({1, 2}), {0, 2})\n    lu.assertEquals(candidate({3, 3}), {2, 3})\n    lu.assertEquals(candidate({1, 1}), {0, 1})\n    lu.assertEquals(candidate({0, 1}), None)\n    lu.assertEquals(candidate({3, 2}), {2, 2})\n    lu.assertEquals(candidate({1, 5}), {0, 5})\n    lu.assertEquals(candidate({5, 5}), {4, 5})\n    lu.assertEquals(candidate({3, 4}), {2, 4})\n    lu.assertEquals(candidate({1, 2}), {0, 2})\n    lu.assertEquals(candidate({5, 2}), {4, 2})\n    lu.assertEquals(candidate({1, 0}), {0, 0})\n    lu.assertEquals(candidate({4, 6}), {3, 6})\n    lu.assertEquals(candidate({0, 0}), None)\n    lu.assertEquals(candidate({0, 0}), None)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_397872_fix_vidshape", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # xmin, ymin = 0, 0\n-- # xmult = (res2[0]/res1[0])\n-- # ymult = (res2[1]/res1[1])\n-- # if xmult > ymult:\n-- #     xmin = int((res2[0]-(res1[0]*ymult))/2)\n-- # if ymult > xmult:\n-- #     ymin = int((res2[0]-(res1[0]*xmult))/2)\n-- # return xmin, ymin\n-- \n-- Compares two resolutions and get missing x and y coords\nlocal function fix_vidshape(res1, res2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_397872_fix_vidshape.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = fix_vidshape\n    lu.assertEquals(candidate({1920, 1080}, {640, 360}), {0, 0})\n    lu.assertEquals(candidate({1920, 1080}, {1280, 1080}), {0, 0})\n    lu.assertEquals(candidate({1920, 1080}, {1280, 720}), {0, 0})\n    lu.assertEquals(candidate({1920, 1080}, {640, 1080}), {0, 0})\n    lu.assertEquals(candidate({1920, 1080}, {1920, 1080}), {0, 0})\n    lu.assertEquals(candidate({1920, 1080}, {640, 540}), {0, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_398023_remove_underline", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # modified_params = {'def_': 'def', 'type_': 'type', 'format_': 'format'}\n-- # for key, value in modified_params.items():\n-- #     if key in params.keys():\n-- #         param_value = params.pop(key)\n-- #         params[value] = param_value\n-- # return params\n-- \n-- Remove the underline in reserved words.\n-- The parameters def and type are python reserved words so it is necessary\n-- to add a underline to use this words, this method remove the underline\n-- before make a http request.\n-- Args:\n--     params (dict): Url query parameters.\n-- Returns:\n--     (dict): Validated url query parameters.\nlocal function remove_underline(params)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_398023_remove_underline.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = remove_underline\n    lu.assertEquals(candidate({['type_'] = 'type', ['format_'] = 'format'}), {['type'] = 'type', ['format'] = 'format'})\n    lu.assertEquals(candidate({['def_'] = 'def', ['type_'] = 'type', ['format_'] = 'format'}), {['def'] = 'def', ['type'] = 'type', ['format'] = 'format'})\n    lu.assertEquals(candidate({['def_'] = 'def', ['type_'] = 'type', ['format_'] = 'format'}), {['def'] = 'def', ['type'] = 'type', ['format'] = 'format'})\n    lu.assertEquals(candidate({['def_'] = 'def', ['format_'] = 'format'}), {['def'] = 'def', ['format'] = 'format'})\n    lu.assertEquals(candidate({['def_'] = 'def', ['type_'] = 'type'}), {['def'] = 'def', ['type'] = 'type'})\n    lu.assertEquals(candidate({['def_'] = 'def', ['format_'] = 'format'}), {['def'] = 'def', ['format'] = 'format'})\n    lu.assertEquals(candidate({['def_'] = 'def'}), {['def'] = 'def'})\n    lu.assertEquals(candidate({}), {})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_398191__intersect", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return list(set(lst_a) & set(lst_b))\n-- \n--  return the intersection of two lists \nlocal function _intersect(lst_a, lst_b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_398191__intersect.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _intersect\n    lu.assertEquals(candidate({'a', 'b', 'c', 'd'}, {'z', 'y', 'x', 'w'}), {})\n    lu.assertEquals(candidate({'a', 'b', 'c', 'd'}, {'a', 'a', 'a', 'a'}), {'a'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_3983_find_match_characters", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # matched = []\n-- # last_index = 0\n-- # if not string or not pattern:\n-- #     return matched\n-- # if string[0] != pattern[0]:\n-- #     return matched\n-- # for c in pattern:\n-- #     index = string.find(c, last_index)\n-- #     if index < 0:\n-- #         return []\n-- #     matched.append((c, index))\n-- #     last_index = index + 1\n-- # return matched\n-- \n-- Find match match pattern string.\n-- Args:\n--     params: string\n--             pattern\n-- Returns:\n-- Raises:\nlocal function find_match_characters(string, pattern)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_3983_find_match_characters.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = find_match_characters\n    lu.assertEquals(candidate('ab', 'ab'), {{'a', 0}, {'b', 1}})\n    lu.assertEquals(candidate('', 'abc'), {})\n    lu.assertEquals(candidate('aabb', 'ab'), {{'a', 0}, {'b', 2}})\n    lu.assertEquals(candidate('abcz', 'abc'), {{'a', 0}, {'b', 1}, {'c', 2}})\n    lu.assertEquals(candidate('a', 'ab'), {})\n    lu.assertEquals(candidate('', 'a'), {})\n    lu.assertEquals(candidate('ab', 'aba'), {})\n    lu.assertEquals(candidate('abc', 'xyz'), {})\n    lu.assertEquals(candidate('a', ''), {})\n    lu.assertEquals(candidate('b', 'a'), {})\n    lu.assertEquals(candidate('aaab', 'ab'), {{'a', 0}, {'b', 3}})\n    lu.assertEquals(candidate('abab', 'babab'), {})\n    lu.assertEquals(candidate('', ''), {})\n    lu.assertEquals(candidate('a', 'a'), {{'a', 0}})\n    lu.assertEquals(candidate(None, None), {})\n    lu.assertEquals(candidate('abc', ''), {})\n    lu.assertEquals(candidate('aabb', 'aba'), {})\n    lu.assertEquals(candidate('babab', 'abab'), {})\n    lu.assertEquals(candidate('', ''), {})\n    lu.assertEquals(candidate('abc', 'abc'), {{'a', 0}, {'b', 1}, {'c', 2}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_398704__GenerateDeviceHVInfoStr", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # work on a copy\n-- # d = dict(hvinfo)\n-- # hvinfo_str = d.pop(\"driver\")\n-- # for k, v in d.items():\n-- #     hvinfo_str += \",%s=%s\" % (k, v)\n-- # return hvinfo_str\n-- \n-- Construct the -device option string for hvinfo dict\n-- PV disk: virtio-blk-pci,id=disk-1234,bus=pci.0,addr=0x9\n-- PV NIC:  virtio-net-pci,id=nic-1234,bus=pci.0,addr=0x9\n-- SG disk: scsi-generic,id=disk-1234,bus=scsi.0,channel=0,scsi-id=1,lun=0\n-- @type hvinfo: dict\n-- @param hvinfo: dictionary created by _GenerateDeviceHVInfo()\n-- @rtype: string\n-- @return: The constructed string to be passed along with a -device option\nlocal function _GenerateDeviceHVInfoStr(hvinfo)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_398704__GenerateDeviceHVInfoStr.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _GenerateDeviceHVInfoStr\n    lu.assertEquals(candidate({['driver'] = 'virtio-blk-pci', ['id'] = 'disk-1234', ['bus'] = 'pci.0', ['addr'] = '0x9', ['scsi-id'] = '1'}), 'virtio-blk-pci,id=disk-1234,bus=pci.0,addr=0x9,scsi-id=1')\n    lu.assertEquals(candidate({['driver'] = 'scsi-generic', ['id'] = 'disk-1234', ['bus'] = 'scsi.0', ['channel'] = '0', ['scsi-id'] = '1', ['lun'] = '0'}), 'scsi-generic,id=disk-1234,bus=scsi.0,channel=0,scsi-id=1,lun=0')\n    lu.assertEquals(candidate({['driver'] = 'virtio-net-pci', ['id'] = 'nic-1234', ['bus'] = 'pci.0'}), 'virtio-net-pci,id=nic-1234,bus=pci.0')\n    lu.assertEquals(candidate({['driver'] = 'virtio-blk-pci', ['id'] = 'disk-1234', ['bus'] = 'pci.0'}), 'virtio-blk-pci,id=disk-1234,bus=pci.0')\n    lu.assertEquals(candidate({['driver'] = 'scsi-generic', ['id'] = 'disk-1234', ['bus'] = 'scsi.0', ['channel'] = '0', ['scsi-id'] = '1', ['lun'] = '0'}), 'scsi-generic,id=disk-1234,bus=scsi.0,channel=0,scsi-id=1,lun=0')\n    lu.assertEquals(candidate({['driver'] = 'virtio-blk-pci', ['id'] = 'disk-1234', ['bus'] = 'pci.0', ['addr'] = '0x9'}), 'virtio-blk-pci,id=disk-1234,bus=pci.0,addr=0x9')\n    lu.assertEquals(candidate({['driver'] = 'virtio-net-pci', ['id'] = 'nic-1234', ['bus'] = 'pci.0', ['addr'] = '0x9'}), 'virtio-net-pci,id=nic-1234,bus=pci.0,addr=0x9')\n    lu.assertEquals(candidate({['driver'] = 'virtio-blk-pci', ['id'] = 'disk-1234', ['bus'] = 'pci.0', ['addr'] = '0x9'}), 'virtio-blk-pci,id=disk-1234,bus=pci.0,addr=0x9')\n    lu.assertEquals(candidate({['driver'] = 'virtio-net-pci', ['id'] = 'nic-1234', ['bus'] = 'pci.0', ['addr'] = '0x9'}), 'virtio-net-pci,id=nic-1234,bus=pci.0,addr=0x9')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_398734_insert", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # L = list(tup)\n-- # L[loc] = val\n-- # return tuple(L)\n-- \n-- >>> insert(('a', 'b', 'c'), 0, 'x')\n-- ('x', 'b', 'c')\nlocal function insert(tup, loc, val)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_398734_insert.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = insert\n    lu.assertEquals(candidate({'a', 'b', 'c'}, 0, 'x'), {'x', 'b', 'c'})\n    lu.assertEquals(candidate({'a', 'b', 'c'}, 2, 'x'), {'a', 'b', 'x'})\n    lu.assertEquals(candidate({'a', 'b', 'c'}, 0, 'x'), {'x', 'b', 'c'})\n    lu.assertEquals(candidate({'a', 'b', 'c'}, 2, 'z'), {'a', 'b', 'z'})\n    lu.assertEquals(candidate({'a', 'b', 'c'}, 1, 'x'), {'a', 'x', 'c'})\n    lu.assertEquals(candidate({'a', 'b', 'c'}, 1, 'y'), {'a', 'y', 'c'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_399266_expand_id_map", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # unmapped_ids = list(set(all_ids).difference(id_map.keys()))\n-- # for i in unmapped_ids:\n-- #     id_map[i] = i\n-- # return id_map\n-- \n--  Ensures all ids within all_ids are included as keys in the mapping \nlocal function expand_id_map(id_map, all_ids)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_399266_expand_id_map.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = expand_id_map\n    lu.assertEquals(candidate({}, {1, 2, 3}), {[1] = 1, [2] = 2, [3] = 3})\n    lu.assertEquals(candidate({[3] = 100}, {1, 2, 3, 4}), {[1] = 1, [2] = 2, [3] = 100, [4] = 4})\n    lu.assertEquals(candidate({[1] = 100, [2] = 100, [3] = 200}, {1, 2, 3}), {[1] = 100, [2] = 100, [3] = 200})\n    lu.assertEquals(candidate(dict(), {1, 2, 3}), {[1] = 1, [2] = 2, [3] = 3})\n    lu.assertEquals(candidate({[1] = 2}, {1, 2, 3}), {[1] = 2, [2] = 2, [3] = 3})\n    lu.assertEquals(candidate({[1] = 2}, {2, 3}), {[1] = 2, [2] = 2, [3] = 3})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_399332_volume_berbentuk_kubus", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return float(lebar * tinggi * panjang)\n-- \n-- kalkulasi volume kuboid\n-- >>> volume_berbentuk_kubus(1, 1, 1)\n-- 1.0\n-- >>> volume_berbentuk_kubus(1, 2, 3)\n-- 6.0\nlocal function volume_berbentuk_kubus(lebar, tinggi, panjang)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_399332_volume_berbentuk_kubus.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = volume_berbentuk_kubus\n    lu.assertEquals(candidate(1, 2, 3), 6.0)\n    lu.assertEquals(candidate(1, 1, 1), 1.0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_400319_from_time", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # def str_or_stars(i, length):\n-- #     if i is None:\n-- #         return \"*\" * length\n-- #     else:\n-- #         return str(i).rjust(length, \"0\")\n-- # wmi_time = \"\"\n-- # wmi_time += str_or_stars(year, 4)\n-- # wmi_time += str_or_stars(month, 2)\n-- # wmi_time += str_or_stars(day, 2)\n-- # wmi_time += str_or_stars(hours, 2)\n-- # wmi_time += str_or_stars(minutes, 2)\n-- # wmi_time += str_or_stars(seconds, 2)\n-- # wmi_time += \".\"\n-- # wmi_time += str_or_stars(microseconds, 6)\n-- # if timezone is None:\n-- #     wmi_time += \"+\"\n-- # else:\n-- #     try:\n-- #         int(timezone)\n-- #     except ValueError:\n-- #         wmi_time += \"+\"\n-- #     else:\n-- #         if timezone >= 0:\n-- #             wmi_time += \"+\"\n-- #         else:\n-- #             wmi_time += \"-\"\n-- #             timezone = abs(timezone)\n-- #             wmi_time += str_or_stars(timezone, 3)\n-- # return wmi_time\n-- \n-- Convenience wrapper to take a series of date/time elements and return a WMI time\n-- of the form `yyyymmddHHMMSS.mmmmmm+UUU`. All elements may be int, string or\n-- omitted altogether. If omitted, they will be replaced in the output string\n-- by a series of stars of the appropriate length.\n-- :param year: The year element of the date/time\n-- :param month: The month element of the date/time\n-- :param day: The day element of the date/time\n-- :param hours: The hours element of the date/time\n-- :param minutes: The minutes element of the date/time\n-- :param seconds: The seconds element of the date/time\n-- :param microseconds: The microseconds element of the date/time\n-- :param timezone: The timeezone element of the date/time\n-- :returns: A WMI datetime string of the form: `yyyymmddHHMMSS.mmmmmm+UUU`\nlocal function from_time(year, month, day, hours, minutes, seconds, microseconds, timezone)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_400319_from_time.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = from_time\n    lu.assertEquals(candidate(2015, 1, 1, 11, 3, 27, 123456, -1), '20150101110327.123456-001')\n    lu.assertEquals(candidate(2015, 1, 1, 11, 3, 27, 123456), '20150101110327.123456+')\n    lu.assertEquals(candidate(1979, 12, 31, 14, 59, 59, 999999, 0), '19791231145959.999999+')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_400320_find_last", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if s.find(sub) == -1:  # this is the return value for s.find if it's not there\n-- #     return None\n-- # else:\n-- #     ans = s.find(sub)  # found it, so save it\n-- #     ans2 = find_last(s[s.find(sub)+1:], sub)  # search remaining string for the sub\n-- #     if ans2 != None:  # if None, then it wasn't found, so the one already found is last\n-- #         ans += ans2 + 1  # move ans to next occurence, add 1 because 0 of the following string should be counted\n-- #     return ans\n-- \n-- s and sub are non-empty strings\n-- Returns the index of the last occurrence of sub in s.\n-- Returns None if sub does not occur in s\nlocal function find_last(s, sub)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_400320_find_last.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = find_last\n    lu.assertEquals(candidate('123123123', '123123123'), 0)\n    lu.assertEquals(candidate('123123123', '123'), 6)\n    lu.assertEquals(candidate('123123123', '0'), None)\n    lu.assertEquals(candidate('ababc', 'd'), None)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_400967_is_id", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return s and ' ' not in s.strip()\n-- \n--     Return True if `s` is some kind of id.\nlocal function is_id(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_400967_is_id.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_id\n    lu.assertEquals(candidate('a b c d e f g h i j k l m n o'), false)\n    lu.assertEquals(candidate('a b'), false)\n    lu.assertEquals(candidate('foo'), true)\n    lu.assertEquals(candidate('__hello_world'), true)\n    lu.assertEquals(candidate('a b c d e f'), false)\n    lu.assertEquals(candidate('a b c d e f g'), false)\n    lu.assertEquals(candidate(' a b'), false)\n    lu.assertEquals(candidate('x'), true)\n    lu.assertEquals(candidate('a b c d'), false)\n    lu.assertEquals(candidate('a b c d e f g h i j k l m n'), false)\n    lu.assertEquals(candidate('a b c d e f g h i j k l'), false)\n    lu.assertEquals(candidate('123 456'), false)\n    lu.assertEquals(candidate('a b c d e f g h i j k'), false)\n    lu.assertEquals(candidate('12345'), true)\n    lu.assertEquals(candidate('a b c d e f g h'), false)\n    lu.assertEquals(candidate('a b c d e f g h i'), false)\n    lu.assertEquals(candidate('a b c d e'), false)\n    lu.assertEquals(candidate('a b c d e f g h i j'), false)\n    lu.assertEquals(candidate('12345 6789'), false)\n    lu.assertEquals(candidate('a b '), false)\n    lu.assertEquals(candidate('hello_world'), true)\n    lu.assertEquals(candidate('123'), true)\n    lu.assertEquals(candidate('a'), true)\n    lu.assertEquals(candidate('foo'), true)\n    lu.assertEquals(candidate('a b c'), false)\n    lu.assertEquals(candidate('hello world'), false)\n    lu.assertEquals(candidate('_hello_world'), true)\n    lu.assertEquals(candidate('a b c d e f g h i j k l m'), false)\n    lu.assertEquals(candidate('foo bar'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_401452_control_start", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if cmd.lower() in (\"y\", \"yes\"):\n-- #     action = \"instructions\"\n-- # else:\n-- #     action = \"game\"\n-- # return action\n-- \n--     Controls the start state\nlocal function control_start(cmd)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_401452_control_start.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = control_start\n    lu.assertEquals(candidate('no'), 'game')\n    lu.assertEquals(candidate('No'), 'game')\n    lu.assertEquals(candidate('y'), 'instructions')\n    lu.assertEquals(candidate('yeS'), 'instructions')\n    lu.assertEquals(candidate('n'), 'game')\n    lu.assertEquals(candidate('Yes'), 'instructions')\n    lu.assertEquals(candidate('Y'), 'instructions')\n    lu.assertEquals(candidate('y'), 'instructions')\n    lu.assertEquals(candidate('yes'), 'instructions')\n    lu.assertEquals(candidate('YES'), 'instructions')\n    lu.assertEquals(candidate('n'), 'game')\n    lu.assertEquals(candidate('No'), 'game')\n    lu.assertEquals(candidate('yes'), 'instructions')\n    lu.assertEquals(candidate('no'), 'game')\n    lu.assertEquals(candidate('yEs'), 'instructions')\n    lu.assertEquals(candidate('Yes'), 'instructions')\n    lu.assertEquals(candidate('NO'), 'game')\n    lu.assertEquals(candidate('YES'), 'instructions')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_401768__quote_arg", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # If there is a quote in the string, assume relevants parts of the\n-- # # string are already quoted (e.g. '-I\"C:\\\\Program Files\\\\...\"')\n-- # if '\"' not in arg and ' ' in arg:\n-- #     return '\"%s\"' % arg\n-- # return arg\n-- \n--     Quote the argument for safe use in a shell command line.\nlocal function _quote_arg(arg)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_401768__quote_arg.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _quote_arg\n    lu.assertEquals(candidate('-I\"C:/Program Files/...\"'), '-I\"C:/Program Files/...\"')\n    lu.assertEquals(candidate('a'), 'a')\n    lu.assertEquals(candidate('\"foo\" bar'), '\"foo\" bar')\n    lu.assertEquals(candidate(\"'foo'\"), \"'foo'\")\n    lu.assertEquals(candidate('a b c'), '\"a b c\"')\n    lu.assertEquals(candidate('-I\"C:\\\\Program Files\\\\...\\\\include\"'), '-I\"C:\\\\Program Files\\\\...\\\\include\"')\n    lu.assertEquals(candidate('a'), 'a')\n    lu.assertEquals(candidate('C:\\\\Program Files\\\\Microsoft Visual Studio 12.0\\\\VC\\\\bin\\\\cl.exe'), '\"C:\\\\Program Files\\\\Microsoft Visual Studio 12.0\\\\VC\\\\bin\\\\cl.exe\"')\n    lu.assertEquals(candidate('a b'), '\"a b\"')\n    lu.assertEquals(candidate('\"foo bar\"'), '\"foo bar\"')\n    lu.assertEquals(candidate('\"'), '\"')\n    lu.assertEquals(candidate('-I\"C:/Program Files/.../include\"'), '-I\"C:/Program Files/.../include\"')\n    lu.assertEquals(candidate('C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 12.0\\\\VC\\\\bin\\\\cl.exe'), '\"C:\\\\Program Files (x86)\\\\Microsoft Visual Studio 12.0\\\\VC\\\\bin\\\\cl.exe\"')\n    lu.assertEquals(candidate('a\\\\\\\\b'), 'a\\\\\\\\b')\n    lu.assertEquals(candidate('\"foo\"'), '\"foo\"')\n    lu.assertEquals(candidate('-I\"C:\\\\Program Files\\\\...\"'), '-I\"C:\\\\Program Files\\\\...\"')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('a b c'), '\"a b c\"')\n    lu.assertEquals(candidate('a\\\\\\\\\\\\\\\\b'), 'a\\\\\\\\\\\\\\\\b')\n    lu.assertEquals(candidate('a\\\\\"b'), 'a\\\\\"b')\n    lu.assertEquals(candidate('a\\\\b'), 'a\\\\b')\n    lu.assertEquals(candidate('-I\"C:\\\\Program Files\\\\.../include\"'), '-I\"C:\\\\Program Files\\\\.../include\"')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_402036_get_item_details", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # details = {'Estimated Time of Delivery': 'Na', 'priority': 'Na', 'cost': 'Na', 'item': 'Na'}\n-- # if item == u'Medicines' or item == u'Medicine':\n-- #     details = {'priority': 'HP', 'cost': '450', 'Estimated Time of Delivery': '1', 'item': 'Medicines'}\n-- # elif item == u'Food':\n-- #     details = {'priority': 'MP', 'cost': '250', 'Estimated Time of Delivery': '3', \"item\": 'Food'}\n-- # elif item == u'Clothes':\n-- #     details = {'priority': 'LP', 'cost': '150', 'Estimated Time of Delivery': '5', 'item': 'Clothes'}\n-- # return details\n-- \n-- This function finds packgen details using item\n-- :param item: item is a string containing packgen content type\n-- :return: it returns dict of details of packgen.\nlocal function get_item_details(item)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_402036_get_item_details.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_item_details\n    lu.assertEquals(candidate('Clothes'), {['priority'] = 'LP', ['cost'] = '150', ['Estimated Time of Delivery'] = '5', ['item'] = 'Clothes'})\n    lu.assertEquals(candidate('Food'), {['Estimated Time of Delivery'] = '3', ['priority'] = 'MP', ['cost'] = '250', ['item'] = 'Food'})\n    lu.assertEquals(candidate('Medicine'), {['priority'] = 'HP', ['cost'] = '450', ['Estimated Time of Delivery'] = '1', ['item'] = 'Medicines'})\n    lu.assertEquals(candidate('Medicine'), {['Estimated Time of Delivery'] = '1', ['priority'] = 'HP', ['cost'] = '450', ['item'] = 'Medicines'})\n    lu.assertEquals(candidate('Medicines'), {['Estimated Time of Delivery'] = '1', ['priority'] = 'HP', ['cost'] = '450', ['item'] = 'Medicines'})\n    lu.assertEquals(candidate('Clothes'), {['Estimated Time of Delivery'] = '5', ['priority'] = 'LP', ['cost'] = '150', ['item'] = 'Clothes'})\n    lu.assertEquals(candidate('Medicines'), {['Estimated Time of Delivery'] = '1', ['priority'] = 'HP', ['cost'] = '450', ['item'] = 'Medicines'})\n    lu.assertEquals(candidate('Food'), {['Estimated Time of Delivery'] = '3', ['cost'] = '250', ['item'] = 'Food', ['priority'] = 'MP'})\n    lu.assertEquals(candidate('Food'), {['priority'] = 'MP', ['cost'] = '250', ['Estimated Time of Delivery'] = '3', ['item'] = 'Food'})\n    lu.assertEquals(candidate('Clothes'), {['Estimated Time of Delivery'] = '5', ['cost'] = '150', ['item'] = 'Clothes', ['priority'] = 'LP'})\n    lu.assertEquals(candidate('Medicines'), {['Estimated Time of Delivery'] = '1', ['cost'] = '450', ['item'] = 'Medicines', ['priority'] = 'HP'})\n    lu.assertEquals(candidate('Medicines'), {['priority'] = 'HP', ['cost'] = '450', ['Estimated Time of Delivery'] = '1', ['item'] = 'Medicines'})\n    lu.assertEquals(candidate('Food'), {['Estimated Time of Delivery'] = '3', ['priority'] = 'MP', ['cost'] = '250', ['item'] = 'Food'})\n    lu.assertEquals(candidate('Clothes'), {['Estimated Time of Delivery'] = '5', ['priority'] = 'LP', ['cost'] = '150', ['item'] = 'Clothes'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_402199_multi_bracket_validation", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not isinstance(string, str):\n-- #     raise TypeError('It\\'s not a string.')\n-- # brackets = {'{': 0, '}': 0, '[': 0, ']': 0, '(': 0, ')': 0}\n-- # for i in string:\n-- #     for key, val in brackets.items():\n-- #         if i == key:\n-- #             brackets[key] += 1\n-- # if brackets['{'] - brackets['}'] == 0 and\\\n-- #     brackets['['] - brackets[']'] == 0 and\\\n-- #    brackets['('] - brackets[')'] == 0:\n-- #     return True\n-- # return False\n-- \n--     This function return True if brackets are all matched up in string, and     return False if there is unmatched brackets.\nlocal function multi_bracket_validation(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_402199_multi_bracket_validation.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = multi_bracket_validation\n    lu.assertEquals(candidate('(hi)()'), true)\n    lu.assertEquals(candidate('[[[]()]]'), true)\n    lu.assertEquals(candidate('(hi)[(hi)]{[()]}'), true)\n    lu.assertEquals(candidate('(hi)[(hi)]{[()]}(hi)hi'), true)\n    lu.assertEquals(candidate('{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{]}}}}}}}}}}}}}}}}}}}}}}}}}}'), false)\n    lu.assertEquals(candidate('(hi)[(hi)]{[()]}(hi)hi(hi})'), false)\n    lu.assertEquals(candidate('(hi)[(hi)]'), true)\n    lu.assertEquals(candidate('{}'), true)\n    lu.assertEquals(candidate('[({})]'), true)\n    lu.assertEquals(candidate('[({})]([({})])'), true)\n    lu.assertEquals(candidate('(hi)[(hi)]{[()]}()'), true)\n    lu.assertEquals(candidate('[[()]]]'), false)\n    lu.assertEquals(candidate('{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{{}}}}}}}}}}}}}}}}}}}}}}}}'), false)\n    lu.assertEquals(candidate('[]'), true)\n    lu.assertEquals(candidate('[[{}]]'), true)\n    lu.assertEquals(candidate('hi())'), false)\n    lu.assertEquals(candidate('[[[]]]'), true)\n    lu.assertEquals(candidate('hi(hi)'), true)\n    lu.assertEquals(candidate('({[]})'), true)\n    lu.assertEquals(candidate('[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[['), false)\n    lu.assertEquals(candidate('{[()]}'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_402673__reconstruct_token", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if len(key) < 34 or key[-33] != \"_\":\n-- #     return\n-- # token = key[-32:]\n-- # try:\n-- #     int(token, 16)\n-- # except ValueError:\n-- #     return None\n-- # return token.lower()\n-- \n-- Reconstruct a token from a key in a graph ('SomeName_<token>')\nlocal function _reconstruct_token(key)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_402673__reconstruct_token.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _reconstruct_token\n    lu.assertEquals(candidate('foo_1234567'), None)\n    lu.assertEquals(candidate('a_11111'), None)\n    lu.assertEquals(candidate('<KEY>'), None)\n    lu.assertEquals(candidate('some_name_l'), None)\n    lu.assertEquals(candidate('some_name_n'), None)\n    lu.assertEquals(candidate('some_name_b'), None)\n    lu.assertEquals(candidate('foo_123456789012345678901234567890_123'), None)\n    lu.assertEquals(candidate('a_'), None)\n    lu.assertEquals(candidate('some_name_m'), None)\n    lu.assertEquals(candidate('a_b'), None)\n    lu.assertEquals(candidate('SomeName_aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'), 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa')\n    lu.assertEquals(candidate(''), None)\n    lu.assertEquals(candidate('some_name_j'), None)\n    lu.assertEquals(candidate('a_111111111'), None)\n    lu.assertEquals(candidate('some_name_'), None)\n    lu.assertEquals(candidate('some_name_d'), None)\n    lu.assertEquals(candidate('some_name_k'), None)\n    lu.assertEquals(candidate('someKey'), None)\n    lu.assertEquals(candidate('someKey_'), None)\n    lu.assertEquals(candidate('foo'), None)\n    lu.assertEquals(candidate('some_name__'), None)\n    lu.assertEquals(candidate('foo_123456789012345678901234567890'), None)\n    lu.assertEquals(candidate('a_11111111'), None)\n    lu.assertEquals(candidate('a'), None)\n    lu.assertEquals(candidate('foo_bar_baz'), None)\n    lu.assertEquals(candidate('some_name_i'), None)\n    lu.assertEquals(candidate('foo_123'), None)\n    lu.assertEquals(candidate('foo_123456789'), None)\n    lu.assertEquals(candidate('a_1111111111111111'), None)\n    lu.assertEquals(candidate('foo_123_456'), None)\n    lu.assertEquals(candidate('foo_'), None)\n    lu.assertEquals(candidate('SomeName_0123456789abcdef0123456789abcdef'), '0123456789abcdef0123456789abcdef')\n    lu.assertEquals(candidate('some_name_p'), None)\n    lu.assertEquals(candidate('some_name_a'), None)\n    lu.assertEquals(candidate('foo_bar'), None)\n    lu.assertEquals(candidate('some_name_o'), None)\n    lu.assertEquals(candidate('a_111111111111111111'), None)\n    lu.assertEquals(candidate(''), None)\n    lu.assertEquals(candidate('some_name_f'), None)\n    lu.assertEquals(candidate('someKey_0123456789abcdef0123456789abcdef0123456789abcdef'), None)\n    lu.assertEquals(candidate('foo_12345678'), None)\n    lu.assertEquals(candidate('foo'), None)\n    lu.assertEquals(candidate('foo_bar'), None)\n    lu.assertEquals(candidate('some_name'), None)\n    lu.assertEquals(candidate('some_name_h'), None)\n    lu.assertEquals(candidate('some_name_g'), None)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_402950_remove_ssml_tags", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # output_text = ''\n-- # inside_chevrons = False\n-- # for c in parm_text:\n-- #     if c == '<':\n-- #         inside_chevrons = True\n-- #     elif c == '>':\n-- #         inside_chevrons = False\n-- #     elif not inside_chevrons:\n-- #         output_text += c\n-- # return output_text\n-- \n-- Remove the SSML tags from parm text. The tags are surrounded by <chevrons>.\nlocal function remove_ssml_tags(parm_text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_402950_remove_ssml_tags.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = remove_ssml_tags\n    lu.assertEquals(candidate('<emphasis>Hello, World!</emphasis>'), 'Hello, World!')\n    lu.assertEquals(candidate('<break time=\"5s\"/>'), '')\n    lu.assertEquals(candidate('Hello, World!'), 'Hello, World!')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_403090_calculate_height", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # px_height = y_max - y_min\n-- # person_height = distance * px_height / focal_y\n-- # return person_height\n-- \n--     Calculate real person height in centimeters.\nlocal function calculate_height(distance, y_max, y_min, focal_y)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_403090_calculate_height.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = calculate_height\n    lu.assertEquals(candidate(0, 0, 0, 2000), 0)\n    lu.assertEquals(candidate(0, 0, 1000, 2000), 0)\n    lu.assertEquals(candidate(1000, 0, 0, 2000), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_403991_node_vertex_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return '{}.vtx[{}]'.format(mesh_node, vertex_id)\n-- \n-- Returns the full name of the given node vertex\n-- :param mesh_node: str\n-- :param vertex_id: int\n-- :return: str\nlocal function node_vertex_name(mesh_node, vertex_id)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_403991_node_vertex_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = node_vertex_name\n    lu.assertEquals(candidate('mesh', 3), 'mesh.vtx[3]')\n    lu.assertEquals(candidate('MeshNode', 0), 'MeshNode.vtx[0]')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_404803_isQualified", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return name.find(':') != -1\n-- \n--     Check if a property name is qualified\nlocal function isQualified(name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_404803_isQualified.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = isQualified\n    lu.assertEquals(candidate(''), false)\n    lu.assertEquals(candidate('prop'), false)\n    lu.assertEquals(candidate('xmlns'), false)\n    lu.assertEquals(candidate('xlink:type'), true)\n    lu.assertEquals(candidate('xlink:title'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_405193_getStatusWord", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # statusWord = 'owned'\n-- # if status == 0:\n-- #     return 'wished'\n-- # elif status == 1:\n-- #     return 'ordered'\n-- # return statusWord\n-- \n-- Returns the status word from the status code. \nlocal function getStatusWord(status)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_405193_getStatusWord.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = getStatusWord\n    lu.assertEquals(candidate(1), 'ordered')\n    lu.assertEquals(candidate(0), 'wished')\n    lu.assertEquals(candidate(2), 'owned')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_405292_factorial", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if num == 1:\n-- #     return num\n-- # else:\n-- #     return num * factorial(num - 1)\n-- \n-- This function calculates factorial of a number recursively.\n-- n! = n*(n-1)*(n-2)*...*2*1\n-- Parameters\n-- ----------\n-- num : uint64\n--     Input positive integer to calcuate factorial.\n-- Returns\n-- -------\n-- uint64\n--     Factorial of input positive integer.\nlocal function factorial(num)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_405292_factorial.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = factorial\n    lu.assertEquals(candidate(8), 40320)\n    lu.assertEquals(candidate(5), 120)\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(17), 355687428096000)\n    lu.assertEquals(candidate(4), 24)\n    lu.assertEquals(candidate(20), 2432902008176640000)\n    lu.assertEquals(candidate(11), 39916800)\n    lu.assertEquals(candidate(12), 479001600)\n    lu.assertEquals(candidate(10), 3628800)\n    lu.assertEquals(candidate(19), 121645100408832000)\n    lu.assertEquals(candidate(15), 1307674368000)\n    lu.assertEquals(candidate(13), 6227020800)\n    lu.assertEquals(candidate(9), 362880)\n    lu.assertEquals(candidate(18), 6402373705728000)\n    lu.assertEquals(candidate(3), 6)\n    lu.assertEquals(candidate(6), 720)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(14), 87178291200)\n    lu.assertEquals(candidate(7), 5040)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_405585_dms2dd", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # dd = float(degrees) + float(minutes)/60 + float(seconds)/(60*60)\n-- # if direction == 'S' or direction == 'W':\n-- #     dd *= -1\n-- # return dd\n-- \n-- http://en.proft.me/2015/09/20/converting-latitude-and-longitude-decimal-values-p/\n-- Args:\n--     degrees:\n--     minutes:\n--     seconds:\n--     direction:\n-- Returns:\nlocal function dms2dd(degrees, minutes, seconds, direction)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_405585_dms2dd.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = dms2dd\n    lu.assertEquals(candidate(40, 0, 0, 'N'), 40.0)\n    lu.assertEquals(candidate(41, 30, 30, 'N'), 41.50833333333333)\n    lu.assertEquals(candidate(31, 30, 30, 'W'), -31.508333333333333)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_406279_realign_shifted_streams", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # max_shift = max(shifts)\n-- # if max_shift > 0:\n-- #     shift_durations, shift_F0s = shifts\n-- #     tokens = tokens[:-max_shift]\n-- #     durations = durations[shift_durations:]\n-- #     if shift_durations < max_shift:\n-- #         durations = durations[: -(max_shift - shift_durations)]\n-- #     if F0s is not None:\n-- #         F0s = F0s[shift_F0s:]\n-- #         if shift_F0s < max_shift:\n-- #             F0s = F0s[: -(max_shift - shift_F0s)]\n-- # assert len(tokens) == len(durations), f\"{len(tokens)} =! {len(durations)}\"\n-- # if F0s is not None:\n-- #     assert len(tokens) == len(F0s), f\"{len(tokens)} =! {len(F0s)}\"\n-- # return tokens, durations, F0s\n-- \n-- Durations are shifted by 1, F0 by 2\n-- >>> tokens = [\"<s>\", \"t1\",  \"t2\", \"t3\", \"</s>\", \"x\", \"x\"]\n-- >>> durations = [\"<0>\", \"<0>\", \"d1\", \"d2\", \"d3\", \"<0>\", \"x\"]\n-- >>> F0s    = [\"<0>\", \"<0>\", \"<0>\", \"f1\", \"f2\", \"f3\", \"<0>\"]\n-- >>> shifts = [1,2]\n-- >>> realign_shifted_streams(tokens, durations, F0s, shifts)\n-- (['<s>', 't1', 't2', 't3', '</s>'], ['<0>', 'd1', 'd2', 'd3', '<0>'], ['<0>', 'f1', 'f2', 'f3', '<0>'])\nlocal function realign_shifted_streams(tokens, durations, F0s, shifts)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_406279_realign_shifted_streams.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = realign_shifted_streams\n    lu.assertEquals(candidate({'<s>', 't1', 't2', 't3', '</s>', 'x', 'x'}, {'<0>', '<0>', 'd1', 'd2', 'd3', '<0>', 'x'}, {'<0>', '<0>', '<0>', 'f1', 'f2', 'f3', '<0>'}, {1, 2}), {{'<s>', 't1', 't2', 't3', '</s>'}, {'<0>', 'd1', 'd2', 'd3', '<0>'}, {'<0>', 'f1', 'f2', 'f3', '<0>'}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_406838_wait_for_line", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # trailing_ops = ['+', '-', '/', '*', '^', '=',\n-- #                 '>', '<', '/;', '/:', '/.', '&&', '||']\n-- # if any(input_string.rstrip().endswith(op) for op in trailing_ops):\n-- #     return True\n-- # brackets = [('(', ')'), ('[', ']'), ('{', '}')]\n-- # kStart, kEnd, stack = 0, 1, []\n-- # for char in input_string:\n-- #     for bracketPair in brackets:\n-- #         if char == bracketPair[kStart]:\n-- #             stack.append(char)\n-- #         elif char == bracketPair[kEnd]:\n-- #             if len(stack) == 0:\n-- #                 return False\n-- #             if stack.pop() != bracketPair[kStart]:\n-- #                 # Brackets are not balanced, but return False so that a\n-- #                 # parse error can be raised\n-- #                 return False\n-- # if len(stack) == 0 and input_string.count('\"') % 2 == 0:\n-- #     return False\n-- # return True\n-- \n-- Should the intepreter wait for another line of input or try to evaluate the\n-- current line as is.\nlocal function wait_for_line(input_string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_406838_wait_for_line.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = wait_for_line\n    lu.assertEquals(candidate('a = b = c = d = e = f'), false)\n    lu.assertEquals(candidate('(a = b = c) = (d = e = f)'), false)\n    lu.assertEquals(candidate('a + b * (c + d) + e'), false)\n    lu.assertEquals(candidate('a + b * (c + d) + e = f + g'), false)\n    lu.assertEquals(candidate('a = 1, b = 2;'), false)\n    lu.assertEquals(candidate('return 42;'), false)\n    lu.assertEquals(candidate('(a + b * (c + d)) + e = f + g'), false)\n    lu.assertEquals(candidate('(a + b) * (c + d) + e = f + g'), false)\n    lu.assertEquals(candidate('a = b = c'), false)\n    lu.assertEquals(candidate('let a = 1, b = 2;'), false)\n    lu.assertEquals(candidate('a + b * (c + d)'), false)\n    lu.assertEquals(candidate('a = b = c = d = e'), false)\n    lu.assertEquals(candidate('function my_func(a, b) { return a + b; }'), false)\n    lu.assertEquals(candidate('a = b = c = d'), false)\n    lu.assertEquals(candidate('a + b * (c + d) + e = f'), false)\n    lu.assertEquals(candidate('2 + 2;'), false)\n    lu.assertEquals(candidate('var a = 1, b = 2;'), false)\n    lu.assertEquals(candidate('a + ((b * c))'), false)\n    lu.assertEquals(candidate('a + (b * c)'), false)\n    lu.assertEquals(candidate('for (i = 0; i < 10; i++) { print(i); }'), false)\n    lu.assertEquals(candidate('print(1 + 1) && print(2 + 2)'), false)\n    lu.assertEquals(candidate('print(\"Hello, World!\");'), false)\n    lu.assertEquals(candidate('a + b * c'), false)\n    lu.assertEquals(candidate('my_func(1, 2);'), false)\n    lu.assertEquals(candidate('a = b = c = (d = e = f)'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_40712_generate_base_code", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if naval_base and scout_base:\n-- #     return \"A\"\n-- # elif scout_base and pirate_base:\n-- #     return \"G\"\n-- # elif naval_base:\n-- #     return \"N\"\n-- # elif pirate_base:\n-- #     return \"P\"\n-- # elif scout_base:\n-- #     return \"S\"\n-- # else:\n-- #     return \" \"\n-- \n-- Returns a base code, one of:\n-- A -- Naval Base and Scout Base/Outpost\n-- G -- Scout Base/Outpost and Pirate Base\n-- N -- Naval Base\n-- P -- Pirate Base\n-- S -- Scout Base/Outpost\nlocal function generate_base_code(naval_base, scout_base, pirate_base)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_40712_generate_base_code.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = generate_base_code\n    lu.assertEquals(candidate(true, false, true), 'N')\n    lu.assertEquals(candidate(true, true, false), 'A')\n    lu.assertEquals(candidate(true, true, true), 'A')\n    lu.assertEquals(candidate(true, false, false), 'N')\n    lu.assertEquals(candidate(false, false, false), ' ')\n    lu.assertEquals(candidate(false, true, false), 'S')\n    lu.assertEquals(candidate(false, true, true), 'G')\n    lu.assertEquals(candidate(false, false, true), 'P')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_408233_parse", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # query_split = query.split(\"+\")\n-- # return \" \".join(query_split)\n-- \n-- Parse URL query into correct SQL syntax.\n-- :param query: SQL query pulled from URL argument.\n-- :return: Parsed query converted to valid SQL syntax.\nlocal function parse(query)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_408233_parse.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse\n    lu.assertEquals(candidate(\"SELECT * FROM film WHERE length > 60 AND title LIKE '%n'\"), \"SELECT * FROM film WHERE length > 60 AND title LIKE '%n'\")\n    lu.assertEquals(candidate(\"SELECT * FROM film WHERE length > 60 AND title LIKE '%n' AND year = 2004\"), \"SELECT * FROM film WHERE length > 60 AND title LIKE '%n' AND year = 2004\")\n    lu.assertEquals(candidate('SELECT * FROM table_name WHERE column_name = value'), 'SELECT * FROM table_name WHERE column_name = value')\n    lu.assertEquals(candidate(\"SELECT * FROM table_name WHERE column_name LIKE '%value%'\"), \"SELECT * FROM table_name WHERE column_name LIKE '%value%'\")\n    lu.assertEquals(candidate('SELECT * FROM table_name WHERE column_name <= value'), 'SELECT * FROM table_name WHERE column_name <= value')\n    lu.assertEquals(candidate(\"SELECT * FROM table_name WHERE column_name NOT LIKE '%value%'\"), \"SELECT * FROM table_name WHERE column_name NOT LIKE '%value%'\")\n    lu.assertEquals(candidate('SELECT * FROM table_name WHERE column_name >= value'), 'SELECT * FROM table_name WHERE column_name >= value')\n    lu.assertEquals(candidate('SELECT * FROM table_name WHERE column_name!= value'), 'SELECT * FROM table_name WHERE column_name!= value')\n    lu.assertEquals(candidate(\"SELECT * FROM table_name WHERE column_name IN ('value1', 'value2')\"), \"SELECT * FROM table_name WHERE column_name IN ('value1', 'value2')\")\n    lu.assertEquals(candidate('SELECT * FROM table_name WHERE column_name < value'), 'SELECT * FROM table_name WHERE column_name < value')\n    lu.assertEquals(candidate('SELECT * FROM table_name WHERE column_name > value'), 'SELECT * FROM table_name WHERE column_name > value')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_409149_turnOff", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # x1 = int(array[0])\n-- # y1 = int(array[1])\n-- # x2 = int(array[2])\n-- # y2 = int(array[3])\n-- # xmin = min(x1, x2)\n-- # xmax = max(x1, x2)\n-- # ymin = min(y1, y2)\n-- # ymax = max(y1, y2)\n-- # for i in range(xmin, xmax + 1):\n-- #     for j in range(ymin, ymax + 1):\n-- #         a2d[i][j] = 0\n-- # return a2d\n-- \n--     function to turn off lights with given coordinates from array\nlocal function turnOff(array, a2d)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_409149_turnOff.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = turnOff\n    lu.assertEquals(candidate({2, 1, 3, 1}, {{0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0}}), {{0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_40937_extract_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return module_name.split('_v')[0]\n-- \n-- extracts the module name.\n-- :param module_name:\n-- :return: <str> the module name without the version.\nlocal function extract_name(module_name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_40937_extract_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = extract_name\n    lu.assertEquals(candidate('baz_v3'), 'baz')\n    lu.assertEquals(candidate('foo_v2'), 'foo')\n    lu.assertEquals(candidate('foo'), 'foo')\n    lu.assertEquals(candidate('bar'), 'bar')\n    lu.assertEquals(candidate('baz'), 'baz')\n    lu.assertEquals(candidate('qux'), 'qux')\n    lu.assertEquals(candidate('bar_v1'), 'bar')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_409443_get_network_bits", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # count = 0\n-- # for i in mask_binary:\n-- #     if int(i) == 1:\n-- #         count += 1\n-- # return count\n-- \n-- Returns number of network bits of given mask\n-- :param mask_binary: Subnet Mask in binary\n-- :return: Number of network bits\nlocal function get_network_bits(mask_binary)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_409443_get_network_bits.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_network_bits\n    lu.assertEquals(candidate('11111111111111111111111111111111'), 32)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_409453_fibonacci_by_index", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if (index < 0):\n-- #     raise IndexError(\"Invalid Fibonacci Index\")\n-- # else:\n-- #     i = 0    # Value at index i\n-- #     ip1 = 1  # Value at index i+1\n-- #     for count in range(index):\n-- #         ip2 = i + ip1  # i+2 is the sum of i and i+1\n-- #         i = ip1        # Move i+1 to i\n-- #         ip1 = ip2      # Move i+2 to i+1\n-- #     return i\n-- \n-- Get the value at the index of the Fibonacci series.\n-- Austin W. Milne @awbmilne  <austin.milne@uwaterloo.ca>\n-- Parameters\n-- ----------\n-- index: number\n--     The index of the Fibonacci series to return.\n-- Returns\n-- -------\n-- The value from the Fibonacci series at the given index.\nlocal function fibonacci_by_index(index)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_409453_fibonacci_by_index.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = fibonacci_by_index\n    lu.assertEquals(candidate(2), 1)\n    lu.assertEquals(candidate(12), 144)\n    lu.assertEquals(candidate(9), 34)\n    lu.assertEquals(candidate(10), 55)\n    lu.assertEquals(candidate(13), 233)\n    lu.assertEquals(candidate(4), 3)\n    lu.assertEquals(candidate(11), 89)\n    lu.assertEquals(candidate(19), 4181)\n    lu.assertEquals(candidate(22), 17711)\n    lu.assertEquals(candidate(14), 377)\n    lu.assertEquals(candidate(16), 987)\n    lu.assertEquals(candidate(15), 610)\n    lu.assertEquals(candidate(6), 8)\n    lu.assertEquals(candidate(24), 46368)\n    lu.assertEquals(candidate(3), 2)\n    lu.assertEquals(candidate(17), 1597)\n    lu.assertEquals(candidate(23), 28657)\n    lu.assertEquals(candidate(18), 2584)\n    lu.assertEquals(candidate(20), 6765)\n    lu.assertEquals(candidate(21), 10946)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(7), 13)\n    lu.assertEquals(candidate(8), 21)\n    lu.assertEquals(candidate(5), 5)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_409856_MILLISECOND", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return {'$millisecond': expression}\n-- \n-- Returns the millisecond portion of a date as an integer between 0 and 999.\n-- See https://docs.mongodb.com/manual/reference/operator/aggregation/millisecond/\n-- for more details\n-- :param expression: expression or variable of a Date, a Timestamp, or an ObjectID\n-- :return: Aggregation operator\nlocal function MILLISECOND(expression)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_409856_MILLISECOND.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = MILLISECOND\n    lu.assertEquals(candidate(2017), {['$millisecond'] = 2017})\n    lu.assertEquals(candidate(1437400400000), {['$millisecond'] = 1437400400000})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_410378_tickDiff", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # tDiff = t2 - t1\n-- # if tDiff < 0:\n-- #     tDiff += (1 << 32)\n-- # return tDiff\n-- \n-- Returns the microsecond difference between two ticks.\n-- t1:= the earlier tick\n-- t2:= the later tick\n-- ...\n-- print(pigpio.tickDiff(4294967272, 12))\n-- 36\n-- ...\nlocal function tickDiff(t1, t2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_410378_tickDiff.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = tickDiff\n    lu.assertEquals(candidate(1, 2), 1)\n    lu.assertEquals(candidate(-1, 0), 1)\n    lu.assertEquals(candidate(1, 1), 0)\n    lu.assertEquals(candidate(0, 3), 3)\n    lu.assertEquals(candidate(123456, 123456), 0)\n    lu.assertEquals(candidate(123456, 123457), 1)\n    lu.assertEquals(candidate(0, 1), 1)\n    lu.assertEquals(candidate(4294967272, 12), 36)\n    lu.assertEquals(candidate(0, 0), 0)\n    lu.assertEquals(candidate(0, 2), 2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_41072_rename_candidate_hugo", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # name_expr = candidate.split(\".\")\n-- # base_name = name_expr[0]\n-- # if base_name in renamings:\n-- #     base_name = renamings[base_name]\n-- # name_expr[0] = base_name\n-- # result = \".\".join(name_expr)\n-- # return result\n-- \n-- Renames a candidate name according to a renaming map.\nlocal function rename_candidate_hugo(candidate, renamings)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_41072_rename_candidate_hugo.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = rename_candidate_hugo\n    lu.assertEquals(candidate('HLA-A*01:01:01:01', {['HLA-A*01:01:01:01'] = 'HLA-A*01:01:01:01_NewName'}), 'HLA-A*01:01:01:01_NewName')\n    lu.assertEquals(candidate('A.a.a.a.a', {['A'] = 'B'}), 'B.a.a.a.a')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_410947_arxiv_url_sanitizer", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # if its an arxiv pdf url then\n-- # if url.find(\"pdf\") != -1:\n-- #     url = url.replace(\"/pdf\", \"/abs\")\n-- #     url = url.replace(\".pdf\", \"\")\n-- # return url\n-- \n-- as of now, just converts \n-- arxiv.org/pdf/ to arxiv.org/abs\nlocal function arxiv_url_sanitizer(url)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_410947_arxiv_url_sanitizer.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = arxiv_url_sanitizer\n    lu.assertEquals(candidate('http://www.arxiv.org/pdf/cond-mat/0712.1784.pdf'), 'http://www.arxiv.org/abs/cond-mat/0712.1784')\n    lu.assertEquals(candidate('https://arxiv.org/pdf/2004.09813v23.pdf'), 'https://arxiv.org/abs/2004.09813v23')\n    lu.assertEquals(candidate('https://arxiv.org/pdf/cond-mat/0712.1784.pdf'), 'https://arxiv.org/abs/cond-mat/0712.1784')\n    lu.assertEquals(candidate('https://arxiv.org/pdf/cond-mat/0712.1784v1.pdf'), 'https://arxiv.org/abs/cond-mat/0712.1784v1')\n    lu.assertEquals(candidate('https://arxiv.org/pdf/2004.09813v2.pdf'), 'https://arxiv.org/abs/2004.09813v2')\n    lu.assertEquals(candidate('https://arxiv.org/abs/2012.12345.pdf'), 'https://arxiv.org/abs/2012.12345')\n    lu.assertEquals(candidate('http://www.arxiv.org/pdf/cond-mat/0712.1784v1.pdf'), 'http://www.arxiv.org/abs/cond-mat/0712.1784v1')\n    lu.assertEquals(candidate('https://arxiv.org/abs/cond-mat/0712.1784'), 'https://arxiv.org/abs/cond-mat/0712.1784')\n    lu.assertEquals(candidate('https://arxiv.org/pdf/2004.09813.pdf'), 'https://arxiv.org/abs/2004.09813')\n    lu.assertEquals(candidate('https://arxiv.org/pdf/math.GT/0611008.pdf'), 'https://arxiv.org/abs/math.GT/0611008')\n    lu.assertEquals(candidate('http://arxiv.org/pdf/1702.06188'), 'http://arxiv.org/abs/1702.06188')\n    lu.assertEquals(candidate('http://arxiv.org/pdf/1702.06188.pdf'), 'http://arxiv.org/abs/1702.06188')\n    lu.assertEquals(candidate('https://arxiv.org/pdf/2012.12345.pdf'), 'https://arxiv.org/abs/2012.12345')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_411394_append_default_columns", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # columns = []\n-- # for i in range(start, num):\n-- #     columns.append(\"col_\" + str(i + 1))\n-- # return columns\n-- \n-- appends num string to a list as col_[index]\nlocal function append_default_columns(start, num)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_411394_append_default_columns.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = append_default_columns\n    lu.assertEquals(candidate(0, 1), {'col_1'})\n    lu.assertEquals(candidate(0, 2), {'col_1', 'col_2'})\n    lu.assertEquals(candidate(10, 0), {})\n    lu.assertEquals(candidate(0, 2), {'col_1', 'col_2'})\n    lu.assertEquals(candidate(3, 0), {})\n    lu.assertEquals(candidate(2, 5), {'col_3', 'col_4', 'col_5'})\n    lu.assertEquals(candidate(0, 0), {})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_411424__re_word_boundary", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # we can't use \\b as it chokes on unicode. however \\W seems to be okay\n-- # # as shorthand for [^0-9A-Za-z_].\n-- # return r\"(^|\\W)%s(\\W|$)\" % (r,)\n-- \n-- Adds word boundary characters to the start and end of an\n-- expression to require that the match occur as a whole word,\n-- but do so respecting the fact that strings starting or ending\n-- with non-word characters will change word boundaries.\nlocal function _re_word_boundary(r)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_411424__re_word_boundary.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _re_word_boundary\n    lu.assertEquals(candidate('\\\\B'), '(^|\\\\W)\\\\B(\\\\W|$)')\n    lu.assertEquals(candidate('\\\\W+'), candidate('\\\\W+'))\n    lu.assertEquals(candidate('a?'), '(^|\\\\W)a?(\\\\W|$)')\n    lu.assertEquals(candidate('[a-zA-Z]'), '(^|\\\\W)[a-zA-Z](\\\\W|$)')\n    lu.assertEquals(candidate('a[b-]b'), '(^|\\\\W)a[b-]b(\\\\W|$)')\n    lu.assertEquals(candidate('a'), '(^|\\\\W)a(\\\\W|$)')\n    lu.assertEquals(candidate('[cat]'), '(^|\\\\W)[cat](\\\\W|$)')\n    lu.assertEquals(candidate('a[]b'), '(^|\\\\W)a[]b(\\\\W|$)')\n    lu.assertEquals(candidate('[abc]'), '(^|\\\\W)[abc](\\\\W|$)')\n    lu.assertEquals(candidate('[a]'), '(^|\\\\W)[a](\\\\W|$)')\n    lu.assertEquals(candidate('[\\\\d\\\\w]'), '(^|\\\\W)[\\\\d\\\\w](\\\\W|$)')\n    lu.assertEquals(candidate('[ab]'), '(^|\\\\W)[ab](\\\\W|$)')\n    lu.assertEquals(candidate('foo1'), '(^|\\\\W)foo1(\\\\W|$)')\n    lu.assertEquals(candidate('foo\\\\b'), '(^|\\\\W)foo\\\\b(\\\\W|$)')\n    lu.assertEquals(candidate('foo-bar'), '(^|\\\\W)foo-bar(\\\\W|$)')\n    lu.assertEquals(candidate('a[b-Xb-]b'), '(^|\\\\W)a[b-Xb-]b(\\\\W|$)')\n    lu.assertEquals(candidate('cat\\\\s+'), '(^|\\\\W)cat\\\\s+(\\\\W|$)')\n    lu.assertEquals(candidate('\\\\\\\\'), '(^|\\\\W)\\\\\\\\(\\\\W|$)')\n    lu.assertEquals(candidate('x'), '(^|\\\\W)x(\\\\W|$)')\n    lu.assertEquals(candidate('cat'), '(^|\\\\W)cat(\\\\W|$)')\n    lu.assertEquals(candidate('foo\\\\bfoo'), '(^|\\\\W)foo\\\\bfoo(\\\\W|$)')\n    lu.assertEquals(candidate('\\\\d'), '(^|\\\\W)\\\\d(\\\\W|$)')\n    lu.assertEquals(candidate('[^abc]'), '(^|\\\\W)[^abc](\\\\W|$)')\n    lu.assertEquals(candidate('\\\\w'), '(^|\\\\W)\\\\w(\\\\W|$)')\n    lu.assertEquals(candidate('cat\\\\|'), '(^|\\\\W)cat\\\\|(\\\\W|$)')\n    lu.assertEquals(candidate('a+b'), '(^|\\\\W)a+b(\\\\W|$)')\n    lu.assertEquals(candidate('.*'), '(^|\\\\W).*(\\\\W|$)')\n    lu.assertEquals(candidate('\\\\w+'), '(^|\\\\W)\\\\w+(\\\\W|$)')\n    lu.assertEquals(candidate('a\\\\sb'), '(^|\\\\W)a\\\\sb(\\\\W|$)')\n    lu.assertEquals(candidate('abc'), '(^|\\\\W)abc(\\\\W|$)')\n    lu.assertEquals(candidate('cat\\\\\\\\'), '(^|\\\\W)cat\\\\\\\\(\\\\W|$)')\n    lu.assertEquals(candidate('cat\\\\\\\\\\\\\\\\'), '(^|\\\\W)cat\\\\\\\\\\\\\\\\(\\\\W|$)')\n    lu.assertEquals(candidate('a\\\\b'), '(^|\\\\W)a\\\\b(\\\\W|$)')\n    lu.assertEquals(candidate('\\\\[cat\\\\]'), '(^|\\\\W)\\\\[cat\\\\](\\\\W|$)')\n    lu.assertEquals(candidate('a[\\\\s]b'), '(^|\\\\W)a[\\\\s]b(\\\\W|$)')\n    lu.assertEquals(candidate('1foo'), '(^|\\\\W)1foo(\\\\W|$)')\n    lu.assertEquals(candidate('(\\\\w+|\\\\\\\\)'), '(^|\\\\W)(\\\\w+|\\\\\\\\)(\\\\W|$)')\n    lu.assertEquals(candidate('foo'), '(^|\\\\W)foo(\\\\W|$)')\n    lu.assertEquals(candidate('[^ab]'), '(^|\\\\W)[^ab](\\\\W|$)')\n    lu.assertEquals(candidate('123'), '(^|\\\\W)123(\\\\W|$)')\n    lu.assertEquals(candidate('foo-bar-'), '(^|\\\\W)foo-bar-(\\\\W|$)')\n    lu.assertEquals(candidate('\\\\]'), '(^|\\\\W)\\\\](\\\\W|$)')\n    lu.assertEquals(candidate('\\\\['), '(^|\\\\W)\\\\[(\\\\W|$)')\n    lu.assertEquals(candidate('a[b-gB-G]b'), '(^|\\\\W)a[b-gB-G]b(\\\\W|$)')\n    lu.assertEquals(candidate('foo1bar'), '(^|\\\\W)foo1bar(\\\\W|$)')\n    lu.assertEquals(candidate('a*'), '(^|\\\\W)a*(\\\\W|$)')\n    lu.assertEquals(candidate('foo_bar1_'), '(^|\\\\W)foo_bar1_(\\\\W|$)')\n    lu.assertEquals(candidate('foo_bar1'), '(^|\\\\W)foo_bar1(\\\\W|$)')\n    lu.assertEquals(candidate('\\\\\\\\cat'), '(^|\\\\W)\\\\\\\\cat(\\\\W|$)')\n    lu.assertEquals(candidate('[^a]'), '(^|\\\\W)[^a](\\\\W|$)')\n    lu.assertEquals(candidate('\\\\W'), candidate('\\\\W'))\n    lu.assertEquals(candidate('a_b'), '(^|\\\\W)a_b(\\\\W|$)')\n    lu.assertEquals(candidate('\\\\d+'), '(^|\\\\W)\\\\d+(\\\\W|$)')\n    lu.assertEquals(candidate('\\\\W'), '(^|\\\\W)\\\\W(\\\\W|$)')\n    lu.assertEquals(candidate('foo-'), '(^|\\\\W)foo-(\\\\W|$)')\n    lu.assertEquals(candidate('foo_bar'), '(^|\\\\W)foo_bar(\\\\W|$)')\n    lu.assertEquals(candidate('\\\\b'), '(^|\\\\W)\\\\b(\\\\W|$)')\n    lu.assertEquals(candidate('!'), '(^|\\\\W)!(\\\\W|$)')\n    lu.assertEquals(candidate('\\\\w+'), candidate('\\\\w+'))\n    lu.assertEquals(candidate('foo-_bar'), '(^|\\\\W)foo-_bar(\\\\W|$)')\n    lu.assertEquals(candidate('a\\\\.b'), '(^|\\\\W)a\\\\.b(\\\\W|$)')\n    lu.assertEquals(candidate('a[^]b]'), '(^|\\\\W)a[^]b](\\\\W|$)')\n    lu.assertEquals(candidate('1foo_bar1'), '(^|\\\\W)1foo_bar1(\\\\W|$)')\n    lu.assertEquals(candidate('[a-zA-Z0-9_]'), '(^|\\\\W)[a-zA-Z0-9_](\\\\W|$)')\n    lu.assertEquals(candidate('the'), '(^|\\\\W)the(\\\\W|$)')\n    lu.assertEquals(candidate('1foo_bar'), '(^|\\\\W)1foo_bar(\\\\W|$)')\n    lu.assertEquals(candidate('[a-z]'), '(^|\\\\W)[a-z](\\\\W|$)')\n    lu.assertEquals(candidate('a.b'), '(^|\\\\W)a.b(\\\\W|$)')\n    lu.assertEquals(candidate('foo;bar'), '(^|\\\\W)foo;bar(\\\\W|$)')\n    lu.assertEquals(candidate('a[b-g]b'), '(^|\\\\W)a[b-g]b(\\\\W|$)')\n    lu.assertEquals(candidate('a/b'), '(^|\\\\W)a/b(\\\\W|$)')\n    lu.assertEquals(candidate('a[b-Za-y]b'), '(^|\\\\W)a[b-Za-y]b(\\\\W|$)')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_411437__prt_mode_from_unfolding", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if unfolding == 0:\n-- #     return 'fixed'\n-- # else:\n-- #     return 'staggered'\n-- \n--  Return 'fixed' or 'staggered' depending on unfolding flag \nlocal function _prt_mode_from_unfolding(unfolding)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_411437__prt_mode_from_unfolding.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _prt_mode_from_unfolding\n    lu.assertEquals(candidate(0), 'fixed')\n    lu.assertEquals(candidate(1), 'staggered')\n    lu.assertEquals(candidate(-1), 'staggered')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_411619_order_of_execution", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # actions = 0\n-- # r = [*range(1, 1 + count, 1)]\n-- # removed = []\n-- # pos = 0\n-- # while len(r) != 0:\n-- #     pos = (pos + (k - 1)) % len(r)\n-- #     removed.append(r.pop(pos))\n-- #     actions += 1\n-- # print(f\"actions: {actions}\")\n-- # return removed\n-- \n-- This one is linear time.\n-- :param count:\n-- :param k:\n-- :return:\nlocal function order_of_execution(count, k)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_411619_order_of_execution.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = order_of_execution\n    lu.assertEquals(candidate(3, 1), {1, 2, 3})\n    lu.assertEquals(candidate(2, 3), {1, 2})\n    lu.assertEquals(candidate(2, 1), {1, 2})\n    lu.assertEquals(candidate(5, 1), {1, 2, 3, 4, 5})\n    lu.assertEquals(candidate(6, 1), {1, 2, 3, 4, 5, 6})\n    lu.assertEquals(candidate(4, 1), {1, 2, 3, 4})\n    lu.assertEquals(candidate(1, 2), {1})\n    lu.assertEquals(candidate(10, 1), {1, 2, 3, 4, 5, 6, 7, 8, 9, 10})\n    lu.assertEquals(candidate(1, 3), {1})\n    lu.assertEquals(candidate(3, 3), {3, 1, 2})\n    lu.assertEquals(candidate(1, 1), {1})\n    lu.assertEquals(candidate(3, 2), {2, 1, 3})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_41197_contains_word", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return f\" {w} \" in f\" {s} \"\n-- \n--     Checks whether a string contains a certain word\nlocal function contains_word(s, w)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_41197_contains_word.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = contains_word\n    lu.assertEquals(candidate('This is a string with a word in it', 'word'), true)\n    lu.assertEquals(candidate('This is a string with a word in it', 'dog'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_41279_get_attrs", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return foreground + (background << 4) + style\n-- \n-- Get foreground and background attributes.\nlocal function get_attrs(foreground, background, style)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_41279_get_attrs.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_attrs\n    lu.assertEquals(candidate(0, 0, 2), 2)\n    lu.assertEquals(candidate(0, 0, 1), 1)\n    lu.assertEquals(candidate(0, 0, 0), 0)\n    lu.assertEquals(candidate(0, 0, 0), 0)\n    lu.assertEquals(candidate(0, 0, 3), 3)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_413062_matrix_add", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # result = []\n-- # for each in range(len(list1)):\n-- #     inner_list = []\n-- #     for item in range(len(list1[each])):\n-- #         inner_list.append(list1[each][item] + list2[each][item])\n-- #     result.append(inner_list)\n-- # return result\n-- \n-- Accepts two lists-of-lists of numbers and returns one\n-- list-of-lists with each of the corresponding numbers in the\n-- two given lists-of-lists added together.\n-- Example:\n-- >>> matrix1 = [[1, -2], [-3, 4]]\n-- >>> matrix2 = [[2, -1], [0, -1]]\n-- >>> add(matrix1, matrix2)\n-- [[3, -3], [-3, 3]]\n-- >>> matrix1 = [[1, -2, 3], [-4, 5, -6], [7, -8, 9]]\n-- >>> matrix2 = [[1, 1, 0], [1, -2, 3], [-2, 2, -2]]\n-- >>> add(matrix1, matrix2)\n-- [[2, -1, 3], [-3, 3, -3], [5, -6, 7]]\nlocal function matrix_add(list1, list2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_413062_matrix_add.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = matrix_add\n    lu.assertEquals(candidate({{1, -2, 3}, {-4, 5, -6}, {7, -8, 9}}, {{1, 1, 0}, {1, -2, 3}, {-2, 2, -2}}), {{2, -1, 3}, {-3, 3, -3}, {5, -6, 7}})\n    lu.assertEquals(candidate({{1, -2, 3}, {-4, 5, -6}, {7, -8, 9}}, {{1, 1, 0}, {1, -2, 3}, {-2, 2, -2}}), {{2, -1, 3}, {-3, 3, -3}, {5, -6, 7}})\n    lu.assertEquals(candidate({{1, -2}, {-3, 4}}, {{2, -1}, {0, -1}}), {{3, -3}, {-3, 3}})\n    lu.assertEquals(candidate({{1, 2, -3}, {4, -5, 6}}, {{-1, 1, 2}, {-1, 2, 3}, {4, 5, 6}}), {{0, 3, -1}, {3, -3, 9}})\n    lu.assertEquals(candidate({{1, -2}, {-3, 4}}, {{2, -1}, {0, -1}}), {{3, -3}, {-3, 3}})\n    lu.assertEquals(candidate({{1, -2}, {-3, 4}}, {{2, -1}, {0, -1}}), {{3, -3}, {-3, 3}})\n    lu.assertEquals(candidate({{1, 0, 1, 0}, {0, 1, 0, 1}, {1, 0, 1, 0}, {0, 1, 0, 1}}, {{1, 1, 1, 1}, {1, 1, 1, 1}, {1, 1, 1, 1}, {1, 1, 1, 1}}), {{2, 1, 2, 1}, {1, 2, 1, 2}, {2, 1, 2, 1}, {1, 2, 1, 2}})\n    lu.assertEquals(candidate({{1, -2, 3}, {-4, 5, -6}, {7, -8, 9}}, {{1, 1, 0}, {1, -2, 3}, {-2, 2, -2}}), {{2, -1, 3}, {-3, 3, -3}, {5, -6, 7}})\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}}, {{-1, -2, -3}, {-4, -5, -6}, {-7, -8, -9}}), {{0, 0, 0}, {0, 0, 0}, {0, 0, 0}})\n    lu.assertEquals(candidate({{1, -2}, {-3, 4}}, {{2, -1}, {0, -1}}), {{3, -3}, {-3, 3}})\n    lu.assertEquals(candidate({{1, -2, 3}, {-4, 5, -6}, {7, -8, 9}}, {{1, 1, 0}, {1, -2, 3}, {-2, 2, -2}}), {{2, -1, 3}, {-3, 3, -3}, {5, -6, 7}})\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}}, {{-1, -2, -3}, {-4, -5, -6}, {-7, -8, -9}}), {{0, 0, 0}, {0, 0, 0}, {0, 0, 0}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_413287_ConvertPrivateIpv6GoogleAccess", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # choices_to_enum = {\n-- #     'DISABLE':\n-- #         'DISABLE_GOOGLE_ACCESS',\n-- #     'ENABLE_BIDIRECTIONAL_ACCESS':\n-- #         'ENABLE_BIDIRECTIONAL_ACCESS_TO_GOOGLE',\n-- #     'ENABLE_OUTBOUND_VM_ACCESS':\n-- #         'ENABLE_OUTBOUND_VM_ACCESS_TO_GOOGLE',\n-- # }\n-- # return choices_to_enum.get(choice)\n-- \n-- Return PrivateIpv6GoogleAccess enum defined in mixer.\n-- Args:\n--   choice: Enum value of PrivateIpv6GoogleAccess defined in gcloud.\nlocal function ConvertPrivateIpv6GoogleAccess(choice)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_413287_ConvertPrivateIpv6GoogleAccess.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = ConvertPrivateIpv6GoogleAccess\n    lu.assertEquals(candidate('ENABLE_OUTBOUND_VM_ACCESS'), 'ENABLE_OUTBOUND_VM_ACCESS_TO_GOOGLE')\n    lu.assertEquals(candidate('DISABLE'), 'DISABLE_GOOGLE_ACCESS')\n    lu.assertEquals(candidate('ENABLE_BIDIRECTIONAL_ACCESS'), 'ENABLE_BIDIRECTIONAL_ACCESS_TO_GOOGLE')\n    lu.assertEquals(candidate('ENABLE_BIDIRECTIONAL_ACCESS'), 'ENABLE_BIDIRECTIONAL_ACCESS_TO_GOOGLE')\n    lu.assertEquals(candidate('ENABLE_OUTBOUND_VM_ACCESS'), 'ENABLE_OUTBOUND_VM_ACCESS_TO_GOOGLE')\n    lu.assertEquals(candidate('DISABLE'), 'DISABLE_GOOGLE_ACCESS')\n    lu.assertEquals(candidate(None), None)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_413434_song_decoder", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return ' '.join((song.replace(\"WUB\", \" \").strip()).split())\n-- \n-- Return decoded song string\nlocal function song_decoder(song)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_413434_song_decoder.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = song_decoder\n    lu.assertEquals(candidate('AWUB'), 'A')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate(candidate('WUBWUBWUBWUBWUBAWUBBWUBC')), 'A B C')\n    lu.assertEquals(candidate(candidate('WUBAWUBBWUBCWUB')), 'A B C')\n    lu.assertEquals(candidate('AWUBWUBWUBBWUBWUBWUBC'), 'A B C')\n    lu.assertEquals(candidate(candidate('WUBAWUBBWUBC')), 'A B C')\n    lu.assertEquals(candidate(candidate('WUBWUBWUBWUBWUBAWUBBWUBCWUB')), 'A B C')\n    lu.assertEquals(candidate(candidate('WUBWUBWUBAWUBBWUBC')), 'A B C')\n    lu.assertEquals(candidate(candidate('WUBWUBWUBWUBWUBWUBWUBWUBWUBWUB')), '')\n    lu.assertEquals(candidate('AWUBWUBWUBBWUBWUBWUBC'), 'A B C')\n    lu.assertEquals(candidate(candidate('WUBWUBAWUBBWUBC')), 'A B C')\n    lu.assertEquals(candidate(candidate('WUBWUBWUBWUBWUBWUBAWUBBWUBC')), 'A B C')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('WUBAWUBBWUBCWUB'), 'A B C')\n    lu.assertEquals(candidate(candidate('WUBWUBWUBAWUBBWUBCWUBWUB')), 'A B C')\n    lu.assertEquals(candidate(candidate('WUBWUBWUBWUBWUBWUBWUBWUBWUB')), '')\n    lu.assertEquals(candidate('WUBAWUBBWUBCWUB'), 'A B C')\n    lu.assertEquals(candidate('WUBAWUB'), 'A')\n    lu.assertEquals(candidate('AWUBBWUBC'), 'A B C')\n    lu.assertEquals(candidate('WUBWUB'), '')\n    lu.assertEquals(candidate(candidate('AWUBBWUBC')), 'A B C')\n    lu.assertEquals(candidate(candidate('AWUBWUBWUBBWUBWUBWUBC')), 'A B C')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_41418_get_api_interfaces_name_by_type", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # interface_name_list = []\n-- # for interface in api_interfaces:\n-- #     if_name = interface.get(key_name)\n-- #     if if_name and interface.get('type') == type_str:\n-- #         interface_name_list.append(if_name)\n-- # return interface_name_list\n-- \n-- Extract interface names from Interfaces API response by type\n-- :param api_interfaces: Interfaces API response\n-- :param type_str: Type string to match\n-- :param key_name: Optional - Key name to use (default 'name')\n-- :return: List of Interface names matching type_str\nlocal function get_api_interfaces_name_by_type(api_interfaces, type_str, key_name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_41418_get_api_interfaces_name_by_type.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_api_interfaces_name_by_type\n    lu.assertEquals(candidate({{['name'] = 'Ethernet0', ['type'] = 'iana-if-type:ethernetCsmacd'}, {['name'] = 'Ethernet4', ['type'] = 'iana-if-type:ethernetCsmacd'}}, 'iana-if-type:ethernetCsmacd'), {'Ethernet0', 'Ethernet4'})\n    lu.assertEquals(candidate({{['name'] = 'Ethernet0', ['type'] = 'iana-if-type:ethernetCsmacd'}, {['name'] = 'Ethernet4', ['type'] = 'iana-if-type:ethernetCsmacd'}}, 'iana-if-type:ethernetCsmacd', 'name'), {'Ethernet0', 'Ethernet4'})\n    lu.assertEquals(candidate({{['name'] = 'Ethernet0', ['type'] = 'iana-if-type:ethernetCsmacd'}, {['name'] = 'Ethernet4', ['type'] = 'iana-if-type:ethernetCsmacd'}}, 'iana-if-type:ethernetCsmacd', 'type'), {'iana-if-type:ethernetCsmacd', 'iana-if-type:ethernetCsmacd'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_414823_clean_whitespace", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # import re\n-- # # Replace linebreaks with spaces\n-- # text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n-- # # Remove any leeding or trailing whitespace\n-- # text = text.strip()\n-- # # Remove consecutive spaces\n-- # text = re.sub(' +', ' ', text)\n-- # return text\n-- \n--     Remove any extra whitespace and line breaks as needed.\nlocal function clean_whitespace(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_414823_clean_whitespace.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = clean_whitespace\n    lu.assertEquals(candidate('one\\r\\r\\r\\ntwo'), 'one two')\n    lu.assertEquals(candidate('one\\r\\n\\t\\t\\ntwo'), 'one two')\n    lu.assertEquals(candidate('one\\n\\r\\n\\ntwo\\n'), 'one two')\n    lu.assertEquals(candidate('one\\ntwo'), 'one two')\n    lu.assertEquals(candidate('one\\t\\t\\r\\n\\r\\n\\n\\n\\ntwo\\r\\n'), 'one two')\n    lu.assertEquals(candidate(' '), '')\n    lu.assertEquals(candidate('one\\n\\r\\r\\n\\t\\t\\ntwo\\n\\t\\t'), 'one two')\n    lu.assertEquals(candidate('one\\r\\n\\n\\ntwo'), 'one two')\n    lu.assertEquals(candidate(' one '), 'one')\n    lu.assertEquals(candidate('one\\n\\r\\r\\ntwo'), 'one two')\n    lu.assertEquals(candidate('one '), 'one')\n    lu.assertEquals(candidate('  one  '), 'one')\n    lu.assertEquals(candidate(' one'), 'one')\n    lu.assertEquals(candidate('one'), 'one')\n    lu.assertEquals(candidate('one\\n\\ntwo\\n'), 'one two')\n    lu.assertEquals(candidate('one\\r\\n\\r\\r\\ntwo\\r\\n'), 'one two')\n    lu.assertEquals(candidate('one\\t\\t\\t\\ttwo'), 'one two')\n    lu.assertEquals(candidate('one\\n\\ntwo'), 'one two')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_415327_escape_bytes", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # x = value7_uint64be\n-- # x0 = x & 0x000000000FFFFFFF\n-- # x1 = x & 0x00FFFFFFF0000000\n-- # x = x0 | (x1 << 4)\n-- # x0 = x & 0x00003FFF00003FFF\n-- # x1 = x & 0x0FFFC0000FFFC000\n-- # x = x0 | (x1 << 2)\n-- # x0 = x & 0x007F007F007F007F\n-- # x1 = x & 0x3F803F803F803F80\n-- # x = x0 | (x1 << 1) | 0x8080808080808080\n-- # return x\n-- \n-- Escapes seven bytes to eight bytes.\n-- Args:\n--     value7_uint64be(int): Bytes as a 56-bit bigendian unsigned integer.\n-- Returns:\n--     int: Escaped bytes as a 64-bit bigendian unsigned integer.\nlocal function escape_bytes(value7_uint64be)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_415327_escape_bytes.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = escape_bytes\n    lu.assertEquals(candidate(72057594037927935), 18446744073709551615)\n    lu.assertEquals(candidate(0), 9259542123273814144)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_415365_chop_at", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # pos = s.find(sub)\n-- # if pos == -1:\n-- #     return s\n-- # if inclusive:\n-- #     return s[:pos+len(sub)]\n-- # return s[:pos]\n-- \n-- Truncate string ``s`` at the first occurrence of ``sub``.\n-- If ``inclusive`` is true, truncate just after ``sub`` rather than at it.\n-- >>> chop_at(\"plutocratic brats\", \"rat\")\n-- 'plutoc'\n-- >>> chop_at(\"plutocratic brats\", \"rat\", True)\n-- 'plutocrat'\nlocal function chop_at(s, sub, inclusive)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_415365_chop_at.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = chop_at\n    lu.assertEquals(candidate('rat', 'bar'), 'rat')\n    lu.assertEquals(candidate('foo bar', 'bar'), 'foo ')\n    lu.assertEquals(candidate('plutocratic brats', 'bar', true), 'plutocratic brats')\n    lu.assertEquals(candidate('', 'rat', true), '')\n    lu.assertEquals(candidate('plutocratic brats', 'rat'), 'plutoc')\n    lu.assertEquals(candidate('rat', 'rat'), '')\n    lu.assertEquals(candidate('a', 'b'), 'a')\n    lu.assertEquals(candidate('abc', 'bc'), 'a')\n    lu.assertEquals(candidate('a', 'b'), 'a')\n    lu.assertEquals(candidate('abc', 'c'), 'ab')\n    lu.assertEquals(candidate('a', 'a'), '')\n    lu.assertEquals(candidate('', 'rat'), '')\n    lu.assertEquals(candidate('plutocratic brats', 'bar'), 'plutocratic brats')\n    lu.assertEquals(candidate('plutocratic brats', 'tat', true), 'plutocratic brats')\n    lu.assertEquals(candidate('rat', 'bar', true), 'rat')\n    lu.assertEquals(candidate('plutocratic brats', 'rat', true), 'plutocrat')\n    lu.assertEquals(candidate('plutocratic brats', 'rat'), 'plutoc')\n    lu.assertEquals(candidate('abc', 'ab'), '')\n    lu.assertEquals(candidate('a', 'b', true), 'a')\n    lu.assertEquals(candidate('a', 'a'), '')\n    lu.assertEquals(candidate('plutocratic brats', 'tat'), 'plutocratic brats')\n    lu.assertEquals(candidate('plutocratic brats', 'rat', true), 'plutocrat')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_415543_add_host", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for i in range(len(data)):\n-- #     data[i]['image'] = f'http://{host}/{data[i][\"image\"]}'\n-- # return data\n-- \n-- Make the image path absolute for those served by the server.\nlocal function add_host(data, host)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_415543_add_host.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = add_host\n    lu.assertEquals(candidate({{['name'] = 'name1', ['image'] = 'img1'}, {['name'] = 'name2', ['image'] = 'img2'}}, 'localhost:8000'), {{['name'] = 'name1', ['image'] = 'http://localhost:8000/img1'}, {['name'] = 'name2', ['image'] = 'http://localhost:8000/img2'}})\n    lu.assertEquals(candidate({{['image'] = 'foo/bar.png'}, {['image'] = 'foo/baz.png'}}, 'example.com'), {{['image'] = 'http://example.com/foo/bar.png'}, {['image'] = 'http://example.com/foo/baz.png'}})\n    lu.assertEquals(candidate({{['name'] = 'name1', ['image'] = 'img1'}, {['name'] = 'name2', ['image'] = 'img2'}}, 'localhost:9000/api'), {{['name'] = 'name1', ['image'] = 'http://localhost:9000/api/img1'}, {['name'] = 'name2', ['image'] = 'http://localhost:9000/api/img2'}})\n    lu.assertEquals(candidate({{['image'] = 'foo/bar.png'}, {['image'] = 'foo/baz.png'}, {['image'] = 'qux/quux.png'}}, 'example.com'), {{['image'] = 'http://example.com/foo/bar.png'}, {['image'] = 'http://example.com/foo/baz.png'}, {['image'] = 'http://example.com/qux/quux.png'}})\n    lu.assertEquals(candidate({{['name'] = 'name1', ['image'] = 'img1'}, {['name'] = 'name2', ['image'] = 'img2'}}, 'localhost'), {{['name'] = 'name1', ['image'] = 'http://localhost/img1'}, {['name'] = 'name2', ['image'] = 'http://localhost/img2'}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_415824_color2rgba", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # colorstring = colorstring.strip()\n-- # if colorstring[0] == '#':\n-- #     colorstring = colorstring[1:]\n-- # if len(colorstring) != 8:\n-- #     raise ValueError(\"input #%s is not in #RRGGBBAA format\" % colorstring)\n-- # r, g, b, a = colorstring[:2], colorstring[2:4], colorstring[4:6], colorstring[6:]\n-- # r, g, b, a = [int(n, 16) for n in (r, g, b, a)]\n-- # return ((r << 24) + (g << 16) + (b << 8) + a)\n-- \n--  convert #RRGGBB[AA] to an (R, G, B, [A]) tuple \nlocal function color2rgba(colorstring)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_415824_color2rgba.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = color2rgba\n    lu.assertEquals(candidate('#0000ffcc'), 65484)\n    lu.assertEquals(candidate('#0000ff00'), 65280)\n    lu.assertEquals(candidate('#ff0000cc'), 4278190284)\n    lu.assertEquals(candidate('#00ff00cc'), 16711884)\n    lu.assertEquals(candidate('#00000000'), 0)\n    lu.assertEquals(candidate('#ff0000ff'), 4278190335)\n    lu.assertEquals(candidate('#00ff0000'), 16711680)\n    lu.assertEquals(candidate('#00ff00ff'), 16711935)\n    lu.assertEquals(candidate('#ff000000'), 4278190080)\n    lu.assertEquals(candidate('#0000ffff'), 65535)\n    lu.assertEquals(candidate('#000000ff'), 255)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_416641_init_method_normalizer", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return name.lower().replace('_', '').replace('nodeembeddinginitializer', '')\n-- \n-- Normalizes the name of an initialization method.\nlocal function init_method_normalizer(name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_416641_init_method_normalizer.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = init_method_normalizer\n    lu.assertEquals(candidate('kaimingnormal'), 'kaimingnormal')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_416669_is_counting_line_passed", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if line_orientation == 'top':\n-- #     return point[1] < counting_line[0][1]\n-- # elif line_orientation == 'bottom':\n-- #     return point[1] > counting_line[0][1]\n-- # elif line_orientation == 'left':\n-- #     return point[0] < counting_line[0][0]\n-- # elif line_orientation == 'right':\n-- #     return point[0] > counting_line[0][0]\n-- \n-- To check if the point passed the counting line by the x coord if it left/right or y coord if it bottom/top.\n-- :param point: the object location.\n-- :param counting_line: the coordinates list of the area.\n-- :param line_orientation: the string of the orientation of the line.need to be top, bottom, left, right.\n-- :return: True if the point passed the line , False if the point didnt pass the line.\nlocal function is_counting_line_passed(point, counting_line, line_orientation)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_416669_is_counting_line_passed.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_counting_line_passed\n    lu.assertEquals(candidate({0, 0}, {{1, 0}, {0, 0}}, 'right'), false)\n    lu.assertEquals(candidate({0, 0}, {{1, 0}, {0, 0}}, 'top'), false)\n    lu.assertEquals(candidate({0, 0}, {{0, 0}, {0, 1}}, 'left'), false)\n    lu.assertEquals(candidate({0, 0}, {{1, 0}, {0, 0}}, 'left'), true)\n    lu.assertEquals(candidate({0, 0}, {{0, 0}, {0, 1}}, 'top'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_4167_wrap", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # diff = M - m\n-- # while x > M:\n-- #     x = x - diff\n-- # while x < m:\n-- #     x = x + diff\n-- # return x\n-- \n-- :param x: a scalar\n-- :param m: minimum possible value in range\n-- :param M: maximum possible value in range\n-- Wraps ``x`` so m <= x <= M; but unlike ``bound()`` which\n-- truncates, ``wrap()`` wraps x around the coordinate system defined by m,M.\n-- For example, m = -180, M = 180 (degrees), x = 360 --> returns 0.\nlocal function wrap(x, m, M)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_4167_wrap.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = wrap\n    lu.assertEquals(candidate(180, -180, 180), 180)\n    lu.assertEquals(candidate(-10, 0, 100), 90)\n    lu.assertEquals(candidate(3, 1, 5), 3)\n    lu.assertEquals(candidate(181, -180, 180), -179)\n    lu.assertEquals(candidate(13, 10, 20), 13)\n    lu.assertEquals(candidate(-180, -180, 180), -180)\n    lu.assertEquals(candidate(0, 0, 100), 0)\n    lu.assertEquals(candidate(-10, -100, 0), -10)\n    lu.assertEquals(candidate(0, -2, 1), 0)\n    lu.assertEquals(candidate(0, -1, 1), 0)\n    lu.assertEquals(candidate(1, 1, 1), 1)\n    lu.assertEquals(candidate(2, 1, 3), 2)\n    lu.assertEquals(candidate(-100, 0, 100), 0)\n    lu.assertEquals(candidate(5, 1, 5), 5)\n    lu.assertEquals(candidate(0, -1, 5), 0)\n    lu.assertEquals(candidate(3, 1, 3), 3)\n    lu.assertEquals(candidate(3, -1, 0), 0)\n    lu.assertEquals(candidate(3, -2, -1), -1)\n    lu.assertEquals(candidate(0, -100, 0), 0)\n    lu.assertEquals(candidate(-100, 1, 3), 2)\n    lu.assertEquals(candidate(10, 0, 100), 10)\n    lu.assertEquals(candidate(100, 1, 3), 2)\n    lu.assertEquals(candidate(360, -180, 180), 0)\n    lu.assertEquals(candidate(3, 10, 20), 13)\n    lu.assertEquals(candidate(-181, -180, 180), 179)\n    lu.assertEquals(candidate(15, 10, 20), 15)\n    lu.assertEquals(candidate(-3, 0, 1), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_417404_even", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return x % 2 == 0\n-- \n--     Returns 1 if x is even, 0 otherwise.\nlocal function even(x)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_417404_even.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = even\n    lu.assertEquals(candidate(3), 0)\n    lu.assertEquals(candidate(0), 1)\n    lu.assertEquals(candidate(-1), 0)\n    lu.assertEquals(candidate(4), 1)\n    lu.assertEquals(candidate(1), 0)\n    lu.assertEquals(candidate(2), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_417916_tokenize_grapheme_langid", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # index_over = enhanced_word.index('}')\n-- # lang_id = [enhanced_word[:index_over + 1]]\n-- # return lang_id\n-- \n--     tokenizer the langid  of word (For multilingual GBERT)\nlocal function tokenize_grapheme_langid(enhanced_word)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_417916_tokenize_grapheme_langid.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = tokenize_grapheme_langid\n    lu.assertEquals(candidate('hello}'), {'hello}'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_418429_normpath", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Preserve unicode (if path is unicode)\n-- # # slash, dot = (u'/', u'.') if isinstance(path, _unicode) else ('/', '.')\n-- # slash, dot = ('/', '.')\n-- # if path == '':\n-- #     return dot\n-- # initial_slashes = path.startswith('/')\n-- # # POSIX allows one or two initial slashes, but treats three or more\n-- # # as single slash.\n-- # if (initial_slashes and\n-- #         path.startswith('//') and not path.startswith('///')):\n-- #     initial_slashes = 2\n-- # comps = path.split('/')\n-- # new_comps = []\n-- # for comp in comps:\n-- #     if comp in ('', '.'):\n-- #         continue\n-- #     if (comp != '..' or (not initial_slashes and not new_comps) or\n-- #             (new_comps and new_comps[-1] == '..')):\n-- #         new_comps.append(comp)\n-- #     elif new_comps:\n-- #         new_comps.pop()\n-- # comps = new_comps\n-- # path = slash.join(comps)\n-- # if initial_slashes:\n-- #     path = slash*initial_slashes + path\n-- # return path or dot\n-- \n-- Normalize path, eliminating double slashes, etc.\nlocal function normpath(path)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_418429_normpath.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = normpath\n    lu.assertEquals(candidate('foo/bar/..'), 'foo')\n    lu.assertEquals(candidate('/foo/bar/..'), '/foo')\n    lu.assertEquals(candidate('/foo/bar/../../'), '/')\n    lu.assertEquals(candidate('/../foo/bar/../../'), '/')\n    lu.assertEquals(candidate('/a/b/c/../..'), '/a')\n    lu.assertEquals(candidate('/a/b/c'), '/a/b/c')\n    lu.assertEquals(candidate('/a/b/../c'), '/a/c')\n    lu.assertEquals(candidate('\\\\foo\\\\bar'), '\\\\foo\\\\bar')\n    lu.assertEquals(candidate('/a/b//c'), '/a/b/c')\n    lu.assertEquals(candidate('foo'), 'foo')\n    lu.assertEquals(candidate('/'), '/')\n    lu.assertEquals(candidate('/../../../../a'), '/a')\n    lu.assertEquals(candidate('/a/b/c/../../..'), '/')\n    lu.assertEquals(candidate('/foo/bar/../..'), '/')\n    lu.assertEquals(candidate(''), '.')\n    lu.assertEquals(candidate('/a/b/c/../../../..'), '/')\n    lu.assertEquals(candidate('/a/b/c/../../../../..'), '/')\n    lu.assertEquals(candidate('/foo'), '/foo')\n    lu.assertEquals(candidate('/a/../b/c'), '/b/c')\n    lu.assertEquals(candidate('/a/./b/c'), '/a/b/c')\n    lu.assertEquals(candidate('/a/b/c/../../../../../..'), '/')\n    lu.assertEquals(candidate('/a/b/c/'), '/a/b/c')\n    lu.assertEquals(candidate('foo/bar'), 'foo/bar')\n    lu.assertEquals(candidate('a/b/c'), 'a/b/c')\n    lu.assertEquals(candidate('/../../../a'), '/a')\n    lu.assertEquals(candidate('a/b//c'), 'a/b/c')\n    lu.assertEquals(candidate('foo\\\\bar'), 'foo\\\\bar')\n    lu.assertEquals(candidate('/foo/bar'), '/foo/bar')\n    lu.assertEquals(candidate('/a/b/./c'), '/a/b/c')\n    lu.assertEquals(candidate('/../../a'), '/a')\n    lu.assertEquals(candidate('a/b/./c'), 'a/b/c')\n    lu.assertEquals(candidate('/../a'), '/a')\n    lu.assertEquals(candidate('/a/b/c/..'), '/a/b')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_418893_factorial_pythonic", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return 1 if number in (0, 1) else number * factorial_pythonic(number - 1)\n-- \n-- Factorial with reduce function (pythonic approach).\n-- Examples:\n--     >>> assert factorial_pythonic(0) == 1\n--     >>> assert factorial_pythonic(1) == 1\n--     >>> assert factorial_pythonic(2) == 2\n--     >>> assert factorial_pythonic(3) == 6\nlocal function factorial_pythonic(number)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_418893_factorial_pythonic.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = factorial_pythonic\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(3), 6)\n    lu.assertEquals(candidate(0), 1)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(10), 3628800)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_419117_filter_matching_fields", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # other_fields_lowercase = set([f.lower() for f in other_fields])\n-- # return [f for f in fields if f.lower() in other_fields_lowercase]\n-- \n-- return fields which are same as in other_fields list, ignoring the case\nlocal function filter_matching_fields(fields, other_fields)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_419117_filter_matching_fields.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = filter_matching_fields\n    lu.assertEquals(candidate({}, {'FOO', 'BAR'}), {})\n    lu.assertEquals(candidate({}, {}), {})\n    lu.assertEquals(candidate({'foo', 'bar', 'baz'}, {}), {})\n    lu.assertEquals(candidate({'aaa', 'bbb', 'ccc'}, {'aaa', 'bbb', 'ccc'}), {'aaa', 'bbb', 'ccc'})\n    lu.assertEquals(candidate({'foo', 'bar', 'baz'}, {'foo', 'baz'}), {'foo', 'baz'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_420357_fixIncorrectDateEncoding", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if 'T00:00:00Z' in Date:\n-- #     return Date\n-- # return Date + \"T00:00:00Z\"\n-- \n--     Fix date string to make it conform to ISO standard.\nlocal function fixIncorrectDateEncoding(Date)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_420357_fixIncorrectDateEncoding.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = fixIncorrectDateEncoding\n    lu.assertEquals(candidate('2020-12-20'), '2020-12-20T00:00:00Z')\n    lu.assertEquals(candidate('2020-12-20T00:00:00Z'), '2020-12-20T00:00:00Z')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_421139_ctz_naive", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if x == 0:\n-- #     return w\n-- # t = 1\n-- # r = 0\n-- # while x & t == 0:\n-- #     t <<= 1\n-- #     r += 1\n-- # return r\n-- \n--     counting zeros starting at the LSB until a 1-bit is encountered\nlocal function ctz_naive(x, w)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_421139_ctz_naive.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = ctz_naive\n    lu.assertEquals(candidate(9), 0)\n    lu.assertEquals(candidate(3, 8), 0)\n    lu.assertEquals(candidate(8), 3)\n    lu.assertEquals(candidate(1), 0)\n    lu.assertEquals(candidate(3), 0)\n    lu.assertEquals(candidate(5), 0)\n    lu.assertEquals(candidate(11, 8), 0)\n    lu.assertEquals(candidate(10), 1)\n    lu.assertEquals(candidate(6, 8), 1)\n    lu.assertEquals(candidate(13), 0)\n    lu.assertEquals(candidate(12), 2)\n    lu.assertEquals(candidate(15), 0)\n    lu.assertEquals(candidate(262143), 0)\n    lu.assertEquals(candidate(7, 8), 0)\n    lu.assertEquals(candidate(5, 8), 0)\n    lu.assertEquals(candidate(8), 3)\n    lu.assertEquals(candidate(3), 0)\n    lu.assertEquals(candidate(1, 8), 0)\n    lu.assertEquals(candidate(7), 0)\n    lu.assertEquals(candidate(10, 8), 1)\n    lu.assertEquals(candidate(6), 1)\n    lu.assertEquals(candidate(9, 8), 0)\n    lu.assertEquals(candidate(7), 0)\n    lu.assertEquals(candidate(262142), 1)\n    lu.assertEquals(candidate(18708), 2)\n    lu.assertEquals(candidate(2, 8), 1)\n    lu.assertEquals(candidate(4, 8), 2)\n    lu.assertEquals(candidate(16), 4)\n    lu.assertEquals(candidate(13), 0)\n    lu.assertEquals(candidate(11), 0)\n    lu.assertEquals(candidate(11), 0)\n    lu.assertEquals(candidate(2), 1)\n    lu.assertEquals(candidate(5), 0)\n    lu.assertEquals(candidate(0), 16)\n    lu.assertEquals(candidate(14), 1)\n    lu.assertEquals(candidate(4), 2)\n    lu.assertEquals(candidate(2), 1)\n    lu.assertEquals(candidate(15), 0)\n    lu.assertEquals(candidate(10), 1)\n    lu.assertEquals(candidate(4), 2)\n    lu.assertEquals(candidate(1), 0)\n    lu.assertEquals(candidate(8, 8), 3)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_421307_get_file_type", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if file_name is None:\n-- #     return None\n-- # file_type = file_name.split(\".\")[-1].strip()\n-- # if file_type == \"nc\":\n-- #     file_type = \"netcdf\"\n-- # if file_type == \"prmtop\":\n-- #     file_type = \"parm7\"\n-- # if file_type == \"cms\":\n-- #     file_type = \"mae\"\n-- # if file_type == \"cif\":\n-- #     file_type = \"pdbx\"\n-- # return file_type\n-- \n--     Determine file type by extracting suffix of file_name\nlocal function get_file_type(file_name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_421307_get_file_type.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_file_type\n    lu.assertEquals(candidate('test.gro.nc'), 'netcdf')\n    lu.assertEquals(candidate('myfile.pdb'), 'pdb')\n    lu.assertEquals(candidate('test.itp'), 'itp')\n    lu.assertEquals(candidate('a/b/c.cms'), 'mae')\n    lu.assertEquals(candidate('a/b/c.gro'), 'gro')\n    lu.assertEquals(candidate('x/y/z/a/b/c.d'), 'd')\n    lu.assertEquals(candidate('something.txt'), 'txt')\n    lu.assertEquals(candidate('test.gro'), 'gro')\n    lu.assertEquals(candidate('a/b/c.nc'), 'netcdf')\n    lu.assertEquals(candidate('myfile.mae'), 'mae')\n    lu.assertEquals(candidate('test.nc'), 'netcdf')\n    lu.assertEquals(candidate('a/b/c.prmtop'), 'parm7')\n    lu.assertEquals(candidate('test.pdb'), 'pdb')\n    lu.assertEquals(candidate('test.cms'), 'mae')\n    lu.assertEquals(candidate('a/b/c.d'), 'd')\n    lu.assertEquals(candidate('something.netcdf'), 'netcdf')\n    lu.assertEquals(candidate(None), None)\n    lu.assertEquals(candidate('myfile.cms'), 'mae')\n    lu.assertEquals(candidate('a/b/c.pdbx'), 'pdbx')\n    lu.assertEquals(candidate('a/b/c.pdb'), 'pdb')\n    lu.assertEquals(candidate('something.pdb'), 'pdb')\n    lu.assertEquals(candidate('something.nc'), 'netcdf')\n    lu.assertEquals(candidate('something.pdbx'), 'pdbx')\n    lu.assertEquals(candidate('something.mae'), 'mae')\n    lu.assertEquals(candidate('test.prmtop'), 'parm7')\n    lu.assertEquals(candidate('something.prmtop'), 'parm7')\n    lu.assertEquals(candidate('something.sdf'), 'sdf')\n    lu.assertEquals(candidate('test.top'), 'top')\n    lu.assertEquals(candidate('something.cms'), 'mae')\n    lu.assertEquals(candidate('test.cif'), 'pdbx')\n    lu.assertEquals(candidate('a/b/c.cif'), 'pdbx')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_421350_extract", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # line = line.replace(\"\\n\", \"\")\n-- # uin = line[1:line.find(\"\\t\")]\n-- # # fix uin\n-- # uin2 = \"\"\n-- # for c in uin:\n-- #     if c.isdigit():\n-- #         uin2 += c\n-- # uin = uin2\n-- # nick = line[1+line.find(\"\\t\"):]\n-- # nick = nick.replace(\"/\", \"_\")\n-- # nick = nick.replace(\":\", \"_\")\n-- # return (uin, nick)\n-- \n-- Return a tuple (uin, nick) from 'O11111111\tnickname'\nlocal function extract(line)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_421350_extract.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = extract\n    lu.assertEquals(candidate('O22222222\\tNickname:with:colons'), {'22222222', 'Nickname_with_colons'})\n    lu.assertEquals(candidate('O11111111\\tn/ickn:ame'), {'11111111', 'n_ickn_ame'})\n    lu.assertEquals(candidate('O11111111\\tNickname'), {'11111111', 'Nickname'})\n    lu.assertEquals(candidate('O11111111\\tnickname'), {'11111111', 'nickname'})\n    lu.assertEquals(candidate('O44444444\\tNickname/with:mixed/slashes:and:colons'), {'44444444', 'Nickname_with_mixed_slashes_and_colons'})\n    lu.assertEquals(candidate('O12345678\\tmy_cool_nickname'), {'12345678', 'my_cool_nickname'})\n    lu.assertEquals(candidate('O12345678\\tmy/cool/nickname'), {'12345678', 'my_cool_nickname'})\n    lu.assertEquals(candidate('O12345678\\tmy:cool:nickname'), {'12345678', 'my_cool_nickname'})\n    lu.assertEquals(candidate('O33333333\\tNickname/with/slashes'), {'33333333', 'Nickname_with_slashes'})\n    lu.assertEquals(candidate('O12345678\\tnick:name'), {'12345678', 'nick_name'})\n    lu.assertEquals(candidate('O11111111\\tnickname'), {'11111111', 'nickname'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_42140_get_train_valid_test_split_", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # splits = []\n-- # if splits_string.find(\",\") != -1:\n-- #     splits = [float(s) for s in splits_string.split(\",\")]\n-- # elif splits_string.find(\"/\") != -1:\n-- #     splits = [float(s) for s in splits_string.split(\"/\")]\n-- # else:\n-- #     splits = [float(splits_string)]\n-- # while len(splits) < 3:\n-- #     splits.append(0.0)\n-- # splits = splits[:3]\n-- # splits_sum = sum(splits)\n-- # assert splits_sum > 0.0\n-- # splits = [split / splits_sum for split in splits]\n-- # splits_index = [0]\n-- # for index, split in enumerate(splits):\n-- #     splits_index.append(splits_index[index] + int(round(split * float(size))))\n-- # diff = splits_index[-1] - size\n-- # for index in range(1, len(splits_index)):\n-- #     splits_index[index] -= diff\n-- # assert len(splits_index) == 4\n-- # assert splits_index[-1] == size\n-- # return splits_index\n-- \n-- Get dataset splits from comma or '/' separated string list.\nlocal function get_train_valid_test_split_(splits_string, size)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_42140_get_train_valid_test_split_.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_train_valid_test_split_\n    lu.assertEquals(candidate('0.6,0.2,0.2', 10), {0, 6, 8, 10})\n    lu.assertEquals(candidate('0.6/0.2/0.2', 10), {0, 6, 8, 10})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_421692_convert_target_counters", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if x[1] == str(0):  # If the counter in question is of the F0xx format -> remove 0 and keep the rest of the numbers\n-- #     cpy = x.replace(\"F\", \"\").replace(\"G\", \"\").replace(\"B\", \"\").replace(\"0\", \"\", 1)\n-- # else:  # If the counter is of the Fxxx format keep all the numbers (F29G034 -> 29034)\n-- #     cpy = x.replace(\"F\", \"\").replace(\"G\", \"\").replace(\"B\", \"\")\n-- # return cpy\n-- \n-- This function converts the FxxxGxxx xml format to the final format. E.g: F29G034 -> 29034,\n-- F08G069 -> 8069.\nlocal function convert_target_counters(x)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_421692_convert_target_counters.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = convert_target_counters\n    lu.assertEquals(candidate('F08G069'), '8069')\n    lu.assertEquals(candidate('F08G069'), '8069')\n    lu.assertEquals(candidate('F29G034'), '29034')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_421969_sum_scores", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return sum(scores.values())\n-- \n-- Sums all the scores\nlocal function sum_scores(scores, query)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_421969_sum_scores.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sum_scores\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2}, 'a b'), 3)\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3}, 'a b c'), 6)\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2}, 'a b'), 3)\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3}, 'c a b'), 6)\n    lu.assertEquals(candidate({}, 'a'), 0)\n    lu.assertEquals(candidate({['a'] = 1}, 'a'), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_422550_solution", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return str(sum(array))[:10]\n-- \n-- Returns the first ten digits of the sum of the array elements.\n-- >>> import os\n-- >>> sum = 0\n-- >>> array = []\n-- >>> with open(os.path.dirname(__file__) + \"/num.txt\",\"r\") as f:\n-- ...     for line in f:\n-- ...         array.append(int(line))\n-- ...\n-- >>> solution(array)\n-- '5537376230'\nlocal function solution(array)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_422550_solution.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = solution\n    lu.assertEquals(candidate(list(range(1, 101))), '5050')\n    lu.assertEquals(candidate(list(range(1, 1001))), '500500')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_422804_charencode", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # encoded = ''\n-- # for char in string:\n-- #     encoded = encoded + \",\" + str(ord(char))\n-- # return encoded[1:]\n-- \n-- String.CharCode\nlocal function charencode(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_422804_charencode.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = charencode\n    lu.assertEquals(candidate('z'), '122')\n    lu.assertEquals(candidate('abcde'), '97,98,99,100,101')\n    lu.assertEquals(candidate('ABC'), '65,66,67')\n    lu.assertEquals(candidate('aA1'), '97,65,49')\n    lu.assertEquals(candidate('AaA1'), '65,97,65,49')\n    lu.assertEquals(candidate('AB'), '65,66')\n    lu.assertEquals(candidate('A1a'), '65,49,97')\n    lu.assertEquals(candidate('Aa1'), '65,97,49')\n    lu.assertEquals(candidate('Aa1A'), '65,97,49,65')\n    lu.assertEquals(candidate('abcdefghi'), '97,98,99,100,101,102,103,104,105')\n    lu.assertEquals(candidate('123'), '49,50,51')\n    lu.assertEquals(candidate('Hello'), '72,101,108,108,111')\n    lu.assertEquals(candidate('1a'), '49,97')\n    lu.assertEquals(candidate('Hello, World!'), '72,101,108,108,111,44,32,87,111,114,108,100,33')\n    lu.assertEquals(candidate('Aa'), '65,97')\n    lu.assertEquals(candidate('abc'), '97,98,99')\n    lu.assertEquals(candidate('1aA'), '49,97,65')\n    lu.assertEquals(candidate('abcdef'), '97,98,99,100,101,102')\n    lu.assertEquals(candidate('abc123'), '97,98,99,49,50,51')\n    lu.assertEquals(candidate('a'), '97')\n    lu.assertEquals(candidate('Z'), '90')\n    lu.assertEquals(candidate('ab'), '97,98')\n    lu.assertEquals(candidate('a'), '97')\n    lu.assertEquals(candidate('A1Aa'), '65,49,65,97')\n    lu.assertEquals(candidate('1A1a'), '49,65,49,97')\n    lu.assertEquals(candidate('a1'), '97,49')\n    lu.assertEquals(candidate('A'), '65')\n    lu.assertEquals(candidate(' '), '32')\n    lu.assertEquals(candidate('abc,def,ghi'), '97,98,99,44,100,101,102,44,103,104,105')\n    lu.assertEquals(candidate('a,b,c'), '97,44,98,44,99')\n    lu.assertEquals(candidate('aA'), '97,65')\n    lu.assertEquals(candidate('A1aA'), '65,49,97,65')\n    lu.assertEquals(candidate('1AaA'), '49,65,97,65')\n    lu.assertEquals(candidate('1Aa'), '49,65,97')\n    lu.assertEquals(candidate('abcd'), '97,98,99,100')\n    lu.assertEquals(candidate('A'), '65')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_422870_calc_bits", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Generate the register bits\n-- # if size == 1:\n-- #     ret_str = \"[\" + str(offset) + \"]\"\n-- # else:\n-- #     top_bit = offset + size - 1\n-- #     ret_str = \"[\" + str(top_bit) + \":\" + str(offset) + \"]\"\n-- # return ret_str\n-- \n-- Generates the string of where the bits of that property in a register live \n-- Parameters\n-- ----------\n-- offset : int,  \n-- size : int, \n-- Returns \n-- ----------\n-- ret_str : str, the string\nlocal function calc_bits(offset, size)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_422870_calc_bits.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = calc_bits\n    lu.assertEquals(candidate(16, 2), '[17:16]')\n    lu.assertEquals(candidate(15, 1), '[15]')\n    lu.assertEquals(candidate(0, 16), '[15:0]')\n    lu.assertEquals(candidate(0, 32), '[31:0]')\n    lu.assertEquals(candidate(16, 1), '[16]')\n    lu.assertEquals(candidate(4, 4), '[7:4]')\n    lu.assertEquals(candidate(1, 1), '[1]')\n    lu.assertEquals(candidate(0, 2), '[1:0]')\n    lu.assertEquals(candidate(0, 1), '[0]')\n    lu.assertEquals(candidate(0, 4), '[3:0]')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_423409_pretty_print", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # indented_uc = '    ' + parsed_file['uncommented_content'].replace('\\n', '\\n    ')\n-- # return '{origpath} ({artifact}):\\n{indented_uc}'.format(\n-- #     origpath=parsed_file['origpath'],\n-- #     artifact=parsed_file['artifact'],\n-- #     indented_uc=indented_uc,\n-- # ).strip()\n-- \n--     We just print so it looks decent in a terminal\nlocal function pretty_print(parsed_file)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_423409_pretty_print.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = pretty_print\n    lu.assertEquals(candidate({['origpath'] = 'foo/bar', ['artifact'] = 'foo/bar.baz', ['uncommented_content'] = 'foobarbaz\\nfoobarbaz\\n'}), 'foo/bar (foo/bar.baz):\\n    foobarbaz\\n    foobarbaz')\n    lu.assertEquals(candidate({['origpath'] = 'foo/bar', ['artifact'] = 'foo/bar.baz', ['uncommented_content'] = 'foobarbaz\\n'}), 'foo/bar (foo/bar.baz):\\n    foobarbaz')\n    lu.assertEquals(candidate({['origpath'] = 'foo/bar', ['artifact'] = 'foo/bar.baz', ['uncommented_content'] = 'foobarbaz'}), 'foo/bar (foo/bar.baz):\\n    foobarbaz')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_423580__jinja2_filter_datetime", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # words = data.split(\" \")\n-- # return \" \".join(words[:100])\n-- \n--  Extract an excert of a post\nlocal function _jinja2_filter_datetime(data)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_423580__jinja2_filter_datetime.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _jinja2_filter_datetime\n    lu.assertEquals(candidate(\"Hello World! I'm a Pythonista!\"), \"Hello World! I'm a Pythonista!\")\n    lu.assertEquals(candidate('2019-01-01 10:00:00'), '2019-01-01 10:00:00')\n    lu.assertEquals(candidate('A long time ago in a galaxy far, far away...'), 'A long time ago in a galaxy far, far away...')\n    lu.assertEquals(candidate('2019-06-13 10:00:00.1234567890123456789'), '2019-06-13 10:00:00.1234567890123456789')\n    lu.assertEquals(candidate('2019-06-13 10:00:00.123456'), '2019-06-13 10:00:00.123456')\n    lu.assertEquals(candidate('Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.'), 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.')\n    lu.assertEquals(candidate('Hello World!'), 'Hello World!')\n    lu.assertEquals(candidate('2019-06-13 10:00:00'), '2019-06-13 10:00:00')\n    lu.assertEquals(candidate(\"Hello World! I'm a Pythonista! I'm a Pythonista! I'm a Pythonista! I'm a Pythonista! I'm a Pythonista!\"), \"Hello World! I'm a Pythonista! I'm a Pythonista! I'm a Pythonista! I'm a Pythonista! I'm a Pythonista!\")\n    lu.assertEquals(candidate('There was a time...'), 'There was a time...')\n    lu.assertEquals(candidate('Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.'), 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.')\n    lu.assertEquals(candidate('2019-06-13 10:00'), '2019-06-13 10:00')\n    lu.assertEquals(candidate(\"Hello World! I'm a Pythonista! I'm a Pythonista!\"), \"Hello World! I'm a Pythonista! I'm a Pythonista!\")\n    lu.assertEquals(candidate('2012-09-22T16:00:00+02:00'), '2012-09-22T16:00:00+02:00')\n    lu.assertEquals(candidate('Once upon a time...'), 'Once upon a time...')\n    lu.assertEquals(candidate('2019-06-13 10:00:00.123456789'), '2019-06-13 10:00:00.123456789')\n    lu.assertEquals(candidate(''), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_423776_yearmonthplusoffset", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # month += offset\n-- # # handle offset and under/overflows - quick and dirty, yes!\n-- # while month < 1:\n-- #     month += 12\n-- #     year -= 1\n-- # while month > 12:\n-- #     month -= 12\n-- #     year += 1\n-- # return year, month\n-- \n--  calculate new year/month from year/month and offset \nlocal function yearmonthplusoffset(year, month, offset)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_423776_yearmonthplusoffset.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = yearmonthplusoffset\n    lu.assertEquals(candidate(2019, 12, 0), {2019, 12})\n    lu.assertEquals(candidate(2018, 1, 3), {2018, 4})\n    lu.assertEquals(candidate(2017, 1, 1), {2017, 2})\n    lu.assertEquals(candidate(2000, 12, 1), {2001, 1})\n    lu.assertEquals(candidate(2000, 1, 13), {2001, 2})\n    lu.assertEquals(candidate(2018, 1, 13), {2019, 2})\n    lu.assertEquals(candidate(2018, 1, 4), {2018, 5})\n    lu.assertEquals(candidate(2017, 1, -1), {2016, 12})\n    lu.assertEquals(candidate(2000, 1, -13), {1998, 12})\n    lu.assertEquals(candidate(2018, 1, 9), {2018, 10})\n    lu.assertEquals(candidate(2018, 1, 11), {2018, 12})\n    lu.assertEquals(candidate(2019, 12, 13), {2021, 1})\n    lu.assertEquals(candidate(2000, 12, 0), {2000, 12})\n    lu.assertEquals(candidate(2000, 12, 12), {2001, 12})\n    lu.assertEquals(candidate(2000, 1, 10), {2000, 11})\n    lu.assertEquals(candidate(2018, 1, 5), {2018, 6})\n    lu.assertEquals(candidate(2017, 1, 11), {2017, 12})\n    lu.assertEquals(candidate(2017, 1, 12), {2018, 1})\n    lu.assertEquals(candidate(2017, 1, 0), {2017, 1})\n    lu.assertEquals(candidate(2017, 1, 13), {2018, 2})\n    lu.assertEquals(candidate(2018, 1, 1), {2018, 2})\n    lu.assertEquals(candidate(2018, 1, 2), {2018, 3})\n    lu.assertEquals(candidate(2018, 1, 6), {2018, 7})\n    lu.assertEquals(candidate(2018, 1, 0), {2018, 1})\n    lu.assertEquals(candidate(2019, 12, -11), {2019, 1})\n    lu.assertEquals(candidate(2000, 1, -12), {1999, 1})\n    lu.assertEquals(candidate(2017, 1, -11), {2016, 2})\n    lu.assertEquals(candidate(2018, 1, 10), {2018, 11})\n    lu.assertEquals(candidate(2000, 1, 1), {2000, 2})\n    lu.assertEquals(candidate(2000, 1, 12), {2001, 1})\n    lu.assertEquals(candidate(2018, 1, 12), {2019, 1})\n    lu.assertEquals(candidate(2019, 12, -1), {2019, 11})\n    lu.assertEquals(candidate(2018, 1, 8), {2018, 9})\n    lu.assertEquals(candidate(2000, 1, 0), {2000, 1})\n    lu.assertEquals(candidate(2019, 12, 1), {2020, 1})\n    lu.assertEquals(candidate(2000, 12, 13), {2002, 1})\n    lu.assertEquals(candidate(2018, 1, 7), {2018, 8})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_424985_sanitize", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return message.replace('`', '').replace('\\n', '')\n-- \n--     Removes backticks (code tag) and linebreaks from a message.\nlocal function sanitize(message)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_424985_sanitize.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sanitize\n    lu.assertEquals(candidate('`hello`'), 'hello')\n    lu.assertEquals(candidate('No backticks'), 'No backticks')\n    lu.assertEquals(candidate('hello'), 'hello')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('`'), '')\n    lu.assertEquals(candidate('\\n'), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_425289_compreso", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return num >= low and num <= high\n-- \n--     checks whether or not a number is between low and high\nlocal function compreso(num, low, high)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_425289_compreso.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = compreso\n    lu.assertEquals(candidate(10, 1, 5), false)\n    lu.assertEquals(candidate(10, 3, 10), true)\n    lu.assertEquals(candidate(3, 3, 6), true)\n    lu.assertEquals(candidate(-5, -3, 1), false)\n    lu.assertEquals(candidate(10, 10, 5), false)\n    lu.assertEquals(candidate(3, 3, 5), true)\n    lu.assertEquals(candidate(5, 3, 10), true)\n    lu.assertEquals(candidate(1, 1, 3), true)\n    lu.assertEquals(candidate(3, -3, 6), true)\n    lu.assertEquals(candidate(3, 1, 5), true)\n    lu.assertEquals(candidate(10, 10, 10), true)\n    lu.assertEquals(candidate(10, 3, 6), false)\n    lu.assertEquals(candidate(-5, -3, 6), false)\n    lu.assertEquals(candidate(5, 10, 10), false)\n    lu.assertEquals(candidate(1, 3, 6), false)\n    lu.assertEquals(candidate(1, 1, 5), true)\n    lu.assertEquals(candidate(1, 2, 3), false)\n    lu.assertEquals(candidate(10, 3, 5), false)\n    lu.assertEquals(candidate(10, -3, 6), false)\n    lu.assertEquals(candidate(2, 1, 3), true)\n    lu.assertEquals(candidate(3, 1, 3), true)\n    lu.assertEquals(candidate(10, -3, 1), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_425575_milli_2_readadble", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # data = msecs / 1000\n-- # seconds = data % 60\n-- # data /= 60\n-- # minutes = data % 60\n-- # data /= 60\n-- # hours = data % 24\n-- # data /= 24\n-- # days = data\n-- # return \"%d days %d hours %d minutes %d seconds\" \\\n-- #        % (days, hours, minutes, seconds)\n-- \n-- Function:  milli_2_readadble\n-- Description:  Converts milliseconds into days, hours, minutes and seconds.\n--     Returns values with appropriate tags.\n-- Arguments:\n--     (input) msecs -> Milliseconds.\nlocal function milli_2_readadble(msecs)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_425575_milli_2_readadble.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = milli_2_readadble\n    lu.assertEquals(candidate(86400000), '1 days 0 hours 0 minutes 0 seconds')\n    lu.assertEquals(candidate(0), '0 days 0 hours 0 minutes 0 seconds')\n    lu.assertEquals(candidate(86400001), '1 days 0 hours 0 minutes 0 seconds')\n    lu.assertEquals(candidate(172800000), '2 days 0 hours 0 minutes 0 seconds')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_425947_exactly", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return f\"{{{length}}}\"\n-- \n--  Match the previous pattern exactly `length` times.\n-- >>> import superexpressive as se\n-- >>> se.exactly(4)\n-- '{4}'\n-- >>> import superexpressive as se\n-- >>> se.DIGIT + se.exactly(6)\n-- '\\\\d{6}'\nlocal function exactly(length)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_425947_exactly.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = exactly\n    lu.assertEquals(candidate(8), '{8}')\n    lu.assertEquals(candidate(7), '{7}')\n    lu.assertEquals(candidate(3), '{3}')\n    lu.assertEquals(candidate(110), '{110}')\n    lu.assertEquals(candidate(6), '{6}')\n    lu.assertEquals(candidate(1), '{1}')\n    lu.assertEquals(candidate(42), '{42}')\n    lu.assertEquals(candidate(10), '{10}')\n    lu.assertEquals(candidate(4), '{4}')\n    lu.assertEquals(candidate(1000000), '{1000000}')\n    lu.assertEquals(candidate(6), '{6}')\n    lu.assertEquals(candidate(1000), '{1000}')\n    lu.assertEquals(candidate(10), '{10}')\n    lu.assertEquals(candidate(4), '{4}')\n    lu.assertEquals(candidate(100), '{100}')\n    lu.assertEquals(candidate(100000), '{100000}')\n    lu.assertEquals(candidate(5), '{5}')\n    lu.assertEquals(candidate(9), '{9}')\n    lu.assertEquals(candidate(2), '{2}')\n    lu.assertEquals(candidate(3), '{3}')\n    lu.assertEquals(candidate(51), '{51}')\n    lu.assertEquals(candidate(30), '{30}')\n    lu.assertEquals(candidate(10000), '{10000}')\n    lu.assertEquals(candidate(222), '{222}')\n    lu.assertEquals(candidate(333), '{333}')\n    lu.assertEquals(candidate(1), '{1}')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_426317_HSVtoRGB", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if s == 0:\n-- #     # Achromatic, i.e. gray\n-- #     gray = v * 255\n-- #     return gray, gray, gray\n-- # # Calculate our \"sectors\"\n-- # h /= 60\n-- # i = int(h)\n-- # # A few intermediate values we may need\n-- # f = h - i\n-- # p = v * (1 - s)\n-- # q = v * (1 - s * f)\n-- # t = v * (1 - s * (1 - f))\n-- # # Figure out which \"sector\" of the HSV cylinder we're in and assign RGB\n-- # if i == 0:\n-- #     r = v\n-- #     g = t\n-- #     b = p\n-- # elif i == 1:\n-- #     r = q\n-- #     g = v\n-- #     b = p\n-- # elif i == 2:\n-- #     r = p\n-- #     g = v\n-- #     b = t\n-- # elif i == 3:\n-- #     r = p\n-- #     g = q\n-- #     b = v\n-- # elif i == 4:\n-- #     r = t\n-- #     g = p\n-- #     b = v\n-- # else:\n-- #     r = v\n-- #     g = p\n-- #     b = q\n-- # r *= 255\n-- # g *= 255\n-- # b *= 255\n-- # return r, g, b\n-- \n-- Convert HSV values to RGB\n-- Algorithm is taken from https://www.cs.rit.edu/~ncs/color/t_convert.html\nlocal function HSVtoRGB(h, s, v)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_426317_HSVtoRGB.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = HSVtoRGB\n    lu.assertEquals(candidate(0, 0, 0), {0, 0, 0})\n    lu.assertEquals(candidate(120, 0, 0), {0, 0, 0})\n    lu.assertEquals(candidate(0, 0, 1), {255, 255, 255})\n    lu.assertEquals(candidate(0, 0, 0), {0, 0, 0})\n    lu.assertEquals(candidate(300, 1, 1), {255, 0, 255})\n    lu.assertEquals(candidate(60, 1, 1), {255, 255, 0})\n    lu.assertEquals(candidate(0, 1, 1), {255, 0, 0})\n    lu.assertEquals(candidate(0, 0, 1), {255, 255, 255})\n    lu.assertEquals(candidate(240, 0, 0), {0, 0, 0})\n    lu.assertEquals(candidate(0, 1, 1), {255, 0, 0})\n    lu.assertEquals(candidate(300, 1, 1), {255, 0, 255})\n    lu.assertEquals(candidate(120, 1, 1), {0, 255, 0})\n    lu.assertEquals(candidate(60, 1, 1), {255, 255, 0})\n    lu.assertEquals(candidate(120, 1, 1), {0, 255, 0})\n    lu.assertEquals(candidate(240, 1, 1), {0, 0, 255})\n    lu.assertEquals(candidate(180, 1, 1), {0, 255, 255})\n    lu.assertEquals(candidate(240, 1, 1), {0, 0, 255})\n    lu.assertEquals(candidate(180, 1, 1), {0, 255, 255})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_426643_remove_keys", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for key in seq:\n-- #     dict_.pop(key, None)\n-- # return dict_\n-- \n-- Gracefully remove keys from dict.\nlocal function remove_keys(dict_, seq)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_426643_remove_keys.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = remove_keys\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3, ['d'] = 4}, {'a', 'b'}), {['c'] = 3, ['d'] = 4})\n    lu.assertEquals(candidate(dict(), {}), dict())\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3, ['d'] = 4}, {'a', 'b', 'c', 'd'}), {})\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3, ['d'] = 4, ['e'] = 5, ['f'] = 6}, {'d', 'c', 'b', 'a', 'e', 'f'}), {})\n    lu.assertEquals(candidate(dict(), {'a', 'b'}), dict())\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3, ['d'] = 4}, {'a', 'e'}), {['b'] = 2, ['c'] = 3, ['d'] = 4})\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3, ['d'] = 4}, {'d', 'c'}), {['a'] = 1, ['b'] = 2})\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3, ['d'] = 4}, {}), {['a'] = 1, ['b'] = 2, ['c'] = 3, ['d'] = 4})\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3, ['d'] = 4}, {'a', 'b'}), {['c'] = 3, ['d'] = 4})\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3, ['d'] = 4, ['e'] = 5, ['f'] = 6}, {'a', 'b', 'c', 'd', 'e', 'f'}), {})\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3, ['d'] = 4}, {'a', 'b', 'c'}), {['d'] = 4})\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3, ['d'] = 4}, {}), {['a'] = 1, ['b'] = 2, ['c'] = 3, ['d'] = 4})\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3, ['d'] = 4}, {'a', 'b', 'c'}), {['d'] = 4})\n    lu.assertEquals(candidate(dict(), {'a'}), dict())\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_426793_natMult3and5", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # sum = 0\n-- # for i in range(limit):\n-- #     if i % 5 == 0 or i % 3 == 0:\n-- #         sum += i\n-- # return sum\n-- \n-- Find the sum of all the multiples of 3 or 5 below 'limit'\nlocal function natMult3and5(limit)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_426793_natMult3and5.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = natMult3and5\n    lu.assertEquals(candidate(100), 2318)\n    lu.assertEquals(candidate(10), 23)\n    lu.assertEquals(candidate(10), 23)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(1000), 233168)\n    lu.assertEquals(candidate(20), 78)\n    lu.assertEquals(candidate(1000), 233168)\n    lu.assertEquals(candidate(1), 0)\n    lu.assertEquals(candidate(20), 78)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_426978_validate_slice_int", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not isinstance(bound, int) or bound <= 0:\n-- #     raise TypeError('bound must be a positive integer.')\n-- # if include:\n-- #     if not -bound <= the_int < bound:\n-- #         raise ValueError('Slice argument {} does not fit with bound {}'.format(the_int, bound))\n-- # else:\n-- #     if not -bound < the_int <= bound:\n-- #         raise ValueError('Slice argument {} does not fit with bound {}'.format(the_int, bound))\n-- # if the_int < 0:\n-- #     return the_int + bound\n-- # return the_int\n-- \n-- Ensure that the given integer makes sense as a slice entry, and move to\n-- a normalized form.\n-- Parameters\n-- ----------\n-- the_int : int\n-- bound : int\n-- include : bool\n-- Returns\n-- -------\n-- int\nlocal function validate_slice_int(the_int, bound, include)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_426978_validate_slice_int.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = validate_slice_int\n    lu.assertEquals(candidate(-5, 10), 5)\n    lu.assertEquals(candidate(-5, 10, true), 5)\n    lu.assertEquals(candidate(9, 10), 9)\n    lu.assertEquals(candidate(0, 10), 0)\n    lu.assertEquals(candidate(0, 100), 0)\n    lu.assertEquals(candidate(1, 10, true), 1)\n    lu.assertEquals(candidate(99, 100), 99)\n    lu.assertEquals(candidate(1, 10), 1)\n    lu.assertEquals(candidate(-50, 100), 50)\n    lu.assertEquals(candidate(-1, 10, true), 9)\n    lu.assertEquals(candidate(-1, 10), 9)\n    lu.assertEquals(candidate(-10, 10), 0)\n    lu.assertEquals(candidate(-1, 100), 99)\n    lu.assertEquals(candidate(5, 10, true), 5)\n    lu.assertEquals(candidate(0, 10, true), 0)\n    lu.assertEquals(candidate(-10, 10, true), 0)\n    lu.assertEquals(candidate(9, 10, true), 9)\n    lu.assertEquals(candidate(50, 100), 50)\n    lu.assertEquals(candidate(10, 10, false), 10)\n    lu.assertEquals(candidate(2, 10), 2)\n    lu.assertEquals(candidate(-100, 100), 0)\n    lu.assertEquals(candidate(5, 10), 5)\n    lu.assertEquals(candidate(-3, 10), 7)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_427698__num_starting_hashes", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not line:\n-- #     return 0\n-- # for n, char in enumerate(line, start=0):\n-- #     if char != \"#\":\n-- #         return n\n-- # return len(line)\n-- \n-- Return the number of hashes (#) at the beginning of the line.\nlocal function _num_starting_hashes(line)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_427698__num_starting_hashes.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _num_starting_hashes\n    lu.assertEquals(candidate('hello, world! #'), 0)\n    lu.assertEquals(candidate('# hello, world!'), 1)\n    lu.assertEquals(candidate('# hello, world! #'), 1)\n    lu.assertEquals(candidate(''), 0)\n    lu.assertEquals(candidate(None), 0)\n    lu.assertEquals(candidate(''), 0)\n    lu.assertEquals(candidate('##### hello, world! ###'), 5)\n    lu.assertEquals(candidate('hello, world!'), 0)\n    lu.assertEquals(candidate('one'), 0)\n    lu.assertEquals(candidate('##### hello, world! #'), 5)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_428261_file_names", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # link = inp_fname if inp_fname.startswith('.') else \".{}\".format(inp_fname)\n-- # return link, \"dot{}.symlink\".format(link)\n-- \n-- Return the linkname and filename for dotfile based on user input.\nlocal function file_names(inp_fname)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_428261_file_names.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = file_names\n    lu.assertEquals(candidate('.inp_fname'), {'.inp_fname', 'dot.inp_fname.symlink'})\n    lu.assertEquals(candidate(), {'.a', 'dot.a.symlink'})\n    lu.assertEquals(candidate(), {'.a', 'dot.a.symlink'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_428422_greeting", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return 'Hello, ' + name + '!'\n-- \n--     Build the greeting message\nlocal function greeting(name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_428422_greeting.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = greeting\n    lu.assertEquals(candidate('William'), 'Hello, William!')\n    lu.assertEquals(candidate('Alexander'), 'Hello, Alexander!')\n    lu.assertEquals(candidate('Eric'), 'Hello, Eric!')\n    lu.assertEquals(candidate('Anne'), 'Hello, Anne!')\n    lu.assertEquals(candidate('John'), 'Hello, John!')\n    lu.assertEquals(candidate('Robert'), 'Hello, Robert!')\n    lu.assertEquals(candidate('World'), 'Hello, World!')\n    lu.assertEquals(candidate('Sarah'), 'Hello, Sarah!')\n    lu.assertEquals(candidate('Ryan'), 'Hello, Ryan!')\n    lu.assertEquals(candidate('Michael'), 'Hello, Michael!')\n    lu.assertEquals(candidate('Christopher'), 'Hello, Christopher!')\n    lu.assertEquals(candidate('Bryan'), 'Hello, Bryan!')\n    lu.assertEquals(candidate('Luis'), 'Hello, Luis!')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_429755_select_supporting_facts", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if isinstance(scored_sfs, dict):\n-- #     scored_sfs = [(sf, score) for sf, score in scored_sfs.items()]\n-- # scored_sfs.sort(key=lambda tup: tup[1], reverse=True)\n-- # sfs = []\n-- # for r, sf_score in enumerate(scored_sfs):\n-- #     sf, score = sf_score\n-- #     thresh = min_thresholds[min(r, len(min_thresholds)-1)]\n-- #     if score >= thresh:\n-- #         sfs.append(sf)\n-- # return sfs\n-- \n-- select supporting facts according to the provided thresholds\n-- :param scored_sfs: a list of (sentence_id, score)\n-- :param min_thresholds: a list of minimum scores for top ranked supporting facts:\n--        [min_score_for_top_ranked, min_score_for_second_ranked, min_score_for_others]\n-- :return: a list of sentence ids predicted as supporting facts\nlocal function select_supporting_facts(scored_sfs, min_thresholds)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_429755_select_supporting_facts.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = select_supporting_facts\n    lu.assertEquals(candidate({{0, 0.4}, {1, 0.5}, {2, 0.6}}, {0.6, 0.6}), {2})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_431077_check_for_take", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if string.isdigit() and int(string) < 30:\n-- #     return True\n-- # return False\n-- \n-- Helper Function that checks to see if clapperboard elements\n-- follows set of rules to be identified as a \"take\" element\n-- Parameters\n-- ----------\n-- string: String\n--     (The string thay will be checked\n-- Returns\n-- -------\n-- Boolean\n--     True if object follows set of rules\n--     False if not.\nlocal function check_for_take(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_431077_check_for_take.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = check_for_take\n    lu.assertEquals(candidate('23'), true)\n    lu.assertEquals(candidate('25'), true)\n    lu.assertEquals(candidate('1234567'), false)\n    lu.assertEquals(candidate('1234'), false)\n    lu.assertEquals(candidate('18'), true)\n    lu.assertEquals(candidate('123'), false)\n    lu.assertEquals(candidate('foo'), false)\n    lu.assertEquals(candidate('27'), true)\n    lu.assertEquals(candidate('12345678'), false)\n    lu.assertEquals(candidate('12345'), false)\n    lu.assertEquals(candidate(''), false)\n    lu.assertEquals(candidate('21'), true)\n    lu.assertEquals(candidate('28'), true)\n    lu.assertEquals(candidate('172'), false)\n    lu.assertEquals(candidate('123456'), false)\n    lu.assertEquals(candidate('1234567890'), false)\n    lu.assertEquals(candidate('a'), false)\n    lu.assertEquals(candidate('24'), true)\n    lu.assertEquals(candidate('22'), true)\n    lu.assertEquals(candidate('29'), true)\n    lu.assertEquals(candidate('123456789'), false)\n    lu.assertEquals(candidate('15'), true)\n    lu.assertEquals(candidate('16'), true)\n    lu.assertEquals(candidate('20'), true)\n    lu.assertEquals(candidate('19'), true)\n    lu.assertEquals(candidate('17'), true)\n    lu.assertEquals(candidate('11'), true)\n    lu.assertEquals(candidate('30'), false)\n    lu.assertEquals(candidate('26'), true)\n    lu.assertEquals(candidate('100'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_431116_replace", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # fields = template.split(pattern, 1)\n-- # if len(fields) > 1:\n-- #     return '%s%s%s' % (fields[0], subst, fields[1])\n-- # else:\n-- #     return template\n-- \n-- If pattern in the template replaces it with subst.\n-- Returns str object template with replaced patterns. \nlocal function replace(template, pattern, subst)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_431116_replace.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = replace\n    lu.assertEquals(candidate('The quick brown fox jumps over the lazy dog', 'The', 'The'), 'The quick brown fox jumps over the lazy dog')\n    lu.assertEquals(candidate('a test', 'test', 'result'), 'a result')\n    lu.assertEquals(candidate('The quick brown fox jumps over the lazy dog', 'fox', 'fox'), 'The quick brown fox jumps over the lazy dog')\n    lu.assertEquals(candidate('The quick brown fox jumps over the lazy dog', 'lazy', 'fox'), 'The quick brown fox jumps over the fox dog')\n    lu.assertEquals(candidate(candidate('X', 'Y', 'Z'), 'Y', 'X'), 'X')\n    lu.assertEquals(candidate('The quick brown fox jumps over the lazy dog', 'dog', 'cat'), 'The quick brown fox jumps over the lazy cat')\n    lu.assertEquals(candidate('The quick brown fox jumps over the lazy dog', 'dog', 'fox'), 'The quick brown fox jumps over the lazy fox')\n    lu.assertEquals(candidate('The quick brown fox jumps over the lazy dog', 'fox', 'cat'), 'The quick brown cat jumps over the lazy dog')\n    lu.assertEquals(candidate(candidate('X', 'Y', 'Z'), 'Y', 'Z'), 'X')\n    lu.assertEquals(candidate('The quick brown fox jumps over the lazy dog', 'The', 'a'), 'a quick brown fox jumps over the lazy dog')\n    lu.assertEquals(candidate('The quick brown fox jumps over the lazy dog', 'The', 'lazy'), 'lazy quick brown fox jumps over the lazy dog')\n    lu.assertEquals(candidate(candidate('X', 'X', 'X'), 'X', 'Z'), 'Z')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_432490_splitStringIntoChunks", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if len(string) <= length:\n-- #     return [string]\n-- # else:\n-- #     return [string[0+i: length+i]\n-- #             for i in range(0, len(string), length)]\n-- \n--     Split string into chunks of defined size\nlocal function splitStringIntoChunks(string, length)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_432490_splitStringIntoChunks.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = splitStringIntoChunks\n    lu.assertEquals(candidate('abcdefghijklmn', 3), {'abc', 'def', 'ghi', 'jkl', 'mn'})\n    lu.assertEquals(candidate('abcdef'), {'abcdef'})\n    lu.assertEquals(candidate('abcdefghij'), {'abcdefghij'})\n    lu.assertEquals(candidate('abcdefg', 3), {'abc', 'def', 'g'})\n    lu.assertEquals(candidate('This is a test', 14), {'This is a test'})\n    lu.assertEquals(candidate('abcdefghij', 5), {'abcde', 'fghij'})\n    lu.assertEquals(candidate('ab', 1), {'a', 'b'})\n    lu.assertEquals(candidate('hello world!'), {'hello world!'})\n    lu.assertEquals(candidate('a', 1), {'a'})\n    lu.assertEquals(candidate('This is a test'), {'This is a test'})\n    lu.assertEquals(candidate('', 1), {''})\n    lu.assertEquals(candidate('abcdef', 2), {'ab', 'cd', 'ef'})\n    lu.assertEquals(candidate('abcdef', 3), {'abc', 'def'})\n    lu.assertEquals(candidate('a', 2), {'a'})\n    lu.assertEquals(candidate('abc'), {'abc'})\n    lu.assertEquals(candidate('ab', 2), {'ab'})\n    lu.assertEquals(candidate('abcdef', 2), {'ab', 'cd', 'ef'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_432816_get_base_out_of_iri", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if \"#\" in iri:\n-- #     index = iri.find(\"#\")\n-- #     return iri[:index]\n-- # else:\n-- #     # for example if uri looks like:\n-- #     # http://webprotege.stanford.edu/RDwpQ8vbi7HaApq8VoqJUXH\n-- #     index = iri.rfind(\"/\")\n-- #     return iri[:index]\n-- \n-- Give an iri, returns an the ontology base name\n-- Args:\n--     iri\n-- Returns:\n--     str\nlocal function get_base_out_of_iri(iri)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_432816_get_base_out_of_iri.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_base_out_of_iri\n    lu.assertEquals(candidate('http://example.com#'), 'http://example.com')\n    lu.assertEquals(candidate('http://example.com/ontology#'), 'http://example.com/ontology')\n    lu.assertEquals(candidate('http://webprotege.stanford.edu/RDwpQ8vbi7HaApq8VoqJUXH#'), 'http://webprotege.stanford.edu/RDwpQ8vbi7HaApq8VoqJUXH')\n    lu.assertEquals(candidate('http://webprotege.stanford.edu/RDwpQ8vbi7HaApq8VoqJUXH#Human'), 'http://webprotege.stanford.edu/RDwpQ8vbi7HaApq8VoqJUXH')\n    lu.assertEquals(candidate('http://example.com/ontology/term'), 'http://example.com/ontology')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_433197_binaryalert_yara_match", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return rec['NumMatchedRules'] > 0\n-- \n-- author:       Austin Byers (Airbnb CSIRT)\n-- description:  BinaryAlert found a binary matching a YARA rule\n-- reference:    https://binaryalert.io\nlocal function binaryalert_yara_match(rec)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_433197_binaryalert_yara_match.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = binaryalert_yara_match\n    lu.assertEquals(candidate({['NumMatchedRules'] = 1}), true)\n    lu.assertEquals(candidate({['NumMatchedRules'] = 0}), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_433253__fk", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"{}::{}::{}\".format(feature_name, channel, target)\n-- \n-- Construct a dict key for a feature instance\nlocal function _fk(feature_name, channel, target)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_433253__fk.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _fk\n    lu.assertEquals(candidate('A', 'c1', 't1'), 'A::c1::t1')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_433350_addslashes", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return value.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n-- \n-- Adds slashes before quotes. Useful for escaping strings in CSV, for\n-- example. Less useful for escaping JavaScript; use the ``escapejs``\n-- filter instead.\nlocal function addslashes(value)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_433350_addslashes.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = addslashes\n    lu.assertEquals(candidate('and \" quotes'), 'and \\\\\" quotes')\n    lu.assertEquals(candidate('\"123'), '\\\\\"123')\n    lu.assertEquals(candidate(\"foo 'bar'\"), \"foo \\\\'bar\\\\'\")\n    lu.assertEquals(candidate('a\"b'), 'a\\\\\"b')\n    lu.assertEquals(candidate('\\\\\\\\'), '\\\\\\\\\\\\\\\\')\n    lu.assertEquals(candidate('\\\\r\\\\n'), '\\\\\\\\r\\\\\\\\n')\n    lu.assertEquals(candidate('slash /'), 'slash /')\n    lu.assertEquals(candidate('1\\\\2\\\\3'), '1\\\\\\\\2\\\\\\\\3')\n    lu.assertEquals(candidate('\\\\'), '\\\\\\\\')\n    lu.assertEquals(candidate(\"'\"), \"\\\\'\")\n    lu.assertEquals(candidate('\"foo\"'), '\\\\\"foo\\\\\"')\n    lu.assertEquals(candidate('This is a \"test\"'), 'This is a \\\\\"test\\\\\"')\n    lu.assertEquals(candidate('\\\\\\\\'), '\\\\\\\\\\\\\\\\')\n    lu.assertEquals(candidate('\"'), '\\\\\"')\n    lu.assertEquals(candidate('foo\\\\tbar\\\\n\\\\baz\\\\n'), 'foo\\\\\\\\tbar\\\\\\\\n\\\\\\\\baz\\\\\\\\n')\n    lu.assertEquals(candidate('123\"'), '123\\\\\"')\n    lu.assertEquals(candidate('foo \"bar\"'), 'foo \\\\\"bar\\\\\"')\n    lu.assertEquals(candidate('this is a \\\\ test'), 'this is a \\\\\\\\ test')\n    lu.assertEquals(candidate('test\\\\ntest'), 'test\\\\\\\\ntest')\n    lu.assertEquals(candidate('abc\"def'), 'abc\\\\\"def')\n    lu.assertEquals(candidate('This is a \"\"test\"\"'), 'This is a \\\\\"\\\\\"test\\\\\"\\\\\"')\n    lu.assertEquals(candidate('this is a \"test\"'), 'this is a \\\\\"test\\\\\"')\n    lu.assertEquals(candidate('\\\\\"'), '\\\\\\\\\\\\\"')\n    lu.assertEquals(candidate('123'), '123')\n    lu.assertEquals(candidate('this is a test'), 'this is a test')\n    lu.assertEquals(candidate(\"I'm a fan of yours, Johnny.\"), \"I\\\\'m a fan of yours, Johnny.\")\n    lu.assertEquals(candidate('this is \"a test\"'), 'this is \\\\\"a test\\\\\"')\n    lu.assertEquals(candidate('\\\\\\'\"'), '\\\\\\\\\\\\\\'\\\\\"')\n    lu.assertEquals(candidate(\"this is 'a test'\"), \"this is \\\\'a test\\\\'\")\n    lu.assertEquals(candidate('\\\\t'), '\\\\\\\\t')\n    lu.assertEquals(candidate(\"'foo'\"), \"\\\\'foo\\\\'\")\n    lu.assertEquals(candidate(\"\\\\'\"), \"\\\\\\\\\\\\'\")\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_433455_this_exist_not_null", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if (\n-- #     not param or\n-- #     len(param) < 1\n-- # ):\n-- #     return False\n-- # return True\n-- \n-- Check if parameter exists or not\nlocal function this_exist_not_null(param)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_433455_this_exist_not_null.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = this_exist_not_null\n    lu.assertEquals(candidate('not empty'), true)\n    lu.assertEquals(candidate(None), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_433810_megagcd", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if b == 0:\n-- #     return a, 1, 0\n-- # d, x, y = megagcd(b, a % b)\n-- # return d, y, x - y * (a // b)\n-- \n-- needs in diophantic function\nlocal function megagcd(a, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_433810_megagcd.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = megagcd\n    lu.assertEquals(candidate(1, 1), {1, 0, 1})\n    lu.assertEquals(candidate(1234567890123456789, 1234567890123456789), {1234567890123456789, 0, 1})\n    lu.assertEquals(candidate(2, 1), {1, 0, 1})\n    lu.assertEquals(candidate(2, 4), {2, 1, 0})\n    lu.assertEquals(candidate(40, 120), {40, 1, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_433930_build_select", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if where:\n-- #     return (\n-- #         \"SELECT \" + \", \".join(f\"{w}\" for w in to_select) +\n-- #         f\" FROM \\\"{table}\\\" WHERE \" +\n-- #         \" AND \".join(f\"{w} = :{w}\" for w in where)\n-- #     )\n-- # return (\n-- #     \"SELECT \" + \", \".join(f\"{w}\" for w in to_select) + f\" FROM \\\"{table}\\\"\"\n-- # )\n-- \n-- Build a select request.\n-- Parameters\n-- ----------\n-- table : str\n--     Table where query will be directed.\n-- to_set: iterable\n--     The list of columns to select.\n-- where: iterable\n--     The list of conditions to constrain the query.\n-- Returns\n-- -------\n-- str\n--     Built query string.\nlocal function build_select(table, to_select, where)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_433930_build_select.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = build_select\n    lu.assertEquals(candidate('person', {'first_name', 'last_name'}), 'SELECT first_name, last_name FROM \"person\"')\n    lu.assertEquals(candidate('User', {'User.id', 'User.name'}), 'SELECT User.id, User.name FROM \"User\"')\n    lu.assertEquals(candidate('artists', {'name'}, {'genre', 'name'}), 'SELECT name FROM \"artists\" WHERE genre = :genre AND name = :name')\n    lu.assertEquals(candidate('person', {'first_name', 'last_name'}, {'first_name', 'last_name'}), 'SELECT first_name, last_name FROM \"person\" WHERE first_name = :first_name AND last_name = :last_name')\n    lu.assertEquals(candidate('artists', {'name'}, None), 'SELECT name FROM \"artists\"')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_434357_sample_width_to_string", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return {1: 's8', 2: 's16', 4: 's32'}[sample_width]\n-- \n-- Convert sample width (bytes) to ALSA format string.\nlocal function sample_width_to_string(sample_width)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_434357_sample_width_to_string.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sample_width_to_string\n    lu.assertEquals(candidate(2), 's16')\n    lu.assertEquals(candidate(1), 's8')\n    lu.assertEquals(candidate(4), 's32')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_434952_register_dictionary", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return dict(nickname=nickname, email=email, password=password, repeat_password=repeat_password)\n-- \n-- creates the dictionary that can be used for registration\nlocal function register_dictionary(nickname, email, password, repeat_password)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_434952_register_dictionary.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = register_dictionary\n    lu.assertEquals(candidate('a', 'b', 'c', 'd'), {['nickname'] = 'a', ['email'] = 'b', ['password'] = 'c', ['repeat_password'] = 'd'})\n    lu.assertEquals(candidate('A', '<EMAIL>', 'password', 'password'), {['nickname'] = 'A', ['email'] = '<EMAIL>', ['password'] = 'password', ['repeat_password'] = 'password'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_435735_get_wsgi_header", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return 'HTTP_{}'.format(header.upper().replace('-', '_'))\n-- \n-- Returns a WSGI compliant HTTP header.\n-- See https://www.python.org/dev/peps/pep-3333/#environ-variables for\n-- information from the spec.\nlocal function get_wsgi_header(header)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_435735_get_wsgi_header.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_wsgi_header\n    lu.assertEquals(candidate('accept-charset'), 'HTTP_ACCEPT_CHARSET')\n    lu.assertEquals(candidate('dnt'), 'HTTP_DNT')\n    lu.assertEquals(candidate('content-type'), 'HTTP_CONTENT_TYPE')\n    lu.assertEquals(candidate('CoNtEnT-tyPe'), 'HTTP_CONTENT_TYPE')\n    lu.assertEquals(candidate('content-type'), 'HTTP_CONTENT_TYPE')\n    lu.assertEquals(candidate('content-md5'), 'HTTP_CONTENT_MD5')\n    lu.assertEquals(candidate('expect'), 'HTTP_EXPECT')\n    lu.assertEquals(candidate('user-agent'), 'HTTP_USER_AGENT')\n    lu.assertEquals(candidate('authorization'), 'HTTP_AUTHORIZATION')\n    lu.assertEquals(candidate('Host'), 'HTTP_HOST')\n    lu.assertEquals(candidate('if-none-match'), 'HTTP_IF_NONE_MATCH')\n    lu.assertEquals(candidate('content-TYPE'), 'HTTP_CONTENT_TYPE')\n    lu.assertEquals(candidate('content-length'), 'HTTP_CONTENT_LENGTH')\n    lu.assertEquals(candidate('CONTENT-type'), 'HTTP_CONTENT_TYPE')\n    lu.assertEquals(candidate('X-Some-Header-Here'), 'HTTP_X_SOME_HEADER_HERE')\n    lu.assertEquals(candidate('content-type'), candidate('CONTENT-TYPE'))\n    lu.assertEquals(candidate('X-Some-Header'), 'HTTP_X_SOME_HEADER')\n    lu.assertEquals(candidate('cookie'), 'HTTP_COOKIE')\n    lu.assertEquals(candidate('Content-TYPE'), 'HTTP_CONTENT_TYPE')\n    lu.assertEquals(candidate('x-forwarded-proto'), 'HTTP_X_FORWARDED_PROTO')\n    lu.assertEquals(candidate('upgrade-insecure-requests'), 'HTTP_UPGRADE_INSECURE_REQUESTS')\n    lu.assertEquals(candidate('accept-encoding'), 'HTTP_ACCEPT_ENCODING')\n    lu.assertEquals(candidate('accept-language'), 'HTTP_ACCEPT_LANGUAGE')\n    lu.assertEquals(candidate('host'), 'HTTP_HOST')\n    lu.assertEquals(candidate('connection'), 'HTTP_CONNECTION')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_436371__tbl_str_width", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if num >= len(widths):\n-- #     return \"\"\n-- # return ' width=\"%s\"' % widths[num]\n-- \n-- Returns the width-argument for an html table cell using the width from\n-- the list of widths.\n-- Returns width=\"[some number]\" or the empty-string if no width is\n-- specified for this num\nlocal function _tbl_str_width(num, widths)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_436371__tbl_str_width.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _tbl_str_width\n    lu.assertEquals(candidate(3, {20, 30}), '')\n    lu.assertEquals(candidate(0, {10, 20}), ' width=\"10\"')\n    lu.assertEquals(candidate(0, {}), '')\n    lu.assertEquals(candidate(2, {}), '')\n    lu.assertEquals(candidate(3, {20, 30, 40}), '')\n    lu.assertEquals(candidate(0, {}), '')\n    lu.assertEquals(candidate(1000, {}), '')\n    lu.assertEquals(candidate(4, {1, 2, 3}), '')\n    lu.assertEquals(candidate(2, {10, 20}), '')\n    lu.assertEquals(candidate(3, {20}), '')\n    lu.assertEquals(candidate(2, {10}), '')\n    lu.assertEquals(candidate(0, {10}), ' width=\"10\"')\n    lu.assertEquals(candidate(1, {10, 20}), ' width=\"20\"')\n    lu.assertEquals(candidate(1, {}), '')\n    lu.assertEquals(candidate(1, {10}), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_436426_parse_slots", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # slots = content.split(\",\")\n-- # return \",\".join(s.strip() for s in slots)\n-- \n-- Parse the list of slots.\n-- Cleans up spaces between items.\n-- Parameters\n-- ----------\n-- content: :class:`str`\n--     A string containing comma separated values.\n-- Returns\n-- -------\n-- :class:`str`:\n--     The slots string.\nlocal function parse_slots(content)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_436426_parse_slots.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_slots\n    lu.assertEquals(candidate('Slot1,Slot2,Slot3,Slot4,Slot5,Slot6,Slot7,Slot8'), 'Slot1,Slot2,Slot3,Slot4,Slot5,Slot6,Slot7,Slot8')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('Slot1,Slot2'), 'Slot1,Slot2')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_436791___equivalent_lists", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if len(list1) != len(list2):\n-- #     return False\n-- # for item in list1:\n-- #     if item not in list2:\n-- #         return False\n-- # for item in list2:\n-- #     if item not in list1:\n-- #         return False\n-- # return True\n-- \n-- Return True if list1 and list2 contain the same items.\nlocal function __equivalent_lists(list1, list2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_436791___equivalent_lists.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = __equivalent_lists\n    lu.assertEquals(candidate(list({1, 2, 3, 4}), list({2, 4, 3, 1})), true)\n    lu.assertEquals(candidate(list({1, 2, 3, 4}), list({4, 3, 2, 1})), true)\n    lu.assertEquals(candidate(list({1}), list()), false)\n    lu.assertEquals(candidate(list({1, 2, 3}), list({1, 3, 4})), false)\n    lu.assertEquals(candidate(list({1, 2}), list({2, 1})), true)\n    lu.assertEquals(candidate(list({1, 2, 3, 4}), list({2, 4, 1, 3})), true)\n    lu.assertEquals(candidate(list({1, 2, 3, 4}), list({4, 3, 1, 2})), true)\n    lu.assertEquals(candidate(list({1, 2, 3}), list({2, 1, 3})), true)\n    lu.assertEquals(candidate(list({1}), list({1})), true)\n    lu.assertEquals(candidate(list({1, 2}), list({1, 3})), false)\n    lu.assertEquals(candidate(list({1, 2, 3}), list({3, 1, 2})), true)\n    lu.assertEquals(candidate(list(), list()), true)\n    lu.assertEquals(candidate(list({1, 2, 3, 4}), list({3, 4, 1, 2})), true)\n    lu.assertEquals(candidate(list({1, 2, 3}), list({1, 3, 2})), true)\n    lu.assertEquals(candidate(list({1, 2, 3, 4}), list({3, 4, 2, 1})), true)\n    lu.assertEquals(candidate(list({1}), list({2})), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_43689_detect_graphql", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"query\" in payload\n-- \n-- \nlocal function detect_graphql(payload)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_43689_detect_graphql.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = detect_graphql\n    lu.assertEquals(candidate('this is some text'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_437285_valid", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return 0 <= x < n and 0 <= y < m\n-- \n-- Check if the coordinates are in bounds.\n-- :param x:\n-- :param y:\n-- :param n:\n-- :param m:\n-- :return:\nlocal function valid(x, y, n, m)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_437285_valid.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = valid\n    lu.assertEquals(candidate(0, 0, 3, 2), true)\n    lu.assertEquals(candidate(1, 3, 3, 3), false)\n    lu.assertEquals(candidate(2, 3, 3, 3), false)\n    lu.assertEquals(candidate(2, 2, 3, 3), true)\n    lu.assertEquals(candidate(0, 3, 3, 3), false)\n    lu.assertEquals(candidate(-1, 1, 3, 3), false)\n    lu.assertEquals(candidate(0, -1, 3, 3), false)\n    lu.assertEquals(candidate(-1, -1, 3, 2), false)\n    lu.assertEquals(candidate(3, 0, 3, 2), false)\n    lu.assertEquals(candidate(4, 4, 3, 3), false)\n    lu.assertEquals(candidate(2, 0, 3, 2), true)\n    lu.assertEquals(candidate(2, 1, 3, 3), true)\n    lu.assertEquals(candidate(2, 1, 3, 2), true)\n    lu.assertEquals(candidate(-1, 0, 3, 3), false)\n    lu.assertEquals(candidate(3, 1, 3, 3), false)\n    lu.assertEquals(candidate(3, 0, 3, 3), false)\n    lu.assertEquals(candidate(1, 1, 3, 3), true)\n    lu.assertEquals(candidate(3, 1, 3, 2), false)\n    lu.assertEquals(candidate(4, 3, 3, 2), false)\n    lu.assertEquals(candidate(1, 1, 3, 2), true)\n    lu.assertEquals(candidate(1, -1, 3, 3), false)\n    lu.assertEquals(candidate(0, 0, 3, 3), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_437295_get_max_key", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return max(d, key=d.get)\n-- \n-- Return the key from the dict with the max value.\nlocal function get_max_key(d)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_437295_get_max_key.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_max_key\n    lu.assertEquals(candidate({['b'] = 2}), 'b')\n    lu.assertEquals(candidate({['a'] = 1}), 'a')\n    lu.assertEquals(candidate({['c'] = 3}), 'c')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_43748__is_summary", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return l.startswith(\"Found \") and l.endswith(\"source files)\\n\")\n-- \n-- Checks if the line is the summary line 'Found X errors in Y files (checked Z source files)'\nlocal function _is_summary(l)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_43748__is_summary.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _is_summary\n    lu.assertEquals(candidate('Found 3 errors in 2 files (checked 11 source files)\\nother line'), false)\n    lu.assertEquals(candidate('Found 3 errors in 2 files (checked 11 source files)\\n\\n'), false)\n    lu.assertEquals(candidate('Found 0 errors in 0 files (checked 24 source files)\\n'), true)\n    lu.assertEquals(candidate('Found 0 errors in 1 files (checked 5 source files)\\n'), true)\n    lu.assertEquals(candidate('Found 3 errors in 2 files (checked 11 source files)\\n'), true)\n    lu.assertEquals(candidate('Found 1 errors in 1 files (checked 1 source files)\\n'), true)\n    lu.assertEquals(candidate('Found 10 errors in 100 files (checked 1000 source files)\\n'), true)\n    lu.assertEquals(candidate('Found 1 error in 1 files (checked 10 source files)\\n'), true)\n    lu.assertEquals(candidate('Found 3 errors in 2 files (checked 11 source files\\n'), false)\n    lu.assertEquals(candidate('Found 1 errors in 1 files (checked 1 source files) \\n'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_437626_progessbar", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # length = 20\n-- # progress = int(round(length * new / float(tot)))\n-- # percent = round(new/float(tot) * 100.0, 1)\n-- # bar = '=' * progress + '-' * (length - progress)\n-- # return '[%s] %s %s\\r' % (bar, percent, '%')\n-- \n-- Builds progressbar\n-- Args:\n--     new: current progress\n--     tot: total length of the download\n-- Returns:\n--     progressbar as a string of length 20\nlocal function progessbar(new, tot)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_437626_progessbar.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = progessbar\n    lu.assertEquals(candidate(100, 100), '[====================] 100.0 %\\r')\n    lu.assertEquals(candidate(0, 100), '[--------------------] 0.0 %\\r')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_437635_get_str_bytes_length", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return len(value.encode(\"utf-8\"))\n-- \n--     - source: https://stackoverflow.com/a/30686735/8445442\nlocal function get_str_bytes_length(value)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_437635_get_str_bytes_length.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_str_bytes_length\n    lu.assertEquals(candidate('test'), 4)\n    lu.assertEquals(candidate(''), 0)\n    lu.assertEquals(candidate('1234567890'), 10)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_437844_is_identity_matrix", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # result = len(L) == len(L[0])\n-- # for i in range(len(L)):\n-- #     for j in range(len(L)):\n-- #         if i == j:\n-- #             result *= (L[i][j] == 1)\n-- #         else:\n-- #             result *= (L[i][j] == 0)\n-- # return result\n-- \n--     Returns True if the input matrix is an identity matrix, False otherwise.\nlocal function is_identity_matrix(L)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_437844_is_identity_matrix.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_identity_matrix\n    lu.assertEquals(candidate({{1, 0, 0, 0}, {0, 1, 0, 0}, {0, 0, 1, 0}, {0, 0, 0, 1}}), true)\n    lu.assertEquals(candidate({{1, 1, 1}, {0, 1, 0}, {0, 0, 1}}), false)\n    lu.assertEquals(candidate({{0, 1, 2, 3}, {4, 5, 6, 7}, {8, 9, 10, 11}, {12, 13, 14, 15}}), false)\n    lu.assertEquals(candidate({{1, 0, 0, 0}, {0, 1, 1, 0}, {0, 0, 1, 0}, {0, 0, 0, 1}}), false)\n    lu.assertEquals(candidate({{1, 0, 0, 0}, {0, 1, 0, 0}, {0, 0, 1, 0}, {0, 0, 0, 1}}), true)\n    lu.assertEquals(candidate({{1, 0, 0, 0}, {0, 1, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 1}}), false)\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5, 6}, {7, 8, 9}}), false)\n    lu.assertEquals(candidate({{1, 0, 0, 0}, {0, 1, 0, 0}, {0, 0, 0, 1}, {0, 0, 1, 0}}), false)\n    lu.assertEquals(candidate({{1, 0, 0}, {0, 1, 0}, {0, 0, 1}}), true)\n    lu.assertEquals(candidate({{1, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 1, 0}, {0, 0, 0, 1}}), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_438500_pax_to_human_time", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for x in ['ns', 'us', 'ms', 's', 'ks', 'Ms', 'G', 'T']:\n-- #     if num < 1000.0:\n-- #         return \"%3.3f %s\" % (num, x)\n-- #     num /= 1000.0\n-- # return \"%3.1f %s\" % (num, 's')\n-- \n-- Converts a pax time to a human-readable representation\nlocal function pax_to_human_time(num)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_438500_pax_to_human_time.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = pax_to_human_time\n    lu.assertEquals(candidate(1000000000000000000000), '1.000 T')\n    lu.assertEquals(candidate(1100000000), '1.100 s')\n    lu.assertEquals(candidate(1000000000000), '1.000 ks')\n    lu.assertEquals(candidate(999), '999.000 ns')\n    lu.assertEquals(candidate(1000), '1.000 us')\n    lu.assertEquals(candidate(999999), '999.999 us')\n    lu.assertEquals(candidate(10000), '10.000 us')\n    lu.assertEquals(candidate(123), '123.000 ns')\n    lu.assertEquals(candidate(1000000000), '1.000 s')\n    lu.assertEquals(candidate(100), '100.000 ns')\n    lu.assertEquals(candidate(12345), '12.345 us')\n    lu.assertEquals(candidate(1), '1.000 ns')\n    lu.assertEquals(candidate(12345), '12.345 us')\n    lu.assertEquals(candidate(10), '10.000 ns')\n    lu.assertEquals(candidate(1000000), '1.000 ms')\n    lu.assertEquals(candidate(1000000000000000000), '1.000 G')\n    lu.assertEquals(candidate(1000000000000000), '1.000 Ms')\n    lu.assertEquals(candidate(0), '0.000 ns')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_43932_area", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # A = 0\n-- # for i in range(len(p)):\n-- #     A += p[i - 1][0] * p[i][1] - p[i][0] * p[i - 1][1]\n-- # return A / 2.\n-- \n-- Area of a polygone\n-- :param p: list of the points taken in any orientation,\n--           p[0] can differ from p[-1]\n-- :returns: area\n-- :complexity: linear\nlocal function area(p)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_43932_area.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = area\n    lu.assertEquals(candidate({{0, 0}, {2, 0}, {0, 2}, {0, 0}}), 2)\n    lu.assertEquals(candidate({{0, 0}, {2, 0}, {2, 2}, {0, 2}}), 4)\n    lu.assertEquals(candidate({{0, 0}, {2, 0}, {2, 1}, {0, 1}}), 2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_439474_dms_to_dmm", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return d, m + s / 60\n-- \n-- Converts degrees, minutes and decimal seconds to degrees and decimal minutes.\n-- Example: (41, 24, 12.2) -> (41, 24.2033)\n-- :param int d: degrees\n-- :param int m: minutes\n-- :param float s: decimal seconds\n-- :rtype: str\nlocal function dms_to_dmm(d, m, s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_439474_dms_to_dmm.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = dms_to_dmm\n    lu.assertEquals(candidate(-10, 0, 0), {-10, 0})\n    lu.assertEquals(candidate(41, 24, 0), {41, 24})\n    lu.assertEquals(candidate(10, 0, 0), {10, 0})\n    lu.assertEquals(candidate(0, 0, 0), {0, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_439518_fnSeconds_To_Hours", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # num_hrs = int(time_period/(60.*60.))\n-- # time_period = time_period - num_hrs*60.*60.\n-- # num_mins = int(time_period/60.)\n-- # num_secs = time_period - num_mins*60.\n-- # return num_hrs, num_mins, num_secs\n-- \n-- Convert from seconds to hours, minutes and seconds.\n-- Date: 16 October 2016\n-- originally in AstroFunctions.py\nlocal function fnSeconds_To_Hours(time_period)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_439518_fnSeconds_To_Hours.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = fnSeconds_To_Hours\n    lu.assertEquals(candidate(3600), {1, 0, 0})\n    lu.assertEquals(candidate(60), {0, 1, 0})\n    lu.assertEquals(candidate(0), {0, 0, 0})\n    lu.assertEquals(candidate(30), {0, 0, 30})\n    lu.assertEquals(candidate(90), {0, 1, 30})\n    lu.assertEquals(candidate(1), {0, 0, 1})\n    lu.assertEquals(candidate(0), {0, 0, 0})\n    lu.assertEquals(candidate(3600), {1, 0, 0})\n    lu.assertEquals(candidate(3723), {1, 2, 3})\n    lu.assertEquals(candidate(86400), {24, 0, 0})\n    lu.assertEquals(candidate(120), {0, 2, 0})\n    lu.assertEquals(candidate(1), {0, 0, 1})\n    lu.assertEquals(candidate(60), {0, 1, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_439784_containsDuplicate", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if len(nums) == 0 or len(nums) == 1:\n-- #     return False\n-- # d = {}\n-- # for i in nums:\n-- #     if i in d:\n-- #         return True\n-- #     d[i] = 0\n-- # return False\n-- \n-- :type nums: List[int]\n-- :rtype: bool\nlocal function containsDuplicate(nums)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_439784_containsDuplicate.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = containsDuplicate\n    lu.assertEquals(candidate(list({1, 2, 3})), false)\n    lu.assertEquals(candidate({1, 2}), false)\n    lu.assertEquals(candidate({}), false)\n    lu.assertEquals(candidate(list({1, 2})), false)\n    lu.assertEquals(candidate(list()), false)\n    lu.assertEquals(candidate(list({1, 2, 3, 4})), false)\n    lu.assertEquals(candidate({1}), false)\n    lu.assertEquals(candidate({1, 2, 3}), false)\n    lu.assertEquals(candidate(list({1, 1, 1, 3, 3, 4, 3, 2, 4, 2})), true)\n    lu.assertEquals(candidate(list({1})), false)\n    lu.assertEquals(candidate({1, 2, 2}), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_440003_map_range", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # mapped = (value - from_lower) * (to_upper - to_lower) / (\n-- #     from_upper - from_lower\n-- # ) + to_lower\n-- # return round(min(max(mapped, to_lower), to_upper))\n-- \n-- Map a value in one range to another.\nlocal function map_range(value, from_lower, from_upper, to_lower, to_upper)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_440003_map_range.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = map_range\n    lu.assertEquals(candidate(5, 1, 10, 10, 10), 10)\n    lu.assertEquals(candidate(5, 1, 10, 1, 1), 1)\n    lu.assertEquals(candidate(7, 1, 10, 1, 10), 7)\n    lu.assertEquals(candidate(10, 1, 10, 0, 1), 1)\n    lu.assertEquals(candidate(6, 0, 10, 100, 200), 160)\n    lu.assertEquals(candidate(5, 1, 10, 5, 5), 5)\n    lu.assertEquals(candidate(50, 0, 100, 0, 200), 100)\n    lu.assertEquals(candidate(0, 0, 10, 0, 100), 0)\n    lu.assertEquals(candidate(15, 0, 10, 100, 200), 200)\n    lu.assertEquals(candidate(7, 0, 10, 100, 200), 170)\n    lu.assertEquals(candidate(10, 0, 10, 100, 0), 0)\n    lu.assertEquals(candidate(5, 0, 10, 0, 100), 50)\n    lu.assertEquals(candidate(1, 1, 10, 1, 10), 1)\n    lu.assertEquals(candidate(13, 0, 10, 100, 200), 200)\n    lu.assertEquals(candidate(16, 0, 10, 100, 200), 200)\n    lu.assertEquals(candidate(10, 0, 10, 100, 200), 200)\n    lu.assertEquals(candidate(0, 0, 100, 0, 100), 0)\n    lu.assertEquals(candidate(11, 0, 10, 100, 200), 200)\n    lu.assertEquals(candidate(50, 0, 100, 0, 100), 50)\n    lu.assertEquals(candidate(10, 1, 10, 1, 10), 10)\n    lu.assertEquals(candidate(0, 1, 10, 1, 10), 1)\n    lu.assertEquals(candidate(10, 1, 10, 10, 20), 20)\n    lu.assertEquals(candidate(5, 0, 10, 100, 200), 150)\n    lu.assertEquals(candidate(6, 1, 10, 1, 10), 6)\n    lu.assertEquals(candidate(2, 10, 1, 100, 10), 10)\n    lu.assertEquals(candidate(12, 0, 10, 100, 200), 200)\n    lu.assertEquals(candidate(9, 1, 10, 1, 10), 9)\n    lu.assertEquals(candidate(-5, 0, 10, 0, 100), 0)\n    lu.assertEquals(candidate(100, 0, 100, 0, 100), 100)\n    lu.assertEquals(candidate(14, 0, 10, 100, 200), 200)\n    lu.assertEquals(candidate(0, 0, 100, -100, 100), -100)\n    lu.assertEquals(candidate(5, 1, 10, 1, 10), 5)\n    lu.assertEquals(candidate(10, 0, 10, 0, 100), 100)\n    lu.assertEquals(candidate(4, 1, 10, 1, 10), 4)\n    lu.assertEquals(candidate(10, 10, 1, 1, 100), 1)\n    lu.assertEquals(candidate(100, 0, 100, 0, 200), 200)\n    lu.assertEquals(candidate(10, 1, 10, 0, 100), 100)\n    lu.assertEquals(candidate(2, 1, 10, 1, 10), 2)\n    lu.assertEquals(candidate(0, 0, 100, 0, 200), 0)\n    lu.assertEquals(candidate(100, 0, 100, -200, -100), -100)\n    lu.assertEquals(candidate(4, 0, 10, 100, 200), 140)\n    lu.assertEquals(candidate(3, 1, 10, 1, 10), 3)\n    lu.assertEquals(candidate(9, 0, 10, 100, 200), 190)\n    lu.assertEquals(candidate(8, 1, 10, 1, 10), 8)\n    lu.assertEquals(candidate(50, 0, 100, 100, 200), 150)\n    lu.assertEquals(candidate(8, 0, 10, 100, 200), 180)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_44003_vmul", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return [v1*v2 for v1, v2 in zip(vec1, vec2)]\n-- \n-- Return element wise multiplication\nlocal function vmul(vec1, vec2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_44003_vmul.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = vmul\n    lu.assertEquals(candidate({1, -2, 3}, {1, 2, 3}), {1, -4, 9})\n    lu.assertEquals(candidate({1, 2, 3}, {1, 2, 3}), {1, 4, 9})\n    lu.assertEquals(candidate({1, 2, -3}, {1, 2, 3}), {1, 4, -9})\n    lu.assertEquals(candidate({1000, 2000, 3000}, {1, 2, 3}), {1000, 4000, 9000})\n    lu.assertEquals(candidate({-1, 2, 3}, {1, -2, 3}), {-1, -4, 9})\n    lu.assertEquals(candidate({1, 2, 3}, {10, 20, 30}), {10, 40, 90})\n    lu.assertEquals(candidate({2}, {2}), {4})\n    lu.assertEquals(candidate({3, 1, 5}, {1, 2, 0}), {3, 2, 0})\n    lu.assertEquals(candidate({-3, 0, 1}, {1, -1, 3}), {-3, 0, 3})\n    lu.assertEquals(candidate({1, 2, 3}, {1, 2, 3}), {1, 4, 9})\n    lu.assertEquals(candidate({1, 2, 3}, {-1, 2, -3}), {-1, 4, -9})\n    lu.assertEquals(candidate({1, 2, 3}, {100, 200, 300}), {100, 400, 900})\n    lu.assertEquals(candidate({1, 2, 3}, {1, 0, 3}), {1, 0, 9})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_440770_filter_python_file", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # python_files = []\n-- # for i in files:\n-- #     parent_dir = i.split(\"/\")[0]\n-- #     if parent_dir == \"simulations\" and i.endswith(\".py\"):\n-- #         python_files.append(i)\n-- # return python_files\n-- \n-- filter python files from simulation folder\nlocal function filter_python_file(files)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_440770_filter_python_file.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = filter_python_file\n    lu.assertEquals(candidate({'simulations/example.py', 'simulations/example_1.py', 'simulations/example_2.py'}), {'simulations/example.py', 'simulations/example_1.py', 'simulations/example_2.py'})\n    lu.assertEquals(candidate({'simulations/test_a.py', 'simulations/test_b.py', 'simulations/test_c.txt'}), {'simulations/test_a.py', 'simulations/test_b.py'})\n    lu.assertEquals(candidate({'simulations/file1.py', 'simulations/subdir/file2.py', 'file3.py', 'simulations/subdir/file4.pyc'}), {'simulations/file1.py', 'simulations/subdir/file2.py'})\n    lu.assertEquals(candidate({'simulations/file1.py', 'simulations/subdir/file2.py', 'file3.py'}), {'simulations/file1.py', 'simulations/subdir/file2.py'})\n    lu.assertEquals(candidate({'simulations/test_a.py', 'simulations/test_b.py'}), {'simulations/test_a.py', 'simulations/test_b.py'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_441120_get_method_color", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # color = {}\n-- # color['Random'] = 'blue'\n-- # color['Target'] = 'cyan'\n-- # color['Minority'] = 'cyan'\n-- # color['Loss'] = 'yellow'\n-- # color['BoostIn'] = 'orange'\n-- # color['LeafInfSP'] = 'brown'\n-- # color['TREX'] = 'green'\n-- # color['TreeSim'] = 'mediumseagreen'\n-- # color['InputSim'] = 'gray'\n-- # color['LOO'] = 'red'\n-- # color['SubSample'] = 'rebeccapurple'\n-- # color['LeafInfluence'] = 'brown'\n-- # color['LeafRefit'] = 'gray'\n-- # assert method in color, f'{method} not in color dict'\n-- # return color[method]\n-- \n--     Return color given the method name.\nlocal function get_method_color(method)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_441120_get_method_color.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_method_color\n    lu.assertEquals(candidate('Random'), 'blue')\n    lu.assertEquals(candidate('SubSample'), 'rebeccapurple')\n    lu.assertEquals(candidate('BoostIn'), 'orange')\n    lu.assertEquals(candidate('Loss'), 'yellow')\n    lu.assertEquals(candidate('LOO'), 'red')\n    lu.assertEquals(candidate('TreeSim'), 'mediumseagreen')\n    lu.assertEquals(candidate('Target'), 'cyan')\n    lu.assertEquals(candidate('InputSim'), 'gray')\n    lu.assertEquals(candidate('LeafInfSP'), 'brown')\n    lu.assertEquals(candidate('TREX'), 'green')\n    lu.assertEquals(candidate('Minority'), 'cyan')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_441315_filter_func", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Tuples are saved as lists in Spark dataframes, convert them back to tuples\n-- # # and use set for faster searching\n-- # bg = set([tuple(b) for b in bigrams])\n-- # # Look for specific bigrams in the list\n-- # return (('complimentary', 'copy') in bg) or \\\n-- #        (('discount', 'exchange') in bg) or \\\n-- #        (('exchange', 'product') in bg) or \\\n-- #        (('exchange', 'review') in bg) or \\\n-- #        (('exchange', 'unbiased') in bg) or \\\n-- #        (('exchange', 'free') in bg) or \\\n-- #        (('exchange', 'honest') in bg) or \\\n-- #        (('exchange', 'true') in bg) or \\\n-- #        (('exchange', 'truth') in bg) or \\\n-- #        (('fair', 'review') in bg) or \\\n-- #        (('free', 'discount') in bg) or \\\n-- #        (('free', 'exchange') in bg) or \\\n-- #        (('free', 'sample') in bg) or \\\n-- #        (('free', 'unbiased') in bg) or \\\n-- #        (('honest', 'feedback') in bg) or \\\n-- #        (('honest', 'unbiased') in bg) or \\\n-- #        (('opinion', 'state') in bg) or \\\n-- #        (('opinion', 'own') in bg) or \\\n-- #        (('provide', 'exchange') in bg) or \\\n-- #        (('provide', 'sample') in bg) or \\\n-- #        (('provided', 'sample') in bg) or \\\n-- #        (('provided', 'exchange') in bg) or \\\n-- #        (('receive', 'free') in bg) or \\\n-- #        (('receive', 'free') in bg) or \\\n-- #        (('received', 'free') in bg) or \\\n-- #        (('received', 'sample') in bg) or \\\n-- #        (('return', 'unbiased') in bg) or \\\n-- #        (('review', 'sample') in bg) or \\\n-- #        (('sample', 'product') in bg) or \\\n-- #        (('sample', 'unbiased') in bg) or \\\n-- #        (('sample', 'free') in bg) or \\\n-- #        (('send', 'sample') in bg) or \\\n-- #        (('unbiased', 'review') in bg) or \\\n-- #        (('unbiased', 'opinion') in bg) or \\\n-- #        (('unbiased', 'view') in bg)\n-- \n-- Given a list of bigrams from a review's text, returns true if the review is\n-- incentivized by using a lookup list\n-- :param bigrams: Bigrams from the review's text, lemmatized for better matching\n-- :return: True if the review is incentivized, False otherwise\nlocal function filter_func(bigrams)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_441315_filter_func.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = filter_func\n    lu.assertEquals(candidate({{'free', 'discount'}, {'free', 'exchange'}}), true)\n    lu.assertEquals(candidate({{'unbiased', 'view'}}), true)\n    lu.assertEquals(candidate({{'return', 'unbiased'}, {'review', 'sample'}}), true)\n    lu.assertEquals(candidate({{'opinion', 'state'}, {'opinion', 'own'}}), true)\n    lu.assertEquals(candidate({{'complimentary', 'copy'}, {'discount', 'exchange'}}), true)\n    lu.assertEquals(candidate({{'sample', 'product'}, {'sample', 'unbiased'}}), true)\n    lu.assertEquals(candidate({{'provide', 'exchange'}, {'provided', 'sample'}}), true)\n    lu.assertEquals(candidate({{'free', 'sample'}, {'free', 'unbiased'}}), true)\n    lu.assertEquals(candidate({{'honest', 'feedback'}, {'honest', 'unbiased'}}), true)\n    lu.assertEquals(candidate({{'receive', 'free'}, {'received', 'sample'}}), true)\n    lu.assertEquals(candidate({{'unbiased', 'review'}, {'unbiased', 'opinion'}}), true)\n    lu.assertEquals(candidate({{'sample', 'free'}, {'send', 'sample'}}), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_441869_is_candidate_word", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # discarded_words = [\"a\", \"an\", \"the\"]  # can enhance this list\n-- # if len(word) <= 2 or word.lower() in discarded_words:\n-- #     return False\n-- # return True\n-- \n--     check a word is correct candidate word for identifying pronoun\nlocal function is_candidate_word(word)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_441869_is_candidate_word.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_candidate_word\n    lu.assertEquals(candidate('The Dog'), true)\n    lu.assertEquals(candidate('the cat'), true)\n    lu.assertEquals(candidate(''), false)\n    lu.assertEquals(candidate('the Cat'), true)\n    lu.assertEquals(candidate('the'), false)\n    lu.assertEquals(candidate('a'), false)\n    lu.assertEquals(candidate('cat'), true)\n    lu.assertEquals(candidate('apple'), true)\n    lu.assertEquals(candidate('The Cat'), true)\n    lu.assertEquals(candidate('The dog'), true)\n    lu.assertEquals(candidate('an'), false)\n    lu.assertEquals(candidate('The'), false)\n    lu.assertEquals(candidate('the Dog'), true)\n    lu.assertEquals(candidate('dog'), true)\n    lu.assertEquals(candidate(' '), false)\n    lu.assertEquals(candidate('banana'), true)\n    lu.assertEquals(candidate('the dog'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_441928_mtoft", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # ft = m * (12 * 25.4 / 1000) ** -1\n-- # return ft\n-- \n-- Convertie les meters en feets\n-- note: 12 [in] = 1 [ft] and 1 [in] = 25.4 [mm] and 1000 [mm] = 1 [m]\n-- :param m: length [m]\n-- :return ft: length [ft]\nlocal function mtoft(m)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_441928_mtoft.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = mtoft\n    lu.assertEquals(candidate(0), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_442368_remove_leading_zeros", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # ret_val = \"\"\n-- # for n in numeric_string:\n-- #     if n != \"0\":\n-- #         ret_val += n\n-- # return ret_val\n-- \n-- >>> remove_leading_zeros(\"0033\")\n-- '33'\nlocal function remove_leading_zeros(numeric_string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_442368_remove_leading_zeros.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = remove_leading_zeros\n    lu.assertEquals(candidate('000'), '')\n    lu.assertEquals(candidate('00'), '')\n    lu.assertEquals(candidate('1234'), '1234')\n    lu.assertEquals(candidate('0123'), '123')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_442462_count_leading_spaces", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return len(string) - len(string.lstrip(\" \"))\n-- \n-- Count the number of spaces in a string before any other character.\n-- :param string: input string\n-- :return: number of spaces\nlocal function count_leading_spaces(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_442462_count_leading_spaces.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = count_leading_spaces\n    lu.assertEquals(candidate('  hello  \\tworld'), 2)\n    lu.assertEquals(candidate('  hello'), 2)\n    lu.assertEquals(candidate('  hello'), 2)\n    lu.assertEquals(candidate('hello\\tworld'), 0)\n    lu.assertEquals(candidate('hello'), 0)\n    lu.assertEquals(candidate('  hello  '), 2)\n    lu.assertEquals(candidate('     hello     '), 5)\n    lu.assertEquals(candidate('  hello   world'), 2)\n    lu.assertEquals(candidate('  hello  '), 2)\n    lu.assertEquals(candidate(''), 0)\n    lu.assertEquals(candidate('  hello world'), 2)\n    lu.assertEquals(candidate('hello   '), 0)\n    lu.assertEquals(candidate('This is a string without leading spaces.'), 0)\n    lu.assertEquals(candidate('    hello'), 4)\n    lu.assertEquals(candidate('    hello'), 4)\n    lu.assertEquals(candidate('hello'), 0)\n    lu.assertEquals(candidate('   hello'), 3)\n    lu.assertEquals(candidate('hello    '), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_442522_editDistance", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # ignore the case\n-- # s1 = s1.upper()\n-- # s2 = s2.upper()\n-- # if len(s1) > len(s2):\n-- #     s1, s2 = s2, s1\n-- # distances = list(range(len(s1) + 1))\n-- # for i2, c2 in enumerate(s2):\n-- #     distances_ = [i2+1]\n-- #     for i1, c1 in enumerate(s1):\n-- #         if c1 == c2:\n-- #             distances_.append(distances[i1])\n-- #         else:\n-- #             distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n-- #     distances = distances_\n-- # return distances[-1]\n-- \n--  Calculate edit distance between 2 strings\n-- Args:\n--     s1: string 1\n--     s2: string 2\n-- Returns:\n--     Edit distance between s1 and s2\nlocal function editDistance(s1, s2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_442522_editDistance.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = editDistance\n    lu.assertEquals(candidate('aaab', 'aaa'), 1)\n    lu.assertEquals(candidate('aaac', 'aaa'), 1)\n    lu.assertEquals(candidate('aab', 'aaa'), 1)\n    lu.assertEquals(candidate('a', 'bb'), 2)\n    lu.assertEquals(candidate('aaa', 'aaa'), 0)\n    lu.assertEquals(candidate('a', 'a'), 0)\n    lu.assertEquals(candidate('', ''), 0)\n    lu.assertEquals(candidate('foo', 'fO0baR'), 4)\n    lu.assertEquals(candidate('aaa', 'aaab'), 1)\n    lu.assertEquals(candidate('a', 'ccc'), 3)\n    lu.assertEquals(candidate('ABC', 'ABC'), 0)\n    lu.assertEquals(candidate('bb', 'a'), 2)\n    lu.assertEquals(candidate('a', 'b'), 1)\n    lu.assertEquals(candidate('', 'a'), 1)\n    lu.assertEquals(candidate('foo', 'bar'), 3)\n    lu.assertEquals(candidate('b', 'a'), 1)\n    lu.assertEquals(candidate('aaa', 'aab'), 1)\n    lu.assertEquals(candidate('sunday', 'saturday'), 3)\n    lu.assertEquals(candidate('kitten', 'sitting'), 3)\n    lu.assertEquals(candidate('ccc', 'a'), 3)\n    lu.assertEquals(candidate('a', ''), 1)\n    lu.assertEquals(candidate('aaa', 'aaac'), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_442974_label_to_int", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if label == 'female':\n-- #     return 0\n-- # else:\n-- #     return 1\n-- # # end if\n-- \n-- Label to tensor\n-- :param label:\n-- :return:\nlocal function label_to_int(label)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_442974_label_to_int.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = label_to_int\n    lu.assertEquals(candidate('male'), 1)\n    lu.assertEquals(candidate('female'), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_443062_partition", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # This is NOT correct\n-- # first = ''\n-- # second = ''\n-- # for x in s:\n-- #     pos = s.find(x)\n-- #     if pos % 2 == 0:\n-- #         first = first + x\n-- #     else:\n-- #         second = second + x\n-- # return [first, second]\n-- \n-- Returns: a list splitting s in two parts\n-- The 1st element of the list consists of all characters in even\n-- positions (starting at 0), while the 2nd element is the odd\n-- positions.\n-- Examples:\n--     partition('abcde') is ['ace','bd']\n--     partition('aabb') is ['ab', 'ab']\n-- Parameter s: the string to partition\n-- Precondition: s is a string\nlocal function partition(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_443062_partition.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = partition\n    lu.assertEquals(candidate('abcde'), {'ace', 'bd'})\n    lu.assertEquals(candidate('12345'), {'135', '24'})\n    lu.assertEquals(candidate('a'), {'a', ''})\n    lu.assertEquals(candidate(''), {'', ''})\n    lu.assertEquals(candidate('abcde'), {'ace', 'bd'})\n    lu.assertEquals(candidate(''), {'', ''})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_443442_column", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return [row[i] for row in matrix]\n-- \n-- Gets column of matrix. \n-- INPUTS:     \n--     Matrix, Int of column to look at\n-- RETURNS:    \n--     Array of the column\nlocal function column(matrix, i)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_443442_column.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = column\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5, 6}}, 1), {2, 5})\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5, 6}}, 0), {1, 4})\n    lu.assertEquals(candidate({{1, 2, 3}, {4, 5, 6}}, 2), {3, 6})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_444222__get_num_to_fold", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return int(round(ngates * (stretch - 1.0) / 2.0))\n-- \n-- Returns the number of gates to fold to achieve the desired (approximate)\n-- stretch factor.\n-- Args:\n--     stretch: Floating point value to stretch the circuit by.\n--     ngates: Number of gates in the circuit to stretch.\nlocal function _get_num_to_fold(stretch, ngates)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_444222__get_num_to_fold.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _get_num_to_fold\n    lu.assertEquals(candidate(1.0, 1), 0)\n    lu.assertEquals(candidate(3.5, 5), 6)\n    lu.assertEquals(candidate(3.0, 5), 5)\n    lu.assertEquals(candidate(1.5, 2), 0)\n    lu.assertEquals(candidate(2.0, 3), 2)\n    lu.assertEquals(candidate(1.0, 5), 0)\n    lu.assertEquals(candidate(2.0, 1000), 500)\n    lu.assertEquals(candidate(2.0, 100), 50)\n    lu.assertEquals(candidate(1.1, 10), 1)\n    lu.assertEquals(candidate(1.4, 12), 2)\n    lu.assertEquals(candidate(1.6, 12), 4)\n    lu.assertEquals(candidate(1.0, 100), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_444241_bin2int", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # print('bin conversion', bin, int(bin, 2))\n-- # return int(bin, 2)\n-- \n-- convert the binary (as string) to integer\nlocal function bin2int(bin)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_444241_bin2int.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = bin2int\n    lu.assertEquals(candidate('10111110011'), 1523)\n    lu.assertEquals(candidate('10101110'), 174)\n    lu.assertEquals(candidate('100101100101110111'), 153975)\n    lu.assertEquals(candidate('10101110010111'), 11159)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_444506_rename_dupe_cols", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # cols = [str(c) for c in cols]\n-- # new_cols = []\n-- # for i in range(len(cols)):\n-- #     c = cols.pop(0)\n-- #     while c in new_cols:\n-- #         try:\n-- #             num = int(''.join(x for x in c if x.isdigit()))\n-- #             c = c.replace(str(num), str(num + 1))\n-- #         except:\n-- #             c = c + str(2)\n-- #     new_cols.append(c)\n-- # return new_cols\n-- \n-- Renames duplicate columns in order of occurrence.\n-- columns [0, 0, 0, 0]\n-- turn into [0, 1, 2, 3]\n-- columns [name10, name10, name10, name10]\n-- turn into [name10, name11, name12, name13]\n-- :param cols: iterable of columns\n-- :return: unique columns with digits incremented.\nlocal function rename_dupe_cols(cols)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_444506_rename_dupe_cols.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = rename_dupe_cols\n    lu.assertEquals(candidate({'name10', 'name10', 'name10', 'name10', 'name10', 'name10'}), {'name10', 'name11', 'name12', 'name13', 'name14', 'name15'})\n    lu.assertEquals(candidate({'name1', 'name2', 'name3', 'name4', 'name5', 'name6', 'name7', 'name8'}), {'name1', 'name2', 'name3', 'name4', 'name5', 'name6', 'name7', 'name8'})\n    lu.assertEquals(candidate({'name1', 'name2', 'name3', 'name4', 'name5', 'name6', 'name7', 'name8', 'name9'}), {'name1', 'name2', 'name3', 'name4', 'name5', 'name6', 'name7', 'name8', 'name9'})\n    lu.assertEquals(candidate({'name10', 'name10', 'name10', 'name10'}), {'name10', 'name11', 'name12', 'name13'})\n    lu.assertEquals(candidate({'name10', 'name11', 'name12', 'name13', 'name14', 'name15'}), {'name10', 'name11', 'name12', 'name13', 'name14', 'name15'})\n    lu.assertEquals(candidate(list('abc')), list('abc'))\n    lu.assertEquals(candidate({'name1', 'name1', 'name1', 'name1', 'name1', 'name1', 'name1', 'name1'}), {'name1', 'name2', 'name3', 'name4', 'name5', 'name6', 'name7', 'name8'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_445003_add_trailing_slash", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not s.endswith('/'):\n-- #     s = s + '/'\n-- # return (s)\n-- \n-- Add trailing slash. \nlocal function add_trailing_slash(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_445003_add_trailing_slash.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = add_trailing_slash\n    lu.assertEquals(candidate('fiddle/'), 'fiddle/')\n    lu.assertEquals(candidate('eggplant/'), 'eggplant/')\n    lu.assertEquals(candidate('durian'), 'durian/')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_445676__format_time", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # US_IN_SECOND = 1000.0 * 1000.0\n-- # US_IN_MS = 1000.0\n-- # if time_us >= US_IN_SECOND:\n-- #     return '{:.3f}s'.format(time_us / US_IN_SECOND)\n-- # if time_us >= US_IN_MS:\n-- #     return '{:.3f}ms'.format(time_us / US_IN_MS)\n-- # return '{:.3f}us'.format(time_us)\n-- \n-- Defines how to format time in FunctionEvent\nlocal function _format_time(time_us)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_445676__format_time.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _format_time\n    lu.assertEquals(candidate(100), '100.000us')\n    lu.assertEquals(candidate(1000), '1.000ms')\n    lu.assertEquals(candidate(10000), '10.000ms')\n    lu.assertEquals(candidate(100000), '100.000ms')\n    lu.assertEquals(candidate(1000000), '1.000s')\n    lu.assertEquals(candidate(100000000), '100.000s')\n    lu.assertEquals(candidate(10000000), '10.000s')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_445722_org_add_payload", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # add_payload = org_default_payload\n-- # add_payload[\"action\"] = \"member_added\"\n-- # return add_payload\n-- \n-- Provide an organization payload for adding a member.\nlocal function org_add_payload(org_default_payload)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_445722_org_add_payload.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = org_add_payload\n    lu.assertEquals(candidate({['action'] = 'member_added'}), {['action'] = 'member_added'})\n    lu.assertEquals(candidate({['action'] = 'member_removed'}), {['action'] = 'member_added'})\n    lu.assertEquals(candidate({['action'] = 'member_removed'}), {['action'] = 'member_added'})\n    lu.assertEquals(candidate({['action'] = 'member_added'}), {['action'] = 'member_added'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_445953_dopant_site", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if 'M_Cd/' in poscar:\n-- #     return 'M_Cd'\n-- # if 'M_S/' in poscar:\n-- #     return 'M_S'\n-- # if 'M_Se/' in poscar:\n-- #     return 'M_Se'\n-- # if 'M_Te/' in poscar:\n-- #     return 'M_Te'\n-- # if 'M_i_Cd_site/' in poscar:\n-- #     return 'M_i_Cd_site'\n-- # if 'M_i_S_site/' in poscar:\n-- #     return 'M_i_S_site'\n-- # if 'M_i_Se_site/' in poscar:\n-- #     return 'M_i_Se_site'\n-- # if 'M_i_Te_site/' in poscar:\n-- #     return 'M_i_Te_site'\n-- # if 'M_i_other/' in poscar:\n-- #     return 'M_i_old'\n-- # if 'M_i_neutral_site/' in poscar:\n-- #     return 'M_i_old'\n-- # else:\n-- #     raise Exception(\"Unknown dopant site given by %s\" % poscar)\n-- \n-- With the given VASP POSCAR file, determine the impurity location\n-- given the folder structure\n-- Inputs\n-- ------\n-- poscar:  File path for the relevant POSCAR file\n-- Outputs\n-- -------\n-- impurity type of the defect as defined by collaborators\nlocal function dopant_site(poscar)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_445953_dopant_site.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = dopant_site\n    lu.assertEquals(candidate('M_i_S_site/POSCAR'), 'M_i_S_site')\n    lu.assertEquals(candidate('M_i_Se_site/POSCAR'), 'M_i_Se_site')\n    lu.assertEquals(candidate('M_i_Cd_site/POSCAR'), 'M_i_Cd_site')\n    lu.assertEquals(candidate('M_Se/POSCAR'), 'M_Se')\n    lu.assertEquals(candidate('M_Cd/POSCAR'), 'M_Cd')\n    lu.assertEquals(candidate('M_i_other/POSCAR'), 'M_i_old')\n    lu.assertEquals(candidate('M_i_neutral_site/POSCAR'), 'M_i_old')\n    lu.assertEquals(candidate('M_Cd/POSCAR'), 'M_Cd')\n    lu.assertEquals(candidate('M_i_Te_site/POSCAR'), 'M_i_Te_site')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_446431_decolonize", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return val.strip(\":\")\n-- \n-- Remove the colon at the end of the word\n-- This will be used by the unique word of\n-- template class to sanitize attr accesses\nlocal function decolonize(val)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_446431_decolonize.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = decolonize\n    lu.assertEquals(candidate(str(400)), '400')\n    lu.assertEquals(candidate(str(1600)), '1600')\n    lu.assertEquals(candidate(str(2100)), '2100')\n    lu.assertEquals(candidate(str(1900)), '1900')\n    lu.assertEquals(candidate(str(1100)), '1100')\n    lu.assertEquals(candidate(str(200)), '200')\n    lu.assertEquals(candidate(str(600)), '600')\n    lu.assertEquals(candidate(str(800)), '800')\n    lu.assertEquals(candidate('Hello'), 'Hello')\n    lu.assertEquals(candidate(str(300)), '300')\n    lu.assertEquals(candidate('Hello '), 'Hello ')\n    lu.assertEquals(candidate(str(100)), '100')\n    lu.assertEquals(candidate(str(1000)), '1000')\n    lu.assertEquals(candidate(str(1400)), '1400')\n    lu.assertEquals(candidate(str(1500)), '1500')\n    lu.assertEquals(candidate(str(500)), '500')\n    lu.assertEquals(candidate(str(700)), '700')\n    lu.assertEquals(candidate(':foo'), 'foo')\n    lu.assertEquals(candidate('Hello :  '), 'Hello :  ')\n    lu.assertEquals(candidate('  '), '  ')\n    lu.assertEquals(candidate(':FOO'), 'FOO')\n    lu.assertEquals(candidate('Foo:'), 'Foo')\n    lu.assertEquals(candidate(str(2000)), '2000')\n    lu.assertEquals(candidate('FOO'), 'FOO')\n    lu.assertEquals(candidate(str(1800)), '1800')\n    lu.assertEquals(candidate(str(900)), '900')\n    lu.assertEquals(candidate('FOO:'), 'FOO')\n    lu.assertEquals(candidate(str(1200)), '1200')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('foo:'), 'foo')\n    lu.assertEquals(candidate(' '), ' ')\n    lu.assertEquals(candidate('foo'), 'foo')\n    lu.assertEquals(candidate(str(1700)), '1700')\n    lu.assertEquals(candidate(':Foo'), 'Foo')\n    lu.assertEquals(candidate('Foo'), 'Foo')\n    lu.assertEquals(candidate(str(1300)), '1300')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_446564__nipype_logging_config", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return {\n-- #     \"workflow_level\": \"INFO\",  # possible options:\n-- #     \"filemanip_level\": \"INFO\",  # INFO (default) | DEBUG\n-- #     \"interface_level\": \"INFO\",\n-- #     \"log_directory\": cachedir,\n-- # }\n-- \n-- This Function takes in...\n-- :param wfrun: cachedir\n-- :return:\nlocal function _nipype_logging_config(cachedir)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_446564__nipype_logging_config.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _nipype_logging_config\n    lu.assertEquals(candidate('testdir'), {['workflow_level'] = 'INFO', ['filemanip_level'] = 'INFO', ['interface_level'] = 'INFO', ['log_directory'] = 'testdir'})\n    lu.assertEquals(candidate('foo'), {['workflow_level'] = 'INFO', ['filemanip_level'] = 'INFO', ['interface_level'] = 'INFO', ['log_directory'] = 'foo'})\n    lu.assertEquals(candidate('cachedir'), {['workflow_level'] = 'INFO', ['filemanip_level'] = 'INFO', ['interface_level'] = 'INFO', ['log_directory'] = 'cachedir'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_446882_sort_012", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # current_index = 0\n-- # zero_index = 0\n-- # two_index = len(input_list) - 1\n-- # while current_index <= two_index:\n-- #     if input_list[current_index] is 2:\n-- #         input_list[current_index], input_list[two_index] = input_list[two_index], input_list[current_index]\n-- #         two_index -= 1\n-- #         continue\n-- #     if input_list[current_index] is 0:\n-- #         input_list[current_index], input_list[zero_index] = input_list[zero_index], input_list[current_index]\n-- #         zero_index += 1\n-- #     current_index += 1\n-- # return input_list\n-- \n-- Sort a list containing the integers 0, 1, and 2, in a single traversal\n-- :param input_list: list\n-- :return: list\nlocal function sort_012(input_list)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_446882_sort_012.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sort_012\n    lu.assertEquals(candidate({1, 0, 2}), {0, 1, 2})\n    lu.assertEquals(candidate({2, 0, 1}), {0, 1, 2})\n    lu.assertEquals(candidate({1, 2, 0}), {0, 1, 2})\n    lu.assertEquals(candidate({0, 2, 1}), {0, 1, 2})\n    lu.assertEquals(candidate({0, 0, 2}), {0, 0, 2})\n    lu.assertEquals(candidate({2, 2, 2}), {2, 2, 2})\n    lu.assertEquals(candidate({2, 1, 0}), {0, 1, 2})\n    lu.assertEquals(candidate({0, 0, 0}), {0, 0, 0})\n    lu.assertEquals(candidate({0, 1, 2}), {0, 1, 2})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_446968_format_markdown", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # try:\n-- #     fmt = content.format(**params)\n-- # except KeyError:\n-- #     fmt = content\n-- # return fmt\n-- \n-- Format content with config parameters.\n-- Arguments:\n--     content {str} -- Unformatted content\n-- Returns:\n--     {str} -- Formatted content\nlocal function format_markdown(content, params)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_446968_format_markdown.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = format_markdown\n    lu.assertEquals(candidate('The following is a **{document}** document.', {['document'] = 'document'}), 'The following is a **document** document.')\n    lu.assertEquals(candidate('The following is a **markdown** document.', {['document'] = 'document'}), 'The following is a **markdown** document.')\n    lu.assertEquals(candidate('The following is a **{markdown}** document.', {}), 'The following is a **{markdown}** document.')\n    lu.assertEquals(candidate('The following is a **markdown** document.', {['document'] = 'document', ['markdown'] = '**markdown**'}), 'The following is a **markdown** document.')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_447141_distance_between_points", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return ((first_point[0] - second_point[0]) ** 2 + (first_point[1] - second_point[1]) ** 2) ** 0.5\n-- \n-- Calculates the Distance between 2 points. (x^2 + y^2) ^ 0.5\n-- :param first_point: tuple of x and y value of point 1\n-- :param second_point: tuple of x and y value of point 2\n-- :return: Float value of the distance between the 2 points\n-- :rtype: float\nlocal function distance_between_points(first_point, second_point)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_447141_distance_between_points.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = distance_between_points\n    lu.assertEquals(candidate({0, 1}, {1, 0}), 1.4142135623730951)\n    lu.assertEquals(candidate({-1, 0}, {3, 0}), 4.0)\n    lu.assertEquals(candidate({0, 0}, {3, 4}), 5.0)\n    lu.assertEquals(candidate({0, 0}, {1, 1}), 1.4142135623730951)\n    lu.assertEquals(candidate({-1, -1}, {1, 1}), 2.8284271247461903)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_448228_ind_dict2list", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # l = list(range(len(dic)))\n-- # for item, index in dic.items():\n-- #     l[index] = item\n-- # return l\n-- \n-- :param dic: dictionary form object ot index, starting from zero\n-- :return:\nlocal function ind_dict2list(dic)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_448228_ind_dict2list.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = ind_dict2list\n    lu.assertEquals(candidate({['x'] = 0, ['y'] = 1, ['z'] = 2}), {'x', 'y', 'z'})\n    lu.assertEquals(candidate(dict(zip(list('abcde'), range(5)))), list('abcde'))\n    lu.assertEquals(candidate(dict()), {})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_448750_simple_decompression", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # string = \"\"\n-- # i = 0\n-- # len_string = len(compressed_string)\n-- # while True:\n-- #     if compressed_string[i].isdigit():\n-- #         s = i\n-- #         while i < len_string and compressed_string[i].isdigit():\n-- #             i += 1\n-- #         n = int(compressed_string[s:i])\n-- #         string += string[-1]*(n-1)\n-- #     else:\n-- #         string += compressed_string[i]\n-- #         i += 1\n-- #     if i == len_string:\n-- #         break\n-- # return string\n-- \n-- Decompression for `simple_compression(string)`\nlocal function simple_decompression(compressed_string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_448750_simple_decompression.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = simple_decompression\n    lu.assertEquals(candidate('c3'), 'ccc')\n    lu.assertEquals(candidate('a4'), 'aaaa')\n    lu.assertEquals(candidate('b1'), 'b')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_448838_pad_diff", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # h_diff = desired_height - actual_height\n-- # w_diff = desired_width - actual_width\n-- # padding = (0, 0, w_diff, h_diff)  # left, top, right, bottom\n-- # return padding\n-- \n--  Pads img_arr width or height < samples_size with zeros \nlocal function pad_diff(actual_height, actual_width, desired_height, desired_width)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_448838_pad_diff.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = pad_diff\n    lu.assertEquals(candidate(10, 10, 15, 15), {0, 0, 5, 5})\n    lu.assertEquals(candidate(2, 2, 4, 4), {0, 0, 2, 2})\n    lu.assertEquals(candidate(2, 2, 3, 3), {0, 0, 1, 1})\n    lu.assertEquals(candidate(100, 100, 100, 200), {0, 0, 100, 0})\n    lu.assertEquals(candidate(200, 300, 200, 300), {0, 0, 0, 0})\n    lu.assertEquals(candidate(100, 100, 200, 100), {0, 0, 0, 100})\n    lu.assertEquals(candidate(4, 6, 4, 6), {0, 0, 0, 0})\n    lu.assertEquals(candidate(1, 1, 3, 2), {0, 0, 1, 2})\n    lu.assertEquals(candidate(1, 1, 2, 2), {0, 0, 1, 1})\n    lu.assertEquals(candidate(100, 100, 100, 100), {0, 0, 0, 0})\n    lu.assertEquals(candidate(10, 10, 10, 10), {0, 0, 0, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_44905_calc_intersection", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # p1, p2, p3, p4 = r1[0], r1[1], r2[0], r2[1]\n-- # x1, y1 = p1[0], p1[1]\n-- # x2, y2 = p2[0], p2[1]\n-- # x3, y3 = p3[0], p3[1]\n-- # x4, y4 = p4[0], p4[1]\n-- # denom = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)\n-- # if denom == 0:\n-- #     return None  # parallel\n-- # else:\n-- #     tmp_x = (x1 * y2 - y1 * x2) * (x3 - x4) - (x1 - x2) * (x3 * y4 - y3 * x4)\n-- #     tmp_y = (x1 * y2 - y1 * x2) * (y3 - y4) - (y1 - y2) * (x3 * y4 - y3 * x4)\n-- #     return tuple([tmp_x / denom, tmp_y / denom])\n-- \n-- Calculates the intersection of 2 rects\n-- Maths from https://es.wikipedia.org/wiki/Intersecci%C3%B3n_de_dos_rectas\n-- :param r1:\n-- :param r2:\n-- :return:\nlocal function calc_intersection(r1, r2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_44905_calc_intersection.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = calc_intersection\n    lu.assertEquals(candidate({{0, 0}, {0, 1}}, {{0, 0}, {1, 0}}), {0, 0})\n    lu.assertEquals(candidate({{0, 0}, {0, 1}}, {{-1, 0}, {1, 0}}), {0, 0})\n    lu.assertEquals(candidate({{1, 1}, {2, 2}}, {{-1, 1}, {1, 3}}), None)\n    lu.assertEquals(candidate({{1, 1}, {3, 3}}, {{1, 2}, {3, 2}}), {2, 2})\n    lu.assertEquals(candidate({{0, 0}, {0, 1}}, {{0, 0}, {1, 1}}), {0, 0})\n    lu.assertEquals(candidate({{0, 0}, {0, 1}}, {{-1, 1}, {1, 1}}), {0, 1})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_449923_get_as_clause_multicol", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # column_str = []\n-- # for col in columns_to_query_lst:\n-- #     column_str.append(col)\n-- # for col in update_param_list:\n-- #     column_str.append(col)\n-- # as_clause = \",\".join(column_str)\n-- # as_clause = \"as c(\" + as_clause + \")\"\n-- # return as_clause\n-- \n-- get_as_clause will return tuple of column names of intermediate table c of postgresql query.\n-- :param columns_to_query_lst: columns for where clause.E.g [col1]\n-- :param update_param_list: new column names for columns to be updated.E.g [updatecol2,updatecol3]\n-- :return as_clause: string. E.g \"as c(col1,updatecol2,updatecol3)\"\nlocal function get_as_clause_multicol(columns_to_query_lst, update_param_list)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_449923_get_as_clause_multicol.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_as_clause_multicol\n    lu.assertEquals(candidate({'col1'}, {'updatecol2', 'updatecol3'}), 'as c(col1,updatecol2,updatecol3)')\n    lu.assertEquals(candidate({'col1', 'col2'}, {'updatecol2', 'updatecol3'}), 'as c(col1,col2,updatecol2,updatecol3)')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_450025_word_count", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not text:\n-- #     return 0\n-- # count = 0\n-- # inside_word = False\n-- # for char in text:\n-- #     if char.isspace():\n-- #         inside_word = False\n-- #     elif not inside_word:\n-- #         count += 1\n-- #         inside_word = True\n-- # return count\n-- \n--  The word-count of the given text. Goes through the string exactly\n-- once and has constant memory usage. Not super sophisticated though.\nlocal function word_count(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_450025_word_count.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = word_count\n    lu.assertEquals(candidate('a b c d e '), 5)\n    lu.assertEquals(candidate('a'), 1)\n    lu.assertEquals(candidate('\\nfoo'), 1)\n    lu.assertEquals(candidate('foo\\tbar'), 2)\n    lu.assertEquals(candidate('a b c d e f g h i j k l m n o p q r s t u v w x y z'), 26)\n    lu.assertEquals(candidate('a b'), 2)\n    lu.assertEquals(candidate(''), 0)\n    lu.assertEquals(candidate('a b c d e f g'), 7)\n    lu.assertEquals(candidate('foo\\n  \\n  bar'), 2)\n    lu.assertEquals(candidate('foo  bar  baz'), 3)\n    lu.assertEquals(candidate('foo bar baz quux'), 4)\n    lu.assertEquals(candidate('foo\\rbar'), 2)\n    lu.assertEquals(candidate(''), 0)\n    lu.assertEquals(candidate('foo '), 1)\n    lu.assertEquals(candidate('\\n\\n\\n\\n\\n'), 0)\n    lu.assertEquals(candidate('foo\\n'), 1)\n    lu.assertEquals(candidate(None), 0)\n    lu.assertEquals(candidate('a b c d e'), 5)\n    lu.assertEquals(candidate('a'), 1)\n    lu.assertEquals(candidate('\\n'), 0)\n    lu.assertEquals(candidate('a b c d e f '), 6)\n    lu.assertEquals(candidate(''), 0)\n    lu.assertEquals(candidate('Hello  World!'), 2)\n    lu.assertEquals(candidate('a b c d e f g h '), 8)\n    lu.assertEquals(candidate('foo \\n bar \\n baz'), 3)\n    lu.assertEquals(candidate('\\t\\n\\n\\n\\t\\t'), 0)\n    lu.assertEquals(candidate('a b c'), 3)\n    lu.assertEquals(candidate('a b c d e f g '), 7)\n    lu.assertEquals(candidate('foo\\n  bar'), 2)\n    lu.assertEquals(candidate(''), 0)\n    lu.assertEquals(candidate('foo\\n\\nbar'), 2)\n    lu.assertEquals(candidate('foo\\tbar\\tbaz?'), 3)\n    lu.assertEquals(candidate(' foo'), 1)\n    lu.assertEquals(candidate('a b c d e f g h'), 8)\n    lu.assertEquals(candidate('foo\\nbar\\nbaz'), 3)\n    lu.assertEquals(candidate('a b c d e f g h i '), 9)\n    lu.assertEquals(candidate('foo'), 1)\n    lu.assertEquals(candidate(''), 0)\n    lu.assertEquals(candidate('foo bar baz'), 3)\n    lu.assertEquals(candidate('a b c d'), 4)\n    lu.assertEquals(candidate('a b c d e f g h i'), 9)\n    lu.assertEquals(candidate('foo  \\t\\tbar'), 2)\n    lu.assertEquals(candidate('foo\\nbar'), 2)\n    lu.assertEquals(candidate(' a b  c '), 3)\n    lu.assertEquals(candidate('a b c d e f'), 6)\n    lu.assertEquals(candidate('foo bar'), 2)\n    lu.assertEquals(candidate('foo\\tbar\\tbaz'), 3)\n    lu.assertEquals(candidate('a b c d '), 4)\n    lu.assertEquals(candidate('foo'), 1)\n    lu.assertEquals(candidate('Hello\\n\\nWorld!'), 2)\n    lu.assertEquals(candidate('Hello World!'), 2)\n    lu.assertEquals(candidate('  '), 0)\n    lu.assertEquals(candidate('  foo'), 1)\n    lu.assertEquals(candidate('foo\\t\\tbar'), 2)\n    lu.assertEquals(candidate('foo   bar  \\t\\n  baz'), 3)\n    lu.assertEquals(candidate('Hello\\tWorld!'), 2)\n    lu.assertEquals(candidate('foo bar baz'), 3)\n    lu.assertEquals(candidate(' a b '), 2)\n    lu.assertEquals(candidate('a b'), 2)\n    lu.assertEquals(candidate('foo\\r\\nbar'), 2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_45007_process_value", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if len(value) > 0 and value[0] == value[-1] == '\"':\n-- #     return value[1:-1]\n-- # return value\n-- \n-- Returns a processed value for an environment variable.\nlocal function process_value(value)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_45007_process_value.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = process_value\n    lu.assertEquals(candidate('\"a\"'), 'a')\n    lu.assertEquals(candidate('\"a'), '\"a')\n    lu.assertEquals(candidate('\"a\"b'), '\"a\"b')\n    lu.assertEquals(candidate('a'), 'a')\n    lu.assertEquals(candidate('\"a b\"'), 'a b')\n    lu.assertEquals(candidate('\"a\"'), 'a')\n    lu.assertEquals(candidate('a\"b\"'), 'a\"b\"')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('a\"'), 'a\"')\n    lu.assertEquals(candidate('a\"b'), 'a\"b')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_450178_startWithArabic", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return Instr[:1].isdigit()\n-- \n--     this function return true if the given string starts with am Arabic numeral\nlocal function startWithArabic(Instr)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_450178_startWithArabic.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = startWithArabic\n    lu.assertEquals(candidate('1999'), true)\n    lu.assertEquals(candidate('12345'), true)\n    lu.assertEquals(candidate('7'), true)\n    lu.assertEquals(candidate('1'), true)\n    lu.assertEquals(candidate('123'), true)\n    lu.assertEquals(candidate('8'), true)\n    lu.assertEquals(candidate('10000000'), true)\n    lu.assertEquals(candidate('999'), true)\n    lu.assertEquals(candidate('7000'), true)\n    lu.assertEquals(candidate('x'), false)\n    lu.assertEquals(candidate(''), false)\n    lu.assertEquals(candidate('0'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_450346_search_escape", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return url.replace('{', '{{').replace('}', '}}')\n-- \n-- Escape URLs such that preexisting { and } are handled properly.\n-- Will obviously trash a properly-formatted qutebrowser URL.\nlocal function search_escape(url)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_450346_search_escape.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = search_escape\n    lu.assertEquals(candidate('http://example.com/?q=a}b'), 'http://example.com/?q=a}}b')\n    lu.assertEquals(candidate('some/url/with/{special characters}'), 'some/url/with/{{special characters}}')\n    lu.assertEquals(candidate('some/url/with/no/{characters}'), 'some/url/with/no/{{characters}}')\n    lu.assertEquals(candidate('some/url/with/no/special/characters'), 'some/url/with/no/special/characters')\n    lu.assertEquals(candidate('https://duckduckgo.com/?q=foo+bar&t=h_'), 'https://duckduckgo.com/?q=foo+bar&t=h_')\n    lu.assertEquals(candidate('http://example.com/?q=a{'), 'http://example.com/?q=a{{')\n    lu.assertEquals(candidate('http://example.com/?q=}'), 'http://example.com/?q=}}')\n    lu.assertEquals(candidate('http://example.com/?q=a}'), 'http://example.com/?q=a}}')\n    lu.assertEquals(candidate('https://duckduckgo.com/?q=foo+bar&t=h_'), 'https://duckduckgo.com/?q=foo+bar&t=h_')\n    lu.assertEquals(candidate('http://example.com/?q=a}b}'), 'http://example.com/?q=a}}b}}')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_450375_get_y_indicator_variable_index", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return i*n + j\n-- \n-- Map the i,j indices to the sequential indicator variable index\n-- for the y_{ij} variable.\n-- This is basically the (2-dimensional) 'array equation' (as per\n-- row-major arrays in C for example).\n-- Note that for MiniSat+, the variables are juist indexed sequentially\n-- and we are mapping the y_{ij} to y_r for 0 <= r < m*n variables.\n-- This function gets the sequential index for a y_{ij} variable.\n-- Parameters:\n--     i, j   - indices for y indicator variable\n--     m - order of tableau a (0 <= i,k < m)\n--     n - order of tableau b (0 <= j,l < n)\n-- Return value:\n--     index r of indicator variable y_{r} corresponding to y_{ij}\nlocal function get_y_indicator_variable_index(i, j, m, n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_450375_get_y_indicator_variable_index.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_y_indicator_variable_index\n    lu.assertEquals(candidate(0, 0, 0, 0), 0)\n    lu.assertEquals(candidate(1, 1, 2, 2), 3)\n    lu.assertEquals(candidate(0, 3, 5, 6), 3)\n    lu.assertEquals(candidate(0, 0, 5, 6), 0)\n    lu.assertEquals(candidate(0, 1, 1, 2), 1)\n    lu.assertEquals(candidate(0, 0, 1, 1), 0)\n    lu.assertEquals(candidate(0, 2, 5, 6), 2)\n    lu.assertEquals(candidate(1, 0, 1, 1), 1)\n    lu.assertEquals(candidate(0, 0, 3, 4), 0)\n    lu.assertEquals(candidate(1, 0, 2, 1), 1)\n    lu.assertEquals(candidate(0, 0, 1, 2), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_450502_default", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not input_str:\n-- #     return name\n-- # return input_str.strip()\n-- \n--     Return default if no input_str, otherwise stripped input_str.\nlocal function default(input_str, name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_450502_default.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = default\n    lu.assertEquals(candidate('', 'name'), 'name')\n    lu.assertEquals(candidate('abc', None), 'abc')\n    lu.assertEquals(candidate('  hi  ', 'candidate'), 'hi')\n    lu.assertEquals(candidate(None, 'name'), 'name')\n    lu.assertEquals(candidate('  ', None), '')\n    lu.assertEquals(candidate('  abc  ', None), 'abc')\n    lu.assertEquals(candidate(None, None), None)\n    lu.assertEquals(candidate(None, 'bob'), 'bob')\n    lu.assertEquals(candidate('1', None), '1')\n    lu.assertEquals(candidate('  bob  ', 'bob'), 'bob')\n    lu.assertEquals(candidate('name', 'name'), 'name')\n    lu.assertEquals(candidate('\\t   \\t', 'candidate'), '')\n    lu.assertEquals(candidate('   str   ', 'candidate'), 'str')\n    lu.assertEquals(candidate('bob', 'bob'), 'bob')\n    lu.assertEquals(candidate('  ', 'bob'), '')\n    lu.assertEquals(candidate(None, 'candidate'), 'candidate')\n    lu.assertEquals(candidate('\\n\\n', 'candidate'), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_451058_est_palindrome3", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # i: int\n-- # for i in range(0, len(s)//2):\n-- #     if s[i] != s[-i-1]:\n-- #         return False\n-- # return True\n-- \n--  ... cf. ci-dessus ...\nlocal function est_palindrome3(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_451058_est_palindrome3.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = est_palindrome3\n    lu.assertEquals(candidate('abb'), false)\n    lu.assertEquals(candidate('aabaa'), true)\n    lu.assertEquals(candidate('abbbbbbbbbbba'), true)\n    lu.assertEquals(candidate('abbaabb'), false)\n    lu.assertEquals(candidate('abbbbba'), true)\n    lu.assertEquals(candidate('0'), true)\n    lu.assertEquals(candidate('toto0'), false)\n    lu.assertEquals(candidate('abbbbbbbaa'), false)\n    lu.assertEquals(candidate('abbbaabb'), false)\n    lu.assertEquals(candidate('abbaabba'), true)\n    lu.assertEquals(candidate(''), true)\n    lu.assertEquals(candidate('aba'), true)\n    lu.assertEquals(candidate('00'), true)\n    lu.assertEquals(candidate('abbba'), true)\n    lu.assertEquals(candidate('ab'), false)\n    lu.assertEquals(candidate('abbbbbbba'), true)\n    lu.assertEquals(candidate('bab'), true)\n    lu.assertEquals(candidate('abba'), true)\n    lu.assertEquals(candidate('a'), true)\n    lu.assertEquals(candidate('abbab'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_451156_dhash_hamming_distance", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # difference = (int(dhash1, 16)) ^ (int(dhash2, 16))\n-- # return bin(difference).count(\"1\")\n-- \n-- Calculate the hamming distance between two dhash values\n-- :param dhash1: str, the dhash of an image returned by `calculate_dhash`\n-- :param dhash2: str, the dhash of an image returned by `calculate_dhash`\n-- :return: int, the hamming distance between two dhash values\nlocal function dhash_hamming_distance(dhash1, dhash2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_451156_dhash_hamming_distance.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = dhash_hamming_distance\n    lu.assertEquals(candidate('1234567890abcdef1234567890abcdef', '1234567890abcdef1234567890abcdef'), 0)\n    lu.assertEquals(candidate('1234567890abcdef1234567890abcdee', '1234567890abcdef1234567890abcdee'), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_451382_findimagenumber", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # split the file so that name is a string equal to OBSDATE+number\n-- # name = filename.split('/')[-1].split('.')[0]\n-- # return int(name[9:])\n-- \n-- find the number for each image file\nlocal function findimagenumber(filename)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_451382_findimagenumber.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = findimagenumber\n    lu.assertEquals(candidate('Data/OBSDATE0001.jpg'), 1)\n    lu.assertEquals(candidate('Data/OBSDATE0010.jpg'), 10)\n    lu.assertEquals(candidate('OBSDATE0001.fits'), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_451777__rescale_score_by_abs", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if -1e-5 < min_score and max_score < 1e-5:\n-- #     return .5\n-- # elif max_score == min_score and min_score < 0:\n-- #     return 0.\n-- # elif max_score == min_score and max_score > 0:\n-- #     return 1.\n-- # top = max(abs(max_score), abs(min_score))\n-- # return (score + top) / (2. * top)\n-- \n-- Normalizes an attribution score to the range [0., 1.], where a score\n-- score of 0. is mapped to 0.5.\n-- :param score: An attribution score\n-- :param max_score: The maximum possible attribution score\n-- :param min_score: The minimum possible attribution score\n-- :return: The normalized score\nlocal function _rescale_score_by_abs(score, max_score, min_score)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_451777__rescale_score_by_abs.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _rescale_score_by_abs\n    lu.assertEquals(candidate(0, 1, 0), 0.5)\n    lu.assertEquals(candidate(-1, 0, 0), 0.5)\n    lu.assertEquals(candidate(1.0, 1.0, -1.0), 1.0)\n    lu.assertEquals(candidate(0, 0, 0), 0.5)\n    lu.assertEquals(candidate(1, 1, 0), 1)\n    lu.assertEquals(candidate(1.0, 1.0, 0.0), 1.0)\n    lu.assertEquals(candidate(0.0, 1.0, -1.0), 0.5)\n    lu.assertEquals(candidate(0.0, 0.0, 0.0), 0.5)\n    lu.assertEquals(candidate(0, 1e-05, 0), 0.5)\n    lu.assertEquals(candidate(0, 0, -1), 0.5)\n    lu.assertEquals(candidate(-1.0, 1.0, 0.0), 0.0)\n    lu.assertEquals(candidate(-1.0, 1.0, -1.0), 0.0)\n    lu.assertEquals(candidate(0.0, 1.0, 0.0), 0.5)\n    lu.assertEquals(candidate(0, 0, 1), 0.5)\n    lu.assertEquals(candidate(0, 0, 0), 0.5)\n    lu.assertEquals(candidate(1, 0, 0), 0.5)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_451986_update_dependencies", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # split dependencies away from their version numbers since we need the names\n-- # # in order to evaluate duplication\n-- # dependency_names = set(dependency.split()[0] for dependency in new_dependencies)\n-- # index_dependency_names = set(index.split()[0] for index in existing_dependencies)\n-- # repeated_packages = index_dependency_names.intersection(dependency_names)\n-- # if len(repeated_packages) > 0:\n-- #     for index_dependency in existing_dependencies:\n-- #         for dependency in repeated_packages:\n-- #             if index_dependency.startswith(dependency):\n-- #                 existing_dependencies.remove(index_dependency)\n-- # existing_dependencies.extend(new_dependencies)\n-- # return existing_dependencies\n-- \n-- Update the source package's existing dependencies.\n-- When a user passes additional dependencies from the command line,\n-- these dependencies will be added to the source package's existing dependencies.\n-- If the dependencies passed from the command line are existing dependencies,\n-- these existing dependencies are overwritten.\n-- Positional arguments:\n-- new_dependencies (List[str]) -- the dependencies passed from the command line\n-- existing_dependencies (List[str]) -- the dependencies found in the source\n--     package's index.json file\nlocal function update_dependencies(new_dependencies, existing_dependencies)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_451986_update_dependencies.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = update_dependencies\n    lu.assertEquals(candidate({'test', 'tests', 'testing'}, {}), {'test', 'tests', 'testing'})\n    lu.assertEquals(candidate({'foo', 'bar', 'baz'}, {}), {'foo', 'bar', 'baz'})\n    lu.assertEquals(candidate({}, {'foo', 'bar', 'baz'}), {'foo', 'bar', 'baz'})\n    lu.assertEquals(candidate({'test', 'tests', 'testing'}, {'test'}), {'test', 'tests', 'testing'})\n    lu.assertEquals(candidate({'foo 4.5.6'}, {'hello 1.2.3', 'goodbye 3.2.1', 'foo 4.5.6'}), {'hello 1.2.3', 'goodbye 3.2.1', 'foo 4.5.6'})\n    lu.assertEquals(candidate({}, {}), {})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_452069_convertBoolean", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if value:\n-- #     jvalue = \"true\"\n-- # else:\n-- #     jvalue = \"false\"\n-- # return jvalue\n-- \n-- Convert Python bool to JS boolean.\n-- Args:\n--     value (bool): True/False\nlocal function convertBoolean(value)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_452069_convertBoolean.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = convertBoolean\n    lu.assertEquals(candidate(false), 'false')\n    lu.assertEquals(candidate(true), 'true')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_452718_hash_generator", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # hash_val = 0\n-- # n = len(tokens) - 1\n-- # base = len(token_to_id) ** n\n-- # for x in tokens:\n-- #     hash_val += token_to_id[x] * base\n-- #     base /= len(token_to_id)\n-- # return hash_val\n-- \n-- Generate hash for tokens in 'tokens' using 'token_to_id'.\n-- Args:\n--     token_to_id: dict. A dictionary which maps each token to a unique ID.\n--     tokens: list(str). A list of tokens.\n-- Returns:\n--     int. Hash value generated for tokens in 'tokens' using 'token_to_id'.\nlocal function hash_generator(token_to_id, tokens)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_452718_hash_generator.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = hash_generator\n    lu.assertEquals(candidate(dict(zip('abcdefghijklmnopqrstuvwxyz', range(26))), 'a'), 0)\n    lu.assertEquals(candidate(dict(zip('abc', range(5))), 'a'), 0)\n    lu.assertEquals(candidate(dict(zip('abc', range(3))), 'a'), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_45275_RPL_ADMINEMAIL", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"<\" + sender + \">: \" + message\n-- \n--  Reply Code 259 \nlocal function RPL_ADMINEMAIL(sender, receipient, message)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_45275_RPL_ADMINEMAIL.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = RPL_ADMINEMAIL\n    lu.assertEquals(candidate('foo', 'bar', 'baz'), '<foo>: baz')\n    lu.assertEquals(candidate('foo', 'bar', 'baz'), '<foo>: baz')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_453163_piece_placed", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if player == 0:\n-- #     board[x][y] = 1\n-- # elif player == 1:\n-- #     board[x][y] = 2\n-- # return board\n-- \n-- This function determines the piece played.\n-- It takes the coordinates of the piece, the player number, and the board.\n-- The pieces are zeros or ones and the function returns the piece on the board based on the number.\nlocal function piece_placed(x, y, player, board)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_453163_piece_placed.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = piece_placed\n    lu.assertEquals(candidate(1, 1, 1, {{0, 0, 0}, {0, 0, 0}, {0, 0, 0}}), {{0, 0, 0}, {0, 2, 0}, {0, 0, 0}})\n    lu.assertEquals(candidate(0, 0, 0, {{0, 0, 0}, {0, 0, 0}, {0, 0, 0}}), {{1, 0, 0}, {0, 0, 0}, {0, 0, 0}})\n    lu.assertEquals(candidate(1, 1, 1, {{1, 0, 0}, {0, 0, 1}, {0, 0, 0}}), {{1, 0, 0}, {0, 2, 1}, {0, 0, 0}})\n    lu.assertEquals(candidate(0, 0, 0, {{0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0}}), {{1, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_453517_do_math", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if operator == \"+\":\n-- #     return int(b) + int(a)\n-- # elif operator == \"-\":\n-- #     return int(b) - int(a)\n-- # elif operator == \"*\":\n-- #     return int(b) * int(a)\n-- # elif operator == \"/\":\n-- #     return int(b) // int(a)\n-- \n-- Helper function that performs computation between two numbers.\nlocal function do_math(a, b, operator)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_453517_do_math.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = do_math\n    lu.assertEquals(candidate(1, 3, '*'), 3)\n    lu.assertEquals(candidate(2, 4, '*'), 8)\n    lu.assertEquals(candidate(1, 2, '*'), 2)\n    lu.assertEquals(candidate(1, 1, '/'), 1)\n    lu.assertEquals(candidate(1, 1, '+'), 2)\n    lu.assertEquals(candidate(1, 3, '-'), 2)\n    lu.assertEquals(candidate(1, 1, '*'), 1)\n    lu.assertEquals(candidate(2, 4, '-'), 2)\n    lu.assertEquals(candidate(1, 2, '+'), 3)\n    lu.assertEquals(candidate(1, 1, '-'), 0)\n    lu.assertEquals(candidate(2, 3, '+'), 5)\n    lu.assertEquals(candidate(2, 4, '/'), 2)\n    lu.assertEquals(candidate(2, 4, '+'), 6)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_453870_get_version", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # assert isinstance(v, tuple)\n-- # assert len(v) == 5\n-- # assert v[3] in ('alpha', 'beta', 'rc', 'final')\n-- # parts = 2 if v[2] == 0 else 3\n-- # main = '.'.join(str(i) for i in v[:parts])\n-- # sub = ''\n-- # if v[3] != 'final':\n-- #     mapping = {'alpha': 'a', 'beta': 'b', 'rc': 'c'}\n-- #     sub = mapping[v[3]] + str(v[4])\n-- # return str(main + sub)\n-- \n-- Generate a PEP386  compliant version\n-- Stolen from django.utils.version.get_version\n-- :param v tuple: A five part tuple indicating the version\n-- :returns str: Compliant version\nlocal function get_version(v)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_453870_get_version.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_version\n    lu.assertEquals(candidate({1, 2, 3, 'beta', 1}), '1.2.3b1')\n    lu.assertEquals(candidate({1, 0, 1, 'alpha', 0}), '1.0.1a0')\n    lu.assertEquals(candidate({0, 1, 0, 'rc', 100}), '0.1c100')\n    lu.assertEquals(candidate({1, 2, 0, 'beta', 11}), '1.2b11')\n    lu.assertEquals(candidate({1, 2, 0, 'final', 0}), '1.2')\n    lu.assertEquals(candidate({0, 1, 0, 'rc', 0}), '0.1c0')\n    lu.assertEquals(candidate({0, 1, 0, 'rc', 10}), '0.1c10')\n    lu.assertEquals(candidate({1, 1, 0, 'final', 0}), '1.1')\n    lu.assertEquals(candidate({1, 2, 0, 'rc', 1}), '1.2c1')\n    lu.assertEquals(candidate({1, 2, 0, 'alpha', 2}), '1.2a2')\n    lu.assertEquals(candidate({0, 1, 1, 'final', 0}), '0.1.1')\n    lu.assertEquals(candidate({1, 0, 1, 'beta', 0}), '1.0.1b0')\n    lu.assertEquals(candidate({1, 1, 0, 'alpha', 1}), '1.1a1')\n    lu.assertEquals(candidate({1, 2, 0, 'beta', 2}), '1.2b2')\n    lu.assertEquals(candidate({0, 1, 0, 'rc', 1}), '0.1c1')\n    lu.assertEquals(candidate({1, 0, 1, 'rc', 0}), '1.0.1c0')\n    lu.assertEquals(candidate({1, 0, 0, 'final', 0}), '1.0')\n    lu.assertEquals(candidate({0, 1, 0, 'final', 1}), '0.1')\n    lu.assertEquals(candidate({1, 2, 1, 'final', 11}), '1.2.1')\n    lu.assertEquals(candidate({1, 2, 0, 'alpha', 0}), '1.2a0')\n    lu.assertEquals(candidate({0, 1, 0, 'beta', 0}), '0.1b0')\n    lu.assertEquals(candidate({1, 1, 0, 'beta', 3}), '1.1b3')\n    lu.assertEquals(candidate({0, 1, 0, 'final', 100}), '0.1')\n    lu.assertEquals(candidate({1, 1, 0, 'rc', 3}), '1.1c3')\n    lu.assertEquals(candidate({0, 1, 1, 'beta', 2}), '0.1.1b2')\n    lu.assertEquals(candidate({1, 2, 0, 'alpha', 11}), '1.2a11')\n    lu.assertEquals(candidate({2, 1, 0, 'final', 0}), '2.1')\n    lu.assertEquals(candidate({0, 1, 0, 'final', 10}), '0.1')\n    lu.assertEquals(candidate({0, 1, 0, 'alpha', 0}), '0.1a0')\n    lu.assertEquals(candidate({0, 1, 0, 'rc', 1000}), '0.1c1000')\n    lu.assertEquals(candidate({1, 0, 1, 'final', 0}), '1.0.1')\n    lu.assertEquals(candidate({2, 1, 1, 'final', 0}), '2.1.1')\n    lu.assertEquals(candidate({1, 2, 1, 'final', 0}), '1.2.1')\n    lu.assertEquals(candidate({1, 2, 0, 'rc', 11}), '1.2c11')\n    lu.assertEquals(candidate({0, 1, 0, 'final', 0}), '0.1')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_454038_norm_mac", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # mac = mac.lower().replace(':', '')\n-- # return ':'.join(mac[i:i + 2] for i in range(0, len(mac), 2))\n-- \n-- Normalize a MAC Address from the pypowervm format to the neutron format.\n-- That means that the format will be converted to lower case and will have\n-- colons added.\n-- :param mac: A pypowervm mac address.  E.g. 1234567890AB\n-- :returns: A mac that matches the standard neutron format.\n--           E.g. 12:34:56:78:90:ab\nlocal function norm_mac(mac)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_454038_norm_mac.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = norm_mac\n    lu.assertEquals(candidate('1234567890AB'), '12:34:56:78:90:ab')\n    lu.assertEquals(candidate('1234567890AB'), '12:34:56:78:90:ab')\n    lu.assertEquals(candidate('1234:5678:90AB'), '12:34:56:78:90:ab')\n    lu.assertEquals(candidate('1234567890ab'), '12:34:56:78:90:ab')\n    lu.assertEquals(candidate('1234567890AB'), '12:34:56:78:90:ab')\n    lu.assertEquals(candidate('1234567890AB'), '12:34:56:78:90:ab')\n    lu.assertEquals(candidate('1234567890ab'), '12:34:56:78:90:ab')\n    lu.assertEquals(candidate('000000000000'), '00:00:00:00:00:00')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_454515_contains_common_item_2", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # array1_dict = {}\n-- # for item in arr1:\n-- #     array1_dict[item] = True\n-- # for item2 in arr2:\n-- #     if array1_dict.get(item2, False):\n-- #         return True\n-- # return False\n-- \n-- loop through first array and create dictionary object where the keys are the items in the array\n-- loop through the second array and check if item in second array exists in the created dictionary\nlocal function contains_common_item_2(arr1, arr2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_454515_contains_common_item_2.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = contains_common_item_2\n    lu.assertEquals(candidate({'apples', 'carrots', 'pears'}, {'oranges', 'bananas', 'apples'}), true)\n    lu.assertEquals(candidate({'a', 'b', 'c', 'd'}, {'a', 'y', 'x'}), true)\n    lu.assertEquals(candidate({}, {'z', 'y', 'x'}), false)\n    lu.assertEquals(candidate({'a', 'b', 'c', 'd'}, {'x', 'y', 'z'}), false)\n    lu.assertEquals(candidate({'a', 'b', 'c', 'd'}, {}), false)\n    lu.assertEquals(candidate({'a', 'b', 'c', 'd'}, {'b', 'c', 'd'}), true)\n    lu.assertEquals(candidate({'b', 'a', 'c', 'd'}, {'z', 'y', 'x'}), false)\n    lu.assertEquals(candidate({}, {}), false)\n    lu.assertEquals(candidate({'a', 'b', 'c', 'd'}, {'z', 'y', 'x'}), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_454709_port_to_ip_mapping", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return {\"vEth0_{}\".format(index): \"192.167.10.{}\".format(index + 1)}\n-- \n--     A user defined mapping port_id (kni) to ipv4.\nlocal function port_to_ip_mapping(index)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_454709_port_to_ip_mapping.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = port_to_ip_mapping\n    lu.assertEquals(candidate(0), {['vEth0_0'] = '192.167.10.1'})\n    lu.assertEquals(candidate(2), {['vEth0_2'] = '192.167.10.3'})\n    lu.assertEquals(candidate(3), {['vEth0_3'] = '192.167.10.4'})\n    lu.assertEquals(candidate(3), {['vEth0_3'] = '192.167.10.4'})\n    lu.assertEquals(candidate(7), {['vEth0_7'] = '192.167.10.8'})\n    lu.assertEquals(candidate(5), {['vEth0_5'] = '192.167.10.6'})\n    lu.assertEquals(candidate(5), {['vEth0_5'] = '192.167.10.6'})\n    lu.assertEquals(candidate(1), {['vEth0_1'] = '192.167.10.2'})\n    lu.assertEquals(candidate(4), {['vEth0_4'] = '192.167.10.5'})\n    lu.assertEquals(candidate(6), {['vEth0_6'] = '192.167.10.7'})\n    lu.assertEquals(candidate(1), {['vEth0_1'] = '192.167.10.2'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_455809__handle_sort_key", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # sort_keys_extra = {'alarm': ['name', 'user_id', 'project_id'],\n-- #                    'meter': ['user_id', 'project_id'],\n-- #                    'resource': ['user_id', 'project_id', 'timestamp'],\n-- #                    }\n-- # sort_keys = sort_keys_extra[model_name]\n-- # if not sort_key:\n-- #     return sort_keys\n-- # # NOTE(Fengqian): We need to put the sort key from user\n-- # # in the first place of sort keys list.\n-- # try:\n-- #     sort_keys.remove(sort_key)\n-- # except ValueError:\n-- #     pass\n-- # finally:\n-- #     sort_keys.insert(0, sort_key)\n-- # return sort_keys\n-- \n-- Generate sort keys according to the passed in sort key from user.\n-- :param model_name: Database model name be query.(alarm, meter, etc.)\n-- :param sort_key: sort key passed from user.\n-- return: sort keys list\nlocal function _handle_sort_key(model_name, sort_key)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_455809__handle_sort_key.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _handle_sort_key\n    lu.assertEquals(candidate('alarm', 'name'), {'name', 'user_id', 'project_id'})\n    lu.assertEquals(candidate('meter', 'user_id'), {'user_id', 'project_id'})\n    lu.assertEquals(candidate('meter', 'user_id'), {'user_id', 'project_id'})\n    lu.assertEquals(candidate('resource', None), {'user_id', 'project_id', 'timestamp'})\n    lu.assertEquals(candidate('meter', 'user_id'), {'user_id', 'project_id'})\n    lu.assertEquals(candidate('resource', 'user_id'), {'user_id', 'project_id', 'timestamp'})\n    lu.assertEquals(candidate('meter', None), {'user_id', 'project_id'})\n    lu.assertEquals(candidate('alarm', 'name'), {'name', 'user_id', 'project_id'})\n    lu.assertEquals(candidate('resource', 'user_id'), {'user_id', 'project_id', 'timestamp'})\n    lu.assertEquals(candidate('alarm', None), {'name', 'user_id', 'project_id'})\n    lu.assertEquals(candidate('resource', None), {'user_id', 'project_id', 'timestamp'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_45618_nearest_square", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # answer = 0\n-- # while (answer+1)**2 < num:\n-- #     answer += 1\n-- # return answer**2\n-- \n-- Function to calculate nearest square of any number.\n-- Args: \n--     num (int): Any integer\n-- Returns: \n--     Int: Nearest Square of num\nlocal function nearest_square(num)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_45618_nearest_square.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = nearest_square\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(2), 1)\n    lu.assertEquals(candidate(5), 4)\n    lu.assertEquals(candidate(3), 1)\n    lu.assertEquals(candidate(17), 16)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_456370_path_inside_dir", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return ((directory == \"\" and path != \"\")\n-- #         or path.rstrip(\"/\").startswith(directory.rstrip(\"/\") + \"/\"))\n-- \n-- Returns True if the specified @path is inside @directory,\n-- performing component-wide comparison. Otherwise returns False.\nlocal function path_inside_dir(path, directory)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_456370_path_inside_dir.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = path_inside_dir\n    lu.assertEquals(candidate('foo/bar/baz', 'foo'), true)\n    lu.assertEquals(candidate('some/path', ''), true)\n    lu.assertEquals(candidate('foo/bar', 'foo\\\\bar\\\\baz\\\\quux'), false)\n    lu.assertEquals(candidate('some/path/', 'other/path'), false)\n    lu.assertEquals(candidate('some/path', 'other/path'), false)\n    lu.assertEquals(candidate('/a/b/c/d/e', '/a/b/c'), true)\n    lu.assertEquals(candidate('foo\\\\bar', 'foo\\\\bar\\\\baz'), false)\n    lu.assertEquals(candidate('foo/bar', 'foo/bar/baz'), false)\n    lu.assertEquals(candidate('foo\\\\bar', 'foo\\\\bar'), false)\n    lu.assertEquals(candidate('some/path/', 'other/directory'), false)\n    lu.assertEquals(candidate('foo\\\\bar', 'foo\\\\bar\\\\baz\\\\quux'), false)\n    lu.assertEquals(candidate('foo\\\\bar', 'foo/bar/baz/quux'), false)\n    lu.assertEquals(candidate('some/path', 'some/path/path'), false)\n    lu.assertEquals(candidate('some/path', 'some/path/'), false)\n    lu.assertEquals(candidate('foo/bar', 'foo/bar'), false)\n    lu.assertEquals(candidate('some/path/', ''), true)\n    lu.assertEquals(candidate('foo/bar', 'foo'), true)\n    lu.assertEquals(candidate('', 'some/path'), false)\n    lu.assertEquals(candidate('foo/bar', 'foo\\\\bar\\\\baz'), false)\n    lu.assertEquals(candidate('some/path', 'some/directory/path'), false)\n    lu.assertEquals(candidate('foo/bar', 'foo/bar/baz/quux'), false)\n    lu.assertEquals(candidate('foo\\\\bar', 'foo/bar/baz'), false)\n    lu.assertEquals(candidate('some/path', 'other/directory'), false)\n    lu.assertEquals(candidate('', ''), false)\n    lu.assertEquals(candidate('/a/b/c/d/e', '/a/b/c/'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_45640_num_digits", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return len(str(number))\n-- \n-- Returns number of digits in an integer.\n-- :param number: Integer\n-- :return: Number of digits\nlocal function num_digits(number)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_45640_num_digits.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = num_digits\n    lu.assertEquals(candidate(3), 1)\n    lu.assertEquals(candidate(223100), 6)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(1000), 4)\n    lu.assertEquals(candidate(0), 1)\n    lu.assertEquals(candidate(23), 2)\n    lu.assertEquals(candidate(123), 3)\n    lu.assertEquals(candidate(12345), 5)\n    lu.assertEquals(candidate(99999999), 8)\n    lu.assertEquals(candidate(1234567890), 10)\n    lu.assertEquals(candidate(223), 3)\n    lu.assertEquals(candidate(123456), 6)\n    lu.assertEquals(candidate(999999), 6)\n    lu.assertEquals(candidate(10), 2)\n    lu.assertEquals(candidate(9999), 4)\n    lu.assertEquals(candidate(22310), 5)\n    lu.assertEquals(candidate(2231), 4)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_457283_unf_gas_density_kgm3", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # m = gamma_gas * 0.029\n-- # p_Pa = 10 ** 6 * p_MPaa\n-- # rho_gas = p_Pa * m / (z * 8.31 * t_K)\n-- # return rho_gas\n-- \n-- Equation for gas density\n-- :param t_K: temperature\n-- :param p_MPaa: pressure\n-- :param gamma_gas: specific gas density by air\n-- :param z: z-factor\n-- :return: gas density\nlocal function unf_gas_density_kgm3(t_K, p_MPaa, gamma_gas, z)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_457283_unf_gas_density_kgm3.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = unf_gas_density_kgm3\n    lu.assertEquals(candidate(293.15, 101325, 0, 0.5), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_457405_count_tilings", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if n < 5:\n-- #     # handle recursive base case\n-- #     return 2**(n - 1)\n-- # else:\n-- #     # place each tile at end of row and recurse on remainder\n-- #     return (count_tilings(n - 1) +\n-- #             count_tilings(n - 2) +\n-- #             count_tilings(n - 3) +\n-- #             count_tilings(n - 4))\n-- \n-- Returns the number of unique ways to tile a row of length n >= 1.\nlocal function count_tilings(n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_457405_count_tilings.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = count_tilings\n    lu.assertEquals(candidate(4), 8)\n    lu.assertEquals(candidate(5), 15)\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(1), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_458323_parse_boolean", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # arg = str(arg).lower()\n-- # if 'true'.startswith(arg):\n-- #     return True\n-- # return False\n-- \n-- Returns boolean representation of argument.\nlocal function parse_boolean(arg)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_458323_parse_boolean.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_boolean\n    lu.assertEquals(candidate(0), false)\n    lu.assertEquals(candidate('TRUE'), true)\n    lu.assertEquals(candidate('true'), true)\n    lu.assertEquals(candidate('True'), true)\n    lu.assertEquals(candidate(false), false)\n    lu.assertEquals(candidate('true'), true)\n    lu.assertEquals(candidate('FALSE'), false)\n    lu.assertEquals(candidate('false'), false)\n    lu.assertEquals(candidate('TRUE'), true)\n    lu.assertEquals(candidate('True'), true)\n    lu.assertEquals(candidate('False'), false)\n    lu.assertEquals(candidate('FALSE'), false)\n    lu.assertEquals(candidate('false'), false)\n    lu.assertEquals(candidate('0'), false)\n    lu.assertEquals(candidate(true), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_45854_check_fields_to_join", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if 'oa' in fields_to_join:\n-- #     oa = True\n-- # else:\n-- #     oa = False\n-- # if 'lad' in fields_to_join:\n-- #     lad = True\n-- # else:\n-- #     lad = False\n-- # if 'gor' in fields_to_join:\n-- #     gor = True\n-- # else:\n-- #     gor = False\n-- # return oa, lad, gor\n-- \n-- Check which fields have been passed to be joined\nlocal function check_fields_to_join(fields_to_join)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_45854_check_fields_to_join.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = check_fields_to_join\n    lu.assertEquals(candidate({'oa', 'lad', 'gor'}), {true, true, true})\n    lu.assertEquals(candidate({'oa', 'lad'}), {true, true, false})\n    lu.assertEquals(candidate({'oa', 'gor'}), {true, false, true})\n    lu.assertEquals(candidate({'lad', 'gor'}), {false, true, true})\n    lu.assertEquals(candidate({}), {false, false, false})\n    lu.assertEquals(candidate({'lad'}), {false, true, false})\n    lu.assertEquals(candidate({'oa'}), {true, false, false})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_458724_pageHeader", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # pageHeader = \"\"\"\n-- #     <div class=\"page-header\" id=\"  \">\n-- #         <h1>%(headline)s<br><small>%(tagline)s</small></h1>\n-- #     </div>\"\"\" % locals()\n-- # return pageHeader\n-- \n-- *Generate a pageHeader - TBS style*\n-- **Key Arguments:**\n--     - ``headline`` -- the headline text\n--     - ``tagline`` -- the tagline text for below the headline\n-- **Return:**\n--     - ``pageHeader`` -- the pageHeader\nlocal function pageHeader(headline, tagline)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_458724_pageHeader.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = pageHeader\n    lu.assertEquals(candidate('headline', 'tagline'), '\\n        <div class=\"page-header\" id=\"  \">\\n            <h1>headline<br><small>tagline</small></h1>\\n        </div>')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_46021_gauss_sum", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return (number * (number + 1)) / 2\n-- \n--  Computes the gaussian sum for an input number. \nlocal function gauss_sum(number)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_46021_gauss_sum.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = gauss_sum\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(10), 55)\n    lu.assertEquals(candidate(6), 21)\n    lu.assertEquals(candidate(20), 210)\n    lu.assertEquals(candidate(2), 3)\n    lu.assertEquals(candidate(7), 28)\n    lu.assertEquals(candidate(5), 15)\n    lu.assertEquals(candidate(2), 3)\n    lu.assertEquals(candidate(4), 10)\n    lu.assertEquals(candidate(3), 6)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_46688_get_variable_sites", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # num_sites = 10\n-- # if index % 2 == 1:\n-- #     return [1 + index % num_sites]\n-- # else:\n-- #     return range(1, num_sites + 1)\n-- \n-- Get a list of sites where data[index] are stored\nlocal function get_variable_sites(index)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_46688_get_variable_sites.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_variable_sites\n    lu.assertEquals(candidate(21), {2})\n    lu.assertEquals(candidate(25), {6})\n    lu.assertEquals(candidate(19), {10})\n    lu.assertEquals(candidate(1), {2})\n    lu.assertEquals(candidate(23), {4})\n    lu.assertEquals(candidate(7), {8})\n    lu.assertEquals(candidate(15), {6})\n    lu.assertEquals(candidate(3), {4})\n    lu.assertEquals(candidate(17), {8})\n    lu.assertEquals(candidate(11), {2})\n    lu.assertEquals(candidate(5), {6})\n    lu.assertEquals(candidate(9), {10})\n    lu.assertEquals(candidate(13), {4})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_4747_create_url", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return url + \"/\" + str(data)\n-- \n-- Method which creates new url from base url\n-- :param url: base url\n-- :param data: data to append to base url\n-- :return: new url\nlocal function create_url(url, data)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_4747_create_url.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = create_url\n    lu.assertEquals(candidate('http://example.com', 'test'), 'http://example.com/test')\n    lu.assertEquals(candidate('http://www.codewars.com', 'users/mattC'), 'http://www.codewars.com/users/mattC')\n    lu.assertEquals(candidate('https://www.reddit.com/r/dailyprogrammer/comments/2ofb7m/20140926_challenge_177_easy_morse_code', 'upvote'), 'https://www.reddit.com/r/dailyprogrammer/comments/2ofb7m/20140926_challenge_177_easy_morse_code/upvote')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_49061_format32BitHexStr", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # remove \"0x\" or \"OX\"  prefix if it had any\n-- # hexStr = hexStr.replace(\"0x\", \"\").replace(\"0X\", \"\")\n-- # hexStr = hexStr[0:8].zfill(8)\n-- # hexStr = hexStr.upper()\n-- # hexStr = \"0x\" + hexStr\n-- # return hexStr\n-- \n-- format the given string which represents a valid 32-bit hexadecimal number.\n-- prefix \"0x\" will be added and will replace any valid prefix.\n-- alphabetic letter will be formatted into upper case.\n-- \"0\" will be used to fill the hexadecimal number if this number is represented as less than 8-letter.\n-- Exmaple usage:\n-- input: 0Xff  -> output:0x000000FF\n-- input: Ab -> output: 0x000000AB\n-- input 0xAf -> output: 0x000000AF\n-- :param hexStr:  a valid string representing a 32-bit hexadecimal number\n-- :return: a formatted string representing 32-bit hexadecimal number as described\nlocal function format32BitHexStr(hexStr)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_49061_format32BitHexStr.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = format32BitHexStr\n    lu.assertEquals(candidate('0xAF'), '0x000000AF')\n    lu.assertEquals(candidate('0xAB'), '0x000000AB')\n    lu.assertEquals(candidate('0XAB'), '0x000000AB')\n    lu.assertEquals(candidate('aB'), '0x000000AB')\n    lu.assertEquals(candidate('AB'), '0x000000AB')\n    lu.assertEquals(candidate('0XFF'), '0x000000FF')\n    lu.assertEquals(candidate('FF'), '0x000000FF')\n    lu.assertEquals(candidate('AF'), '0x000000AF')\n    lu.assertEquals(candidate('0xFF'), '0x000000FF')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_5015_SEARCH", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # .lower() isn't always correct for unicode. See http://stackoverflow.com/a/29247821/328565\n-- # return within_text.lower().index(find_text.lower(), start_num - 1) + 1\n-- \n-- Returns the position at which a string is first found within text, ignoring case.\n-- Find is case-sensitive. The returned position is 1 if within_text starts with find_text.\n-- Start_num specifies the character at which to start the search, defaulting to 1 (the first\n-- character of within_text).\n-- If find_text is not found, or start_num is invalid, raises ValueError.\n-- >>> SEARCH(\"e\", \"Statements\", 6)\n-- 7\n-- >>> SEARCH(\"margin\", \"Profit Margin\")\n-- 8\n-- >>> SEARCH(\" \", \"Profit Margin\")\n-- 7\n-- >>> SEARCH('\"', 'The \"boss\" is here.')\n-- 5\n-- >>> SEARCH(\"gle\", \"Google\")\n-- 4\n-- >>> SEARCH(\"GLE\", \"Google\")\n-- 4\nlocal function SEARCH(find_text, within_text, start_num)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_5015_SEARCH.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = SEARCH\n    lu.assertEquals(candidate(' ', 'Profit Margin'), 7)\n    lu.assertEquals(candidate('GLE', 'Google'), 4)\n    lu.assertEquals(candidate('e', 'Statements', 6), 7)\n    lu.assertEquals(candidate('\"', 'The \"boss\" is here.'), 5)\n    lu.assertEquals(candidate('e', 'Statements', 6), 7)\n    lu.assertEquals(candidate('\"', 'The \"boss\" is here.'), 5)\n    lu.assertEquals(candidate('gle', 'Google'), 4)\n    lu.assertEquals(candidate('margin', 'Profit Margin'), 8)\n    lu.assertEquals(candidate(' ', 'Profit Margin'), 7)\n    lu.assertEquals(candidate('gle', 'Google'), 4)\n    lu.assertEquals(candidate('e', 'Statements', 6), 7)\n    lu.assertEquals(candidate('margin', 'Profit Margin'), 8)\n    lu.assertEquals(candidate('GLE', 'Google'), 4)\n    lu.assertEquals(candidate('g', 'Google', 4), 4)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_50398_task2", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Do computation\n-- # pass\n-- # # Return success and multiple args for next task\n-- # return True, ('arg1', 'arg2', 'arg3')\n-- \n--     Second task that depends on the output of the first task.\nlocal function task2(arg1)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_50398_task2.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = task2\n    lu.assertEquals(candidate(102), {true, {'arg1', 'arg2', 'arg3'}})\n    lu.assertEquals(candidate(104), {true, {'arg1', 'arg2', 'arg3'}})\n    lu.assertEquals(candidate(105), {true, {'arg1', 'arg2', 'arg3'}})\n    lu.assertEquals(candidate(101), {true, {'arg1', 'arg2', 'arg3'}})\n    lu.assertEquals(candidate(1), {true, {'arg1', 'arg2', 'arg3'}})\n    lu.assertEquals(candidate(3), {true, {'arg1', 'arg2', 'arg3'}})\n    lu.assertEquals(candidate(103), {true, {'arg1', 'arg2', 'arg3'}})\n    lu.assertEquals(candidate(2), {true, {'arg1', 'arg2', 'arg3'}})\n    lu.assertEquals(candidate(4), {true, {'arg1', 'arg2', 'arg3'}})\n    lu.assertEquals(candidate(5), {true, {'arg1', 'arg2', 'arg3'}})\n    lu.assertEquals(candidate(109), {true, {'arg1', 'arg2', 'arg3'}})\n    lu.assertEquals(candidate(108), {true, {'arg1', 'arg2', 'arg3'}})\n    lu.assertEquals(candidate(100), {true, {'arg1', 'arg2', 'arg3'}})\n    lu.assertEquals(candidate(107), {true, {'arg1', 'arg2', 'arg3'}})\n    lu.assertEquals(candidate(106), {true, {'arg1', 'arg2', 'arg3'}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_50616_suffixed_file_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # reverse split, with max of 1 split:\n-- # string_parts = file_path.rsplit('.', 1)\n-- # return string_parts[0] + suffix_string + '.' + string_parts[1]\n-- \n--  Returns file path with appended string (preserving file type)\n-- :param file_path: (string) either relative or absolute path of file\n-- :param suffix_string: (string) string to append to the original file name\n-- :return: (string) suffixed file path\n-- example:\n--     append_path(\"foo.html.bar.html\", \"_BAZ\")\n--     >>> \"foo.html.bar.html_BAZ.html\"\nlocal function suffixed_file_name(file_path, suffix_string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_50616_suffixed_file_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = suffixed_file_name\n    lu.assertEquals(candidate('foo.html', ''), 'foo.html')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_50913__normalize_block_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # block_name = ''.join(block_name.split())\n-- # block_name = block_name.replace('-', '')\n-- # return block_name.replace('_', '').lower()\n-- \n-- Implements Unicode name normalization for block names.\n-- Removes white space, '-', '_' and forces lower case.\nlocal function _normalize_block_name(block_name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_50913__normalize_block_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _normalize_block_name\n    lu.assertEquals(candidate('A B C'), 'abc')\n    lu.assertEquals(candidate('Block_Name_ '), 'blockname')\n    lu.assertEquals(candidate('block-name'), 'blockname')\n    lu.assertEquals(candidate('foo bar'), 'foobar')\n    lu.assertEquals(candidate('foo\\nbar'), 'foobar')\n    lu.assertEquals(candidate('ABC_DEF'), 'abcdef')\n    lu.assertEquals(candidate('foo'), 'foo')\n    lu.assertEquals(candidate('foo-bar'), 'foobar')\n    lu.assertEquals(candidate('block name\\n\\r\\t'), 'blockname')\n    lu.assertEquals(candidate('\\t\\n\\rfoo-bar'), 'foobar')\n    lu.assertEquals(candidate('ABC-DEF'), 'abcdef')\n    lu.assertEquals(candidate('foo\\tbar'), 'foobar')\n    lu.assertEquals(candidate('Block-Name\\n\\r\\t'), 'blockname')\n    lu.assertEquals(candidate('Block_Name-'), 'blockname')\n    lu.assertEquals(candidate('A  B  C'), 'abc')\n    lu.assertEquals(candidate('A-B-C'), 'abc')\n    lu.assertEquals(candidate('Block_Name_'), 'blockname')\n    lu.assertEquals(candidate('block_name'), 'blockname')\n    lu.assertEquals(candidate('foo_bar'), 'foobar')\n    lu.assertEquals(candidate('A_B_C'), 'abc')\n    lu.assertEquals(candidate('Block-Name \\n\\r\\t'), 'blockname')\n    lu.assertEquals(candidate('ABC  DEF'), 'abcdef')\n    lu.assertEquals(candidate('  block-name \\n\\n\\t'), 'blockname')\n    lu.assertEquals(candidate('block name\\t'), 'blockname')\n    lu.assertEquals(candidate('blockname'), 'blockname')\n    lu.assertEquals(candidate('ABC'), 'abc')\n    lu.assertEquals(candidate('foo-bar_1'), 'foobar1')\n    lu.assertEquals(candidate('Block_Name'), 'blockname')\n    lu.assertEquals(candidate('foo-1_bar'), 'foo1bar')\n    lu.assertEquals(candidate('foo-bar-1'), 'foobar1')\n    lu.assertEquals(candidate('o-ya'), 'oya')\n    lu.assertEquals(candidate('block-name\\r\\n\\t'), 'blockname')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_51126_get_tag_line", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for line in lines:\n-- #     for prefix in tag_prefixes:\n-- #         full_tag = prefix + revision\n-- #         if line.startswith(full_tag):\n-- #             return line\n-- # return None\n-- \n-- Get the revision hash for the tag matching the given project revision in\n-- the given lines containing revision hashes. Uses the given array of tag\n-- prefix strings if provided. For example, given an array of tag prefixes\n-- [\"checker-framework-\", \"checkers-\"] and project revision \"2.0.0\", the\n-- tags named \"checker-framework-2.0.0\" and \"checkers-2.0.0\" are sought.\nlocal function get_tag_line(lines, revision, tag_prefixes)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_51126_get_tag_line.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_tag_line\n    lu.assertEquals(candidate({'checkers-2.1.5', 'checkers-2.1.6'}, '2.1.5', {'checker-framework-', 'checkers-'}), 'checkers-2.1.5')\n    lu.assertEquals(candidate({'checker-framework-1.0.0', 'checker-framework-1.0.1'}, '1.0.1', {'checker-framework-'}), 'checker-framework-1.0.1')\n    lu.assertEquals(candidate({'checkers-2.1.6', 'checkers-2.1.5'}, '2.1.6', {'checker-framework-', 'checkers-'}), 'checkers-2.1.6')\n    lu.assertEquals(candidate({'checker-framework-2.0.0', 'checker-framework-2.0.1'}, '2.0.1', {'checker-framework-'}), 'checker-framework-2.0.1')\n    lu.assertEquals(candidate({'a', 'b', 'c'}, 'd', {'e', 'f'}), None)\n    lu.assertEquals(candidate({'checker-framework-2.0.0', 'checker-framework-2.0.1', 'checker-framework-2.0.2'}, '2.0.2', {'checker-framework-', 'checkers-'}), 'checker-framework-2.0.2')\n    lu.assertEquals(candidate({'checker-framework-2.1.10', 'checker-framework-2.1.11', 'checker-framework-2.2.0'}, '2.2.0', {'checker-framework-', 'checkers-'}), 'checker-framework-2.2.0')\n    lu.assertEquals(candidate({'checker-framework-2.0.0', 'checker-framework-2.0.1', 'checker-framework-2.0.2'}, '2.0.2', {}), None)\n    lu.assertEquals(candidate({'checker-framework-2.1.10', 'checker-framework-2.1.11', 'checker-framework-2.2.0'}, '2.2.0', {'checker-framework-'}), 'checker-framework-2.2.0')\n    lu.assertEquals(candidate({'checkers-2.1.5', 'checkers-2.1.6'}, '2.1.6', {'checker-framework-', 'checkers-'}), 'checkers-2.1.6')\n    lu.assertEquals(candidate({'checker-framework-2.1.6', 'checker-framework-2.1.5'}, '2.1.6', {'checker-framework-', 'checkers-'}), 'checker-framework-2.1.6')\n    lu.assertEquals(candidate({'checker-framework-2.0.0', 'checker-framework-2.0.1'}, '2.0.1', {'checkers-'}), None)\n    lu.assertEquals(candidate({'checkers-2.1.6', 'checkers-2.1.5'}, '2.1.5', {'checker-framework-', 'checkers-'}), 'checkers-2.1.5')\n    lu.assertEquals(candidate({'checker-framework-2.0.0', 'checker-framework-2.0.1', 'checker-framework-2.0.2'}, '2.0.2', {'checker-framework-'}), 'checker-framework-2.0.2')\n    lu.assertEquals(candidate({'checker-framework-2.1.6', 'checker-framework-2.1.5'}, '2.1.5', {'checker-framework-', 'checkers-'}), 'checker-framework-2.1.5')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_51134_extract_class_label", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # _, class_dir, _ = filename.split(\"/\")\n-- # return int(class_dir.split(\".\")[0])\n-- \n--     arg:\n-- filename: string, e.g.\n-- 'images/001.Black_footed_Albatross/Black_footed_Albatross_0001_2950163169.jpg'\n--     return:\n-- A class label as integer, e.g. 1\nlocal function extract_class_label(filename)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_51134_extract_class_label.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = extract_class_label\n    lu.assertEquals(candidate('images/002.Laysan_Albatross/Laysan_Albatross_0004_2950165039.jpg'), 2)\n    lu.assertEquals(candidate('images/007.Parakeet_Auklet/Parakeet_Auklet_0002_2950163991.jpg'), 7)\n    lu.assertEquals(candidate('images/001.Black_footed_Albatross/Black_footed_Albatross_0001_2950163169.jpg'), 1)\n    lu.assertEquals(candidate('images/003.Sooty_Albatross/Sooty_Albatross_0005_2950164871.jpg'), 3)\n    lu.assertEquals(candidate('images/006.Least_Auklet/Least_Auklet_0001_2950163844.jpg'), 6)\n    lu.assertEquals(candidate('images/004.Groove_billed_Ani/Groove_billed_Ani_0001_2950166344.jpg'), 4)\n    lu.assertEquals(candidate('images/005.Crested_Auklet/Crested_Auklet_0005_2950164507.jpg'), 5)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_51820_getcardinals", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # cardinals = [val for val in range(minv, maxv) if val % stepv == 0]\n-- # if len(cardinals) > 10:\n-- #     return [\n-- #         cardinal[1] for cardinal\n-- #         in enumerate(cardinals) if cardinal[0] % 2 > 0\n-- #     ]\n-- # return cardinals\n-- \n-- Get lats and longs to mark on map\n-- :param minv:\n-- :type minv: float\n-- :param maxv:\n-- :type maxv: float\n-- :param stepv:\n-- :type stepv: int\n-- :return:\n-- :rtype: list\nlocal function getcardinals(minv, maxv, stepv)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_51820_getcardinals.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = getcardinals\n    lu.assertEquals(candidate(-5, 5, 1), {-5, -4, -3, -2, -1, 0, 1, 2, 3, 4})\n    lu.assertEquals(candidate(0, 360, 360), {0})\n    lu.assertEquals(candidate(1, 5, 1), {1, 2, 3, 4})\n    lu.assertEquals(candidate(0, 100, 20), {0, 20, 40, 60, 80})\n    lu.assertEquals(candidate(2, 9, 2), {2, 4, 6, 8})\n    lu.assertEquals(candidate(0, 360, 90), {0, 90, 180, 270})\n    lu.assertEquals(candidate(0, 10, 6), {0, 6})\n    lu.assertEquals(candidate(1, 10, 1), {1, 2, 3, 4, 5, 6, 7, 8, 9})\n    lu.assertEquals(candidate(10, 100, 10), {10, 20, 30, 40, 50, 60, 70, 80, 90})\n    lu.assertEquals(candidate(0, 10, 1), {0, 1, 2, 3, 4, 5, 6, 7, 8, 9})\n    lu.assertEquals(candidate(0, 5, 2), {0, 2, 4})\n    lu.assertEquals(candidate(0, 5, 3), {0, 3})\n    lu.assertEquals(candidate(0, 10, 2), {0, 2, 4, 6, 8})\n    lu.assertEquals(candidate(0, 5, 1), {0, 1, 2, 3, 4})\n    lu.assertEquals(candidate(10, 100, 200), {})\n    lu.assertEquals(candidate(20, 100, 10), {20, 30, 40, 50, 60, 70, 80, 90})\n    lu.assertEquals(candidate(360, 0, 360), {})\n    lu.assertEquals(candidate(0, 10, 5), {0, 5})\n    lu.assertEquals(candidate(0, 10, 4), {0, 4, 8})\n    lu.assertEquals(candidate(10, 100, 100), {})\n    lu.assertEquals(candidate(0, 5, 4), {0, 4})\n    lu.assertEquals(candidate(0, 10, 3), {0, 3, 6, 9})\n    lu.assertEquals(candidate(1, 11, 1), {1, 2, 3, 4, 5, 6, 7, 8, 9, 10})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_51944_validate_stdout", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Todo: support should_panic tests (Implementation on hermit side with custom panic handler)\n-- # if \"!!!PANIC!!!\" in stdout:\n-- #     return False\n-- # return True\n-- \n-- :param stdout:\n-- :return: true if stdout does not indicate test failure\nlocal function validate_stdout(stdout)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_51944_validate_stdout.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = validate_stdout\n    lu.assertEquals(candidate('Hello World'), true)\n    lu.assertEquals(candidate('Hello!!!PANIC!!! World'), false)\n    lu.assertEquals(candidate('Hello!!!PANIC!!!'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_52012_sqlite3_column_affinity", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # ct = column_type.lower()\n-- # if \"int\" in ct:\n-- #     return \"INTEGER\"\n-- # elif \"char\" in ct or \"clob\" in ct or \"text\" in ct:\n-- #     return \"TEXT\"\n-- # elif \"blob\" in ct or ct == \"\":\n-- #     return \"NONE\"\n-- # elif \"real\" in ct or \"floa\" in ct or \"doub\" in ct:\n-- #     return \"REAL\"\n-- # else:\n-- #     return \"NUMERIC\"\n-- \n-- Return the sqlite3 column affinity corresponding to a type string.\nlocal function sqlite3_column_affinity(column_type)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_52012_sqlite3_column_affinity.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sqlite3_column_affinity\n    lu.assertEquals(candidate('BLOB(12)'), 'NONE')\n    lu.assertEquals(candidate('VARCHAR(12)'), 'TEXT')\n    lu.assertEquals(candidate('CHARACTER VARYING'), 'TEXT')\n    lu.assertEquals(candidate('CHARACTER VARYING(12)'), 'TEXT')\n    lu.assertEquals(candidate('not a number'), 'NUMERIC')\n    lu.assertEquals(candidate('blob(12)'), 'NONE')\n    lu.assertEquals(candidate('DOUBLE'), 'REAL')\n    lu.assertEquals(candidate('DOUBLE(12)'), 'REAL')\n    lu.assertEquals(candidate('CHARACTER'), 'TEXT')\n    lu.assertEquals(candidate('CHARACTER(12)'), 'TEXT')\n    lu.assertEquals(candidate('text'), 'TEXT')\n    lu.assertEquals(candidate('TEXT'), 'TEXT')\n    lu.assertEquals(candidate('DOUBLE PRECISION(12)'), 'REAL')\n    lu.assertEquals(candidate(str('1234567890123456789012345678901234567890')), 'NUMERIC')\n    lu.assertEquals(candidate('REAL'), 'REAL')\n    lu.assertEquals(candidate('CLOB'), 'TEXT')\n    lu.assertEquals(candidate(str(-1234567890123456789012345678901234567890)), 'NUMERIC')\n    lu.assertEquals(candidate('Int'), 'INTEGER')\n    lu.assertEquals(candidate(str('-1234567890123456789012345678901234567890')), 'NUMERIC')\n    lu.assertEquals(candidate('BLOB'), 'NONE')\n    lu.assertEquals(candidate('FLOAT'), 'REAL')\n    lu.assertEquals(candidate('CHAR(12)'), 'TEXT')\n    lu.assertEquals(candidate('CLOB(12)'), 'TEXT')\n    lu.assertEquals(candidate('FLOAT(12)'), 'REAL')\n    lu.assertEquals(candidate(str(1234567890123456789012345678901234567890)), 'NUMERIC')\n    lu.assertEquals(candidate('VARCHAR'), 'TEXT')\n    lu.assertEquals(candidate('DOUBLE PRECISION'), 'REAL')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_5259_without_end_slash", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return url.rstrip(\"/\")\n-- \n-- Makes sure there is no end slash at the end of a url.\nlocal function without_end_slash(url)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_5259_without_end_slash.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = without_end_slash\n    lu.assertEquals(candidate('www.python.org/books'), 'www.python.org/books')\n    lu.assertEquals(candidate('http://python.org/'), 'http://python.org')\n    lu.assertEquals(candidate('http://www.hackerrank.com/challenges/nested-list/'), 'http://www.hackerrank.com/challenges/nested-list')\n    lu.assertEquals(candidate('http://python.org/books'), 'http://python.org/books')\n    lu.assertEquals(candidate('http://www.python.org/'), 'http://www.python.org')\n    lu.assertEquals(candidate('python.org/books'), 'python.org/books')\n    lu.assertEquals(candidate('www.python.org/'), 'www.python.org')\n    lu.assertEquals(candidate('python.org/books/'), 'python.org/books')\n    lu.assertEquals(candidate('http://python.org'), 'http://python.org')\n    lu.assertEquals(candidate('http://www.python.org/books'), 'http://www.python.org/books')\n    lu.assertEquals(candidate('www.python.org'), 'www.python.org')\n    lu.assertEquals(candidate('http://www.python.org/books/'), 'http://www.python.org/books')\n    lu.assertEquals(candidate('python.org/'), 'python.org')\n    lu.assertEquals(candidate('http://www.python.org'), 'http://www.python.org')\n    lu.assertEquals(candidate('www.python.org/books/'), 'www.python.org/books')\n    lu.assertEquals(candidate('http://python.org/books/'), 'http://python.org/books')\n    lu.assertEquals(candidate('http://www.hackerrank.com/challenges/nested-list'), 'http://www.hackerrank.com/challenges/nested-list')\n    lu.assertEquals(candidate('python.org'), 'python.org')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_52634__clear_data", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # need_fields = list(need_fields_tuple)\n-- # result = data.copy()\n-- # for fields in data:\n-- #     try:\n-- #         need_fields.remove(fields)\n-- #     except ValueError:\n-- #         result.pop(fields)\n-- # if len(need_fields) != 0:\n-- #     return False\n-- # return result\n-- \n-- Check and clear data\nlocal function _clear_data(data, need_fields_tuple)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_52634__clear_data.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _clear_data\n    lu.assertEquals(candidate({['A'] = 1, ['B'] = 2, ['C'] = 3}, {'A', 'C'}), {['A'] = 1, ['C'] = 3})\n    lu.assertEquals(candidate({['A'] = 1, ['B'] = 2, ['C'] = 3}, {'A', 'B'}), {['A'] = 1, ['B'] = 2})\n    lu.assertEquals(candidate({['A'] = 1, ['B'] = 2, ['C'] = 3}, {}), {})\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3}, {'a', 'b'}), {['a'] = 1, ['b'] = 2})\n    lu.assertEquals(candidate({['A'] = 1, ['B'] = 2, ['C'] = 3}, {'C', 'B'}), {['C'] = 3, ['B'] = 2})\n    lu.assertEquals(candidate({['a'] = 1, ['b'] = 2, ['c'] = 3}, {}), {})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_52943_Hubble", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return ((Om) * (1 + z)**3. + (1 - Om))**0.5\n-- \n--  LCDM AP parameter auxiliary function \nlocal function Hubble(Om, z)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_52943_Hubble.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = Hubble\n    lu.assertEquals(candidate(1, 0), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_5314_int_to_float", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # fifty_three_ones = 0xFFFFFFFFFFFFFFFF >> (64 - 53)\n-- # fifty_three_zeros = float(1 << 53)\n-- # return (value & fifty_three_ones) / fifty_three_zeros\n-- \n-- Converts a uniformly random [[64-bit computing|64-bit]]\n-- integer to uniformly random floating point number on interval <math>[0, 1)</math>.\nlocal function int_to_float(value)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_5314_int_to_float.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = int_to_float\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(18446744073709551615), 0.9999999999999999)\n    lu.assertEquals(candidate(-1), 0.9999999999999999)\n    lu.assertEquals(candidate(0), 0.0)\n    lu.assertEquals(candidate(0), 0.0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_54360_has_seven", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if k < 10:\n-- #     return k == 7\n-- # if k % 10 == 7:\n-- #     return True\n-- # return has_seven(k // 10)\n-- \n-- Returns True if at least one of the digits of k is a 7, False otherwise.\n-- >>> has_seven(3)\n-- False\n-- >>> has_seven(7)\n-- True\n-- >>> has_seven(2734)\n-- True\n-- >>> has_seven(2634)\n-- False\n-- >>> has_seven(734)\n-- True\n-- >>> has_seven(7777)\n-- True\n-- >>> from construct_check import check\n-- >>> check(HW_SOURCE_FILE, 'has_seven',\n-- ...       ['Assign', 'AugAssign'])\n-- True\nlocal function has_seven(k)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_54360_has_seven.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = has_seven\n    lu.assertEquals(candidate(7), true)\n    lu.assertEquals(candidate(734), true)\n    lu.assertEquals(candidate(11235813), false)\n    lu.assertEquals(candidate(2734), true)\n    lu.assertEquals(candidate(3), false)\n    lu.assertEquals(candidate(7777), true)\n    lu.assertEquals(candidate(1234), false)\n    lu.assertEquals(candidate(300000000), false)\n    lu.assertEquals(candidate(2634), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_5448_POS", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # POSText = \"\"\n-- # if POSInt == 42:\n-- #     POSText = \"F\"\n-- # return (POSText)\n-- \n-- Returns 'F' if position indicator is present. The AllSportCG sends a * in a specific position to indicate which\n-- team has posession, and this changes that character to an 'F'. Using font Mattbats, F is a football.\nlocal function POS(POSInt)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_5448_POS.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = POS\n    lu.assertEquals(candidate(24), '')\n    lu.assertEquals(candidate(29), '')\n    lu.assertEquals(candidate(14), '')\n    lu.assertEquals(candidate(21), '')\n    lu.assertEquals(candidate(20), '')\n    lu.assertEquals(candidate(42), 'F')\n    lu.assertEquals(candidate(0), '')\n    lu.assertEquals(candidate(13), '')\n    lu.assertEquals(candidate(19), '')\n    lu.assertEquals(candidate(7), '')\n    lu.assertEquals(candidate(25), '')\n    lu.assertEquals(candidate(27), '')\n    lu.assertEquals(candidate(34), '')\n    lu.assertEquals(candidate(), 'F')\n    lu.assertEquals(candidate(39), '')\n    lu.assertEquals(candidate(28), '')\n    lu.assertEquals(candidate(5), '')\n    lu.assertEquals(candidate(37), '')\n    lu.assertEquals(candidate(2), '')\n    lu.assertEquals(candidate(11), '')\n    lu.assertEquals(candidate(12), '')\n    lu.assertEquals(candidate(18), '')\n    lu.assertEquals(candidate(41), '')\n    lu.assertEquals(candidate(23), '')\n    lu.assertEquals(candidate(40), '')\n    lu.assertEquals(candidate(4), '')\n    lu.assertEquals(candidate(3), '')\n    lu.assertEquals(candidate(36), '')\n    lu.assertEquals(candidate(43), '')\n    lu.assertEquals(candidate(17), '')\n    lu.assertEquals(candidate(30), '')\n    lu.assertEquals(candidate(9), '')\n    lu.assertEquals(candidate(), '')\n    lu.assertEquals(candidate(32), '')\n    lu.assertEquals(candidate(42), 'F')\n    lu.assertEquals(candidate(10), '')\n    lu.assertEquals(candidate(35), '')\n    lu.assertEquals(candidate(38), '')\n    lu.assertEquals(candidate(16), '')\n    lu.assertEquals(candidate(22), '')\n    lu.assertEquals(candidate(1), '')\n    lu.assertEquals(candidate(6), '')\n    lu.assertEquals(candidate(26), '')\n    lu.assertEquals(candidate(8), '')\n    lu.assertEquals(candidate(31), '')\n    lu.assertEquals(candidate(15), '')\n    lu.assertEquals(candidate(33), '')\n    lu.assertEquals(candidate(3), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_55281_get_name_from_selector", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if selector.startswith(\"name=\"):\n-- #     return selector[len(\"name=\"):]\n-- # if selector.startswith(\"&\"):\n-- #     return selector[len(\"&\"):]\n-- # return selector\n-- \n--     A basic method to get the name from a name selector.\nlocal function get_name_from_selector(selector)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_55281_get_name_from_selector.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_name_from_selector\n    lu.assertEquals(candidate('name=x'), 'x')\n    lu.assertEquals(candidate('&'), '')\n    lu.assertEquals(candidate('x'), 'x')\n    lu.assertEquals(candidate('&x'), 'x')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_55787_get_custom_kickstart_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return os_type + server_serial_number + \"_ks.cfg\"\n-- \n-- This function is to generate a name for the custom kickstart file based on the type of OS and server serial number\n-- Arguments:\n--     os_type {string}              -- Type of the opertaing system - RHEL\n--     server_serial_number {string} -- Server serial number\n-- Returns:\n--     string -- custom kickstart filename\nlocal function get_custom_kickstart_name(os_type, server_serial_number)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_55787_get_custom_kickstart_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_custom_kickstart_name\n    lu.assertEquals(candidate('RHEL', '8888888888'), 'RHEL8888888888_ks.cfg')\n    lu.assertEquals(candidate('RHEL', '123456'), 'RHEL123456_ks.cfg')\n    lu.assertEquals(candidate('RHEL', 'C2251756072'), 'RHELC2251756072_ks.cfg')\n    lu.assertEquals(candidate('RHEL', '7777777777'), 'RHEL7777777777_ks.cfg')\n    lu.assertEquals(candidate('RHEL', 'C2251756072'), 'RHELC2251756072_ks.cfg')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_56082_drop_role", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return f\"DROP ROLE IF EXISTS {role};\"\n-- \n-- Helper method to construct SQL: drop role.\nlocal function drop_role(role)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_56082_drop_role.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = drop_role\n    lu.assertEquals(candidate(), 'DROP ROLE IF EXISTS my_role;')\n    lu.assertEquals(candidate('a'), 'DROP ROLE IF EXISTS a;')\n    lu.assertEquals(candidate('my_role'), 'DROP ROLE IF EXISTS my_role;')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_56324_dice_roll", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # num, sides = 1, 6\n-- # if arg.count(\"x\") > 1:\n-- #     return None\n-- # if \"x\" in arg:\n-- #     num, sides = arg.split(\"x\")\n-- # else:\n-- #     num = arg\n-- # try:\n-- #     num = int(num)\n-- #     sides = int(sides)\n-- # except ValueError:\n-- #     return None\n-- # if num < 1 or sides < 1:\n-- #     return None\n-- # return num, sides\n-- \n--  Dice roll as number of rolls (eg 6) or as num and sides (2x6)\nlocal function dice_roll(arg)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_56324_dice_roll.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = dice_roll\n    lu.assertEquals(candidate('1x-2'), None)\n    lu.assertEquals(candidate('x5y6'), None)\n    lu.assertEquals(candidate('5x10'), {5, 10})\n    lu.assertEquals(candidate('5x6 3'), None)\n    lu.assertEquals(candidate('5 6'), None)\n    lu.assertEquals(candidate('6'), {6, 6})\n    lu.assertEquals(candidate('5x6x3x9x8'), None)\n    lu.assertEquals(candidate('x5x6'), None)\n    lu.assertEquals(candidate('2x0'), None)\n    lu.assertEquals(candidate(' '), None)\n    lu.assertEquals(candidate('5y6x'), None)\n    lu.assertEquals(candidate('4x6'), {4, 6})\n    lu.assertEquals(candidate('18x20'), {18, 20})\n    lu.assertEquals(candidate('x4'), None)\n    lu.assertEquals(candidate('5x6x3'), None)\n    lu.assertEquals(candidate('5x12'), {5, 12})\n    lu.assertEquals(candidate('0x1'), None)\n    lu.assertEquals(candidate('5x6y3'), None)\n    lu.assertEquals(candidate('-1x2'), None)\n    lu.assertEquals(candidate('1x1'), {1, 1})\n    lu.assertEquals(candidate('2x4'), {2, 4})\n    lu.assertEquals(candidate('\\n'), None)\n    lu.assertEquals(candidate('1x0'), None)\n    lu.assertEquals(candidate('5y6'), None)\n    lu.assertEquals(candidate('2'), {2, 6})\n    lu.assertEquals(candidate('2x'), None)\n    lu.assertEquals(candidate(''), None)\n    lu.assertEquals(candidate('\\t'), None)\n    lu.assertEquals(candidate('0x0'), None)\n    lu.assertEquals(candidate('2x6'), {2, 6})\n    lu.assertEquals(candidate('3x6'), {3, 6})\n    lu.assertEquals(candidate('5x6x3x9'), None)\n    lu.assertEquals(candidate('1x2'), {1, 2})\n    lu.assertEquals(candidate('5x6x'), None)\n    lu.assertEquals(candidate('-1x-2'), None)\n    lu.assertEquals(candidate('1x10'), {1, 10})\n    lu.assertEquals(candidate('5y6x3'), None)\n    lu.assertEquals(candidate('5x6'), {5, 6})\n    lu.assertEquals(candidate('5x6y'), None)\n    lu.assertEquals(candidate('1x100'), {1, 100})\n    lu.assertEquals(candidate('x5x6y'), None)\n    lu.assertEquals(candidate('10x4'), {10, 4})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_56391_charCodeAt", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # try:\n-- #     return ord(src[pos])\n-- # except IndexError:\n-- #     return None\n-- \n-- Returns the Unicode value of the character at the specified location.\n-- @param - index The zero-based index of the desired character.\n-- If there is no character at the specified index, NaN is returned.\n-- This was added for compatibility with python\nlocal function charCodeAt(src, pos)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_56391_charCodeAt.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = charCodeAt\n    lu.assertEquals(candidate('', 0), None)\n    lu.assertEquals(candidate('abc', -2), 98)\n    lu.assertEquals(candidate('abc', -3), 97)\n    lu.assertEquals(candidate('', -1), None)\n    lu.assertEquals(candidate('hello world', 21), None)\n    lu.assertEquals(candidate('abc', 3), None)\n    lu.assertEquals(candidate('abc', 3), None)\n    lu.assertEquals(candidate('abc', 1), 98)\n    lu.assertEquals(candidate('hello world', 1), 101)\n    lu.assertEquals(candidate('abc', 0), 97)\n    lu.assertEquals(candidate('abc', -4), None)\n    lu.assertEquals(candidate('abc', 2), 99)\n    lu.assertEquals(candidate('hello world', 2), 108)\n    lu.assertEquals(candidate('\ud83d\ude0a\ud83d\udc0d', 3), None)\n    lu.assertEquals(candidate('abc', 2), 99)\n    lu.assertEquals(candidate('\ud83d\ude0a\ud83d\udc0d', -4), None)\n    lu.assertEquals(candidate('abc', 0), 97)\n    lu.assertEquals(candidate('abc', 1), 98)\n    lu.assertEquals(candidate('', 2), None)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_56448_filter", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if s == None:\n-- #     return \"\"\n-- # s = s.replace(\">\", \"&gt;\")\n-- # s = s.replace(\"<\", \"&lt;\")\n-- # return s\n-- \n--  Filters a plain text and makes it acceptable for docbook \nlocal function filter(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_56448_filter.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = filter\n    lu.assertEquals(candidate('abc'), 'abc')\n    lu.assertEquals(candidate('<><>'), '&lt;&gt;&lt;&gt;')\n    lu.assertEquals(candidate('>'), '&gt;')\n    lu.assertEquals(candidate('<>'), '&lt;&gt;')\n    lu.assertEquals(candidate('foo'), 'foo')\n    lu.assertEquals(candidate('<Hello world>'), '&lt;Hello world&gt;')\n    lu.assertEquals(candidate('<'), '&lt;')\n    lu.assertEquals(candidate('a < b'), 'a &lt; b')\n    lu.assertEquals(candidate(None), '')\n    lu.assertEquals(candidate('this is plain text'), 'this is plain text')\n    lu.assertEquals(candidate('Hello world!'), 'Hello world!')\n    lu.assertEquals(candidate('><'), '&gt;&lt;')\n    lu.assertEquals(candidate('a'), 'a')\n    lu.assertEquals(candidate(''), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_56655_bool_to_string", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # s = str(b).lower()\n-- # if s in [\"true\", \"false\"]:\n-- #     return s\n-- # raise TypeError(\"Value must be True or False.\")\n-- \n-- Convert a boolean type to string.\n-- Args:\n--     b (bool): A Boolean.\n-- Raises:\n--     TypeError\n-- Returns:\n--     str: String representation of a bool type.\nlocal function bool_to_string(b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_56655_bool_to_string.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = bool_to_string\n    lu.assertEquals(candidate(bool(false)), candidate(false))\n    lu.assertEquals(candidate(bool(true)), candidate(true))\n    lu.assertEquals(candidate(true), 'true')\n    lu.assertEquals(candidate(false), 'false')\n    lu.assertEquals(candidate(true), candidate(bool(true)))\n    lu.assertEquals(candidate(bool(false)), 'false')\n    lu.assertEquals(candidate(false), candidate(bool(false)))\n    lu.assertEquals(candidate(bool(true)), 'true')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_56738_sum_of_powers", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return sum(pow(number, power) for number in numbers)\n-- \n-- Sums each number raised to power\n-- Ex: sum_of_powers([2, 3, 4], 2) = 2^2 + 3^2 + 4^2 = 29\nlocal function sum_of_powers(numbers, power)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_56738_sum_of_powers.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sum_of_powers\n    lu.assertEquals(candidate({1, 2, 3}, 3), 36)\n    lu.assertEquals(candidate({2, 3, 4}, 2), 29)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_56766_factorial", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # result = 1\n-- # for i in range(1, k + 1):\n-- #     result *= i\n-- # return result\n-- \n-- Returns the factorial of the number, the product of all positive integers smaller or equal to the number.\n-- By convention an empty product is considered 1, meaning factorial(0) will return 1.\n-- :param k: A positive integer\n-- :return: The factorial of that integer\nlocal function factorial(k)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_56766_factorial.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = factorial\n    lu.assertEquals(candidate(17), 355687428096000)\n    lu.assertEquals(candidate(9), 362880)\n    lu.assertEquals(candidate(20), 2432902008176640000)\n    lu.assertEquals(candidate(5), 120)\n    lu.assertEquals(candidate(11), 39916800)\n    lu.assertEquals(candidate(14), 87178291200)\n    lu.assertEquals(candidate(10), 3628800)\n    lu.assertEquals(candidate(0), 1)\n    lu.assertEquals(candidate(12), 479001600)\n    lu.assertEquals(candidate(8), 40320)\n    lu.assertEquals(candidate(13), 6227020800)\n    lu.assertEquals(candidate(15), 1307674368000)\n    lu.assertEquals(candidate(6), 720)\n    lu.assertEquals(candidate(18), 6402373705728000)\n    lu.assertEquals(candidate(4), 24)\n    lu.assertEquals(candidate(3), 6)\n    lu.assertEquals(candidate(16), 20922789888000)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(7), 5040)\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(19), 121645100408832000)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_57139_build_type_flag", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not compiler or not build_type:\n-- #     return \"\"\n-- # if str(compiler) == 'Visual Studio':\n-- #     if build_type == 'Debug':\n-- #         return '-Zi'\n-- # else:\n-- #     if build_type == 'Debug':\n-- #         return '-g'\n-- #     elif build_type == 'Release' and str(compiler) == 'gcc':\n-- #         return '-s'\n-- # return \"\"\n-- \n-- returns flags specific to the build type (Debug, Release, etc.)\n-- (-s, -g, /Zi, etc.)\nlocal function build_type_flag(compiler, build_type)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_57139_build_type_flag.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = build_type_flag\n    lu.assertEquals(candidate('Visual Studio', 'Release'), '')\n    lu.assertEquals(candidate('Visual Studio', 'Debug'), '-Zi')\n    lu.assertEquals(candidate('foo', 'RelWithDebInfo'), '')\n    lu.assertEquals(candidate(None, 'Release'), '')\n    lu.assertEquals(candidate(None, 'Release'), '')\n    lu.assertEquals(candidate('Visual Studio', 'RelWithDebInfo'), '')\n    lu.assertEquals(candidate('Visual Studio', None), '')\n    lu.assertEquals(candidate('foo', None), '')\n    lu.assertEquals(candidate('foo', 'Release'), '')\n    lu.assertEquals(candidate(None, 'RelWithDebInfo'), '')\n    lu.assertEquals(candidate(None, 'Debug'), '')\n    lu.assertEquals(candidate(None, None), '')\n    lu.assertEquals(candidate('Visual Studio', 'Debug'), '-Zi')\n    lu.assertEquals(candidate('Visual Studio', None), '')\n    lu.assertEquals(candidate('gcc', 'Debug'), '-g')\n    lu.assertEquals(candidate('gcc', None), '')\n    lu.assertEquals(candidate('gcc', 'Release'), '-s')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_57161_remove_cdata_tags_from_every_node", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # content = content.replace(']]>', '>')\n-- # content = content.replace('<![CDATA[', '<')\n-- # return content\n-- \n-- [removes a CDATA tag from every node in the document] \nlocal function remove_cdata_tags_from_every_node(content)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_57161_remove_cdata_tags_from_every_node.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = remove_cdata_tags_from_every_node\n    lu.assertEquals(candidate('<![CDATA[this is a cdata]]>'), '<this is a cdata>')\n    lu.assertEquals(candidate('this is not a cdata'), 'this is not a cdata')\n    lu.assertEquals(candidate('<div>Hi there</div>'), '<div>Hi there</div>')\n    lu.assertEquals(candidate('I am not a CDATA tag!'), 'I am not a CDATA tag!')\n    lu.assertEquals(candidate('<a><b><![CDATA[this is a cdata]]></b></a>'), '<a><b><this is a cdata></b></a>')\n    lu.assertEquals(candidate('<a><b><![CDATA[this is not a cdata]]></b><![CDATA[so this one too]]></a>'), '<a><b><this is not a cdata></b><so this one too></a>')\n    lu.assertEquals(candidate('<![CDATA[this is not a cdata]]><![CDATA[so this one too]]>'), '<this is not a cdata><so this one too>')\n    lu.assertEquals(candidate(''), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_57396_convert_case", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return ''.join([a.title() for a in s.split(\"_\") if a])\n-- \n--     Given a string in snake case, conver to CamelCase\nlocal function convert_case(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_57396_convert_case.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = convert_case\n    lu.assertEquals(candidate('this_is_snake_case'), 'ThisIsSnakeCase')\n    lu.assertEquals(candidate('James'), 'James')\n    lu.assertEquals(candidate('sally_brown'), 'SallyBrown')\n    lu.assertEquals(candidate('this_is_snake_case_too'), 'ThisIsSnakeCaseToo')\n    lu.assertEquals(candidate('Sally_Brown'), 'SallyBrown')\n    lu.assertEquals(candidate('sAlly_bRoWn'), 'SallyBrown')\n    lu.assertEquals(candidate('this__is__snake__case_too'), 'ThisIsSnakeCaseToo')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_57398_correct_sentence", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if text[0].islower() == True:\n-- #     text = text[0].upper() + text[1:]\n-- # if text[-1] != \".\":\n-- #     text = text + \".\"\n-- # return text\n-- \n-- returns a corrected sentence which starts with a capital letter\n-- and ends with a dot.\nlocal function correct_sentence(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_57398_correct_sentence.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = correct_sentence\n    lu.assertEquals(candidate('hello world.'), 'Hello world.')\n    lu.assertEquals(candidate('hello world'), 'Hello world.')\n    lu.assertEquals(candidate('hello world.'), 'Hello world.')\n    lu.assertEquals(candidate('hello world'), 'Hello world.')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_58289_api_repo_url", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return 'https://api.github.com/orgs/{}/repos'.format(org_name)\n-- \n-- With the supplied organization name, constructs a GitHub API URL\n-- :param org_name: GitHub organization name\n-- :return: URL to GitHub API to query org's repos\nlocal function api_repo_url(org_name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_58289_api_repo_url.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = api_repo_url\n    lu.assertEquals(candidate('github'), 'https://api.github.com/orgs/github/repos')\n    lu.assertEquals(candidate('org2'), 'https://api.github.com/orgs/org2/repos')\n    lu.assertEquals(candidate('google'), 'https://api.github.com/orgs/google/repos')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_5841__root_sort_key", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # try:\n-- #     return root.pattern\n-- # except AttributeError:\n-- #     return root\n-- \n-- Allow root comparison when sorting.\n-- Args:\n--     root (str or re.Pattern): Root.\n-- Returns:\n--     str: Comparable root string.\nlocal function _root_sort_key(root)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_5841__root_sort_key.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _root_sort_key\n    lu.assertEquals(candidate('^/a/b/?$'), '^/a/b/?$')\n    lu.assertEquals(candidate('^/foo/bar/baz/quux/a[b]c.*$'), '^/foo/bar/baz/quux/a[b]c.*$')\n    lu.assertEquals(candidate('^/foo/bar/baz/quux/a[b]c.*'), '^/foo/bar/baz/quux/a[b]c.*')\n    lu.assertEquals(candidate('.*'), '.*')\n    lu.assertEquals(candidate('foo'), 'foo')\n    lu.assertEquals(candidate('/?$'), '/?$')\n    lu.assertEquals(candidate('^/$'), '^/$')\n    lu.assertEquals(candidate('^/foo/bar/baz/quux/.*'), '^/foo/bar/baz/quux/.*')\n    lu.assertEquals(candidate('root_2'), 'root_2')\n    lu.assertEquals(candidate('^/foo/bar/baz/quux$'), '^/foo/bar/baz/quux$')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_59500_lr_schedule_adam", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # for Adam Optimizer\n-- # lr = 1e-3\n-- # if epoch > 350:\n-- #     lr = 1e-5\n-- # elif epoch > 300:\n-- #     lr = 1e-4\n-- # elif epoch > 200:\n-- #     lr = 2e-4\n-- # elif epoch > 100:\n-- #     lr = 5e-4\n-- # print('Learning rate: ', lr)\n-- # return lr\n-- \n-- Learning Rate Schedule\n-- # Arguments\n--     epoch (int): The number of epochs\n-- # Returns\n--     lr (float32): learning rate\nlocal function lr_schedule_adam(epoch)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_59500_lr_schedule_adam.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = lr_schedule_adam\n    lu.assertEquals(candidate(100), 0.001)\n    lu.assertEquals(candidate(400), 1e-05)\n    lu.assertEquals(candidate(200), 0.0005)\n    lu.assertEquals(candidate(10), 0.001)\n    lu.assertEquals(candidate(0), 0.001)\n    lu.assertEquals(candidate(50), 0.001)\n    lu.assertEquals(candidate(50), 0.001)\n    lu.assertEquals(candidate(200), 0.0005)\n    lu.assertEquals(candidate(149), 0.0005)\n    lu.assertEquals(candidate(0), 0.001)\n    lu.assertEquals(candidate(151), 0.0005)\n    lu.assertEquals(candidate(99), 0.001)\n    lu.assertEquals(candidate(49), 0.001)\n    lu.assertEquals(candidate(150), 0.0005)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_60112_set_to_dm_limits", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # updated = [tuple(min(i, limit) for i in ptt) for ptt in ptt_list]\n-- # return updated\n-- \n-- Check that the values for piston, tip, and tilt are not exceeding the hardware\n-- limit and reset to limit if limit is exceeded. These limits are the same as what\n-- the IrisAO GUI has set.\n-- :param ppt_list: list, of tuples existing of piston, tip, tilt, values for each\n--                  segment in a pupil, in DM units\n-- :param limit: float, in DM units. Default = 5.\n-- :return: list of tuples of the piston, tip, tilt values in DM units for each segment listed\n--          such that none of the values exceed the limit\nlocal function set_to_dm_limits(ptt_list, limit)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_60112_set_to_dm_limits.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = set_to_dm_limits\n    lu.assertEquals(candidate({{0.2, 0.5, 0.3, 1000.0}, {0.5, 0.5, 0.3, 0.1}, {0.4, 0.5, 0.3, 0.1}}), {{0.2, 0.5, 0.3, 5.0}, {0.5, 0.5, 0.3, 0.1}, {0.4, 0.5, 0.3, 0.1}})\n    lu.assertEquals(candidate({{0.2, 0.5, 0.3, 0.2}, {0.5, 0.5, 0.3, 0.1}, {0.4, 0.5, 0.3, 0.1}}), {{0.2, 0.5, 0.3, 0.2}, {0.5, 0.5, 0.3, 0.1}, {0.4, 0.5, 0.3, 0.1}})\n    lu.assertEquals(candidate({{0.2, 0.5, 0.3, 0.2}, {0.5, 0.5, 0.3, 0.1}, {0.4, 0.5, 0.3, 0.1}}, 1.0), {{0.2, 0.5, 0.3, 0.2}, {0.5, 0.5, 0.3, 0.1}, {0.4, 0.5, 0.3, 0.1}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_60491_prime_factors", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # i = 2\n-- # factors = []\n-- # while i * i <= n:\n-- #     if n % i:\n-- #         i += 1\n-- #     else:\n-- #         n //= i\n-- #         factors.append(i)\n-- # if n > 1:\n-- #     factors.append(n)\n-- # return factors\n-- \n-- Compute the prime factors of the given number\n-- :param n: Number you want to compute the prime factors (intger)\n-- :return: Prime factors of the given number (list of integer)\nlocal function prime_factors(n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_60491_prime_factors.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = prime_factors\n    lu.assertEquals(candidate(15), {3, 5})\n    lu.assertEquals(candidate(21), {3, 7})\n    lu.assertEquals(candidate(4), {2, 2})\n    lu.assertEquals(candidate(6), {2, 3})\n    lu.assertEquals(candidate(19), {19})\n    lu.assertEquals(candidate(13), {13})\n    lu.assertEquals(candidate(12), {2, 2, 3})\n    lu.assertEquals(candidate(26), {2, 13})\n    lu.assertEquals(candidate(3), {3})\n    lu.assertEquals(candidate(8), {2, 2, 2})\n    lu.assertEquals(candidate(10), {2, 5})\n    lu.assertEquals(candidate(0), {})\n    lu.assertEquals(candidate(2), {2})\n    lu.assertEquals(candidate(24), {2, 2, 2, 3})\n    lu.assertEquals(candidate(18), {2, 3, 3})\n    lu.assertEquals(candidate(1), {})\n    lu.assertEquals(candidate(16), {2, 2, 2, 2})\n    lu.assertEquals(candidate(20), {2, 2, 5})\n    lu.assertEquals(candidate(23), {23})\n    lu.assertEquals(candidate(14), {2, 7})\n    lu.assertEquals(candidate(22), {2, 11})\n    lu.assertEquals(candidate(11), {11})\n    lu.assertEquals(candidate(9), {3, 3})\n    lu.assertEquals(candidate(25), {5, 5})\n    lu.assertEquals(candidate(5), {5})\n    lu.assertEquals(candidate(17), {17})\n    lu.assertEquals(candidate(7), {7})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_6077_get_index_str", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if i < 0 or i > n:\n-- #     raise ValueError(\"N >= i or i > 0 is required\")\n-- # lm = len(str(n))\n-- # res = str(i)\n-- # while lm > len(res):\n-- #     res = \"0\" + res\n-- # return res\n-- \n-- To convert an int 'i' to a string.\n-- Parameters\n-- ----------\n-- n : int\n--     Order to put 0 if necessary.\n-- i : int\n--     The number to convert.\n-- Returns\n-- -------\n-- res : str\n--     The number as a string.\n-- Examples\n-- --------\n-- ```python\n--     getIndexStr(100,15)\n-- ```\n-- Out:\n-- ```\n--     '015'\n-- ```\nlocal function get_index_str(n, i)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_6077_get_index_str.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_index_str\n    lu.assertEquals(candidate(10, 1), '01')\n    lu.assertEquals(candidate(100, 100), '100')\n    lu.assertEquals(candidate(10, 10), '10')\n    lu.assertEquals(candidate(10, 0), '00')\n    lu.assertEquals(candidate(100, 0), '000')\n    lu.assertEquals(candidate(10, 9), '09')\n    lu.assertEquals(candidate(10, 10), '10')\n    lu.assertEquals(candidate(1, 1), str(1))\n    lu.assertEquals(candidate(20, 10), '10')\n    lu.assertEquals(candidate(100, 15), '015')\n    lu.assertEquals(candidate(100, 99), '099')\n    lu.assertEquals(candidate(0, 0), '0')\n    lu.assertEquals(candidate(10, 2), '02')\n    lu.assertEquals(candidate(10, 10), '10')\n    lu.assertEquals(candidate(1, 0), '0')\n    lu.assertEquals(candidate(100, 15), '015')\n    lu.assertEquals(candidate(1, 1), '1')\n    lu.assertEquals(candidate(100, 15), '015')\n    lu.assertEquals(candidate(1, 1), '1')\n    lu.assertEquals(candidate(100, 10), '010')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_61119__topo_to_sphere", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # sph_phi = (0.5 - radius) * 180\n-- # sph_theta = -theta\n-- # return sph_phi, sph_theta\n-- \n-- Convert 2D topo coordinates to spherical.\nlocal function _topo_to_sphere(theta, radius)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_61119__topo_to_sphere.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _topo_to_sphere\n    lu.assertEquals(candidate(0.0, 0.5), {0.0, 0.0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_6150__dof", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return (2.0 * mean_tau ** 4.) / (sd_tau2 ** 2.)\n-- \n-- Returns the degrees of freedom for the chi-2 distribution from the mean and\n-- variance of the uncertainty model, as reported in equation 5.5 of Al Atik\n-- (2015)\nlocal function _dof(mean_tau, sd_tau2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_6150__dof.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _dof\n    lu.assertEquals(candidate(), candidate(0.1, 0.001))\n    lu.assertEquals(candidate(1.0, 1.0), 2.0)\n    lu.assertEquals(candidate(1.0, 1.0), 2.0)\n    lu.assertEquals(candidate(0.1), candidate(0.1, 0.001))\n    lu.assertEquals(candidate(2.0, 8.0), 0.5)\n    lu.assertEquals(candidate(1.0, 2.0), 0.5)\n    lu.assertEquals(candidate(2.0, 2.0), 8.0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_61922_http_verifier", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not any(x in url[:10] for x in ['http://', 'https://']):\n-- #     return 'http://' + url\n-- # else:\n-- #     return url\n-- \n-- verifies if the url starts with\n-- http:// or https://. If not, http://\n-- is put in the start of url\n-- :param url: url to be verified\n-- :return: url with http://\nlocal function http_verifier(url)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_61922_http_verifier.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = http_verifier\n    lu.assertEquals(candidate('http://www.codewars.com'), 'http://www.codewars.com')\n    lu.assertEquals(candidate('http://www.codewars.com/users/GiacomoSorbi?ref=codewars'), 'http://www.codewars.com/users/GiacomoSorbi?ref=codewars')\n    lu.assertEquals(candidate('http://foo.bar'), 'http://foo.bar')\n    lu.assertEquals(candidate('http://www.codewars.com/users/GiacomoSorbi?ref=codewars&foo=bar'), 'http://www.codewars.com/users/GiacomoSorbi?ref=codewars&foo=bar')\n    lu.assertEquals(candidate('http://google.com'), 'http://google.com')\n    lu.assertEquals(candidate('http://www.codewars.com/users/GiacomoSorbi?ref=other'), 'http://www.codewars.com/users/GiacomoSorbi?ref=other')\n    lu.assertEquals(candidate('https://www.foo.bar'), 'https://www.foo.bar')\n    lu.assertEquals(candidate('https://youtube.com'), 'https://youtube.com')\n    lu.assertEquals(candidate('http://www.python.org/'), 'http://www.python.org/')\n    lu.assertEquals(candidate('codewars.com'), 'http://codewars.com')\n    lu.assertEquals(candidate('www.foo.bar'), 'http://www.foo.bar')\n    lu.assertEquals(candidate('https://foo.bar'), 'https://foo.bar')\n    lu.assertEquals(candidate('www.codewars.com'), 'http://www.codewars.com')\n    lu.assertEquals(candidate('http://www.codewars.com/users/GiacomoSorbi?ref=mycodewars'), 'http://www.codewars.com/users/GiacomoSorbi?ref=mycodewars')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_61942_write_var_length", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Result goes into an array. Since we are starting with the least\n-- # # significant byte, we will eventually have to reverse this to correctly\n-- # # order the output character bytes:\n-- # result_array = []\n-- # # We have at least one value, right? :\n-- # result_array.append(var & 0x7F)\n-- # var >>= 7  # shift right\n-- # # If 'var' still has any value greater than 0, 'and' it with 0x7f and\n-- # # then 'or' with 0x80, then put it out to result.  Then we shift right\n-- # # for the next iteration:\n-- # while var > 0x0:\n-- #     result_array.append((var & 0x7F) | 0x80)\n-- #     var >>= 7\n-- # # Reverse the array, after all, we *did* put the least significant byte\n-- # # in their first!\n-- # result_array.reverse()\n-- # # Make an empty list of to gather the values appended as characters.\n-- # # Return the string-joined array:\n-- # hex_string_array = []\n-- # for n in result_array:\n-- #     hex_string_array.append(chr(n))\n-- # return \"\".join(hex_string_array)\n-- \n--  Take a numerical value, and convert it to a 7-bit packed string\n-- with high bit of each byte set as a flag to indicate to the reader that\n-- the value that follows in the following byte is to be consumed as well.\nlocal function write_var_length(var)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_61942_write_var_length.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = write_var_length\n    lu.assertEquals(candidate(23), '\\x17')\n    lu.assertEquals(candidate(1), '\\x01')\n    lu.assertEquals(candidate(127), '\\x7f')\n    lu.assertEquals(candidate(268435455), '\u00ff\u00ff\u00ff\\x7f')\n    lu.assertEquals(candidate(16384), '\\x81\\x80\\x00')\n    lu.assertEquals(candidate(16383), '\u00ff\\x7f')\n    lu.assertEquals(candidate(268435456), '\\x81\\x80\\x80\\x80\\x00')\n    lu.assertEquals(candidate(0), '\\x00')\n    lu.assertEquals(candidate(128), '\\x81\\x00')\n    lu.assertEquals(candidate(0), '\\x00')\n    lu.assertEquals(candidate(127), chr(127))\n    lu.assertEquals(candidate(2097151), '\u00ff\u00ff\\x7f')\n    lu.assertEquals(candidate(1), '\\x01')\n    lu.assertEquals(candidate(127), '\\x7f')\n    lu.assertEquals(candidate(2097152), '\\x81\\x80\\x80\\x00')\n    lu.assertEquals(candidate(1), chr(1))\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_62278_fibonacci", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if n <= 1:\n-- #     return n\n-- # else:\n-- #     return fibonacci(n-1) + fibonacci(n-2)\n-- \n-- Returns the n-th number in the Fibonacci sequence.\n-- Parameters\n-- ----------\n-- n: int\n--    The n-th number in the Fibonacci sequence.\nlocal function fibonacci(n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_62278_fibonacci.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = fibonacci\n    lu.assertEquals(candidate(12), 144)\n    lu.assertEquals(candidate(18), 2584)\n    lu.assertEquals(candidate(2), 1)\n    lu.assertEquals(candidate(19), 4181)\n    lu.assertEquals(candidate(8), 21)\n    lu.assertEquals(candidate(7), 13)\n    lu.assertEquals(candidate(22), 17711)\n    lu.assertEquals(candidate(25), 75025)\n    lu.assertEquals(candidate(14), 377)\n    lu.assertEquals(candidate(15), 610)\n    lu.assertEquals(candidate(21), 10946)\n    lu.assertEquals(candidate(13), 233)\n    lu.assertEquals(candidate(11), 89)\n    lu.assertEquals(candidate(26), 121393)\n    lu.assertEquals(candidate(24), 46368)\n    lu.assertEquals(candidate(6), 8)\n    lu.assertEquals(candidate(9), 34)\n    lu.assertEquals(candidate(1), 1)\n    lu.assertEquals(candidate(20), 6765)\n    lu.assertEquals(candidate(3), 2)\n    lu.assertEquals(candidate(27), 196418)\n    lu.assertEquals(candidate(30), 832040)\n    lu.assertEquals(candidate(17), 1597)\n    lu.assertEquals(candidate(29), 514229)\n    lu.assertEquals(candidate(28), 317811)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(5), 5)\n    lu.assertEquals(candidate(4), 3)\n    lu.assertEquals(candidate(23), 28657)\n    lu.assertEquals(candidate(16), 987)\n    lu.assertEquals(candidate(10), 55)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_62303_genSubparts", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # length = len(string)\n-- # res = []\n-- # for i in range(1, length):\n-- #     res.append((string[0:i], string[i:]))\n-- # return res\n-- \n-- Partition a string into all possible two parts, e.g.\n-- given \"abcd\", generate [(\"a\", \"bcd\"), (\"ab\", \"cd\"), (\"abc\", \"d\")]\n-- For string of length 1, return empty list\nlocal function genSubparts(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_62303_genSubparts.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = genSubparts\n    lu.assertEquals(candidate(''), {})\n    lu.assertEquals(candidate(''), {})\n    lu.assertEquals(candidate('abcd'), {{'a', 'bcd'}, {'ab', 'cd'}, {'abc', 'd'}})\n    lu.assertEquals(candidate('abcd'), {{'a', 'bcd'}, {'ab', 'cd'}, {'abc', 'd'}})\n    lu.assertEquals(candidate('abcd'), {{'a', 'bcd'}, {'ab', 'cd'}, {'abc', 'd'}})\n    lu.assertEquals(candidate('abc'), {{'a', 'bc'}, {'ab', 'c'}})\n    lu.assertEquals(candidate('abcd'), {{'a', 'bcd'}, {'ab', 'cd'}, {'abc', 'd'}})\n    lu.assertEquals(candidate('a'), {})\n    lu.assertEquals(candidate(''), {})\n    lu.assertEquals(candidate('abcd'), {{'a', 'bcd'}, {'ab', 'cd'}, {'abc', 'd'}})\n    lu.assertEquals(candidate('ab'), {{'a', 'b'}})\n    lu.assertEquals(candidate('abcd'), {{'a', 'bcd'}, {'ab', 'cd'}, {'abc', 'd'}})\n    lu.assertEquals(candidate(''), {})\n    lu.assertEquals(candidate('abc'), {{'a', 'bc'}, {'ab', 'c'}})\n    lu.assertEquals(candidate('a'), {})\n    lu.assertEquals(candidate('ab'), {{'a', 'b'}})\n    lu.assertEquals(candidate('abc'), {{'a', 'bc'}, {'ab', 'c'}})\n    lu.assertEquals(candidate('abc'), {{'a', 'bc'}, {'ab', 'c'}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_63573_check_bounds", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if volts > 4055:\n-- #     return 4055\n-- # if volts < 45:\n-- #     return 45\n-- # return volts\n-- \n-- Limit voltage.\n-- Voltage limits are 45 (1.1%) and 4055 (99.0%) of a 4095 max.\n-- Valve calibrated so that at 45 (1.1%) it's fully shutoff.\nlocal function check_bounds(volts)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_63573_check_bounds.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = check_bounds\n    lu.assertEquals(candidate(500), 500)\n    lu.assertEquals(candidate(4046), 4046)\n    lu.assertEquals(candidate(4037), 4037)\n    lu.assertEquals(candidate(4033), 4033)\n    lu.assertEquals(candidate(4055), 4055)\n    lu.assertEquals(candidate(4044), 4044)\n    lu.assertEquals(candidate(4048), 4048)\n    lu.assertEquals(candidate(4034), 4034)\n    lu.assertEquals(candidate(4056), 4055)\n    lu.assertEquals(candidate(45), 45)\n    lu.assertEquals(candidate(4050), 4050)\n    lu.assertEquals(candidate(0), 45)\n    lu.assertEquals(candidate(4041), 4041)\n    lu.assertEquals(candidate(4054), 4054)\n    lu.assertEquals(candidate(4045), 4045)\n    lu.assertEquals(candidate(1), 45)\n    lu.assertEquals(candidate(4047), 4047)\n    lu.assertEquals(candidate(4049), 4049)\n    lu.assertEquals(candidate(4042), 4042)\n    lu.assertEquals(candidate(4040), 4040)\n    lu.assertEquals(candidate(3000), 3000)\n    lu.assertEquals(candidate(4035), 4035)\n    lu.assertEquals(candidate(4095), 4055)\n    lu.assertEquals(candidate(4039), 4039)\n    lu.assertEquals(candidate(4036), 4036)\n    lu.assertEquals(candidate(4096), 4055)\n    lu.assertEquals(candidate(4038), 4038)\n    lu.assertEquals(candidate(4043), 4043)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_63833_multiline_test", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # line = line.rpartition('=')  # partition the line in before, sep, after\n-- # line = ''.join(line[0:2])  # use all including separator\n-- # line = line.rstrip()  # strip spaces\n-- # if line.endswith('='):\n-- #     return True\n-- # else:\n-- #     return False\n-- \n-- test if the current line is a multiline with \"=\" at the end\n-- :param line: 'O1 3 -0.01453 1.66590 0.10966 11.00 0.05 ='\n-- :type line: string\n-- >>> line = 'C1    1    0.278062    0.552051    0.832431    11.00000    0.02895    0.02285 ='\n-- >>> multiline_test(line)\n-- True\n-- >>> line = 'C1    1    0.278062    0.552051    0.832431    11.00000    0.05 '\n-- >>> multiline_test(line)\n-- False\nlocal function multiline_test(line)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_63833_multiline_test.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = multiline_test\n    lu.assertEquals(candidate('O1 3 -0.01453 1.66590 0.10966 11.00 0.05 ='), true)\n    lu.assertEquals(candidate('O1 3 -0.01453 1.66590 0.10966 11.00 0.05 '), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_64387_cleanForIRI", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # iri = \"\"\n-- # for c in string:\n-- #     if c.isalnum() or c in [\"-\", \".\", \"_\", \"~\"]:\n-- #         iri += c\n-- # return iri\n-- \n-- Cleans a string to be suitable for use as an IRI (punctation we dont want is removed)\nlocal function cleanForIRI(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_64387_cleanForIRI.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = cleanForIRI\n    lu.assertEquals(candidate('123 456'), '123456')\n    lu.assertEquals(candidate('123'), '123')\n    lu.assertEquals(candidate('hello world!'), 'helloworld')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_65087_trim_lost_U", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # keepgoing = 1\n-- # for lostuseq in LOSTUSEQS:\n-- #     if keepgoing:\n-- #         if len(seq_F) < len(lostuseq):\n-- #             return seq_F, qual_F\n-- #         if seq_F[:len(lostuseq)] == lostuseq:\n-- #             seq_F = seq_F[len(lostuseq):]\n-- #             qual_F = qual_F[len(lostuseq):]\n-- #         # if LOSTUSEQ[0] found, also look for LOSTUSEQ[1] etc.\n-- #         else:\n-- #             keepgoing = 0\n-- # return seq_F, qual_F\n-- \n--  test for lost U at the 3' end of the PCR primer sequence \nlocal function trim_lost_U(seq_F, qual_F, LOSTUSEQS)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_65087_trim_lost_U.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = trim_lost_U\n    lu.assertEquals(candidate('AACC', 'DEB', {'AACC'}), {'', ''})\n    lu.assertEquals(candidate('AACC', 'DEB', {'AACC', 'ACTA'}), {'', ''})\n    lu.assertEquals(candidate('CGT', 'DEB', {'AACT'}), {'CGT', 'DEB'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_65211_findDuplicate", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # slow, fast = nums[0], nums[0]\n-- # while True:\n-- #     fast = nums[nums[fast]]\n-- #     slow = nums[slow]\n-- #     if fast == slow:\n-- #         break\n-- # fast = nums[0]\n-- # while fast != slow:\n-- #     fast = nums[fast]\n-- #     slow = nums[slow]\n-- # return slow\n-- \n-- :type nums: List[int]\n-- :rtype: int\nlocal function findDuplicate(nums)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_65211_findDuplicate.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = findDuplicate\n    lu.assertEquals(candidate({1, 1, 2}), 1)\n    lu.assertEquals(candidate({1, 3, 4, 2, 2}), 2)\n    lu.assertEquals(candidate({1, 1}), 1)\n    lu.assertEquals(candidate({1, 1, 2, 2, 3, 3, 4, 4}), 1)\n    lu.assertEquals(candidate({3, 1, 3, 4, 2}), 3)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_65436_remove_min_line", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for y, y_elt in enumerate(matrix):\n-- #     for x, x_elt in enumerate(y_elt):\n-- #         matrix[y][x] = x_elt - list_min_line[y]\n-- # return matrix\n-- \n-- Soustraction des min par ligne\nlocal function remove_min_line(matrix, list_min_line)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_65436_remove_min_line.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = remove_min_line\n    lu.assertEquals(candidate({{4, 1, 4, 4}, {4, 4, 1, 4}, {4, 4, 4, 1}}, {1, 1, 1}), {{3, 0, 3, 3}, {3, 3, 0, 3}, {3, 3, 3, 0}})\n    lu.assertEquals(candidate({{1, 2, 3, 4}}, {1}), {{0, 1, 2, 3}})\n    lu.assertEquals(candidate({{2, 2, 2}, {5, 5, 5}}, {2, 2, 2}), {{0, 0, 0}, {3, 3, 3}})\n    lu.assertEquals(candidate({{3, 4, 5, 6, 7, 8, 9, 0}}, {1, 2, 3, 4}), {{2, 3, 4, 5, 6, 7, 8, -1}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_65809_misencode", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return text.encode(\"cp1250\").decode(\"latin2\")\n-- \n-- Take a properly represented text, encode into win1250 and decode\n-- back into latin2 (iso-8859-2) so it could be encoded back as such over the wire.\n-- Has to be used when querying database for data stored by original application,\n-- represented by MisencodedChar/TextField.\nlocal function misencode(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_65809_misencode.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = misencode\n    lu.assertEquals(candidate('abc'), 'abc')\n    lu.assertEquals(candidate('\u017b'), '\u017b')\n    lu.assertEquals(candidate('x'), 'x')\n    lu.assertEquals(candidate('\u0119\u0142'), '\u0119\u0142')\n    lu.assertEquals(candidate('\u0119'), '\u0119')\n    lu.assertEquals(candidate('a'), 'a')\n    lu.assertEquals(candidate('x\\n\\r\\t a'), 'x\\n\\r\\t a')\n    lu.assertEquals(candidate('x\\n\\r\\t'), 'x\\n\\r\\t')\n    lu.assertEquals(candidate('x '), 'x ')\n    lu.assertEquals(candidate('abcd'), 'abcd')\n    lu.assertEquals(candidate('\u00f3'), '\u00f3')\n    lu.assertEquals(candidate('\u0142\u0119'), '\u0142\u0119')\n    lu.assertEquals(candidate('\u017c'), '\u017c')\n    lu.assertEquals(candidate(' '), ' ')\n    lu.assertEquals(candidate('x\\ta'), 'x\\ta')\n    lu.assertEquals(candidate('\u0118'), '\u0118')\n    lu.assertEquals(candidate('\u0142'), '\u0142')\n    lu.assertEquals(candidate('ab'), 'ab')\n    lu.assertEquals(candidate('\u0142'), '\u0142')\n    lu.assertEquals(candidate('\u0119'), '\u0119')\n    lu.assertEquals(candidate('x\\na'), 'x\\na')\n    lu.assertEquals(candidate('\u0141'), '\u0141')\n    lu.assertEquals(candidate('x\\n\\r a\\n'), 'x\\n\\r a\\n')\n    lu.assertEquals(candidate('x a'), 'x a')\n    lu.assertEquals(candidate('x\\r\\na'), 'x\\r\\na')\n    lu.assertEquals(candidate('x\\n\\r'), 'x\\n\\r')\n    lu.assertEquals(candidate('x\\n\\r\\t\\na'), 'x\\n\\r\\t\\na')\n    lu.assertEquals(candidate('\u00d3'), '\u00d3')\n    lu.assertEquals(candidate('\u0119'), '\u0119')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('ab\u010d'), 'ab\u010d')\n    lu.assertEquals(candidate('x\\n\\ra\\n'), 'x\\n\\ra\\n')\n    lu.assertEquals(candidate('\u0143'), '\u0143')\n    lu.assertEquals(candidate('\u0144'), '\u0144')\n    lu.assertEquals(candidate('x\\n\\ra'), 'x\\n\\ra')\n    lu.assertEquals(candidate('x\\n\\r a'), 'x\\n\\r a')\n    lu.assertEquals(candidate('\u0118'), '\u0118')\n    lu.assertEquals(candidate('ab\u010d'), 'ab\u010d')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_66931___validate_float_fields", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # try:\n-- #     float_field = float(value)\n-- #     return float_field\n-- # except ValueError:\n-- #     raise TypeError(error_msg)\n-- \n-- Validate float values from a dictionary.\n-- Parameters\n-- ----------\n-- value : float\n--     Value to be validated.\n-- error_msg : str\n--     Error message for an invalid value.\n-- Returns\n-- -------\n-- float\n--     Validated value.\n-- Raises\n-- ------\n-- TypeError\n--     Raised when the value is not valid (namely, when it is data that cannot be cast to float).\nlocal function __validate_float_fields(value, error_msg)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_66931___validate_float_fields.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = __validate_float_fields\n    lu.assertEquals(candidate(-273.15, ''), -273.15)\n    lu.assertEquals(candidate(0, ''), 0)\n    lu.assertEquals(candidate('1', 'Should be valid.'), 1.0)\n    lu.assertEquals(candidate(1.0, 'Error: Invalid value'), 1.0)\n    lu.assertEquals(candidate(1, 'Input value cannot be cast to float.'), 1.0)\n    lu.assertEquals(candidate(float('inf'), ''), float('inf'))\n    lu.assertEquals(candidate(10, 'Input value is not a float.'), 10.0)\n    lu.assertEquals(candidate('1', 'Error: Invalid value'), 1.0)\n    lu.assertEquals(candidate(1.0, 'Input value cannot be cast to float.'), 1.0)\n    lu.assertEquals(candidate(1, 'Error: Invalid value'), 1.0)\n    lu.assertEquals(candidate(-10, ''), -10)\n    lu.assertEquals(candidate(10.0, 'Input value is not a float.'), 10.0)\n    lu.assertEquals(candidate(1, ''), 1)\n    lu.assertEquals(candidate(-1, 'Input value cannot be cast to float.'), -1.0)\n    lu.assertEquals(candidate(-1.0, 'Input value cannot be cast to float.'), -1.0)\n    lu.assertEquals(candidate(3.14159, ''), 3.14159)\n    lu.assertEquals(candidate(1.0, 'Should be valid.'), 1.0)\n    lu.assertEquals(candidate('1.0', 'Should be valid.'), 1.0)\n    lu.assertEquals(candidate(0, 'Input value cannot be cast to float.'), 0.0)\n    lu.assertEquals(candidate(0.0, 'Input value cannot be cast to float.'), 0.0)\n    lu.assertEquals(candidate(1, 'Should be valid.'), 1.0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_67171_remove_unencodable", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # s = str_.replace(\"\\xb2\", \"\")\n-- # s = s.replace(\"\\u2013\", \"\")\n-- # s = s.replace(\"\\u2019\", \"\")\n-- # return s\n-- \n-- :type str_: str\n-- :param str_: string to remove unencodable character\n-- :return: string removed unencodable character\nlocal function remove_unencodable(str_)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_67171_remove_unencodable.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = remove_unencodable\n    lu.assertEquals(candidate('\u201cHe said, \u201cLet us go to the movies tonight\u201d\u201d'), '\u201cHe said, \u201cLet us go to the movies tonight\u201d\u201d')\n    lu.assertEquals(candidate('The British Beatles were an English rock band formed in London in 1960 by <NAME>, <NAME> and <NAME>'), 'The British Beatles were an English rock band formed in London in 1960 by <NAME>, <NAME> and <NAME>')\n    lu.assertEquals(candidate('The American actress and author, <NAME>, is perhaps best known for her role as <NAME> in the 1994 film, Selma.'), 'The American actress and author, <NAME>, is perhaps best known for her role as <NAME> in the 1994 film, Selma.')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_6730_char_to_bool", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if letter.upper() == 'J':\n-- #     return True\n-- # elif letter.upper() == 'N':\n-- #     return False\n-- # else:\n-- #     raise ValueError('Ongeldige letter, alleen J of N toegestaan.')\n-- \n-- Transform character (J/N) to Bool.\nlocal function char_to_bool(letter)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_6730_char_to_bool.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = char_to_bool\n    lu.assertEquals(candidate('j'), true)\n    lu.assertEquals(candidate('N'), false)\n    lu.assertEquals(candidate('J'), true)\n    lu.assertEquals(candidate('n'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_67323_is_belong_train_set", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # topic_num_in_test_set = ['3', '45']\n-- # if fname.split('_')[0] in topic_num_in_test_set:\n-- #     return False\n-- # else:\n-- #     return True\n-- \n--     Args:\n-- fname: string, file name without dir path\n--     Returns:\n-- boolean\nlocal function is_belong_train_set(fname)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_67323_is_belong_train_set.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_belong_train_set\n    lu.assertEquals(candidate('2_2_1.txt'), true)\n    lu.assertEquals(candidate('3_1_1.txt'), false)\n    lu.assertEquals(candidate('2_1_1.txt'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_67419_get_index_of_char", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return [x for x, v in enumerate(my_string) if v == char]\n-- \n-- Returns all indices of all appearances of char in str\n-- :param file_name:\n-- :param char:\n-- :return:\nlocal function get_index_of_char(my_string, char)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_67419_get_index_of_char.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_index_of_char\n    lu.assertEquals(candidate('123123123', '2'), {1, 4, 7})\n    lu.assertEquals(candidate('abcdefg', 'z'), {})\n    lu.assertEquals(candidate('123123123', '1'), {0, 3, 6})\n    lu.assertEquals(candidate('abc\\nabc', '\\n'), {3})\n    lu.assertEquals(candidate('', ''), {})\n    lu.assertEquals(candidate('abcdef', 'z'), {})\n    lu.assertEquals(candidate('123123123', 'x'), {})\n    lu.assertEquals(candidate('a1b2c3', 'x'), {})\n    lu.assertEquals(candidate('abcdefg', 'd'), {3})\n    lu.assertEquals(candidate('abc', 'd'), {})\n    lu.assertEquals(candidate('1abc2', 'd'), {})\n    lu.assertEquals(candidate('abcabc', 'z'), {})\n    lu.assertEquals(candidate('abc', 'c'), {2})\n    lu.assertEquals(candidate('abc', 'c'), {2})\n    lu.assertEquals(candidate('abcdefg', 'a'), {0})\n    lu.assertEquals(candidate('a1b2c3', '1'), {1})\n    lu.assertEquals(candidate('aaaaaaaaaa', 'a'), {0, 1, 2, 3, 4, 5, 6, 7, 8, 9})\n    lu.assertEquals(candidate('abc', 'b'), {1})\n    lu.assertEquals(candidate('1abc2', 'b'), {2})\n    lu.assertEquals(candidate('123123123', '3'), {2, 5, 8})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_67437_check_dead", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if left_hp <= 0:\n-- #     return True\n-- # elif right_hp <= 0:\n-- #     return True\n-- # else:\n-- #     return False\n-- \n-- Method to check if either player is dead\n-- :param left_hp: Hit points of priority player\n-- :param right_hp: Hit points of right player\n-- :return: True if somebody is dead, else False\nlocal function check_dead(left_hp, right_hp)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_67437_check_dead.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = check_dead\n    lu.assertEquals(candidate(-1, 1), true)\n    lu.assertEquals(candidate(0, 5), true)\n    lu.assertEquals(candidate(1, 1), false)\n    lu.assertEquals(candidate(0, 0), true)\n    lu.assertEquals(candidate(1, 0), true)\n    lu.assertEquals(candidate(5, 0), true)\n    lu.assertEquals(candidate(1, -1), true)\n    lu.assertEquals(candidate(0, 1), true)\n    lu.assertEquals(candidate(5, 5), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_67598_strip_str", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return string.strip('\"').strip(\"'\").strip()\n-- \n-- Strip string.\nlocal function strip_str(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_67598_strip_str.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = strip_str\n    lu.assertEquals(candidate(candidate(candidate('  a  '))), candidate('  a  '))\n    lu.assertEquals(candidate('  '), '')\n    lu.assertEquals(candidate(\"Hello'World\"), \"Hello'World\")\n    lu.assertEquals(candidate('Hello World!    '), 'Hello World!')\n    lu.assertEquals(candidate('    Hello World!'), 'Hello World!')\n    lu.assertEquals(candidate('Hello \" World'), 'Hello \" World')\n    lu.assertEquals(candidate('  hello  world  '), 'hello  world')\n    lu.assertEquals(candidate(\"'hello  world'\"), 'hello  world')\n    lu.assertEquals(candidate(' Hello World'), 'Hello World')\n    lu.assertEquals(candidate(candidate(candidate(\"' a '\"))), candidate(\"' a '\"))\n    lu.assertEquals(candidate('\"hello  world\"'), 'hello  world')\n    lu.assertEquals(candidate(\"'hello'\"), 'hello')\n    lu.assertEquals(candidate(\"  'hello'  'world'  \"), \"'hello'  'world'\")\n    lu.assertEquals(candidate('hi'), 'hi')\n    lu.assertEquals(candidate('Hello World'), 'Hello World')\n    lu.assertEquals(candidate(\"Hello 'World\"), \"Hello 'World\")\n    lu.assertEquals(candidate('Hello World!'), 'Hello World!')\n    lu.assertEquals(candidate('Hello \" World!'), 'Hello \" World!')\n    lu.assertEquals(candidate(\"Hello'World! \"), \"Hello'World!\")\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('  hi  '), 'hi')\n    lu.assertEquals(candidate('    Hello World!    '), 'Hello World!')\n    lu.assertEquals(candidate('hi'), 'hi')\n    lu.assertEquals(candidate('Hello World '), 'Hello World')\n    lu.assertEquals(candidate('Hello\" World'), 'Hello\" World')\n    lu.assertEquals(candidate('  \"hello\"  \"world\"  '), '\"hello\"  \"world\"')\n    lu.assertEquals(candidate(\"Hello'World\"), \"Hello'World\")\n    lu.assertEquals(candidate('\"hello\"'), 'hello')\n    lu.assertEquals(candidate(' Hello World '), 'Hello World')\n    lu.assertEquals(candidate(\"''\"), '')\n    lu.assertEquals(candidate('\"\"'), '')\n    lu.assertEquals(candidate(\"Hello' World\"), \"Hello' World\")\n    lu.assertEquals(candidate('  hello  '), 'hello')\n    lu.assertEquals(candidate('  \"hello\"  \\'world\\'  '), '\"hello\"  \\'world\\'')\n    lu.assertEquals(candidate('hello'), 'hello')\n    lu.assertEquals(candidate('Hello World!'), 'Hello World!')\n    lu.assertEquals(candidate('Hello\"World'), 'Hello\"World')\n    lu.assertEquals(candidate(candidate(candidate('\" a \"'))), candidate('\" a \"'))\n    lu.assertEquals(candidate('Hello \"World'), 'Hello \"World')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_68506_fib", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # a, b = 0, 1\n-- # for i in range(n - 1):\n-- #     a, b = b, a + b\n-- # return a\n-- \n--  Calculate the nth digit of Fibonacci\n-- 0 1 1 2 3 5 8 13 21 34 ... \nlocal function fib(n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_68506_fib.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = fib\n    lu.assertEquals(candidate(2), 1)\n    lu.assertEquals(candidate(0), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_68615_is_even", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return number % 2 == 0\n-- \n-- Check if a number is even.\nlocal function is_even(number)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_68615_is_even.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_even\n    lu.assertEquals(candidate(5), false)\n    lu.assertEquals(candidate(0), true)\n    lu.assertEquals(candidate(4), true)\n    lu.assertEquals(candidate(1), false)\n    lu.assertEquals(candidate(2), true)\n    lu.assertEquals(candidate(10), true)\n    lu.assertEquals(candidate(3), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_68774_extract", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return token[1:-1]\n-- \n-- Extract label from transition.\nlocal function extract(token)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_68774_extract.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = extract\n    lu.assertEquals(candidate(\"'bc'\"), 'bc')\n    lu.assertEquals(candidate(\"''\"), '')\n    lu.assertEquals(candidate('[a]'), 'a')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate(\"'a'\"), 'a')\n    lu.assertEquals(candidate('a'), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_68849_removeElement", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # nums[:] = [num for num in nums if num != val]\n-- # return len(nums)\n-- \n-- :type nums: List[int]\n-- :type val: int\n-- :rtype: int\nlocal function removeElement(nums, val)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_68849_removeElement.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = removeElement\n    lu.assertEquals(candidate({0, 1, 2, 2, 3, 0, 4, 2}, 2), 5)\n    lu.assertEquals(candidate({1, 2, 3, 3}, 3), 2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_69513_comma_counter", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # total = 0\n-- # for f in field:\n-- #     if f == ',':\n-- #         total += 1\n-- # return total\n-- \n-- :param field:\n-- :return:\nlocal function comma_counter(field)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_69513_comma_counter.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = comma_counter\n    lu.assertEquals(candidate('a,b,c,d,e'), 4)\n    lu.assertEquals(candidate('hello,,world'), 2)\n    lu.assertEquals(candidate(''), 0)\n    lu.assertEquals(candidate('hello'), 0)\n    lu.assertEquals(candidate('hello,world'), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_69832_strip_all", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return list(map(lambda x: x.strip(), lst))\n-- \n--  Strips leading and trailing whitespace from all strings in a list.\n-- Args:\n--     lst (list of str): The list of strings to strip.\n-- Returns:\n--     list of str: The list of stripped strings.\nlocal function strip_all(lst)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_69832_strip_all.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = strip_all\n    lu.assertEquals(candidate({' \\r\\n one \\r\\n', ' two \\r\\n', ' three \\r\\n'}), {'one', 'two', 'three'})\n    lu.assertEquals(candidate({'  ', '', '  hello  ', '  world', '  ', '', '  '}), {'', '', 'hello', 'world', '', '', ''})\n    lu.assertEquals(candidate({'    one   ', '   two  ', '  three'}), {'one', 'two', 'three'})\n    lu.assertEquals(candidate({'', '', ''}), {'', '', ''})\n    lu.assertEquals(candidate({'one', ' two', ' three'}), {'one', 'two', 'three'})\n    lu.assertEquals(candidate({}), {})\n    lu.assertEquals(candidate({}), {})\n    lu.assertEquals(candidate({'     Hello,     World!     ', 'Hello, World!'}), {'Hello,     World!', 'Hello, World!'})\n    lu.assertEquals(candidate({'hello', 'world'}), {'hello', 'world'})\n    lu.assertEquals(candidate({'', 'hello  ', '  world'}), {'', 'hello', 'world'})\n    lu.assertEquals(candidate({' \\n one\\n', ' two \\n', ' three \\n'}), {'one', 'two', 'three'})\n    lu.assertEquals(candidate({' one ', ' two ', ' three'}), {'one', 'two', 'three'})\n    lu.assertEquals(candidate({'  ', '  ', '  '}), {'', '', ''})\n    lu.assertEquals(candidate({'  hello  ', 'world', ''}), {'hello', 'world', ''})\n    lu.assertEquals(candidate({'one', '', 'two', 'three'}), {'one', '', 'two', 'three'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_70041_format_timedelta", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # hours = int(seconds / (60 * 60))\n-- # minutes = int(seconds % (60 * 60) / 60)\n-- # return f\"{hours}h {minutes}m\"\n-- \n-- Returns a formatted message that is displayed whenever a command wants to display a duration\nlocal function format_timedelta(seconds)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_70041_format_timedelta.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = format_timedelta\n    lu.assertEquals(candidate(3661), '1h 1m')\n    lu.assertEquals(candidate(0), '0h 0m')\n    lu.assertEquals(candidate(7200), '2h 0m')\n    lu.assertEquals(candidate(86399), '23h 59m')\n    lu.assertEquals(candidate(3600), '1h 0m')\n    lu.assertEquals(candidate(60), '0h 1m')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_71150_max_divisible", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # while a % b == 0:\n-- #     a = a / b\n-- # return a\n-- \n-- Keep dividing(a/b) till it's divisible(a % b == 0)\n-- e.g.\n-- Input: a = 300; b = 2\n-- Output: 75\n-- :param a:\n-- :param b:\n-- :return:\nlocal function max_divisible(a, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_71150_max_divisible.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = max_divisible\n    lu.assertEquals(candidate(100, 4), 25)\n    lu.assertEquals(candidate(24, 4), 6)\n    lu.assertEquals(candidate(300, 100), 3)\n    lu.assertEquals(candidate(15, 3), 5)\n    lu.assertEquals(candidate(100, 25), 4)\n    lu.assertEquals(candidate(300, 2), 75)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_72364_get_xi_from_ARPS_simulation", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # [topo_or_wind, N, dx, xi, sigma, ext] = simulation.split('_')\n-- # xi = xi.split('xi')[1]\n-- # return (xi)\n-- \n-- Extract xi from full name of ARPS files\nlocal function get_xi_from_ARPS_simulation(simulation)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_72364_get_xi_from_ARPS_simulation.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_xi_from_ARPS_simulation\n    lu.assertEquals(candidate('wind_N2000_dx0.05_xi00001_sigma0.001_small'), '00001')\n    lu.assertEquals(candidate('wind_N1000_dx0.05_xi00100_sigma0.001_small'), '00100')\n    lu.assertEquals(candidate('topo_N100_dx0.02_xi00100_sigma0.001_small'), '00100')\n    lu.assertEquals(candidate('topo_N100_dx0.02_xi00200_sigma0.001_small'), '00200')\n    lu.assertEquals(candidate('topo_N100_dx0.02_xi00300_sigma0.001_small'), '00300')\n    lu.assertEquals(candidate('wind_N2000_dx0.05_xi00300_sigma0.001_small'), '00300')\n    lu.assertEquals(candidate('wind_N2000_dx0.05_xi00200_sigma0.001_small'), '00200')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_72597_plurality", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # max_cnt = -1e90\n-- # max_vid = None\n-- # for vid in d:\n-- #     if d[vid] > max_cnt:\n-- #         max_cnt = d[vid]\n-- #         max_vid = vid\n-- # return max_vid\n-- \n-- Return, for input dict d mapping vids to (real) counts, vid with largest count.\n-- (Tie-breaking done arbitrarily here.)\nlocal function plurality(d)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_72597_plurality.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = plurality\n    lu.assertEquals(candidate({[1] = 10}), 1)\n    lu.assertEquals(candidate(dict({{0, 1}, {1, 2}, {2, 1}, {3, 1}, {4, 1}, {5, 1}})), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_72678_f", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return x**3 - 2*x + 2\n-- \n--         Defining Function\nlocal function f(x)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_72678_f.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = f\n    lu.assertEquals(candidate(0), 2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_73015_init_loop_state", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # file_row_counter = 0\n-- # file_counter += 1\n-- # file_data = []\n-- # file_data.append([\n-- #     'Date',\n-- #     'Weight (lb)',\n-- #     'Fat mass (lb)'\n-- # ])\n-- # return (file_row_counter, file_counter, file_data)\n-- \n-- Initialize the file row counter, the file counter,\n-- and the list representing file data.\n-- Janky, I know, but this needed to be done in 2 spots.\nlocal function init_loop_state(file_counter)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_73015_init_loop_state.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = init_loop_state\n    lu.assertEquals(candidate(0), {0, 1, {{'Date', 'Weight (lb)', 'Fat mass (lb)'}}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_73332__make_even", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return (n >> 1) << 1\n-- \n-- Return largest even integer less than or equal to `n`.\nlocal function _make_even(n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_73332__make_even.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _make_even\n    lu.assertEquals(candidate(29), 28)\n    lu.assertEquals(candidate(0), 0)\n    lu.assertEquals(candidate(10), 10)\n    lu.assertEquals(candidate(8), 8)\n    lu.assertEquals(candidate(11), 10)\n    lu.assertEquals(candidate(13), 12)\n    lu.assertEquals(candidate(5), 4)\n    lu.assertEquals(candidate(24), 24)\n    lu.assertEquals(candidate(21), 20)\n    lu.assertEquals(candidate(15), 14)\n    lu.assertEquals(candidate(22), 22)\n    lu.assertEquals(candidate(30), 30)\n    lu.assertEquals(candidate(32), 32)\n    lu.assertEquals(candidate(6), 6)\n    lu.assertEquals(candidate(23), 22)\n    lu.assertEquals(candidate(17), 16)\n    lu.assertEquals(candidate(20), 20)\n    lu.assertEquals(candidate(19), 18)\n    lu.assertEquals(candidate(16), 16)\n    lu.assertEquals(candidate(1), 0)\n    lu.assertEquals(candidate(2), 2)\n    lu.assertEquals(candidate(3), 2)\n    lu.assertEquals(candidate(25), 24)\n    lu.assertEquals(candidate(14), 14)\n    lu.assertEquals(candidate(7), 6)\n    lu.assertEquals(candidate(4), 4)\n    lu.assertEquals(candidate(18), 18)\n    lu.assertEquals(candidate(12), 12)\n    lu.assertEquals(candidate(28), 28)\n    lu.assertEquals(candidate(4), candidate(candidate(4)))\n    lu.assertEquals(candidate(31), 30)\n    lu.assertEquals(candidate(9), 8)\n    lu.assertEquals(candidate(27), 26)\n    lu.assertEquals(candidate(26), 26)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_73550__desktop_escape", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # escapes = {' ': R'\\s', '\\n': R'\\n', '\\t': R'\\t', '\\\\': R'\\\\'}\n-- # s = str(s)\n-- # for unescaped, escaped in escapes.items():\n-- #     s = s.replace(unescaped, escaped)\n-- # return s\n-- \n-- Escape a filepath for use in a .desktop file\nlocal function _desktop_escape(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_73550__desktop_escape.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _desktop_escape\n    lu.assertEquals(candidate('\\\\v'), '\\\\\\\\v')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('\\\\\\\\s'), '\\\\\\\\\\\\\\\\s')\n    lu.assertEquals(candidate('C:\\\\Users\\\\foo\\\\Desktop\\\\bar'), 'C:\\\\\\\\Users\\\\\\\\foo\\\\\\\\Desktop\\\\\\\\bar')\n    lu.assertEquals(candidate('abc'), 'abc')\n    lu.assertEquals(candidate('\\\\\\\\\\\\a'), '\\\\\\\\\\\\\\\\\\\\\\\\a')\n    lu.assertEquals(candidate('\\\\\\\\\\\\s'), '\\\\\\\\\\\\\\\\\\\\\\\\s')\n    lu.assertEquals(candidate('\\\\\\\\\\\\0'), '\\\\\\\\\\\\\\\\\\\\\\\\0')\n    lu.assertEquals(candidate('\\\\\\\\a'), '\\\\\\\\\\\\\\\\a')\n    lu.assertEquals(candidate('C:\\\\Users\\\\foo\\\\Desktop\\\\tab\\\\t'), 'C:\\\\\\\\Users\\\\\\\\foo\\\\\\\\Desktop\\\\\\\\tab\\\\\\\\t')\n    lu.assertEquals(candidate('abcde'), 'abcde')\n    lu.assertEquals(candidate('\\\\s'), '\\\\\\\\s')\n    lu.assertEquals(candidate('C:\\\\Users\\\\foo\\\\Desktop\\\\newline\\\\n'), 'C:\\\\\\\\Users\\\\\\\\foo\\\\\\\\Desktop\\\\\\\\newline\\\\\\\\n')\n    lu.assertEquals(candidate('C:\\\\Users\\\\foo\\\\Desktop\\\\backslash\\\\backslash'), 'C:\\\\\\\\Users\\\\\\\\foo\\\\\\\\Desktop\\\\\\\\backslash\\\\\\\\backslash')\n    lu.assertEquals(candidate('\\\\\\\\f'), '\\\\\\\\\\\\\\\\f')\n    lu.assertEquals(candidate('\\\\\\\\\\\\f'), '\\\\\\\\\\\\\\\\\\\\\\\\f')\n    lu.assertEquals(candidate('\\\\\\\\\\\\v'), '\\\\\\\\\\\\\\\\\\\\\\\\v')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('\\\\\\\\v'), '\\\\\\\\\\\\\\\\v')\n    lu.assertEquals(candidate('a'), 'a')\n    lu.assertEquals(candidate('\\\\0'), '\\\\\\\\0')\n    lu.assertEquals(candidate('\\\\\\\\0'), '\\\\\\\\\\\\\\\\0')\n    lu.assertEquals(candidate('ab'), 'ab')\n    lu.assertEquals(candidate('\\\\f'), '\\\\\\\\f')\n    lu.assertEquals(candidate('\\\\a'), '\\\\\\\\a')\n    lu.assertEquals(candidate('abcd'), 'abcd')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_73986_manhattan_distance", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return abs(point1_x - point2_x) + abs(point1_y-point2_y)\n-- \n--  It is the sum of absolute values of differences in the point 1's x and y coordinates and the\n-- point 2's x and y coordinates respectively \nlocal function manhattan_distance(point1_x, point1_y, point2_x, point2_y)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_73986_manhattan_distance.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = manhattan_distance\n    lu.assertEquals(candidate(1, 1, 0, 0), 2)\n    lu.assertEquals(candidate(0, 0, -3, -4), 7)\n    lu.assertEquals(candidate(-2, -2, -4, -4), 4)\n    lu.assertEquals(candidate(10, 10, 0, 0), 20)\n    lu.assertEquals(candidate(0, 0, -10, 10), 20)\n    lu.assertEquals(candidate(1, 1, 3, 4), 5)\n    lu.assertEquals(candidate(1, 1, 1, 1), 0)\n    lu.assertEquals(candidate(0, 0, 10, -10), 20)\n    lu.assertEquals(candidate(0, 0, 10, 10), 20)\n    lu.assertEquals(candidate(3, -4, 0, 0), 7)\n    lu.assertEquals(candidate(0, 0, 0, 0), 0)\n    lu.assertEquals(candidate(0, 0, 3, 4), 7)\n    lu.assertEquals(candidate(1, 2, 3, 4), 4)\n    lu.assertEquals(candidate(-3, 4, 0, 0), 7)\n    lu.assertEquals(candidate(-1, -1, -1, -1), 0)\n    lu.assertEquals(candidate(0, 0, -10, -10), 20)\n    lu.assertEquals(candidate(-10, 10, 0, 0), 20)\n    lu.assertEquals(candidate(2, 2, 2, 2), 0)\n    lu.assertEquals(candidate(-10, -10, 0, 0), 20)\n    lu.assertEquals(candidate(1, 1, 2, 2), 2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_74366_to_unicode_repr", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Python 2-3 compatible\n-- # return u\"u'\" + u\"\".join([u\"\\\\u%04x\" % ord(l) for l in _letter]) + u\"'\"\n-- \n--  helpful in situations where browser/app may recognize Unicode encoding\n-- in the \u0b8e type syntax but not actual unicode glyph/code-point\nlocal function to_unicode_repr(_letter)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_74366_to_unicode_repr.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = to_unicode_repr\n    lu.assertEquals(candidate('\u0b8e\u0b8e'), \"u'\\\\u0b8e\\\\u0b8e'\")\n    lu.assertEquals(candidate('\u0905\u0906\u0907\u0908\u0909\u090a'), \"u'\\\\u0905\\\\u0906\\\\u0907\\\\u0908\\\\u0909\\\\u090a'\")\n    lu.assertEquals(candidate(''), \"u''\")\n    lu.assertEquals(candidate('\u0b8e\u0b8e\u0b8e'), \"u'\\\\u0b8e\\\\u0b8e\\\\u0b8e'\")\n    lu.assertEquals(candidate('\u0b8e'), \"u'\\\\u0b8e'\")\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_76381_str_list", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # temp = input_Str.split(' ')  # define temp list to save the result\n-- # for i in range(len(temp)):\n-- #     temp[i] = float(temp[i])\n-- # return temp\n-- \n-- convert string to list\nlocal function str_list(input_Str)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_76381_str_list.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = str_list\n    lu.assertEquals(candidate('0 0 1 1 2 3 5 8'), {0.0, 0.0, 1.0, 1.0, 2.0, 3.0, 5.0, 8.0})\n    lu.assertEquals(candidate('1.0 2.0 3.0'), {1.0, 2.0, 3.0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_77434_get_str_ip", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \".\".join([str(_) for _ in list_ip])\n-- \n--     turns a list of 4 integers into IP address format.\nlocal function get_str_ip(list_ip)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_77434_get_str_ip.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_str_ip\n    lu.assertEquals(candidate({192, 168, 1, 0}), '192.168.1.0')\n    lu.assertEquals(candidate({10, 0, 1, 0}), '10.0.1.0')\n    lu.assertEquals(candidate({10, 0, 0, 1}), '10.0.0.1')\n    lu.assertEquals(candidate({192, 168, 0, 1}), '192.168.0.1')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_77523_get_prefix", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return url[0:url.rfind(\"/\") + 1]\n-- \n-- @Description:\n-- get the prefix of a url\n-- to form the sub-level url\n-- ---------\n-- @Param:\n-- url:str\n-- -------\n-- @Returns:\n-- a substr end where '/' last time appears\n-- -------\nlocal function get_prefix(url)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_77523_get_prefix.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_prefix\n    lu.assertEquals(candidate('http://github.com/'), 'http://github.com/')\n    lu.assertEquals(candidate('http://github.com/this_is_a_test.py/this_is_another_test.py/'), 'http://github.com/this_is_a_test.py/this_is_another_test.py/')\n    lu.assertEquals(candidate('http://www.example.com/some/path/'), 'http://www.example.com/some/path/')\n    lu.assertEquals(candidate('https://www.coursera.org/api/onDemandProgrammingScripting'), 'https://www.coursera.org/api/')\n    lu.assertEquals(candidate('http://data.pr4e.org/data/animals.txt'), 'http://data.pr4e.org/data/')\n    lu.assertEquals(candidate('http://github.com/this_is_a_test.py'), 'http://github.com/')\n    lu.assertEquals(candidate('http://172.16.31.10/data.php'), 'http://172.16.31.10/')\n    lu.assertEquals(candidate('http://github.com/this_is_a_test.py/this_is_another_test.py/?some_variable=some_value'), 'http://github.com/this_is_a_test.py/this_is_another_test.py/')\n    lu.assertEquals(candidate('https://www.coursera.org/api/onDemandProgrammingScripting?a=1&b=2'), 'https://www.coursera.org/api/')\n    lu.assertEquals(candidate('http://www.example.com/'), 'http://www.example.com/')\n    lu.assertEquals(candidate('https://www.coursera.org/api/onDemandProgrammingScripting?a=1'), 'https://www.coursera.org/api/')\n    lu.assertEquals(candidate('http://www.example.com/some/path'), 'http://www.example.com/some/')\n    lu.assertEquals(candidate('http://github.com/this_is_a_test.py/this_is_another_test.py/?some_variable=some_value&other_variable=other_value'), 'http://github.com/this_is_a_test.py/this_is_another_test.py/')\n    lu.assertEquals(candidate('http://data.pr4e.org/data/intro-short.txt'), 'http://data.pr4e.org/data/')\n    lu.assertEquals(candidate('http://data.pr4e.org/intro-short.txt'), 'http://data.pr4e.org/')\n    lu.assertEquals(candidate('github.com'), '')\n    lu.assertEquals(candidate('http://172.16.31.10/data.php'), 'http://172.16.31.10/')\n    lu.assertEquals(candidate('http://stackoverflow.com/questions/tagged/python'), 'http://stackoverflow.com/questions/tagged/')\n    lu.assertEquals(candidate('http://github.com/this_is_a_test.py/this_is_another_test.py'), 'http://github.com/this_is_a_test.py/')\n    lu.assertEquals(candidate('http://coursera.org/api/onDemandProgrammingScripting'), 'http://coursera.org/api/')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_77581_has_no_trailing_zeroes", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return len(str(int(string))) == len(string)\n-- \n-- True if string has no trailing zeroes, False otherwise.\n-- PARAMETERS:\n--     string : str\n-- RETURNS: bool\nlocal function has_no_trailing_zeroes(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_77581_has_no_trailing_zeroes.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = has_no_trailing_zeroes\n    lu.assertEquals(candidate('040'), false)\n    lu.assertEquals(candidate('4'), true)\n    lu.assertEquals(candidate('40'), true)\n    lu.assertEquals(candidate('0'), true)\n    lu.assertEquals(candidate('0400'), false)\n    lu.assertEquals(candidate('400'), true)\n    lu.assertEquals(candidate('000'), false)\n    lu.assertEquals(candidate('04'), false)\n    lu.assertEquals(candidate('00'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_77664_sumDigits", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # sum = 0\n-- # letters = 0\n-- # for char in s:\n-- #     try:\n-- #         d = int(char)  # throws ValueError: invalid literal for int() with base 10: 'a'\n-- #         sum += d\n-- #     except:\n-- #         letters = letters + 1\n-- # ===========================================================================\n-- #   # Debug\n-- #   print(sum)\n-- #   print(letters)\n-- # # ===========================================================================\n-- # return sum\n-- \n--  Assumes s is a string\n-- Returns the sum of the decimal digits in s\n-- For example, if s is 'a2b3c' it returns 5\nlocal function sumDigits(s)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_77664_sumDigits.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sumDigits\n    lu.assertEquals(candidate('a2b3c'), 5)\n    lu.assertEquals(candidate('a2b3c0'), 5)\n    lu.assertEquals(candidate('1 2 3 4'), 10)\n    lu.assertEquals(candidate('a2b3c'), 5)\n    lu.assertEquals(candidate('a2b3c1'), 6)\n    lu.assertEquals(candidate('12 34'), 10)\n    lu.assertEquals(candidate('a2b3c'), 5)\n    lu.assertEquals(candidate(''), 0)\n    lu.assertEquals(candidate('987654321'), 45)\n    lu.assertEquals(candidate('1'), 1)\n    lu.assertEquals(candidate('a'), 0)\n    lu.assertEquals(candidate('1234'), 10)\n    lu.assertEquals(candidate('xyz'), 0)\n    lu.assertEquals(candidate('0123456789'), 45)\n    lu.assertEquals(candidate(''), 0)\n    lu.assertEquals(candidate('123'), 6)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_7831_mixed_radix_to_base_10", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # res = x[0]\n-- # for i in range(1, len(x)):\n-- #     res *= b[i]\n-- #     res += x[i]\n-- # return res\n-- \n-- Convert the `mixed radix`_ integer with digits `x` and bases `b` to base 10.\n-- Args:\n--     x (list): a list of digits ordered by increasing place values\n--     b (list): a list of bases corresponding to the digits\n-- Examples:\n--     Generally, the base 10 representation of the mixed radix number :math:`x_n\\ldots x_1` where :math:`x_i` is a digit in place value :math:`i` with base :math:`b_i` is\n--     .. math::\n--         \\sum_{i=1}^nx_i\\prod_{j=i+1}^nb_j = x_n + b_nx_{n-1} + b_nb_{n-1}x_{n-2} + \\cdots + b_n\\cdots b_2x_1\n--     Convert 111 with bases :math:`(b_1,b_2,b_3)=(2,3,4)` to base 10:\n--     >>> from fem.discrete.combinatorics import mixed_radix_to_base_10\n--     >>> mixed_radix_to_base_10([1,1,1], [2,3,4])\n--     17\n-- .. _mixed radix:\n--     https://en.wikipedia.org/wiki/Mixed_radix\nlocal function mixed_radix_to_base_10(x, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_7831_mixed_radix_to_base_10.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = mixed_radix_to_base_10\n    lu.assertEquals(candidate({1, 1, 1}, {2, 3, 4}), 17)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_79249_removeDuplicateChars", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # duplicateless_string = \"\"\n-- # char_set = set()\n-- # for char in a_string:\n-- #     if char not in char_set:\n-- #         char_set.add(char)\n-- #         duplicateless_string += char\n-- # return duplicateless_string\n-- \n-- assumes a-string is a string\n-- returns a string, a_string with any duplicate characters removed\nlocal function removeDuplicateChars(a_string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_79249_removeDuplicateChars.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = removeDuplicateChars\n    lu.assertEquals(candidate('aaabbbccc'), 'abc')\n    lu.assertEquals(candidate('aabbcde'), 'abcde')\n    lu.assertEquals(candidate('abcde'), 'abcde')\n    lu.assertEquals(candidate('aaaaaa'), 'a')\n    lu.assertEquals(candidate('aabbcdef'), 'abcdef')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_79326_fix_forts", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # city_components = report_city.split(\" \")\n-- # if city_components[0].strip().lower() == \"ft.\":\n-- #     city_components[0] = \"Fort\"\n-- # return \" \".join(city_components)\n-- \n--  Changes Ft. -> Fort.\nlocal function fix_forts(report_city)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_79326_fix_forts.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = fix_forts\n    lu.assertEquals(candidate('Ft. Myers, Missouri  '), 'Fort Myers, Missouri  ')\n    lu.assertEquals(candidate('Ft. Myers'), 'Fort Myers')\n    lu.assertEquals(candidate('Ft. Myers  '), 'Fort Myers  ')\n    lu.assertEquals(candidate('Ft.  Myers '), 'Fort  Myers ')\n    lu.assertEquals(candidate('Ft. Myers, Missouri'), 'Fort Myers, Missouri')\n    lu.assertEquals(candidate('Ft.  Myers, Missouri '), 'Fort  Myers, Missouri ')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_79846_failed", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \"Failed: \" + message\n-- \n-- Simply attaches a failed marker to a message\n-- :param message: The message\n-- :return: String\nlocal function failed(message)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_79846_failed.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = failed\n    lu.assertEquals(candidate('test'), 'Failed: test')\n    lu.assertEquals(candidate('2'), 'Failed: 2')\n    lu.assertEquals(candidate('Message'), 'Failed: Message')\n    lu.assertEquals(candidate('This test passed!'), 'Failed: This test passed!')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_80983_get_color", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # n %= 8\n-- # colors = [\n-- #     'b',  # blue\n-- #     'g',  # green\n-- #     'r',  # red\n-- #     'c',  # cyan\n-- #     'm',  # magenta\n-- #     'y',  # yellow\n-- #     'k',  # black\n-- #     'w',  # white\n-- # ]\n-- # return colors[n]\n-- \n--  Valid matplotlib colors. Could be used to automatically pick colors.\n-- :param n:   an integer\n-- :return:    a valid matploglib color string\nlocal function get_color(n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_80983_get_color.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_color\n    lu.assertEquals(candidate(23), 'w')\n    lu.assertEquals(candidate(5), candidate(13))\n    lu.assertEquals(candidate(17), 'g')\n    lu.assertEquals(candidate(3), 'c')\n    lu.assertEquals(candidate(31), 'w')\n    lu.assertEquals(candidate(1), candidate(17))\n    lu.assertEquals(candidate(3), candidate(11))\n    lu.assertEquals(candidate(2), candidate(10))\n    lu.assertEquals(candidate(-1), 'w')\n    lu.assertEquals(candidate(7), 'w')\n    lu.assertEquals(candidate(4), candidate(20))\n    lu.assertEquals(candidate(13), 'y')\n    lu.assertEquals(candidate(9), 'g')\n    lu.assertEquals(candidate(20), 'm')\n    lu.assertEquals(candidate(25), 'g')\n    lu.assertEquals(candidate(4), 'm')\n    lu.assertEquals(candidate(4), candidate(12))\n    lu.assertEquals(candidate(7), candidate(15))\n    lu.assertEquals(candidate(0), candidate(16))\n    lu.assertEquals(candidate(16), 'b')\n    lu.assertEquals(candidate(24), 'b')\n    lu.assertEquals(candidate(6), 'k')\n    lu.assertEquals(candidate(6), candidate(14))\n    lu.assertEquals(candidate(1), candidate(9))\n    lu.assertEquals(candidate(5), candidate(21))\n    lu.assertEquals(candidate(5), 'y')\n    lu.assertEquals(candidate(12), 'm')\n    lu.assertEquals(candidate(21), 'y')\n    lu.assertEquals(candidate(14), 'k')\n    lu.assertEquals(candidate(30), 'k')\n    lu.assertEquals(candidate(2), candidate(18))\n    lu.assertEquals(candidate(22), 'k')\n    lu.assertEquals(candidate(7), candidate(23))\n    lu.assertEquals(candidate(10), 'r')\n    lu.assertEquals(candidate(1), 'g')\n    lu.assertEquals(candidate(8), 'b')\n    lu.assertEquals(candidate(3), candidate(19))\n    lu.assertEquals(candidate(27), 'c')\n    lu.assertEquals(candidate(0), 'b')\n    lu.assertEquals(candidate(29), 'y')\n    lu.assertEquals(candidate(15), 'w')\n    lu.assertEquals(candidate(19), 'c')\n    lu.assertEquals(candidate(26), 'r')\n    lu.assertEquals(candidate(2), 'r')\n    lu.assertEquals(candidate(28), 'm')\n    lu.assertEquals(candidate(0), candidate(8))\n    lu.assertEquals(candidate(18), 'r')\n    lu.assertEquals(candidate(11), 'c')\n    lu.assertEquals(candidate(6), candidate(22))\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_82442_get_params", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # params = {}\n-- # for param in leftovers:\n-- #     tokens = param.split(\"=\")\n-- #     if len(tokens) != 2:\n-- #         continue\n-- #     key = tokens[0].replace(\"--\", \"\")\n-- #     value = tokens[1]\n-- #     params[key] = value\n-- # return params\n-- \n-- Turn arguments leftovers into service params.\nlocal function get_params(leftovers)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_82442_get_params.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_params\n    lu.assertEquals(candidate({}), {})\n    lu.assertEquals(candidate({'--param1', '--param2', 'b'}), {})\n    lu.assertEquals(candidate({'--param1', 'a', '--param2', 'b', 'c'}), {})\n    lu.assertEquals(candidate({'--foo=1', '--bar=2', '--baz', 'baz=3'}), {['foo'] = '1', ['bar'] = '2', ['baz'] = true, ['baz'] = '3'})\n    lu.assertEquals(candidate({'--foo=1', '--bar=2', 'baz=3'}), {['foo'] = '1', ['bar'] = '2', ['baz'] = '3'})\n    lu.assertEquals(candidate({'--foo=1', '--bar=2'}), {['foo'] = '1', ['bar'] = '2'})\n    lu.assertEquals(candidate({'--foo=1', '--bar=2', '--baz', '--baz=3'}), {['foo'] = '1', ['bar'] = '2', ['baz'] = true, ['baz'] = '3'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_82529_filter_file_paths_by_extension", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # valid_file_paths = []\n-- # for file_path in file_paths:\n-- #     ext = \".%s\" % ext if ext[0] != '.' else ext\n-- #     if not file_path.endswith(ext):\n-- #         continue\n-- #     valid_file_paths.append(file_path)\n-- # return valid_file_paths\n-- \n-- Filters out file paths that do not have an appropriate extension.\n-- :param file_paths: list of file path strings\n-- :param ext: valid extension\nlocal function filter_file_paths_by_extension(file_paths, ext)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_82529_filter_file_paths_by_extension.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = filter_file_paths_by_extension\n    lu.assertEquals(candidate({'a.csv', 'b.csv', 'c.csv', 'd.txt', 'e.txt', 'f.json', 'g.csv'}, '  '), {})\n    lu.assertEquals(candidate({}), {})\n    lu.assertEquals(candidate({'my_file_0.csv'}), {'my_file_0.csv'})\n    lu.assertEquals(candidate({'a.csv', 'b.csv', 'c.csv', 'd.txt', 'e.txt', 'f.json', 'g.csv'}, 'csvx'), {})\n    lu.assertEquals(candidate({'my_file_0.csv', 'my_file_1.csv', 'my_file_2.csv', 'my_file_3.txt', 'my_file_4.png'}), {'my_file_0.csv', 'my_file_1.csv', 'my_file_2.csv'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_82780_is_bidirectional_conversion", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if letter_id not in letter_case:\n-- #     return False\n-- # # Check one-to-one mapping\n-- # mapped_value = letter_case[letter_id]\n-- # if len(mapped_value) > 1:\n-- #     return False\n-- # # Check two way conversions\n-- # mapped_value_id = ord(mapped_value)\n-- # if mapped_value_id not in reverse_letter_case or len(reverse_letter_case[mapped_value_id]) > 1:\n-- #     return False\n-- # if ord(reverse_letter_case[mapped_value_id]) != letter_id:\n-- #     return False\n-- # return True\n-- \n-- Check that two unicode value are also a mapping value of each other.\n-- :param letter_id: An integer, representing the unicode code point of the character.\n-- :param other_case_mapping: Comparable case mapping table which possible contains\n--                            the return direction of the conversion.\n-- :return: True, if it's a reverible conversion, false otherwise.\nlocal function is_bidirectional_conversion(letter_id, letter_case, reverse_letter_case)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_82780_is_bidirectional_conversion.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_bidirectional_conversion\n    lu.assertEquals(candidate(65, {[65] = 'A'}, {[66] = 'B'}), false)\n    lu.assertEquals(candidate(65, {[65] = 'a', [66] = 'B'}, {[66] = 'b'}), false)\n    lu.assertEquals(candidate(65, {[65] = 'a'}, {[66] = 'B'}), false)\n    lu.assertEquals(candidate(65, {[65] = 'A'}, {[97] = 'B'}), false)\n    lu.assertEquals(candidate(223, {[223] = 'ss'}, {[83] = 'S'}), false)\n    lu.assertEquals(candidate(65, {[65] = 'A'}, {[66] = 'a'}), false)\n    lu.assertEquals(candidate(65, {[65] = 'A'}, {[65] = 'a'}), false)\n    lu.assertEquals(candidate(65, {[65] = 'a'}, {[65] = 'a'}), false)\n    lu.assertEquals(candidate(65, {[65] = 'A'}, {[97] = 'A'}), false)\n    lu.assertEquals(candidate(65, {[65] = 'A'}, {[98] = 'a'}), false)\n    lu.assertEquals(candidate(65, {[65] = 'A'}, {[98] = 'B'}), false)\n    lu.assertEquals(candidate(97, {[65] = 'A'}, {[65] = 'A'}), false)\n    lu.assertEquals(candidate(65, {[65] = 'A', [66] = 'B'}, {[66] = 'b'}), false)\n    lu.assertEquals(candidate(98, {[66] = 'B'}, {[66] = 'B'}), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_8290_dot", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # vx, vy = v[0], v[1]\n-- # ux, uy = u[0], u[1]\n-- # dotproduct = vx*ux + vy*uy\n-- # return dotproduct\n-- \n-- v and u are vectors. v and u -> list\nlocal function dot(v, u)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_8290_dot.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = dot\n    lu.assertEquals(candidate({1, 2}, {3, 4}), 11)\n    lu.assertEquals(candidate(), 11)\n    lu.assertEquals(candidate(), 5)\n    lu.assertEquals(candidate({-1, 2}, {3, 4}), 5)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_8323__get_text_alignment", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # x1, x2, y1, y2 = point1[0], point2[0], point1[1], point2[1]\n-- # if (x1 < x2):\n-- #     ha = 'left'\n-- # else:\n-- #     ha = 'right'\n-- # if (y1 < y2):\n-- #     va = 'bottom'\n-- # else:\n-- #     va = 'top'\n-- # return (ha, va)\n-- \n-- Get the horizontal and vertical text alignment keywords for text placed at the end of a line segment from point1 to point2\n-- args:\n--     point1 - x,y pair\n--     point2 - x,y pair\n-- returns:\n--     ha - horizontal alignment string\n--     va - vertical alignment string\nlocal function _get_text_alignment(point1, point2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_8323__get_text_alignment.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _get_text_alignment\n    lu.assertEquals(candidate({0, 0}, {1, 1}), {'left', 'bottom'})\n    lu.assertEquals(candidate({-1, -1}, {1, 1}), {'left', 'bottom'})\n    lu.assertEquals(candidate({0, 0}, {-1, -1}), {'right', 'top'})\n    lu.assertEquals(candidate({1, 1}, {-1, -1}), {'right', 'top'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_83484_getRefId", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # for i, ref in enumerate(refs):\n-- #     if ref == refname:\n-- #         return i\n-- # return -1\n-- \n--     Get the reference ID for a reference name\nlocal function getRefId(refs, refname)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_83484_getRefId.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = getRefId\n    lu.assertEquals(candidate({'A', 'B', 'C'}, 'A'), 0)\n    lu.assertEquals(candidate({'A', 'B', 'C'}, 'I'), -1)\n    lu.assertEquals(candidate(list('GATTACA'), 'GG'), -1)\n    lu.assertEquals(candidate(list('GATTACA'), 'A'), 1)\n    lu.assertEquals(candidate({'A', 'B', 'C'}, 'E'), -1)\n    lu.assertEquals(candidate(list('GATTACA'), 'GATAA'), -1)\n    lu.assertEquals(candidate({'A', 'B', 'C'}, 'D'), -1)\n    lu.assertEquals(candidate({'A', 'B', 'C'}, 'F'), -1)\n    lu.assertEquals(candidate({'A', 'B', 'C'}, 'L'), -1)\n    lu.assertEquals(candidate({'A', 'B', 'C'}, 'K'), -1)\n    lu.assertEquals(candidate({'A', 'B', 'C'}, 'J'), -1)\n    lu.assertEquals(candidate({'A', 'B', 'C'}, 'G'), -1)\n    lu.assertEquals(candidate({'A', 'B', 'C'}, 'A'), 0)\n    lu.assertEquals(candidate({'A', 'B', 'C'}, 'D'), -1)\n    lu.assertEquals(candidate({'A', 'B', 'C'}, 'H'), -1)\n    lu.assertEquals(candidate({'A', 'B', 'C'}, 'C'), 2)\n    lu.assertEquals(candidate({'A', 'B', 'C'}, 'C'), 2)\n    lu.assertEquals(candidate({'A', 'B', 'C'}, 'B'), 1)\n    lu.assertEquals(candidate(list('GATTACA'), 'G'), 0)\n    lu.assertEquals(candidate({'A', 'B', 'C'}, 'B'), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_83818_solution2", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # left = [0] * len(n)\n-- # right = [0] * len(n)\n-- # pivot = n[i]\n-- # i, li, ri = 0, 0, 0\n-- # while i < len(n):\n-- #     if n[i] < pivot:\n-- #         left[li] = n[i]\n-- #         li += 1\n-- #     if n[i] > pivot:\n-- #         right[ri] = n[i]\n-- #         ri += 1\n-- #     i += 1\n-- # i, j, k = 0, 0, 0\n-- # while i < len(n):\n-- #     while j < li:\n-- #         n[i] = left[j]\n-- #         j += 1\n-- #         i += 1\n-- #     n[i] = pivot\n-- #     i += 1\n-- #     while k < ri:\n-- #         n[i] = right[k]\n-- #         k += 1\n-- #         i += 1\n-- # return n\n-- \n-- given an index i, sort  the array such that elements less then i appear before i\n-- then element at i, then all items that are larger\nlocal function solution2(n, i)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_83818_solution2.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = solution2\n    lu.assertEquals(candidate({1, 4, 3, 2, 6}, 0), {1, 4, 3, 2, 6})\n    lu.assertEquals(candidate({1, 4, 3, 2, 6}, 1), {1, 3, 2, 4, 6})\n    lu.assertEquals(candidate({1, 2, 5, 3, 4}, 2), {1, 2, 3, 4, 5})\n    lu.assertEquals(candidate(list(range(10)), 5), list(range(10)))\n    lu.assertEquals(candidate({1, 4, 3, 2, 6}, 2), {1, 2, 3, 4, 6})\n    lu.assertEquals(candidate({1, 2, 3, 4, 5}, 4), {1, 2, 3, 4, 5})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_84115_solution1", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # parse_inp = [int(n) for n in inp.split('\\n') if len(n) > 0]\n-- # for x in parse_inp:\n-- #     for y in parse_inp:\n-- #         if x + y == 2020:\n-- #             return x * y\n-- \n-- Solves the first part of the challenge\nlocal function solution1(inp)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_84115_solution1.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = solution1\n    lu.assertEquals(candidate('1721\\n979\\n366\\n299\\n675\\n1456'), 514579)\n    lu.assertEquals(candidate('\\n1721\\n979\\n366\\n299\\n675\\n1456\\n'), 514579)\n    lu.assertEquals(candidate('1721\\n    979\\n    366\\n    299\\n    675\\n    1456'), 514579)\n    lu.assertEquals(candidate('\\n1721\\n979\\n366\\n299\\n675\\n1456\\n'), 514579)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_84207_isize", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if n >= 1024 ** 3:\n-- #     return \"%.2f GB\" % (n / 1024.0 ** 3)\n-- # elif n >= 1024 ** 2:\n-- #     return \"%.2f MB\" % (n / 1024.0 ** 2)\n-- # elif n >= 1024:\n-- #     return \"%.2f KB\" % (n / 1024.0)\n-- # else:\n-- #     return \"%d B\" % n\n-- \n-- Get a readable size from a number of bytes.\nlocal function isize(n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_84207_isize.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = isize\n    lu.assertEquals(candidate(1023), '1023 B')\n    lu.assertEquals(candidate(1024), '1.00 KB')\n    lu.assertEquals(candidate(1025), '1.00 KB')\n    lu.assertEquals(candidate(1), '1 B')\n    lu.assertEquals(candidate(0), '0 B')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_84583_min_abs_mod", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if a >= 0:\n-- #     return a - b * ((a + a + b) // (b + b))\n-- # a = -a\n-- # return -(a - b * ((a + a + b) // (b + b)))\n-- \n--     This function returns absolute minimum modulo of a over b.\nlocal function min_abs_mod(a, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_84583_min_abs_mod.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = min_abs_mod\n    lu.assertEquals(candidate(10, 10), 0)\n    lu.assertEquals(candidate(-1, 1), 0)\n    lu.assertEquals(candidate(3, 10), 3)\n    lu.assertEquals(candidate(3, -5), -2)\n    lu.assertEquals(candidate(1, 1), 0)\n    lu.assertEquals(candidate(-15, 5), 0)\n    lu.assertEquals(candidate(-3, 5), 2)\n    lu.assertEquals(candidate(10, 5), 0)\n    lu.assertEquals(candidate(8, 6), 2)\n    lu.assertEquals(candidate(3, 3), 0)\n    lu.assertEquals(candidate(-2, 2), 0)\n    lu.assertEquals(candidate(10, 3), 1)\n    lu.assertEquals(candidate(2, -2), 0)\n    lu.assertEquals(candidate(2, 2), 0)\n    lu.assertEquals(candidate(2, -1), 0)\n    lu.assertEquals(candidate(7, 6), 1)\n    lu.assertEquals(candidate(8, 8), 0)\n    lu.assertEquals(candidate(5, 5), 0)\n    lu.assertEquals(candidate(-2, -1), 0)\n    lu.assertEquals(candidate(4, 4), 0)\n    lu.assertEquals(candidate(-10, 5), 0)\n    lu.assertEquals(candidate(1, 5), 1)\n    lu.assertEquals(candidate(2, 1), 0)\n    lu.assertEquals(candidate(-1, -2), -1)\n    lu.assertEquals(candidate(-2, 1), 0)\n    lu.assertEquals(candidate(6, 6), 0)\n    lu.assertEquals(candidate(0, 1), 0)\n    lu.assertEquals(candidate(1, -1), 0)\n    lu.assertEquals(candidate(7, 5), 2)\n    lu.assertEquals(candidate(-2, -2), 0)\n    lu.assertEquals(candidate(0, 5), 0)\n    lu.assertEquals(candidate(10, 7), 3)\n    lu.assertEquals(candidate(4, 3), 1)\n    lu.assertEquals(candidate(8, 4), 0)\n    lu.assertEquals(candidate(-1, -1), 0)\n    lu.assertEquals(candidate(-1, 2), 1)\n    lu.assertEquals(candidate(1, 3), 1)\n    lu.assertEquals(candidate(2, 5), 2)\n    lu.assertEquals(candidate(4, 2), 0)\n    lu.assertEquals(candidate(-5, 5), 0)\n    lu.assertEquals(candidate(7, 7), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_84972_parse_hex", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # colon_list = hex_string.split(':')\n-- # space_list = hex_string.split()  # multiple spaces act as one delimiter\n-- # if len(colon_list) >= len(space_list):\n-- #     return [x.strip() for x in colon_list]\n-- # return space_list\n-- \n--  Helper function for RA and Dec parsing, takes hex string, returns list of floats.\n-- Not normally called directly by user. TESTS OK 2020-10-24.\n--     :param hex_string: string in either full hex (\"12:34:56.7777\" or \"12 34 56.7777\"),\n--        or degrees (\"234.55\")\n--     :return: list of strings representing floats (hours:min:sec or deg:arcmin:arcsec).\nlocal function parse_hex(hex_string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_84972_parse_hex.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_hex\n    lu.assertEquals(candidate('234.55'), {'234.55'})\n    lu.assertEquals(candidate('123456.777'), {'123456.777'})\n    lu.assertEquals(candidate('12:34:56.7777'), candidate('12 34 56.7777'))\n    lu.assertEquals(candidate('23:45:12.456'), {'23', '45', '12.456'})\n    lu.assertEquals(candidate('-23:45:12.456'), {'-23', '45', '12.456'})\n    lu.assertEquals(candidate('12 34 56.7777'), {'12', '34', '56.7777'})\n    lu.assertEquals(candidate('12 34 56.777 78 90 11.111'), {'12', '34', '56.777', '78', '90', '11.111'})\n    lu.assertEquals(candidate('-234.5555'), {'-234.5555'})\n    lu.assertEquals(candidate('12:34:56'), {'12', '34', '56'})\n    lu.assertEquals(candidate('12 34 56.7777'), {'12', '34', '56.7777'})\n    lu.assertEquals(candidate('12 34 56.777'), {'12', '34', '56.777'})\n    lu.assertEquals(candidate('12:34:56.7777'), {'12', '34', '56.7777'})\n    lu.assertEquals(candidate('12:34:56.777'), {'12', '34', '56.777'})\n    lu.assertEquals(candidate('234.5555'), {'234.5555'})\n    lu.assertEquals(candidate('12 34 56'), {'12', '34', '56'})\n    lu.assertEquals(candidate('12:34:56'), {'12', '34', '56'})\n    lu.assertEquals(candidate('12:34:56.7777'), {'12', '34', '56.7777'})\n    lu.assertEquals(candidate('12 34 56'), {'12', '34', '56'})\n    lu.assertEquals(candidate('234.55'), {'234.55'})\n    lu.assertEquals(candidate('234.55 24.12'), {'234.55', '24.12'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_854_arg_return_greetings", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # message = F\"hello {name}\"\n-- # return message\n-- \n-- This is greeting function with arguments and return greeting message\n-- :param name:\n-- :return:\nlocal function arg_return_greetings(name)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_854_arg_return_greetings.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = arg_return_greetings\n    lu.assertEquals(candidate('Bob'), 'hello Bob')\n    lu.assertEquals(candidate('Milton'), 'hello Milton')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_85909_validate_training_proportion_input", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # training_proportion = float(input)\n-- # if training_proportion > 1 or training_proportion <= 0:\n-- #     raise ValueError(\n-- #         \"Invalid training proportion input. Must be strictly greater than 0 and less than or equal to 1\")\n-- # return training_proportion\n-- \n-- Validates the training proportion input\n-- :param input: The training proportion before parsing\n-- :return: The training proportion after parsing\nlocal function validate_training_proportion_input(input)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_85909_validate_training_proportion_input.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = validate_training_proportion_input\n    lu.assertEquals(candidate(0.1), 0.1)\n    lu.assertEquals(candidate(), 0.3)\n    lu.assertEquals(candidate(0.2), 0.2)\n    lu.assertEquals(candidate(0.3), 0.3)\n    lu.assertEquals(candidate('1'), 1)\n    lu.assertEquals(candidate(1), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_86450_matrix_n_by_n_determinant", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if nb_rows == 1:\n-- #     return matrix[0][0]\n-- # if nb_rows == 2:\n-- #     return matrix[0][0] * matrix[1][1] - matrix[0][1] * matrix[1][0]\n-- # else:\n-- #     determinant = 0\n-- #     for i in range(nb_rows):\n-- #         minor = [[matrix[_][j] for j in range(nb_cols) if j != i] for _ in range(1, nb_rows)]\n-- #         determinant += matrix[0][i] * matrix_n_by_n_determinant(nb_rows - 1, nb_cols - 1, minor) * (-1) ** (\n-- #             1 + i + 1)\n-- #     return determinant\n-- \n-- find the determinant of a n by n matrix\nlocal function matrix_n_by_n_determinant(nb_rows, nb_cols, matrix)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_86450_matrix_n_by_n_determinant.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = matrix_n_by_n_determinant\n    lu.assertEquals(candidate(1, 2, {{1, 2}}), 1)\n    lu.assertEquals(candidate(1, 1, {{5}}), 5)\n    lu.assertEquals(candidate(4, 4, {{1, 2, 3, 4}, {5, 6, 7, 8}, {9, 10, 11, 12}, {13, 14, 15, 16}}), 0)\n    lu.assertEquals(candidate(1, 1, {{1}}), 1)\n    lu.assertEquals(candidate(1, 1, {{2}}), 2)\n    lu.assertEquals(candidate(2, 2, {{2, 3}, {4, 5}}), -2)\n    lu.assertEquals(candidate(1, 3, {{1, 2, 3}}), 1)\n    lu.assertEquals(candidate(2, 2, {{1, 2}, {3, 4}}), -2)\n    lu.assertEquals(candidate(3, 3, {{1, 2, 3}, {4, 5, 6}, {7, 8, 9}}), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_86590_horizontal_index", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # lambda ncols, row, col: ncols * (row - 1) + col\n-- # return ncols * (row - 1) + col\n-- \n--  Compute the array index using horizontal-display logic \nlocal function horizontal_index(ncols, row, col)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_86590_horizontal_index.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = horizontal_index\n    lu.assertEquals(candidate(2, 1, 1), 1)\n    lu.assertEquals(candidate(2, 1, 2), 2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_86759_parse_codesys", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # infos = info.split('\\n')\n-- # data = {}\n-- # for info in infos:\n-- #     if ':' not in info:\n-- #         continue\n-- #     k, v = info.split(':', 1)\n-- #     if '.' in k:\n-- #         continue\n-- #     data[k] = v.strip()\n-- # return data\n-- \n-- Operating System: Nucleus PLUS\n-- Operating System Details: Nucleus PLUS version unknown\n-- Product: 3S-Smart Software Solutions\nlocal function parse_codesys(info)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_86759_parse_codesys.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_codesys\n    lu.assertEquals(candidate('Operating System: Nucleus PLUS\\nOperating System Details: Nucleus PLUS version unknown\\nProduct: 3S-Smart Software Solutions\\n'), {['Operating System'] = 'Nucleus PLUS', ['Operating System Details'] = 'Nucleus PLUS version unknown', ['Product'] = '3S-Smart Software Solutions'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_86814_polynomial_decay_learning_rate", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if step <= decay_steps:\n-- #     delta = float(learning_rate_start - learning_rate_final)\n-- #     lr = delta * (1.0 - float(step) / float(decay_steps)) ** power + learning_rate_final\n-- #     return lr\n-- # return learning_rate_final\n-- \n-- Manual implementation of polynomial decay for learning rate\n-- :param step: which step we're on\n-- :param learning_rate_start: learning rate for epoch 0\n-- :param learning_rate_final: learning rate for epoch decay_steps\n-- :param decay_steps: epoch at which learning rate stops changing\n-- :param power: exponent\n-- :return:\nlocal function polynomial_decay_learning_rate(step, learning_rate_start, learning_rate_final, decay_steps, power)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_86814_polynomial_decay_learning_rate.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = polynomial_decay_learning_rate\n    lu.assertEquals(candidate(25, 1.0, 0.0, 25, 1.0), 0.0)\n    lu.assertEquals(candidate(50, 1.0, 0.0, 25, 1.0), 0.0)\n    lu.assertEquals(candidate(0, 1.0, 0.0, 25, 1.0), 1.0)\n    lu.assertEquals(candidate(10, 0.1, 0.5, 1, 2.0), 0.5)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_87160_steps_cancel_out", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if prev_step is None:\n-- #     return False\n-- # # U2 followed by U2 is a no-op\n-- # if step == prev_step and step.endswith(\"2\"):\n-- #     return True\n-- # # U' followed by U is a no-op\n-- # if prev_step.endswith(\"'\") and not step.endswith(\"'\") and step == prev_step[0:-1]:\n-- #     return True\n-- # # U followed by U' is a no-op\n-- # if not prev_step.endswith(\"'\") and step.endswith(\"'\") and step[0:-1] == prev_step:\n-- #     return True\n-- # return False\n-- \n-- >>> steps_cancel_out(None, \"U\")\n-- False\n-- >>> steps_cancel_out(\"U\", \"U'\")\n-- True\n-- >>> steps_cancel_out(\"U'\", \"U\")\n-- True\n-- >>> steps_cancel_out(\"U2\", \"U2\")\n-- True\n-- >>> steps_cancel_out(\"U\", \"U\")\n-- False\nlocal function steps_cancel_out(prev_step, step)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_87160_steps_cancel_out.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = steps_cancel_out\n    lu.assertEquals(candidate('U2', 'U2'), true)\n    lu.assertEquals(candidate('U', 'U2'), false)\n    lu.assertEquals(candidate('U2', 'U'), false)\n    lu.assertEquals(candidate(\"U'\", 'U'), true)\n    lu.assertEquals(candidate('U', \"U'\"), true)\n    lu.assertEquals(candidate('U', 'U'), false)\n    lu.assertEquals(candidate(None, 'U'), false)\n    lu.assertEquals(candidate('U2', 'D2'), false)\n    lu.assertEquals(candidate(\"U'\", 'U2'), false)\n    lu.assertEquals(candidate('U2', \"U'\"), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_87271_longestPalindrome", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # ngrams = {}\n-- # longest_palin = None\n-- # for i in list(reversed(range(1, len(text)+1))):\n-- #     ngrams[i] = list()\n-- #     ngram = text\n-- #     while len(ngram) >= i:\n-- #         ngrams[i].append(ngram[0:i])\n-- #         ngram = ngram[1:]\n-- #     for text in ngrams[i]:\n-- #         if text == text[::-1]:\n-- #             longest_palin = text\n-- #             break\n-- #         else:\n-- #             continue\n-- #     if longest_palin == None:\n-- #         continue\n-- #     else:\n-- #         break\n-- # return longest_palin\n-- \n--     Finds the longest instance of a palindrome in a string\nlocal function longestPalindrome(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_87271_longestPalindrome.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = longestPalindrome\n    lu.assertEquals(candidate('a'), 'a')\n    lu.assertEquals(candidate('abcba'), 'abcba')\n    lu.assertEquals(candidate('aa'), 'aa')\n    lu.assertEquals(candidate('abba'), 'abba')\n    lu.assertEquals(candidate('abababa'), 'abababa')\n    lu.assertEquals(candidate('racecar'), 'racecar')\n    lu.assertEquals(candidate('bananas'), 'anana')\n    lu.assertEquals(candidate('aba'), 'aba')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_87596_delimiter_check", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if ',' in line:\n-- #     delim = ','\n-- # elif '\\t' in line:\n-- #     delim = '\\t'\n-- # elif ':' in line:\n-- #     delim = ':'\n-- # else:\n-- #     raise ValueError('Unrecognized delimiter, use \",\", \"\\t\", or \":\" instead.')\n-- # return delim\n-- \n-- Determines what delimiter is used in given text file.\n-- Parameters:\n--         -line: str\n--                 line from text file\n-- Returns:\n--         -delim: str\n--                 delimiter used in text file\n-- Notes:\n-- Recognizes only \",\", \"\t\", and \":\" delimiters.\nlocal function delimiter_check(line)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_87596_delimiter_check.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = delimiter_check\n    lu.assertEquals(candidate('a,b,c'), ',')\n    lu.assertEquals(candidate('a\\tb\\tc'), '\\t')\n    lu.assertEquals(candidate('a:b:c'), ':')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_87843_create_fixed_income_regex", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return rf\"\\b{input_symbol}\\b\\\\{{0,1}}D{{0,1}}@{{0,1}}[a-zA-Z0-9]{{1,10}}\"\n-- \n-- Creates a regular expression pattern to match the fixed income symbology.\n-- To create the regular expression patter, the function uses the fact that within the\n-- ICE consolidated feed, all the fixed income instruments are identified by the root\n-- symbol ( a unique mnemonic based on the exchange ticker or the ISIN, where no exchange\n-- ticker is available), prefixed with the type and the optional session indicator. In\n-- addition to this minimal symbology setup, fixed income symbols can present optional\n-- elements such as the \"dirty bond\" marker and the sub-market indicator.\n-- The function only requires the root symbol, prefixed with the type and the optional\n-- session indicator, to generate a regular expression pattern, and takes care to\n-- autonomously extend the pattern to match as well all the optional components of the\n-- symbol.\n-- Parameters\n-- ----------\n-- input_symbol: str\n--     A fixed income symbol consisting of the root symbol prefixed with the type\n--     identifier (B) and optional session indicator.\n-- Returns\n-- -------\n-- str\n--     The regular expression pattern that matches the input symbol as well as all the\n--     optional components.\nlocal function create_fixed_income_regex(input_symbol)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_87843_create_fixed_income_regex.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = create_fixed_income_regex\n    lu.assertEquals(candidate('B\\\\d{1,10}'), '\\\\bB\\\\d{1,10}\\\\b\\\\\\\\{0,1}D{0,1}@{0,1}[a-zA-Z0-9]{1,10}')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_87844_min_value", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return b ^ ((a ^ b) & -(a < b))\n-- \n--  Compute min of a pair of two ints. \nlocal function min_value(a, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_87844_min_value.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = min_value\n    lu.assertEquals(candidate(3, 5), 3)\n    lu.assertEquals(candidate(2, 1), 1)\n    lu.assertEquals(candidate(0, -2), -2)\n    lu.assertEquals(candidate(-1, -2), -2)\n    lu.assertEquals(candidate(2, 3), 2)\n    lu.assertEquals(candidate(3, 4), 3)\n    lu.assertEquals(candidate(3, 1), 1)\n    lu.assertEquals(candidate(100, 1), 1)\n    lu.assertEquals(candidate(2, 2), 2)\n    lu.assertEquals(candidate(1, 100), 1)\n    lu.assertEquals(candidate(0, 0), 0)\n    lu.assertEquals(candidate(1, 1), 1)\n    lu.assertEquals(candidate(1, 3), 1)\n    lu.assertEquals(candidate(3, 3), 3)\n    lu.assertEquals(candidate(5, 3), 3)\n    lu.assertEquals(candidate(1, 2), 1)\n    lu.assertEquals(candidate(3, 2), 2)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_88209_location", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if loc == 'None':\n-- #     return {'parsed': 'None', 'string': 'N/A'}\n-- # return {'parsed': loc, 'string': loc}\n-- \n-- Function to format location\nlocal function location(loc)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_88209_location.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = location\n    lu.assertEquals(candidate('40.780831,-73.965339, 150 ft'), {['parsed'] = '40.780831,-73.965339, 150 ft', ['string'] = '40.780831,-73.965339, 150 ft'})\n    lu.assertEquals(candidate('E'), {['parsed'] = 'E', ['string'] = 'E'})\n    lu.assertEquals(candidate('F'), {['parsed'] = 'F', ['string'] = 'F'})\n    lu.assertEquals(candidate('H'), {['parsed'] = 'H', ['string'] = 'H'})\n    lu.assertEquals(candidate('D'), {['parsed'] = 'D', ['string'] = 'D'})\n    lu.assertEquals(candidate('New York, NY'), {['parsed'] = 'New York, NY', ['string'] = 'New York, NY'})\n    lu.assertEquals(candidate('somewhere'), {['parsed'] = 'somewhere', ['string'] = 'somewhere'})\n    lu.assertEquals(candidate('G'), {['parsed'] = 'G', ['string'] = 'G'})\n    lu.assertEquals(candidate('742 Evergreen Terrace'), {['parsed'] = '742 Evergreen Terrace', ['string'] = '742 Evergreen Terrace'})\n    lu.assertEquals(candidate('New York City'), {['parsed'] = 'New York City', ['string'] = 'New York City'})\n    lu.assertEquals(candidate('Q'), {['parsed'] = 'Q', ['string'] = 'Q'})\n    lu.assertEquals(candidate('New York'), {['parsed'] = 'New York', ['string'] = 'New York'})\n    lu.assertEquals(candidate('L'), {['parsed'] = 'L', ['string'] = 'L'})\n    lu.assertEquals(candidate('CaliforniA, Irvine'), {['parsed'] = 'CaliforniA, Irvine', ['string'] = 'CaliforniA, Irvine'})\n    lu.assertEquals(candidate('P'), {['parsed'] = 'P', ['string'] = 'P'})\n    lu.assertEquals(candidate('M'), {['parsed'] = 'M', ['string'] = 'M'})\n    lu.assertEquals(candidate('A'), {['parsed'] = 'A', ['string'] = 'A'})\n    lu.assertEquals(candidate('UCLA'), {['parsed'] = 'UCLA', ['string'] = 'UCLA'})\n    lu.assertEquals(candidate('University of California, Irvine'), {['parsed'] = 'University of California, Irvine', ['string'] = 'University of California, Irvine'})\n    lu.assertEquals(candidate('R'), {['parsed'] = 'R', ['string'] = 'R'})\n    lu.assertEquals(candidate('40.780831, -73.965339'), {['parsed'] = '40.780831, -73.965339', ['string'] = '40.780831, -73.965339'})\n    lu.assertEquals(candidate('NYC'), {['parsed'] = 'NYC', ['string'] = 'NYC'})\n    lu.assertEquals(candidate('C'), {['parsed'] = 'C', ['string'] = 'C'})\n    lu.assertEquals(candidate('40.780831,-73.965339'), {['parsed'] = '40.780831,-73.965339', ['string'] = '40.780831,-73.965339'})\n    lu.assertEquals(candidate('San Diego State University'), {['parsed'] = 'San Diego State University', ['string'] = 'San Diego State University'})\n    lu.assertEquals(candidate('O'), {['parsed'] = 'O', ['string'] = 'O'})\n    lu.assertEquals(candidate(''), {['parsed'] = '', ['string'] = ''})\n    lu.assertEquals(candidate('J'), {['parsed'] = 'J', ['string'] = 'J'})\n    lu.assertEquals(candidate('K'), {['parsed'] = 'K', ['string'] = 'K'})\n    lu.assertEquals(candidate('University of California, San Diego'), {['parsed'] = 'University of California, San Diego', ['string'] = 'University of California, San Diego'})\n    lu.assertEquals(candidate('N'), {['parsed'] = 'N', ['string'] = 'N'})\n    lu.assertEquals(candidate('None'), {['parsed'] = 'None', ['string'] = 'N/A'})\n    lu.assertEquals(candidate('New York State University'), {['parsed'] = 'New York State University', ['string'] = 'New York State University'})\n    lu.assertEquals(candidate('I'), {['parsed'] = 'I', ['string'] = 'I'})\n    lu.assertEquals(candidate('B'), {['parsed'] = 'B', ['string'] = 'B'})\n    lu.assertEquals(candidate('New York State'), {['parsed'] = 'New York State', ['string'] = 'New York State'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_8834_count_digit", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if n == 0:\n-- #     return 0\n-- # if n % 10 == digit:\n-- #     return count_digit(n//10, digit) + 1\n-- # else:\n-- #     return count_digit(n//10, digit)\n-- \n-- Return how many times digit appears in n.\n-- >>> count_digit(55055, 5)\n-- 4\n-- >>> count_digit(1231421, 1)\n-- 3\n-- >>> count_digit(12, 3)\n-- 0\nlocal function count_digit(n, digit)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_8834_count_digit.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = count_digit\n    lu.assertEquals(candidate(55055, 5), 4)\n    lu.assertEquals(candidate(12345, 6), 0)\n    lu.assertEquals(candidate(1234, 2), 1)\n    lu.assertEquals(candidate(1234, 3), 1)\n    lu.assertEquals(candidate(1231421, 1), 3)\n    lu.assertEquals(candidate(12, 3), 0)\n    lu.assertEquals(candidate(1231421, 0), 0)\n    lu.assertEquals(candidate(1234, 5), 0)\n    lu.assertEquals(candidate(1231421, 5), 0)\n    lu.assertEquals(candidate(1234, 4), 1)\n    lu.assertEquals(candidate(1234, 1), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_89299_get_pkg_vendor_name", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # vendor = pkg.get(\"vendor\")\n-- # if not vendor:\n-- #     purl = pkg.get(\"purl\")\n-- #     if purl:\n-- #         purl_parts = purl.split(\"/\")\n-- #         if purl_parts:\n-- #             vendor = purl_parts[0].replace(\"pkg:\", \"\")\n-- #     else:\n-- #         vendor = \"\"\n-- # name = pkg.get(\"name\")\n-- # return vendor, name\n-- \n-- Method to extract vendor and name information from package. If vendor information is not available\n-- package url is used to extract the package registry provider such as pypi, maven\nlocal function get_pkg_vendor_name(pkg)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_89299_get_pkg_vendor_name.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = get_pkg_vendor_name\n    lu.assertEquals(candidate({['purl'] = 'pkg:pypi/pip', ['type'] = 'pypi', ['name'] = 'pip', ['version'] = '20.3.4', ['scope'] = 'dependencies'}), {'pypi', 'pip'})\n    lu.assertEquals(candidate({['purl'] = 'pkg:pypi/pip@20.3.4', ['type'] = 'pypi', ['name'] = 'pip', ['version'] = '20.3.4', ['scope'] = 'dependencies'}), {'pypi', 'pip'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_8962_is_catalog_record_owner", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if user_id and catalog_record.get('metadata_provider_user') == user_id:\n-- #     return True\n-- # return False\n-- \n-- Does user_id own catalog_record.\n-- :param catalog_record:\n-- :param user_id:\n-- :return:\nlocal function is_catalog_record_owner(catalog_record, user_id)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_8962_is_catalog_record_owner.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_catalog_record_owner\n    lu.assertEquals(candidate({['metadata_provider_user'] = 'a', ['owner'] = 'b'}, 'a'), true)\n    lu.assertEquals(candidate({['metadata_provider_user'] = 'a', ['owner'] = 'a'}, 'b'), false)\n    lu.assertEquals(candidate({['metadata_provider_user'] = 'a', ['owner'] = 'b'}, None), false)\n    lu.assertEquals(candidate({['metadata_provider_user'] = 'a', ['owner'] = 'a'}, None), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_89762_maxSatisfied", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # sum_cust = 0\n-- # count = 0\n-- # start = 0\n-- # for i in range(len(customers)):\n-- #     if grumpy[i] == 0:\n-- #         sum_cust += customers[i]\n-- # win = sum_cust\n-- # if X == 0:\n-- #     return sum_cust\n-- # for i in range(X):\n-- #     if grumpy[i] == 1:\n-- #         count += 1\n-- #         win += customers[i]\n-- # sum_cust = max(sum_cust, win)\n-- # for i in range(X, len(customers)):\n-- #     if grumpy[i] == 1:\n-- #         win += customers[i]\n-- #         count += 1\n-- #     if grumpy[start] == 1:\n-- #         win -= customers[start]\n-- #         count -= 1\n-- #     start += 1\n-- #     if count > X:\n-- #         continue\n-- #     sum_cust = max(win, sum_cust)\n-- # return sum_cust\n-- \n-- :type customers: List[int]\n-- :type grumpy: List[int]\n-- :type X: int\n-- :rtype: int\nlocal function maxSatisfied(customers, grumpy, X)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_89762_maxSatisfied.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = maxSatisfied\n    lu.assertEquals(candidate({0, 0}, {0, 0}, 1), 0)\n    lu.assertEquals(candidate({1, 0, 1, 2, 1, 1, 7, 5}, {0, 1, 0, 1, 0, 1, 0, 1}, 3), 16)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_90472_conv_ls", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # a = list(str(N))\n-- # return list(map(lambda i: int(i), a))\n-- \n-- convert a number in a list of integers\nlocal function conv_ls(N)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_90472_conv_ls.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = conv_ls\n    lu.assertEquals(candidate(5), {5})\n    lu.assertEquals(candidate(7), {7})\n    lu.assertEquals(candidate(256), {2, 5, 6})\n    lu.assertEquals(candidate(123456789), {1, 2, 3, 4, 5, 6, 7, 8, 9})\n    lu.assertEquals(candidate(12345), {1, 2, 3, 4, 5})\n    lu.assertEquals(candidate(2), {2})\n    lu.assertEquals(candidate(9), {9})\n    lu.assertEquals(candidate(1), {1})\n    lu.assertEquals(candidate(19), {1, 9})\n    lu.assertEquals(candidate(123), {1, 2, 3})\n    lu.assertEquals(candidate(21), {2, 1})\n    lu.assertEquals(candidate(100), {1, 0, 0})\n    lu.assertEquals(candidate(12345), {1, 2, 3, 4, 5})\n    lu.assertEquals(candidate(23), {2, 3})\n    lu.assertEquals(candidate(253), {2, 5, 3})\n    lu.assertEquals(candidate(20), {2, 0})\n    lu.assertEquals(candidate(0), {0})\n    lu.assertEquals(candidate(32), {3, 2})\n    lu.assertEquals(candidate(13), {1, 3})\n    lu.assertEquals(candidate(11), {1, 1})\n    lu.assertEquals(candidate(18), {1, 8})\n    lu.assertEquals(candidate(1000), {1, 0, 0, 0})\n    lu.assertEquals(candidate(1234567), {1, 2, 3, 4, 5, 6, 7})\n    lu.assertEquals(candidate(12), {1, 2})\n    lu.assertEquals(candidate(123456), {1, 2, 3, 4, 5, 6})\n    lu.assertEquals(candidate(17), {1, 7})\n    lu.assertEquals(candidate(24), {2, 4})\n    lu.assertEquals(candidate(111), {1, 1, 1})\n    lu.assertEquals(candidate(8), {8})\n    lu.assertEquals(candidate(1234), {1, 2, 3, 4})\n    lu.assertEquals(candidate(22), {2, 2})\n    lu.assertEquals(candidate(26), {2, 6})\n    lu.assertEquals(candidate(4), {4})\n    lu.assertEquals(candidate(15), {1, 5})\n    lu.assertEquals(candidate(25), {2, 5})\n    lu.assertEquals(candidate(6), {6})\n    lu.assertEquals(candidate(257), {2, 5, 7})\n    lu.assertEquals(candidate(3), {3})\n    lu.assertEquals(candidate(10), {1, 0})\n    lu.assertEquals(candidate(16), {1, 6})\n    lu.assertEquals(candidate(14), {1, 4})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_9048_upto", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return text[0: text.find(limit)]\n-- \n--  return all the text up to the limit string \nlocal function upto(limit, text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_9048_upto.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = upto\n    lu.assertEquals(candidate(' ', 'Python is the best choice to start learning'), 'Python')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_90538_rev", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # t = \"\"\n-- # for c in l:\n-- #     t += chr(c + 97)\n-- # return t\n-- \n-- >>> rev([20, 13, 8, 19, 19, 4, 18, 19, 8, 18, 1, 4, 19, 19, 4, 17])\n-- 'unittestisbetter'\nlocal function rev(l)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_90538_rev.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = rev\n    lu.assertEquals(candidate(list('')), '')\n    lu.assertEquals(candidate({20, 13, 8, 19, 19, 4, 18, 19, 8, 18, 1, 4, 19, 19, 4, 17}), 'unittestisbetter')\n    lu.assertEquals(candidate({}), '')\n    lu.assertEquals(candidate({20, 13, 8, 19, 19, 4, 18, 19, 8, 18, 1, 4, 19, 19, 4, 17}), 'unittestisbetter')\n    lu.assertEquals(candidate({20, 13, 8, 19, 19, 4, 18, 19, 8, 18, 1, 4, 19, 19, 4, 17}), 'unittestisbetter')\n    lu.assertEquals(candidate({20, 13, 8, 19, 19, 4, 18, 19, 8, 18, 1, 4, 19, 19, 4, 17}), 'unittestisbetter')\n    lu.assertEquals(candidate({20, 13, 8, 19, 19, 4, 18, 19, 8, 18, 1, 4, 19, 19, 4, 17}), 'unittestisbetter')\n    lu.assertEquals(candidate(list(range(20))), 'abcdefghijklmnopqrst')\n    lu.assertEquals(candidate(list(map(ord, ''))), '')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_90563_smallest_difference", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # abs_diff = abs(value1 - value2)\n-- # if abs_diff > 180:\n-- #     smallest_diff = 360 - abs_diff\n-- # else:\n-- #     smallest_diff = abs_diff\n-- # return smallest_diff\n-- \n-- Finds smallest angle between two bearings\n-- :param value1:\n-- :param value2:\n-- :return:\nlocal function smallest_difference(value1, value2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_90563_smallest_difference.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = smallest_difference\n    lu.assertEquals(candidate(10, 10), 0)\n    lu.assertEquals(candidate(-5, 5), 10)\n    lu.assertEquals(candidate(30, 35), 5)\n    lu.assertEquals(candidate(290, 270), 20)\n    lu.assertEquals(candidate(270, 270), 0)\n    lu.assertEquals(candidate(30, 50), 20)\n    lu.assertEquals(candidate(0, 360), 0)\n    lu.assertEquals(candidate(20, 35), 15)\n    lu.assertEquals(candidate(30, 30), 0)\n    lu.assertEquals(candidate(35, 20), 15)\n    lu.assertEquals(candidate(-10, 10), 20)\n    lu.assertEquals(candidate(35, 30), 5)\n    lu.assertEquals(candidate(20, 50), 30)\n    lu.assertEquals(candidate(15, 45), 30)\n    lu.assertEquals(candidate(-5, 25), 30)\n    lu.assertEquals(candidate(270, 360), 90)\n    lu.assertEquals(candidate(270, 290), 20)\n    lu.assertEquals(candidate(10, 20), 10)\n    lu.assertEquals(candidate(25, -5), 30)\n    lu.assertEquals(candidate(350, 360), 10)\n    lu.assertEquals(candidate(350, 355), 5)\n    lu.assertEquals(candidate(350, 350), 0)\n    lu.assertEquals(candidate(10, 30), 20)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_90785_dist_point", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return (point[1] - line[0]*point[0]-line[1])**2\n-- \n-- Computes the distance square between a point and a line\n-- inputs:\n--   - point : (x,y)\n--   - line  : (slope, intercept)      \nlocal function dist_point(point, line)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_90785_dist_point.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = dist_point\n    lu.assertEquals(candidate({1, 2}, {1, 3}), 4)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_91158_count_leading_empty_lines", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # lines = cell.splitlines(keepends=True)\n-- # if not lines:\n-- #     return 0\n-- # for i, line in enumerate(lines):\n-- #     if line and not line.isspace():\n-- #         return i\n-- # return len(lines)\n-- \n-- Count the number of leading empty cells.\nlocal function count_leading_empty_lines(cell)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_91158_count_leading_empty_lines.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = count_leading_empty_lines\n    lu.assertEquals(candidate('Hello world!'), 0)\n    lu.assertEquals(candidate('\\n# This is a comment\\n# This is another comment\\nhello\\nworld\\n'), 1)\n    lu.assertEquals(candidate('hello world'), 0)\n    lu.assertEquals(candidate('\\n\\nhello\\nworld\\n'), 2)\n    lu.assertEquals(candidate('The first line is not empty'), 0)\n    lu.assertEquals(candidate('    \\n    '), 2)\n    lu.assertEquals(candidate('    Hello, world!\\n    '), 0)\n    lu.assertEquals(candidate('\\nA long cell with leading empty lines\\n\\n'), 1)\n    lu.assertEquals(candidate('\\n# This is a comment\\nhello\\nworld\\n'), 1)\n    lu.assertEquals(candidate('\\n\\n'), 2)\n    lu.assertEquals(candidate('This cell contains one line of text.'), 0)\n    lu.assertEquals(candidate('    \\n    \\n    '), 3)\n    lu.assertEquals(candidate(\"def func():\\n    print('Hi')\\n\"), 0)\n    lu.assertEquals(candidate('\\n\\n\\nhello\\nworld\\n'), 3)\n    lu.assertEquals(candidate('text'), 0)\n    lu.assertEquals(candidate(''), 0)\n    lu.assertEquals(candidate('\\n\\nhello\\nworld\\n'), 2)\n    lu.assertEquals(candidate('\\n\\n\\ntext'), 3)\n    lu.assertEquals(candidate('\\n\\n# This is a comment\\nhello\\nworld\\n'), 2)\n    lu.assertEquals(candidate('hello\\nworld\\n'), 0)\n    lu.assertEquals(candidate('\\n\\n\\n'), 3)\n    lu.assertEquals(candidate('\\n\\n\\n   \\ntext'), 4)\n    lu.assertEquals(candidate('\\nhello\\nworld\\n'), 1)\n    lu.assertEquals(candidate('   '), 1)\n    lu.assertEquals(candidate('\\n\\n    '), 3)\n    lu.assertEquals(candidate(''), 0)\n    lu.assertEquals(candidate('\\n'), 1)\n    lu.assertEquals(candidate(''), 0)\n    lu.assertEquals(candidate('hello world'), 0)\n    lu.assertEquals(candidate('\\n\\n\\nhello\\nworld\\n'), 3)\n    lu.assertEquals(candidate('\\nA long cell with leading empty lines\\n\\n \\n\\n\\n\\n \\n\\n\\n'), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_91177_calc_mi", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if side not in ['right', 'left']:\n-- #     raise ValueError(f'Side must be \"right\" or \"left\", not \"{side}\"')\n-- # if side == 'left':\n-- #     med_head_x *= -1\n-- #     lat_head_x *= -1\n-- #     lat_acetabulum_x *= -1\n-- # mi = (lat_acetabulum_x - lat_head_x) / (med_head_x - lat_head_x)\n-- # # bound within [0, 1]\n-- # mi = max(mi, 0)\n-- # mi = min(mi, 1)\n-- # return mi\n-- \n-- Calculate migration index (MI).\n-- For the right hip, MI = (lat acetabulum - lat head) / (med head - lat head)\n-- For the left hip first need to multiple each point by -1.\n-- Args:\n--     med_head_x: x coordinate of medial head\n--     lat_head_x: x coordinate of lateral head\n--     lat_acetabulum_x: x coordinate of lateral acetabulum\n--     side: side of hip from which points come (i.e., 'right' or 'left')\n-- Raise:\n--     ValueError if side not in ['right', 'left']\n-- Returns:\n--     Migration index in interval [0,1]\nlocal function calc_mi(med_head_x, lat_head_x, lat_acetabulum_x, side)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_91177_calc_mi.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = calc_mi\n    lu.assertEquals(candidate(2, 1, 2, 'right'), 1)\n    lu.assertEquals(candidate(2, 1, 1, 'right'), 0)\n    lu.assertEquals(candidate(2, 1, 2, 'left'), 1)\n    lu.assertEquals(candidate(1, 0, 0, 'right'), 0)\n    lu.assertEquals(candidate(100, 0, 0, 'left'), 0)\n    lu.assertEquals(candidate(0, 1, 0, 'left'), 1)\n    lu.assertEquals(candidate(1, 0, 0, 'left'), 0)\n    lu.assertEquals(candidate(2, 1, 1, 'left'), 0)\n    lu.assertEquals(candidate(100, 0, 0, 'right'), 0)\n    lu.assertEquals(candidate(0, 1, 0, 'right'), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_91262_password_rule_2", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not password:\n-- #     return False\n-- # previous = password[0]\n-- # for char in password:\n-- #     if char < previous:\n-- #         return False\n-- #     previous = char\n-- # return True\n-- \n-- Check if going from left to right, the digits never decrease; they only ever increase or\n-- stay the same (like 111123 or 135679).\n-- :param password: str\n-- :return: bool\nlocal function password_rule_2(password)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_91262_password_rule_2.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = password_rule_2\n    lu.assertEquals(candidate('111122'), true)\n    lu.assertEquals(candidate('1'), true)\n    lu.assertEquals(candidate('333'), true)\n    lu.assertEquals(candidate('223450'), false)\n    lu.assertEquals(candidate('123456'), true)\n    lu.assertEquals(candidate('4444'), true)\n    lu.assertEquals(candidate('111123'), true)\n    lu.assertEquals(candidate(''), false)\n    lu.assertEquals(candidate('22'), true)\n    lu.assertEquals(candidate('666666'), true)\n    lu.assertEquals(candidate('111111'), true)\n    lu.assertEquals(candidate('999999999'), true)\n    lu.assertEquals(candidate(None), false)\n    lu.assertEquals(candidate('55555'), true)\n    lu.assertEquals(candidate('88888888'), true)\n    lu.assertEquals(candidate('135679'), true)\n    lu.assertEquals(candidate('7777777'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_91318_decapitalize", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return f\"{key[:1].lower()}{key[1:]}\"\n-- \n-- This method will be used to lower case the first character of SQS\n-- message attributes being received by Lambda to resolve inconsistencies.\n-- Issue outlined here: https://github.com/boto/boto3/issues/2582\nlocal function decapitalize(key)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_91318_decapitalize.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = decapitalize\n    lu.assertEquals(candidate('Foo'), 'foo')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('foo'), 'foo')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_91803_is_valid_square_input", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if text == \"\" or (text.isdigit() and 1 <= int(text) <= 9):\n-- #     return True\n-- # else:\n-- #     return False\n-- \n-- Checks if an entry square value is a valid input to the sudoku\n-- :param text: Str\n-- :return: True if valid, False if not\nlocal function is_valid_square_input(text)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_91803_is_valid_square_input.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = is_valid_square_input\n    lu.assertEquals(candidate('a'), false)\n    lu.assertEquals(candidate('abc'), false)\n    lu.assertEquals(candidate('1'), true)\n    lu.assertEquals(candidate('-1'), false)\n    lu.assertEquals(candidate('1.0'), false)\n    lu.assertEquals(candidate('11'), false)\n    lu.assertEquals(candidate('10'), false)\n    lu.assertEquals(candidate(' '), false)\n    lu.assertEquals(candidate('2'), true)\n    lu.assertEquals(candidate('9'), true)\n    lu.assertEquals(candidate('00000'), false)\n    lu.assertEquals(candidate('3'), true)\n    lu.assertEquals(candidate(''), true)\n    lu.assertEquals(candidate('100'), false)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_92093_format_makehelp", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return '{}:\\t{}'.format(target, detail)\n-- \n--     return \"{target}:\t{detail}\"\nlocal function format_makehelp(target, detail)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_92093_format_makehelp.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = format_makehelp\n    lu.assertEquals(candidate('all', 'This target builds the following targets: clean, debug, release, and test.'), 'all:\\tThis target builds the following targets: clean, debug, release, and test.')\n    lu.assertEquals(candidate('debug', 'Build with debug symbols'), 'debug:\\tBuild with debug symbols')\n    lu.assertEquals(candidate('clean', 'Removes the build directory'), 'clean:\\tRemoves the build directory')\n    lu.assertEquals(candidate('release-test', 'Build release binary, run unit tests'), 'release-test:\\tBuild release binary, run unit tests')\n    lu.assertEquals(candidate('target', 'detail'), 'target:\\tdetail')\n    lu.assertEquals(candidate('run', 'Runs the program'), 'run:\\tRuns the program')\n    lu.assertEquals(candidate('test', 'this is a test'), 'test:\\tthis is a test')\n    lu.assertEquals(candidate('clean', 'Remove all build artifacts'), 'clean:\\tRemove all build artifacts')\n    lu.assertEquals(candidate('debug-test', 'Build debug binary, run unit tests'), 'debug-test:\\tBuild debug binary, run unit tests')\n    lu.assertEquals(candidate('test', 'Run all tests'), 'test:\\tRun all tests')\n    lu.assertEquals(candidate('release', 'Build without debug symbols'), 'release:\\tBuild without debug symbols')\n    lu.assertEquals(candidate('clean-debug', 'Remove all debug build artifacts'), 'clean-debug:\\tRemove all debug build artifacts')\n    lu.assertEquals(candidate('target', 'detail'), 'target:\\tdetail')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_92403__list_product", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # product = 1\n-- # for num in num_list:\n-- #     product *= num\n-- # return product\n-- \n-- Computes product of all elements in a list.\nlocal function _list_product(num_list)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_92403__list_product.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _list_product\n    lu.assertEquals(candidate({}), 1)\n    lu.assertEquals(candidate(list(range(1, 10))), 362880)\n    lu.assertEquals(candidate(list(range(1, 11))), 3628800)\n    lu.assertEquals(candidate(list(range(1, 21))), 2432902008176640000)\n    lu.assertEquals(candidate(list(range(1, 6))), 120)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_92626_gcd", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if n == 0:\n-- #     result = m\n-- # else:\n-- #     result = gcd(n, m % n)\n-- # return result\n-- \n-- (int, int) -> int\n-- Uses Euclid's method to compute the greatest common factor\n-- (greatest common divisor) of two integers, <m> and <n>\n-- Returns greatest common factor (gcd) of the two integers\nlocal function gcd(m, n)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_92626_gcd.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = gcd\n    lu.assertEquals(candidate(12, 6), 6)\n    lu.assertEquals(candidate(12, 24), 12)\n    lu.assertEquals(candidate(2, 12), 2)\n    lu.assertEquals(candidate(9, 6), 3)\n    lu.assertEquals(candidate(10, 100), 10)\n    lu.assertEquals(candidate(5, 15), 5)\n    lu.assertEquals(candidate(4, 2), 2)\n    lu.assertEquals(candidate(8, 2), 2)\n    lu.assertEquals(candidate(56, 110), 2)\n    lu.assertEquals(candidate(18, 12), 6)\n    lu.assertEquals(candidate(15, 5), 5)\n    lu.assertEquals(candidate(1024, 1024), 1024)\n    lu.assertEquals(candidate(36, 6), 6)\n    lu.assertEquals(candidate(12, 3), 3)\n    lu.assertEquals(candidate(36, 24), 12)\n    lu.assertEquals(candidate(5, 0), 5)\n    lu.assertEquals(candidate(12, 18), 6)\n    lu.assertEquals(candidate(19, 3), 1)\n    lu.assertEquals(candidate(3, 12), 3)\n    lu.assertEquals(candidate(6, 36), 6)\n    lu.assertEquals(candidate(0, 5), 5)\n    lu.assertEquals(candidate(20, 15), 5)\n    lu.assertEquals(candidate(6, 9), 3)\n    lu.assertEquals(candidate(), 10)\n    lu.assertEquals(candidate(6, 12), 6)\n    lu.assertEquals(candidate(30, 10), 10)\n    lu.assertEquals(candidate(12, 12), 12)\n    lu.assertEquals(candidate(2, 4), 2)\n    lu.assertEquals(candidate(0, 2), 2)\n    lu.assertEquals(candidate(0, 0), 0)\n    lu.assertEquals(candidate(6, 18), 6)\n    lu.assertEquals(candidate(24, 36), 12)\n    lu.assertEquals(candidate(12, 2), 2)\n    lu.assertEquals(candidate(2, 0), 2)\n    lu.assertEquals(candidate(2, 8), 2)\n    lu.assertEquals(candidate(12, 0), 12)\n    lu.assertEquals(candidate(2, 10), 2)\n    lu.assertEquals(candidate(18, 6), 6)\n    lu.assertEquals(candidate(1, 1), 1)\n    lu.assertEquals(candidate(1000, 10), 10)\n    lu.assertEquals(candidate(3, 4), 1)\n    lu.assertEquals(candidate(10, 2), 2)\n    lu.assertEquals(candidate(1000, 10000), 1000)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_9273_clean_text_simple", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # pos = string.find('\\x19')\n-- # while pos != -1:\n-- #     string = string[:pos] + string[pos+2:]\n-- #     pos = string.find('\\x19')\n-- # return string\n-- \n-- Remove all \u0019 from the string formatted with simple colors:\n-- \u00198\nlocal function clean_text_simple(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_9273_clean_text_simple.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = clean_text_simple\n    lu.assertEquals(candidate('This \\x191is\\x193 \\x199just\\x193 some\\x193 \\x199text.\\x193'), 'This is just some text.')\n    lu.assertEquals(candidate('This is just some text.'), 'This is just some text.')\n    lu.assertEquals(candidate('\\x190Red\\x198 \\x190Green\\x198 \\x190Blue\\x198 \\x190Yellow\\x198'), 'Red Green Blue Yellow')\n    lu.assertEquals(candidate('\\\\x198\\\\x1913'), '\\\\x198\\\\x1913')\n    lu.assertEquals(candidate('\\\\x198\\\\x198\\\\x19888'), '\\\\x198\\\\x198\\\\x19888')\n    lu.assertEquals(candidate('This \\x199is\\x193 \\x199just\\x193 some\\x193 \\x191text.\\x193'), 'This is just some text.')\n    lu.assertEquals(candidate('\\x194Red\\x198 \\x194Green\\x198 \\x194Blue\\x198 \\x194Yellow\\x198'), 'Red Green Blue Yellow')\n    lu.assertEquals(candidate('\\\\x198888'), '\\\\x198888')\n    lu.assertEquals(candidate('\\\\x1913\\\\x198888'), '\\\\x1913\\\\x198888')\n    lu.assertEquals(candidate('\\\\x198'), '\\\\x198')\n    lu.assertEquals(candidate('\\x194Red\\x198 \\x190Green\\x198 \\x192Blue\\x198 \\x191Yellow\\x198'), 'Red Green Blue Yellow')\n    lu.assertEquals(candidate('\\\\x1988\\\\x1988\\\\x1988'), '\\\\x1988\\\\x1988\\\\x1988')\n    lu.assertEquals(candidate('\\x194Red\\x198 \\x191Green\\x198 \\x192Blue\\x198 \\x193Yellow\\x198'), 'Red Green Blue Yellow')\n    lu.assertEquals(candidate('This \\x191is\\x193 \\x199just\\x193 some\\x193 \\x191text.\\x193'), 'This is just some text.')\n    lu.assertEquals(candidate('\\\\x198\\\\x198\\\\x198'), '\\\\x198\\\\x198\\\\x198')\n    lu.assertEquals(candidate('This \\x199is\\x193 \\x199just\\x193 some\\x193 text.\\x193'), 'This is just some text.')\n    lu.assertEquals(candidate('\\x191Red\\x198 \\x191Green\\x198 \\x191Blue\\x198 \\x191Yellow\\x198'), 'Red Green Blue Yellow')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_92801_parse_hl_lines", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # if not expr:\n-- #     return []\n-- # try:\n-- #     return list(map(int, expr.split()))\n-- # except ValueError:  # pragma: no cover\n-- #     return []\n-- \n-- Support our syntax for emphasizing certain lines of code.\n-- expr should be like '1 2' to emphasize lines 1 and 2 of a code block.\n-- Returns a list of ints, the line numbers to emphasize.\nlocal function parse_hl_lines(expr)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_92801_parse_hl_lines.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_hl_lines\n    lu.assertEquals(candidate('10 '), {10})\n    lu.assertEquals(candidate(' 10'), {10})\n    lu.assertEquals(candidate(' 1'), {1})\n    lu.assertEquals(candidate('1'), {1})\n    lu.assertEquals(candidate(' 10 11 '), {10, 11})\n    lu.assertEquals(candidate('10 11 12 13 '), {10, 11, 12, 13})\n    lu.assertEquals(candidate(' '), {})\n    lu.assertEquals(candidate('10 11'), {10, 11})\n    lu.assertEquals(candidate('1 2'), {1, 2})\n    lu.assertEquals(candidate('1'), {1})\n    lu.assertEquals(candidate(''), {})\n    lu.assertEquals(candidate('10 11 a'), {})\n    lu.assertEquals(candidate(''), {})\n    lu.assertEquals(candidate('10'), {10})\n    lu.assertEquals(candidate('   '), {})\n    lu.assertEquals(candidate('10 11 '), {10, 11})\n    lu.assertEquals(candidate(None), {})\n    lu.assertEquals(candidate('10 11 12 13'), {10, 11, 12, 13})\n    lu.assertEquals(candidate(' 10 11 12 13 '), {10, 11, 12, 13})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_92907_distance", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # (ax, ay) = a\n-- # (bx, by) = b\n-- # return abs(ax - bx) + abs(ay - by)\n-- \n--  Manthatten distance \nlocal function distance(a, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_92907_distance.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = distance\n    lu.assertEquals(candidate({0, 0}, {0, 0}), 0)\n    lu.assertEquals(candidate({-1, 1}, {1, 1}), 2)\n    lu.assertEquals(candidate({1, 1}, {-1, 1}), 2)\n    lu.assertEquals(candidate({1, 1}, {1, 1}), 0)\n    lu.assertEquals(candidate({0, 0}, {0, 1}), 1)\n    lu.assertEquals(candidate({0, 0}, {0, 0}), 0)\n    lu.assertEquals(candidate({1, 1}, {1, -1}), 2)\n    lu.assertEquals(candidate({1, 1}, {2, 2}), 2)\n    lu.assertEquals(candidate({1, 1}, {1, 2}), 1)\n    lu.assertEquals(candidate({1, 1}, {2, 1}), 1)\n    lu.assertEquals(candidate({1, 1}, {1, 1}), 0)\n    lu.assertEquals(candidate({0, 0}, {1, 1}), 2)\n    lu.assertEquals(candidate({1, 1}, {0, 0}), 2)\n    lu.assertEquals(candidate({3, 4}, {3, 4}), 0)\n    lu.assertEquals(candidate({0, 0}, {1, 0}), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_92983_areStringsEqual", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return a.upper() == b.upper()\n-- \n--     Returns if two strings are the same, disregarding cases.\nlocal function areStringsEqual(a, b)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_92983_areStringsEqual.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = areStringsEqual\n    lu.assertEquals(candidate('t', 'test'), false)\n    lu.assertEquals(candidate('TEST', 'test'), true)\n    lu.assertEquals(candidate('test', 'test'), true)\n    lu.assertEquals(candidate('TeSt', 'test'), true)\n    lu.assertEquals(candidate('Hello', 'hello'), true)\n    lu.assertEquals(candidate('Hello', 'world'), false)\n    lu.assertEquals(candidate('', ''), true)\n    lu.assertEquals(candidate('TESt', 'test'), true)\n    lu.assertEquals(candidate('Hello', 'Hello'), true)\n    lu.assertEquals(candidate('tESt', 'test'), true)\n    lu.assertEquals(candidate('test', 'other'), false)\n    lu.assertEquals(candidate('te', 'test'), false)\n    lu.assertEquals(candidate('test', 'TEST'), true)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_93087_binary_search", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # length = len(array)                                             # O(1)\n-- # i = 0                                                           # O(1)\n-- # j = length - 1                                                  # O(1)\n-- # if array[i] > k or array[j] < k:                                # O(1)\n-- #     return -1                                                   # O(1)\n-- # while i <= j:                                                   # O(logN)\n-- #     mid = ((j - i) // 2) + i                                    # O(1)\n-- #     if i == j and array[mid] != k:                              # O(1)\n-- #         return -1                                               # O(1)\n-- #     if array[mid] > k:                                          # O(1)\n-- #         j = mid - 1                                             # O(1)\n-- #     elif array[mid] < k:                                        # O(1)\n-- #         i = mid + 1                                             # O(1)\n-- #     elif array[mid] == k:                                       # O(1)\n-- #         if pos == 'start':                                      # O(1)\n-- #             if mid == i or array[mid - 1] != k:                 # O(1)\n-- #                 return mid                                      # O(1)\n-- #             j = mid - 1                                         # O(1)\n-- #         elif pos == 'end':                                      # O(1)\n-- #             if mid == j or array[mid + 1] != k:                 # O(1)\n-- #                 return mid                                      # O(1)\n-- #             i = mid + 1                                         # O(1)\n-- \n-- Apply binary search to find either a \"start\" or \"end\" of a particular number\n-- >>> binary_search([2, 3, 3, 3, 4], 3, 'start')\n-- 1\n-- >>> binary_search([2, 3, 3, 3, 4], 3, 'end')\n-- 3\n-- >>> binary_search([2, 3, 3, 3, 4], 5, 'start')\n-- -1\n-- >>> binary_search([2, 3, 3, 3, 4], 5, 'end')\n-- -1\n-- >>> binary_search([2, 3, 3, 3, 5], 4, 'start')\n-- -1\nlocal function binary_search(array, k, pos)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_93087_binary_search.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = binary_search\n    lu.assertEquals(candidate({2, 3, 3, 3, 5}, 4, 'start'), -1)\n    lu.assertEquals(candidate({2, 3, 3, 3, 4}, 5, 'start'), -1)\n    lu.assertEquals(candidate({2, 3, 3, 3, 4}, 3, 'start'), 1)\n    lu.assertEquals(candidate({2, 3, 3, 3, 4}, 3, 'end'), 3)\n    lu.assertEquals(candidate({2, 3, 3, 3, 4}, 5, 'end'), -1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_93501_parse_line_contents", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # Split by spaces\n-- # spl = line.split()\n-- # res = []\n-- # for ss in spl:\n-- #     try:\n-- #         float(ss)\n-- #         res.append(float(ss))\n-- #     except ValueError:\n-- #         pass\n-- #     if len(res) == 3:\n-- #         break\n-- # return res[0], res[1], res[2]\n-- \n-- Extract the sigma, gamma, lambda and m from the line:\n-- just read until we get something looking like a float and then store the first three of them\nlocal function parse_line_contents(line)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_93501_parse_line_contents.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_line_contents\n    lu.assertEquals(candidate('sigma: 1.0 gamma: 1.0 lambda: 1 m: 1'), {1.0, 1.0, 1.0})\n    lu.assertEquals(candidate('0.00000000 0.0 0.0'), {0.0, 0.0, 0.0})\n    lu.assertEquals(candidate('lambda: 1 m: 1 sigma: 1 gamma: 1'), {1.0, 1.0, 1.0})\n    lu.assertEquals(candidate('sigma: 1.0 gamma: 1.0 lambda: 1.0 m: 1'), {1.0, 1.0, 1.0})\n    lu.assertEquals(candidate('0.307228 0.010438 12.869096 0.0000000'), {0.307228, 0.010438, 12.869096})\n    lu.assertEquals(candidate('0.307228 0.010438 12.869096'), {0.307228, 0.010438, 12.869096})\n    lu.assertEquals(candidate('1.234567890 2345.678901 3.456789012'), {1.23456789, 2345.678901, 3.456789012})\n    lu.assertEquals(candidate('123.4567890 2.345678901 3.456789012'), {123.456789, 2.345678901, 3.456789012})\n    lu.assertEquals(candidate('0.0 0.1 0.0'), {0.0, 0.1, 0.0})\n    lu.assertEquals(candidate('1.234567890 2.345678901 3.456789012'), {1.23456789, 2.345678901, 3.456789012})\n    lu.assertEquals(candidate('0.0 0.0 0.0'), {0.0, 0.0, 0.0})\n    lu.assertEquals(candidate('123456.7890 2.345678901 3.456789012'), {123456.789, 2.345678901, 3.456789012})\n    lu.assertEquals(candidate('0.307228 0.010438 12.869096 0.0000000'), {0.307228, 0.010438, 12.869096})\n    lu.assertEquals(candidate('0.05 0.0 0.0'), {0.05, 0.0, 0.0})\n    lu.assertEquals(candidate('0.5 0.0 0.0'), {0.5, 0.0, 0.0})\n    lu.assertEquals(candidate('0.00 0.0 0.0'), {0.0, 0.0, 0.0})\n    lu.assertEquals(candidate('0.0 0.0 0.2'), {0.0, 0.0, 0.2})\n    lu.assertEquals(candidate('sigma: 1.0 gamma: 1.0 m: 1 lambda: 1'), {1.0, 1.0, 1.0})\n    lu.assertEquals(candidate('sigma: 1.0 lambda: 1 m: 1 gamma: 1'), {1.0, 1.0, 1.0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_93912_selection_to_string", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return \", \".join([\"{}\".format(v) for _, v in selection.items()])\n-- \n-- Convert dictionary of coordinates to a string for labels.\n-- Parameters\n-- ----------\n-- selection : dict[Any] -> Any\n-- Returns\n-- -------\n-- str\n--     key1: value1, key2: value2, ...\nlocal function selection_to_string(selection)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_93912_selection_to_string.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = selection_to_string\n    lu.assertEquals(candidate(dict()), '')\n    lu.assertEquals(candidate({[1] = 2, [3] = 4, [5] = 6, [7] = 8}), '2, 4, 6, 8')\n    lu.assertEquals(candidate({[1] = 2, [3] = 4}), '2, 4')\n    lu.assertEquals(candidate({[1] = 2}), '2')\n    lu.assertEquals(candidate({[1] = 2, [3] = 4, [5] = 6}), '2, 4, 6')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_94083_rotate_key", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # hexed_key = hex(temp_key)[2:].zfill(8)\n-- # decomposed_key = []\n-- # for round_num in range(3):\n-- #     decomposed_key.append(eval('0x' + hexed_key[:2]))\n-- #     hexed_key = hexed_key[(-len(hexed_key)) + 2:]\n-- # decomposed_key.append(eval('0x' + hexed_key[:2]))\n-- # rotated_key = decomposed_key.copy()\n-- # position_tag = -4\n-- # for byte in decomposed_key:\n-- #     rotated_key[position_tag + 3] = byte\n-- #     position_tag += 1\n-- # return rotated_key\n-- \n-- This function is used to adjust the byte position in the temp_key, the bytes of the temp_key are shifted over one\n-- bytes to the left.\n-- :param temp_key: A 4 bytes value, only accept numeric object.\n-- :return: A new 4 bytes value.\nlocal function rotate_key(temp_key)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_94083_rotate_key.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = rotate_key\n    lu.assertEquals(candidate(65536), {1, 0, 0, 0})\n    lu.assertEquals(candidate(0), {0, 0, 0, 0})\n    lu.assertEquals(candidate(0), {0, 0, 0, 0})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_94301_fmt_jsdoc_union", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return '(' + '|'.join(type_strings) + ')' if len(type_strings) > 1 else type_strings[0]\n-- \n--     Returns a JSDoc union of the given type strings.\nlocal function fmt_jsdoc_union(type_strings)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_94301_fmt_jsdoc_union.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = fmt_jsdoc_union\n    lu.assertEquals(candidate({'(string|number)', '(boolean|number)'}), '((string|number)|(boolean|number))')\n    lu.assertEquals(candidate({'Number', 'boolean', 'String', 'undefined', 'void'}), '(Number|boolean|String|undefined|void)')\n    lu.assertEquals(candidate({'String'}), 'String')\n    lu.assertEquals(candidate({'number', 'undefined'}), '(number|undefined)')\n    lu.assertEquals(candidate({''}), '')\n    lu.assertEquals(candidate({'Number', 'boolean'}), '(Number|boolean)')\n    lu.assertEquals(candidate({'string', 'null'}), '(string|null)')\n    lu.assertEquals(candidate({'String', 'Number', 'Boolean'}), '(String|Number|Boolean)')\n    lu.assertEquals(candidate({'null', 'number'}), '(null|number)')\n    lu.assertEquals(candidate({'String', 'Number'}), '(String|Number)')\n    lu.assertEquals(candidate({'number', 'undefined', 'string', 'null'}), '(number|undefined|string|null)')\n    lu.assertEquals(candidate({'Number', 'Boolean', 'String', 'Array', 'Object', 'Function'}), '(Number|Boolean|String|Array|Object|Function)')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_94317_str_to_list", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # string = string[1:-1].replace(' ', '').split(',')\n-- # return [int(str_id) for str_id in string]\n-- \n-- remove [] and whitespace, then create list of integers to return\nlocal function str_to_list(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_94317_str_to_list.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = str_to_list\n    lu.assertEquals(candidate('[1, 2, 3, 4]'), {1, 2, 3, 4})\n    lu.assertEquals(candidate('[1,2,3,4]'), {1, 2, 3, 4})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_94524__create_mux_ranges", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # ordered = sorted(multiplexer_ids)\n-- # # Anything but ordered[0] - 1\n-- # prev_value = ordered[0]\n-- # ranges = []\n-- # for value in ordered:\n-- #     if value == prev_value + 1:\n-- #         ranges[-1][1] = value\n-- #     else:\n-- #         ranges.append([value, value])\n-- #     prev_value = value\n-- # return ranges\n-- \n-- Create a list of ranges based on a list of single values.\n-- Example:\n--     Input:  [1, 2, 3, 5,      7, 8, 9]\n--     Output: [[1, 3], [5, 5], [7, 9]]\nlocal function _create_mux_ranges(multiplexer_ids)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_94524__create_mux_ranges.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = _create_mux_ranges\n    lu.assertEquals(candidate({1, 2, 3, 5, 7, 8, 9}), {{1, 3}, {5, 5}, {7, 9}})\n    lu.assertEquals(candidate({1, 2, 3, 4, 5}), {{1, 5}})\n    lu.assertEquals(candidate({1, 2}), {{1, 2}})\n    lu.assertEquals(candidate({1, 2, 3, 4, 6, 7, 8, 9, 10}), {{1, 4}, {6, 10}})\n    lu.assertEquals(candidate({1, 2, 3}), {{1, 3}})\n    lu.assertEquals(candidate(list(range(10))), {{0, 9}})\n    lu.assertEquals(candidate({1, 2, 3, 5, 7, 8, 9}), {{1, 3}, {5, 5}, {7, 9}})\n    lu.assertEquals(candidate({1, 3, 5, 7, 9}), {{1, 1}, {3, 3}, {5, 5}, {7, 7}, {9, 9}})\n    lu.assertEquals(candidate({2, 3}), {{2, 3}})\n    lu.assertEquals(candidate({1, 2, 4, 6, 8}), {{1, 2}, {4, 4}, {6, 6}, {8, 8}})\n    lu.assertEquals(candidate({2, 4, 6, 8}), {{2, 2}, {4, 4}, {6, 6}, {8, 8}})\n    lu.assertEquals(candidate(list(range(2, 10))), {{2, 9}})\n    lu.assertEquals(candidate({1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13}), {{1, 10}, {12, 13}})\n    lu.assertEquals(candidate({1, 2, 3, 4, 5, 6, 7, 8, 9}), {{1, 9}})\n    lu.assertEquals(candidate({1, 2, 3, 4}), {{1, 4}})\n    lu.assertEquals(candidate({1, 2, 3, 4, 5, 6, 7}), {{1, 7}})\n    lu.assertEquals(candidate({1, 2, 3, 4, 5, 6, 7, 8}), {{1, 8}})\n    lu.assertEquals(candidate({1, 2, 3, 4, 5, 6, 7, 8, 9, 10}), {{1, 10}})\n    lu.assertEquals(candidate({1, 2, 3, 4, 5, 6}), {{1, 6}})\n    lu.assertEquals(candidate({1, 2, 3, 5, 7, 8, 9}), {{1, 3}, {5, 5}, {7, 9}})\n    lu.assertEquals(candidate({1, 2, 3, 4, 5, 6, 7, 8, 9}), {{1, 9}})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_94584_contar_palabra", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return linea.count(palabra)\n-- \n-- Cuenta cuantas veces se repite una palabra en una cadena de\n-- texto.\n-- :param linea: Cadena de texto.\n-- :linea type: str\n-- :param palabra: Palabra a buscar.\n-- :palabra type: str\n-- :return: Cuantas veces se repite la palabra en la cadena.\n-- :rtype: int\nlocal function contar_palabra(linea, palabra)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_94584_contar_palabra.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = contar_palabra\n    lu.assertEquals(candidate('la casa de papelera es de papel', 'de'), 2)\n    lu.assertEquals(candidate('Caminando por el camino de la victoria', 'camino'), 1)\n    lu.assertEquals(candidate('la casa de papelera es de papel', 'casa'), 1)\n    lu.assertEquals(candidate('Caminando por el camino de la victoria', 'de'), 1)\n    lu.assertEquals(candidate('Caminando por el camino de la victoria', 'victoria'), 1)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_94731_url_to_id", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # character_list = url.split('/')\n-- # return int(character_list[5])\n-- \n--  Takes a tweet url and returns its id \nlocal function url_to_id(url)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_94731_url_to_id.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = url_to_id\n    lu.assertEquals(candidate('https://twitter.com/nasa/status/668083631563360256/'), 668083631563360256)\n    lu.assertEquals(candidate('https://twitter.com/GOP/status/883987787058900992'), 883987787058900992)\n    lu.assertEquals(candidate('https://twitter.com/Interior/status/1365679211592238592'), 1365679211592238592)\n    lu.assertEquals(candidate('http://twitter.com/realDonaldTrump/status/883891012282360832'), 883891012282360832)\n    lu.assertEquals(candidate('http://twitter.com/TheDemocrats/status/883891832571059712'), 883891832571059712)\n    lu.assertEquals(candidate('http://twitter.com/ThePSF/status/293380700514344496'), 293380700514344496)\n    lu.assertEquals(candidate('https://twitter.com/dog_rates/status/457937231478969344'), 457937231478969344)\n    lu.assertEquals(candidate('https://twitter.com/Interior/status/108076710113857283'), 108076710113857283)\n    lu.assertEquals(candidate('https://twitter.com/dog_rates/status/1234567890123456789'), 1234567890123456789)\n    lu.assertEquals(candidate('https://twitter.com/dog_rates/status/881533559209541120'), 881533559209541120)\n    lu.assertEquals(candidate('https://twitter.com/nasa/status/668083631563360256'), 668083631563360256)\n    lu.assertEquals(candidate('https://twitter.com/dog_rates/status/1456963204405214725'), 1456963204405214725)\n    lu.assertEquals(candidate('https://twitter.com/dog_rates/status/1228908675299224064'), 1228908675299224064)\n    lu.assertEquals(candidate('https://twitter.com/ThePSF/status/293380700514344496'), 293380700514344496)\n    lu.assertEquals(candidate('https://twitter.com/Interior/status/1278747526441191425/'), 1278747526441191425)\n    lu.assertEquals(candidate('https://twitter.com/dog_rates/status/1000000000000000000000'), 1000000000000000000000)\n    lu.assertEquals(candidate('https://twitter.com/Interior/status/108076710113857282'), 108076710113857282)\n    lu.assertEquals(candidate('https://twitter.com/dog_rates/status/1000000000000000000'), 1000000000000000000)\n    lu.assertEquals(candidate('https://twitter.com/dog_rates/status/1322712625669947906'), 1322712625669947906)\n    lu.assertEquals(candidate('https://twitter.com/dog_rates/status/1204125929760247808'), 1204125929760247808)\n    lu.assertEquals(candidate('https://twitter.com/ThePSF/status/1204844046511772737'), 1204844046511772737)\n    lu.assertEquals(candidate('https://twitter.com/Interior/status/1278747526441191425'), 1278747526441191425)\n    lu.assertEquals(candidate('https://twitter.com/Interior/status/1278747526441191425/?s=20#fragment'), 1278747526441191425)\n    lu.assertEquals(candidate('https://twitter.com/Interior/status/108076710113857286'), 108076710113857286)\n    lu.assertEquals(candidate('https://twitter.com/realDonaldTrump/status/883907451037186050'), 883907451037186050)\n    lu.assertEquals(candidate('https://twitter.com/Interior/status/108076710113857284'), 108076710113857284)\n    lu.assertEquals(candidate('https://twitter.com/realDonaldTrump/status/883907451037186051'), 883907451037186051)\n    lu.assertEquals(candidate('https://twitter.com/Interior/status/1278747526441191425/?s=20'), 1278747526441191425)\n    lu.assertEquals(candidate('https://twitter.com/realDonaldTrump/status/883907451037186052'), 883907451037186052)\n    lu.assertEquals(candidate('https://twitter.com/ThePSF/status/1204844046511772737'), 1204844046511772737)\n    lu.assertEquals(candidate('https://twitter.com/dog_rates/status/1360580986549242368'), 1360580986549242368)\n    lu.assertEquals(candidate('https://twitter.com/Interior/status/108076710113857281'), 108076710113857281)\n    lu.assertEquals(candidate('https://twitter.com/dog_rates/status/690096206448278528'), 690096206448278528)\n    lu.assertEquals(candidate('https://twitter.com/dog_rates/status/0000000000000000000'), 0)\n    lu.assertEquals(candidate('https://twitter.com/dog_rates/status/889479840728144384'), 889479840728144384)\n    lu.assertEquals(candidate('https://twitter.com/dog_rates/status/761015783402666496'), 761015783402666496)\n    lu.assertEquals(candidate('https://twitter.com/dog_rates/status/9999999999999999999'), 9999999999999999999)\n    lu.assertEquals(candidate('https://twitter.com/dog_rates/status/1234567890123456789'), 1234567890123456789)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_95589_parse_export_env", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # env_vars = env.split(',')\n-- # return dict([tuple(var.split('=')) for var in env_vars])\n-- \n-- Parse environment variables to a dictionary.\n-- Exmple:\n--     env_vars = parse_export_env(job.attr_export_env_to_job)\n--     primary_file = env_vars['PAS_PRIMARY_FILE']\n-- Args:\n--     env: Environment variables.\n-- Returns:\n--     Pairs of the name and value of environment variables.\nlocal function parse_export_env(env)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_95589_parse_export_env.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = parse_export_env\n    lu.assertEquals(candidate('PAS_ENVIRONMENT=1,PAS_PRIMARY_FILE=1,PAS_CONTEXT_ID=1'), {['PAS_ENVIRONMENT'] = '1', ['PAS_PRIMARY_FILE'] = '1', ['PAS_CONTEXT_ID'] = '1'})\n    lu.assertEquals(candidate('PAS_PRIMARY_FILE='), {['PAS_PRIMARY_FILE'] = ''})\n    lu.assertEquals(candidate('PAS_ENVIRONMENT=,PAS_PRIMARY_FILE=,PAS_CONTEXT_ID='), {['PAS_ENVIRONMENT'] = '', ['PAS_PRIMARY_FILE'] = '', ['PAS_CONTEXT_ID'] = ''})\n    lu.assertEquals(candidate('PAS_PRIMARY_FILE=primary_file'), {['PAS_PRIMARY_FILE'] = 'primary_file'})\n    lu.assertEquals(candidate('A=1,B=2,C='), {['A'] = '1', ['B'] = '2', ['C'] = ''})\n    lu.assertEquals(candidate('A=1,B=2,C=3'), {['A'] = '1', ['B'] = '2', ['C'] = '3'})\n    lu.assertEquals(candidate('PAS_PRIMARY_FILE=primary_file1,PAS_PRIMARY_FILE=primary_file2'), {['PAS_PRIMARY_FILE'] = 'primary_file2'})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_957_sparse_add", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # newdict = {}\n-- # keys = set(sv1.keys()) | set(sv2.keys())\n-- # for key in keys:\n-- #     x = sv1.get(key, 0) + sv2.get(key, 0)\n-- #     newdict[key] = x\n-- # return (newdict)\n-- \n-- dict, dict -> dict\n-- Returns a new dictionary that is the sum of the other two.\n-- >>>sparse_add(sv1, sv2)\n-- {0: 5, 1: 6, 2: 9}\nlocal function sparse_add(sv1, sv2)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_957_sparse_add.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = sparse_add\n    lu.assertEquals(candidate({[1] = 2, [3] = 3}, {}), {[1] = 2, [3] = 3})\n    lu.assertEquals(candidate({}, {[1] = 2, [3] = 3}), {[1] = 2, [3] = 3})\n    lu.assertEquals(candidate({[0] = 1, [1] = 2, [2] = 3, [4] = 1}, {[0] = 1, [1] = 2, [2] = 3}), {[0] = 2, [1] = 4, [2] = 6, [4] = 1})\n    lu.assertEquals(candidate({[1] = 2, [3] = 3}, {[1] = 4}), {[1] = 6, [3] = 3})\n    lu.assertEquals(candidate({}, {[0] = 1, [1] = 2, [2] = 3}), {[0] = 1, [1] = 2, [2] = 3})\n    lu.assertEquals(candidate({[0] = 1, [1] = 2, [3] = 3}, {[0] = 1, [1] = 2, [2] = 3}), {[0] = 2, [1] = 4, [2] = 3, [3] = 3})\n    lu.assertEquals(candidate({[0] = 1, [2] = 3}, {[0] = 1, [1] = 2, [2] = 3}), {[0] = 2, [1] = 2, [2] = 6})\n    lu.assertEquals(candidate({[1] = 2, [3] = 3}, {[3] = 4, [5] = 6}), {[1] = 2, [3] = 7, [5] = 6})\n    lu.assertEquals(candidate({}, {}), {})\n    lu.assertEquals(candidate({[0] = 1, [1] = 2, [2] = 3}, {}), {[0] = 1, [1] = 2, [2] = 3})\n    lu.assertEquals(candidate({}, {}), {})\n    lu.assertEquals(candidate({[1] = 2, [3] = 3}, {[4] = 5}), {[1] = 2, [3] = 3, [4] = 5})\n    lu.assertEquals(candidate({}, {[0] = 100}), {[0] = 100})\n    lu.assertEquals(candidate({[0] = 100}, {}), {[0] = 100})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_96148_index_from_weekday", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # weekday_map = {\n-- #     'Sunday': 0,\n-- #     'Monday': 1,\n-- #     'Tuesday': 2,\n-- #     'Wednesday': 3,\n-- #     'Thursday': 4,\n-- #     'Friday': 5,\n-- #     'Saturday': 6\n-- # }\n-- # return weekday_map.get(weekday)\n-- \n-- Returns a numeric index for day of week based on name of day\n-- :param weekday: Name of day (e.g. 'Sunday', 'Monday', etc.)\n-- :return: numeric index\nlocal function index_from_weekday(weekday)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_96148_index_from_weekday.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = index_from_weekday\n    lu.assertEquals(candidate('Sunday'), 0)\n    lu.assertEquals(candidate('Thursday'), 4)\n    lu.assertEquals(candidate('Saturday'), 6)\n    lu.assertEquals(candidate('Wednesday'), 3)\n    lu.assertEquals(candidate('Monday'), 1)\n    lu.assertEquals(candidate('Tuesday'), 2)\n    lu.assertEquals(candidate('Friday'), 5)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_96726_reverse_transform_params", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # params_out = {}\n-- # for param in params_in:\n-- #     key = param.get(\"ParameterKey\")\n-- #     value = param.get(\"ParameterValue\")\n-- #     params_out.update({key: value})\n-- # return params_out\n-- \n-- Merge input parameter key and value into one-line string\n-- Args:\n--     params_in (list): Python list of output params e.g.\n--     {\n--         \"ParameterKey\": \"principal_role\",\n--         \"ParameterValue\": \"$[alfred_ssm_/org/primary/service_catalog/\n--         principal/role_arn]\"\n--     }\n-- Return:\n--     params_out (dict): Python dict of input params e.g.\n--     {\n--         \"principal_role\": \"$[alfred_ssm_/org/primary/service_catalog/\n--         principal/role_arn]\"\n--     }\nlocal function reverse_transform_params(params_in)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_96726_reverse_transform_params.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = reverse_transform_params\n    lu.assertEquals(candidate({{['ParameterKey'] = 'principal_role', ['ParameterValue'] = '$[alfred_ssm_/org/primary/service_catalog/principal/role_arn]'}, {['ParameterKey'] = 'role_arn', ['ParameterValue'] = '$[alfred_ssm_/org/primary/service_catalog/deployer/role_arn]'}}), {['principal_role'] = '$[alfred_ssm_/org/primary/service_catalog/principal/role_arn]', ['role_arn'] = '$[alfred_ssm_/org/primary/service_catalog/deployer/role_arn]'})\n    lu.assertEquals(candidate({{['ParameterKey'] = 'principal_role', ['ParameterValue'] = '$[alfred_ssm_/org/primary/service_catalog/principal/role_arn]'}}), {['principal_role'] = '$[alfred_ssm_/org/primary/service_catalog/principal/role_arn]'})\n    lu.assertEquals(candidate({{['ParameterKey'] = 'principal_role', ['ParameterValue'] = '$[alfred_ssm_/org/primary/service_catalog/principal/role_arn]'}, {['ParameterKey'] = 'data_bucket', ['ParameterValue'] = '$[alfred_ssm_/org/primary/data_lake/bucket]'}}), {['principal_role'] = '$[alfred_ssm_/org/primary/service_catalog/principal/role_arn]', ['data_bucket'] = '$[alfred_ssm_/org/primary/data_lake/bucket]'})\n    lu.assertEquals(candidate({{['ParameterKey'] = 'principal_role', ['ParameterValue'] = '$[alfred_ssm_/org/primary/service_catalog/principal/role_arn]'}}), {['principal_role'] = '$[alfred_ssm_/org/primary/service_catalog/principal/role_arn]'})\n    lu.assertEquals(candidate({{['ParameterKey'] = 'empty', ['ParameterValue'] = ''}}), {['empty'] = ''})\n    lu.assertEquals(candidate({{['ParameterKey'] = 'principal_role', ['ParameterValue'] = '$[alfred_ssm_/org/primary/service_catalog/principal/role_arn]'}, {['ParameterKey'] = 'data_bucket', ['ParameterValue'] = '$[alfred_ssm_/org/primary/data_lake/bucket]'}, {['ParameterKey'] = 'data_bucket', ['ParameterValue'] = '$[alfred_ssm_/org/primary/data_lake/bucket]'}}), {['principal_role'] = '$[alfred_ssm_/org/primary/service_catalog/principal/role_arn]', ['data_bucket'] = '$[alfred_ssm_/org/primary/data_lake/bucket]'})\n    lu.assertEquals(candidate({{['ParameterKey'] = 'principal_role', ['ParameterValue'] = '$[alfred_ssm_/org/primary/service_catalog/principal/role_arn]'}, {['ParameterKey'] = 'role_name', ['ParameterValue'] = '$[alfred_ssm_/org/primary/service_catalog/cross_account_access_role/name]'}}), {['principal_role'] = '$[alfred_ssm_/org/primary/service_catalog/principal/role_arn]', ['role_name'] = '$[alfred_ssm_/org/primary/service_catalog/cross_account_access_role/name]'})\n    lu.assertEquals(candidate({{['ParameterKey'] = 'principal_role', ['ParameterValue'] = '$[alfred_ssm_/org/primary/service_catalog/principal/role_arn]'}, {['ParameterKey'] = 'vpc_cidr', ['ParameterValue'] = '$[alfred_ssm_/org/primary/service_catalog/network/vpc/vpc_cidr]'}}), {['principal_role'] = '$[alfred_ssm_/org/primary/service_catalog/principal/role_arn]', ['vpc_cidr'] = '$[alfred_ssm_/org/primary/service_catalog/network/vpc/vpc_cidr]'})\n    lu.assertEquals(candidate({{['ParameterKey'] = 'principal_role', ['ParameterValue'] = '$[alfred_ssm_/org/primary/service_catalog/principal/role_arn]'}, {['ParameterKey'] = 'another_role', ['ParameterValue'] = '$[alfred_ssm_/org/primary/service_catalog/another_role_arn]'}}), {['principal_role'] = '$[alfred_ssm_/org/primary/service_catalog/principal/role_arn]', ['another_role'] = '$[alfred_ssm_/org/primary/service_catalog/another_role_arn]'})\n    lu.assertEquals(candidate({{['ParameterKey'] = 'principal_role', ['ParameterValue'] = '$[alfred_ssm_/org/primary/service_catalog/principal/role_arn]'}}), {['principal_role'] = '$[alfred_ssm_/org/primary/service_catalog/principal/role_arn]'})\n    lu.assertEquals(candidate({}), {})\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_97036_string_lower", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # return string.lower()\n-- \n-- **string_lower(string)** -> return the lowercase value of the string\n-- * string: (string) string to lower case.\n-- <code>\n--    Example:\n--        string_lower('Linux')\n--    Returns:\n--        'linux'\n-- </code>\nlocal function string_lower(string)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_97036_string_lower.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = string_lower\n    lu.assertEquals(candidate('A'), 'a')\n    lu.assertEquals(candidate('\u00a1'), '\u00a1')\n    lu.assertEquals(candidate('\u00bf'), '\u00bf')\n    lu.assertEquals(candidate('LINUX1234'), 'linux1234')\n    lu.assertEquals(candidate('`~-_=+[]{}\\\\|;:\\'\",<.>/?'), '`~-_=+[]{}\\\\|;:\\'\",<.>/?')\n    lu.assertEquals(candidate('z'), 'z')\n    lu.assertEquals(candidate('Linux1234'), 'linux1234')\n    lu.assertEquals(candidate('LiNuX'), 'linux')\n    lu.assertEquals(candidate('LInux'), 'linux')\n    lu.assertEquals(candidate('!@#$%^&*()'), '!@#$%^&*()')\n    lu.assertEquals(candidate('linux'), 'linux')\n    lu.assertEquals(candidate('\u01dd'), '\u01dd')\n    lu.assertEquals(candidate('Linux'), 'linux')\n    lu.assertEquals(candidate('Z'), 'z')\n    lu.assertEquals(candidate('\u0137'), '\u0137')\n    lu.assertEquals(candidate(''), '')\n    lu.assertEquals(candidate('lInuX'), 'linux')\n    lu.assertEquals(candidate('\u0133'), '\u0133')\n    lu.assertEquals(candidate('LinUxB'), 'linuxb')\n    lu.assertEquals(candidate('\u0135'), '\u0135')\n    lu.assertEquals(candidate('a'), 'a')\n    lu.assertEquals(candidate('linuxB'), 'linuxb')\n    lu.assertEquals(candidate('LinUxB!'), 'linuxb!')\n    lu.assertEquals(candidate('0123456789'), '0123456789')\n    lu.assertEquals(candidate('Linux!'), 'linux!')\n    lu.assertEquals(candidate('\u0138'), '\u0138')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_9742_convert_to_camel", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # components = data.split('_')\n-- # return components[0] + \"\".join(x.title() for x in components[1:])\n-- \n-- Convert snake case (foo_bar_bat) to camel case (fooBarBat).\n-- This is not pythonic, but needed for certain situations\nlocal function convert_to_camel(data)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_9742_convert_to_camel.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = convert_to_camel\n    lu.assertEquals(candidate('foo_bar_bat'), 'fooBarBat')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_98079_eq_11_dimensionless_hrr_rectangular", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # # equation starts\n-- # aa = Q_dot_kW\n-- # bb = rho_0 * c_p_0_kJ_kg_K * T_0 * (g ** 0.5) * (L_A ** 1.5) * L_B\n-- # Q_dot_star_rect = aa / bb\n-- # return Q_dot_star_rect\n-- \n-- Equation 11 in Section 8.3.2.2 PD 7974-1:2019 calculates dimensionless for rectangular fire source.\n-- :param Q_dot_kW: in kW, fire heat release rate.\n-- :param rho_0: in kg/m^3, density of ambient air.\n-- :param c_p_0_kJ_kg_K: in kJ/kg/K, specific heat capacity of ambient air.\n-- :param T_0: in K, ambient air temperature.\n-- :param g: in m/s^2, acceleration due to gravity.\n-- :param L_A: in m, rectangular shape dimension's shorter edge.\n-- :param L_B: in m, rectangular shape dimension's longer edge.\n-- :return Q_dot_star_rect: dimensionless, dimensionless heat release rate\nlocal function eq_11_dimensionless_hrr_rectangular(Q_dot_kW, rho_0, c_p_0_kJ_kg_K, T_0, g, L_A, L_B)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_98079_eq_11_dimensionless_hrr_rectangular.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = eq_11_dimensionless_hrr_rectangular\n    lu.assertEquals(candidate(0, 1, 1, 273.15, 9.807, 2, 2), 0)\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
{"name": "HumanEval_99617_unindent", "language": "lua", "prompt": "-- ## Canonical Python Solution ##\n-- # min_leading_spaces = len(str)\n-- # lines = str.splitlines()\n-- # for line in lines:\n-- #     if line:\n-- #         min_leading_spaces = min(len(line) - len(line.lstrip(' ')),\n-- #                                  min_leading_spaces)\n-- # return '\\n'.join(l[min_leading_spaces:] for l in lines)\n-- \n-- eat leading space in front of lines based on the smallest one\nlocal function unindent(str)\n", "doctests": "keep", "original": "/home/elleven/code/multipl-t/MultiPL-E/datasets/../../multipl_e_target_adaptor/stack-clean-python/HumanEval_99617_unindent.py", "prompt_terminology": "verbatim", "tests": "lu = require('luaunit')\n\nfunction test_humaneval()\nlocal candidate = unindent\n    lu.assertEquals(candidate('\\n    line1\\n    line2\\n    line3\\n    line4\\n    '), '\\nline1\\nline2\\nline3\\nline4\\n')\n    lu.assertEquals(candidate('    This is a\\n    multi-line\\n    string\\n    '), 'This is a\\nmulti-line\\nstring\\n')\n    lu.assertEquals(candidate('\\n    line1\\n\\n    line2\\n\\n    line3\\n    line4\\n    '), '\\nline1\\n\\nline2\\n\\nline3\\nline4\\n')\n    lu.assertEquals(candidate('\\n\\n    line1\\n      line2\\n    line3\\n    line4\\n    '), '\\n\\nline1\\n  line2\\nline3\\nline4\\n')\n    lu.assertEquals(candidate('    This is a\\n    multi-line\\n    string\\n\\n    this is also part of the string\\n\\n        it has a nested indentation\\n    '), 'This is a\\nmulti-line\\nstring\\n\\nthis is also part of the string\\n\\n    it has a nested indentation\\n')\n    lu.assertEquals(candidate('\\n    line1\\n      line2\\n    line3\\n    line4\\n    '), '\\nline1\\n  line2\\nline3\\nline4\\n')\n    lu.assertEquals(candidate('    This is a\\n    multi-line\\n    string\\n\\n    this is also part of the string\\n    '), 'This is a\\nmulti-line\\nstring\\n\\nthis is also part of the string\\n')\nend\n\nos.exit(lu.LuaUnit.run())", "stop_tokens": ["\nlocal", "\nfunction", "\n--", "\n\n"]}
